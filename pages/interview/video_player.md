# 视频播放器

实现一个播放器SDK，要求播放控制界面可定制，提供播放、暂停、停止、Seek等功能。

>## I帧、P帧、B帧

I帧表示关键帧，帧画面的完整保留，解码时只需要本帧数据就可以完成。

P帧表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。

B帧是双向差别帧，也就是B帧记录的是本帧与前后帧的差别，换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面，B帧压缩率高，但是解码时CPU会比较累。

>## 视频如何播放？

视频画面其实是由视频帧组成，分别为I帧、P帧、B帧。也就是说，显示视频画面需要对视频压缩格式进行遍历解码帧数据，遍历的同时对每一帧数据按照播放时间进行显示，每一帧可以理解为一张图片，其实就是对图片数据的显示。

>## 视频如何显示？

不仅是视频，显示问题无处不在，我们所有在手机端看到的画面都和视频一样，是一个不断刷新绘制的过程，底层都是通过使用OpenGL的API，对GPU硬件发出指令，通过图形渲染管线程序，最终在屏幕的每个像素点显示。OpenGL是跨平台的，iOS上是通过GLKView进行渲染显示。

>## 什么是图形渲染管线？

顶点着色器 —— 图元装配 —— 几何着色器 —— 光栅化 —— 片段着色器 —— 测试与混合。  

着色器是一段运行在GPU中的程序；顶点着色器确定绘制图形的形状；图元装配是将顶点着色器传来的顶点数据组装为图元；光栅化是将一个图元转化为一张二维的图片，而这张图片由若干个片段（fragment）组成；片段着色器计算片段的颜色；测试和混合丢弃一些不需要显示的片段。

>## 如何播放全景视频？

在图形渲染管线中，顶点着色器和片元着色器是可编程的，也就是说我们可以通过顶点着色器构建任意事物模型顶点，然后通过过GPU进行绘制，再通过片元着色器给事物上色（纹理贴图）。普通的视频播放可以理解为就是在一个二维的面显示视频画面，二维的面就是顶点着色器构建的，而画面的显示则交给了片元着色器，那么全景视频的显示，其实就是在顶点着色器环节构建一个球面即可实现播放全景视频。

>## iOS上如何对MP4文件播放？

AVFoundation中的AVPlayer提供对视频压缩文件的播放。以下是播放步骤：

1）构建AVPlayerItem实例。  
2）通过KVO监听AVPlayerItem实例加载的状态，加载成功，否则播放失败，最后移除监听。  
3）构建播放器AVPlayer实例。     
4）为AVPlayerItem实例增加AVPlayerItemVideoOutput实例，加入异步队列。  
5）构建屏幕定时器，通过AVPlayerItemVideoOutput实例获取当前视频帧数据，交给OpengGL渲染。  
6）监听播放进度，播放结束。  
7）开始播放。

>## 如何使用FFmpeg对MP4文件播放？

FFmpeg提供ffplay命令可以对几乎所有的视频压缩格式进行播放，包括yuv格式。

>## 播放器设计

播发器类：CPVideoPlayer，提供播放、暂停、停止、Seek等方法。  
协议一：CPVideoPlayerDelegate，回调播放状态及视频数据。  
协议二：CPVideoPlayerDataSource，获取视频地址。  
协议三：CPVideoControlViewProtocol，播放控制界面。

播放器weak引用上面三个协议。

>## 参考链接

[一看就懂的OpenGL ES教程——图形渲染管线的那些事](https://juejin.cn/post/7119135465302654984)  
[YUV图解](https://blog.csdn.net/mydear_11000/article/details/50404084)    
[I帧、P帧、B帧、GOP、IDR 和PTS, DTS之间的关系](https://www.cnblogs.com/yongdaimi/p/10676309.html)  