{"pages/重拾服务端/域名的注册及备案.html":{"url":"pages/重拾服务端/域名的注册及备案.html","title":"域名的注册及备案","keywords":"","body":"域名的注册及备案 一、购买云服务器 打开腾讯云->产品->热门产品->轻量应用服务器 二、购买域名 腾讯云·DNSPod 公安备案指南 腾讯云·控制台 三、域名解析 腾讯云·DNS解析DNSPod DNS.TECH 域名检测 域名已经实名认证，但是显示状态还是注册局暂停解析 四、mac上登录服务器及上传 指定端口号运行gitbook serve --port 80 怎样在Mac上SSH和FTP？完美替代XShell是哪个软件？item2吗？Royal TSX! 没有比它更好 五、centos7.6上安装gitbook CentOS7.6安装Nodejs(Npm) CentOS7系统中node安装配置 make_unique 不是 'std'成员 Centos 7默认gcc版本为4.8，有时需要更高版本的，这里以升级至8.3.1版本为例，分别执行下面三条命令即可，无需手动下载源码编译 Centos7卸载nodejs 在centos7上安装gitbook debug of plugin-mathjax of gitbook 六、FTP Linux 云服务器搭建 FTP 服务 七、Ngnix 连前端都看得懂的《Nginx 入门指南》 Nginx 服务器 SSL 证书安装部署 Centos 安装 Gitbook Centos7.6安装和配置最新版Nginx服务 centOS7.6安装Nginx "},"pages/重拾服务端/Java开发工具.html":{"url":"pages/重拾服务端/Java开发工具.html","title":"Java开发工具","keywords":"","body":"Java开发工具 Spring Boot 我的Spring Boot学习之路！ IntelliJ IDEA IntelliJ IDEA 2022.2.3破解版图文教程mac,windows,linux均适用（2022.11.10亲测有效） "},"pages/mysql/Mac下MYSQL的安装.html":{"url":"pages/mysql/Mac下MYSQL的安装.html","title":"Mac下MYSQL的安装","keywords":"","body":"Mac下MYSQL的安装 环境：macos 11.7.4 一、MYSQL下载 1、进入官网，滑动至最下方，找到Downloads，点击MySQL Community Server。2、点击Archives，选择Product Version:5.7.31，选择Operating System:macOS。3、下载“macOS 10.14 (x86, 64-bit), Compressed TAR Archive”。4、解压缩： cd /usr/local/ sudo mkdir src sudo mv ~/Downloads/mysql-5.7.31-macos10.14-x86_64.tar.gz src sudo tar -xzvf mysql-5.7.31-macos10.14-x86_64.tar.gz sudo ln -sf mysql-5.7.31-macos10.14-x86_64 mysql sudo chown -R chenchangqing:staff mysql* 二、配置环境变量 1、 vi ~/.bash_profile，在 ~/.bashrc 中添加如下配置项。 MYSQL_HOME=/usr/local/mysql export PATH=$PATH:$MYSQL_HOME/bin:$MYSQL_HOME/support-files 2、source ~/.bash_profile。3、mysql --version。 chenchangqingdeMacBook-Pro-2:sdxy chenchangqing$ mysql --version mysql Ver 14.14 Distrib 5.7.31, for macos10.14 (x86_64) using EditLine wrapper 4、错误分析： dyld: Symbol not found: __ZTTNSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE Referenced from: /usr/local/mysql/bin/mysql (which was built for Mac OS X 12.0) Expected in: /usr/lib/libc++.1.dylib in /usr/local/mysql/bin/mysql Abort trap: 6 如果出现以上错误，说明下载的mysql版本和当前的macos系统不匹配，比如“macos 11.7.4”下载了“macOS 13 (x86, 64-bit), Compressed TAR Archive”，就会出现上面的错误。 -bash: /usr/local/mysql/bin/mysql: Bad CPU type in executable。 如果出现以上错误，说明下载的mysql版本与当前macos系统不匹配CPU架构不匹配，比如“macos 11.7.4”下载了“macOS 12 (ARM, 64-bit), Compressed TAR Archive”，就会出现上面的错误。 三、初始化root chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysqld --initialize-insecure 2023-03-24T17:25:46.055794Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details). 2023-03-24T17:25:46.057786Z 0 [Warning] Setting lower_case_table_names=2 because file system for /usr/local/mysql-5.7.31-macos10.14-x86_64/data/ is case insensitive 2023-03-24T17:25:46.237798Z 0 [Warning] InnoDB: New log files created, LSN=45790 2023-03-24T17:25:46.269409Z 0 [Warning] InnoDB: Creating foreign key constraint system tables. 2023-03-24T17:25:46.329085Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: e99f3ed4-ca68-11ed-b222-0a4a56d116f7. 2023-03-24T17:25:46.340828Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened. 2023-03-24T17:25:46.790867Z 0 [Warning] CA certificate ca.pem is self signed. 2023-03-24T17:25:46.937195Z 1 [Warning] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option. 从输出可以看到，mysqld 已经帮我们创建了一个 root 用户，且该 root 用户的 password 为空。 四、启动MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server start Starting MySQL . SUCCESS! 五、登录root chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql -uroot -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 2 Server version: 5.7.31 MySQL Community Server (GPL) Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> 六、给root用户创建密码 mysql> ALTER USER root@localhost IDENTIFIED WITH caching_sha2_password BY '123456'; -> ; ERROR 1524 (HY000): Plugin 'caching_sha2_password' is not loaded MySQL新版默认使用caching_sha2_password作为身份验证插件，而旧版是使用mysql_native_password。当连接MySQL时报错“plugin caching_sha2_password could not be loaded”时，可换回旧版插件。 mysql> ALTER USER root@localhost IDENTIFIED WITH mysql_native_password BY '123456'; Query OK, 0 rows affected (0.00 sec) mysql> FLUSH PRIVILEGES; Query OK, 0 rows affected (0.00 sec) mysql> quit Bye 七、常用命令 1、启动MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server start Starting MySQL SUCCESS! 2、停止MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server stop Shutting down MySQL .. SUCCESS! 3、重启MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server restart ERROR! MySQL server PID file could not be found! Starting MySQL . SUCCESS! 4、检查 MySQL 运行状态 chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server status SUCCESS! MySQL running (1725) 八、参考 https://learnku.com/articles/62379 https://blog.csdn.net/weixin_33728077/article/details/113902283 https://www.cnblogs.com/yjmyzz/p/how-to-install-mysql8-on-mac-using-tar-gz.html "},"pages/jdbc/prepare.html":{"url":"pages/jdbc/prepare.html","title":"准备工作","keywords":"","body":"准备工作 工具 Navicat for MySQL PowerDesigner JDK API "},"pages/jdbc/nature.html":{"url":"pages/jdbc/nature.html","title":"JDBC本质","keywords":"","body":"JDBC本质 JDBC是什么？ Java Database Connectivity（Java语言连接数据库）。 JDBC的本质是什么？ JDBC是SUN公司制定的一套接口（interface）。接口都有调用者和实现者，面向接口调用、面向接口写实现类，这都属于面向接口编程。java.sql.*，这个软件包下有很多的接口。 为什么要面向接口编程？ 解耦合：降低程序的耦合度，提供程序的扩展力。多态机制就是非常典型的面向抽象编程，而不是面向具体编程。 我们建议： Animal a = new Cat(); Animal b = new Dog(); // 喂养的方法 public void feed(Animal a) { // 面向父类型编程 } 我们不建议： Cat a = new Cat(); Dog b = new Dog(); // 喂养的方法 public void feed(Cat a) { // 面向实现类编程 } 为什么SUN制定一套JDBC接口呢？ 因为每一个数据库的底层实现原理不一样，Oracle数据库有自己的原理，MYSQL数据库也有自己的原理，MS SQLServer数据库也有自己的原理，每个数据库都有自己独特的实现原理。 "},"pages/fultter/在MacOS上搭建Flutter开发环境.html":{"url":"pages/fultter/在MacOS上搭建Flutter开发环境.html","title":"在MacOS上搭建Flutter开发环境","keywords":"","body":"在MacOS上搭建Flutter开发环境 问题记录 下载地址错误 在下载fultter的时候，需要下载macOS下的x64，而不是arm64。如果使用了错误的CPU架构类型，运行flutter命令，flutter会提示CPU不支持的信息。 命令行没有翻墙 如果没有翻墙，会导致以下两个错误： HTTP Host availability check is taking a long time...[!] HTTP Host Availability ✗ HTTP host \"https://maven.google.com/\" is not reachable. Reason: An error occurred while checking the HTTP host: Operation timed out ✗ HTTP host \"https://cloud.google.com/\" is not reachable. Reason: An error occurred while checking the HTTP host: Operation timed out Android sdkmanager not found. https://stackoverflow.com/questions/70719767/android-sdkmanager-not-found-update-to-the-latest-android-sdk-and-ensure-that-t ! NO_PROXY is not set https://zhuanlan.zhihu.com/p/474652737 "},"pages/fultter/创建Flutter应用.html":{"url":"pages/fultter/创建Flutter应用.html","title":"创建Flutter应用","keywords":"","body":"创建Flutter应用 1）安装Flutter和Dart插件 2）创建Flutter应用 3）Organization命名不规范错误 发现一个错误，Organization命名不规范，不能出现纯数字，修改如下： 4）Android真机运行 5）iOS模拟器运行 6）在Android Studio上选择多设备运行 效果： 《Flutter实战·第二版》1.3.2 ～ 1.3.3 "},"pages/fultter/第一个Flutter应用.html":{"url":"pages/fultter/第一个Flutter应用.html","title":"第一个Flutter应用","keywords":"","body":"第一个Flutter应用 模板代码分析 1. 导入包 import 'package:flutter/material.dart'; 此行代码作用是导入了 Material UI 组件库。 2. 应用入口 void main() => runApp(MyApp()); 启动Flutter应用：main调用runApp，runApp接受MyApp对象参数，MyApp()是 Flutter应用根组件。 3. 应用结构 class MyApp extends StatelessWidget { @override Widget build(BuildContext context) { return MaterialApp( //应用名称 title: 'Flutter Demo', theme: ThemeData( //蓝色主题 primarySwatch: Colors.blue, ), //应用首页路由 home: MyHomePage(title: 'Flutter Demo Home Page'), ); } } MyApp类代表Flutter应用，继承了StatelessWidget类，是一个widget。 Flutter通过widget提供的build方法来构建UI界面。 MaterialApp是Flutter APP框架，可以设置应用的名称、主题、语言、首页及路由列表等。 home指定了App的首页，是一个widget。 首页 1. MyHomePage类 class MyHomePage extends StatefulWidget { MyHomePage({Key? key, required this.title}) : super(key: key); final String title; @override _MyHomePageState createState() => _MyHomePageState(); } class _MyHomePageState extends State { ... } MyHomePage是应用首页，它继承自StatefulWidget类，表示它是一个有状态的组件（Stateful widget）。 MyHomePage没有提供build方法，但是它有_MyHomePageState状态类，bulid方法被挪到了这个状态类， 2. State类 组件的状态。int _counter = 0; //用于记录按钮点击的总次数 设置状态的自增函数。 void _incrementCounter() { setState(() { _counter++; }); } 点击按钮+时会调用，通过setState通知Flutter状态修改了，然后重新执行build方法重新构建UI。 构建UI界面的build方法 Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: Text(widget.title), ), body: Center( child: Column( mainAxisAlignment: MainAxisAlignment.center, children: [ Text('You have pushed the button this many times:'), Text( '$_counter', style: Theme.of(context).textTheme.headline4, ), ], ), ), floatingActionButton: FloatingActionButton( onPressed: _incrementCounter, tooltip: 'Increment', child: Icon(Icons.add), ), ); } Scaffold是 Material 库中提供的页面脚手架。 body是具体的组件树。 floatingActionButton右下角的加号按钮。 现在，我们将整个计数器执行流程串起来：当右下角的floatingActionButton按钮被点击之后，会调用_incrementCounter方法。在_incrementCounter方法中，首先会自增_counter计数器（状态），然后setState会通知 Flutter 框架状态发生变化，接着，Flutter 框架会调用build方法以新的状态重新构建UI，最终显示在设备屏幕上。 3. build方法放在State的原因 为什么要将build方法放在State中，而不是放在StatefulWidget中？ 状态访问不便。 将build方法放在widget中，由于构建UI需要访问State的属性，例如上面的_counter，也就是说build方法需要依赖State ，并且公开_counter，这就会导致对状态的修改将会变的不可控。反之，build放在State中，可以直接访问状态，并且拿到_counter，这会非常方便。 继承StatefulWidget不便。 子类在调用父类build方法时，需要依赖父类State类，这是不合理的，因为父类的状态是父类内部的实现细节，不应该暴露给子类。 模板代码分析 "},"pages/fultter/Widget简介.html":{"url":"pages/fultter/Widget简介.html","title":"Widget简介","keywords":"","body":"Widget简介 Widget概念 描述一个UI元素的配置信息，比如对于Text来讲，文本的内容、对齐方式、文本样式都是它的配置信息。 Widget类的声明： @immutable // 不可变的 abstract class`Widget`extends DiagnosticableTree { const Widget({ this.key }); final Key? key; @protected @factory Element createElement(); @override String toStringShort() { final String type = objectRuntimeType(this, 'Widget'); return key == null ? type : '$type-$key'; } @override void debugFillProperties(DiagnosticPropertiesBuilder properties) { super.debugFillProperties(properties); properties.defaultDiagnosticsTreeStyle = DiagnosticsTreeStyle.dense; } @override @nonVirtual bool operator ==(Object other) => super == other; @override @nonVirtual int get hashCode => super.hashCode; static bool canUpdate(Widget oldWidget,`Widget`newWidget) { return oldWidget.runtimeType == newWidget.runtimeType && oldWidget.key == newWidget.key; } ... } @immutable代表Widget是不可变的，限制Widget中的属性必须使用final修饰。 为啥不允许属性修改呢？属性修改会重新创建新的Widget实例，这没有意义，还有就是可变属性应该交给State管理。 key的作用是决定下一次build是否复用旧的widget，决定的条件在canUpdate()方法。 createElement构建UI树时，生成对应节点的Element对象。一个widget可以对应多个Element。 canUpdate(...)：是否用新的widget对象去更新旧UI树上所对应的Element对象的配置，返回false的话，会重新创建新的Element。 Widget类是一个抽象类，其中最核心的就是定义了createElement()接口。 Flutter中的四棵树 Widget 只是描述一个UI元素的配置信息，那么真正的布局、绘制是由谁来完成的呢？Flutter 框架的的处理流程是这样的： 根据Widget树生成一个 Element 树，Element 树中的节点都继承自 Element 类。 根据 Element 树生成Render树（渲染树），渲染树中的节点都继承自RenderObject 类。 根据渲染树生成 Layer 树，然后上屏显示，Layer 树中的节点都继承自 Layer 类。 真正的布局和渲染逻辑在Render树中，Element 是Widget和 RenderObject 的粘合剂，可以理解为一个中间代理。 Widget类 1. StatelessWidget 继承Widget类，重写了createElement()方法，对应StatelessElement类。 用于不需要维护状态的场景，有一个build方法用来构建UI。 2. BuildContext build方法的context参数，表示当前widget在widget树中的上下文，每个widget对应一个context。 context是当前widget在widget树中位置中执行”相关操作“的一个句柄。 3. StatefulWidget 对应StatefulElement，多了一个createState()方法。 StatefulElement中可能会多次调用createState()来创建状态（State）对象。 createState()用于创建和StatefulWidget相关的状态，它在StatefulWidget的生命周期中可能会被多次调用。 在StatefulWidget中，State对象和StatefulElement具有一一对应的关系。 State 1. 简介 一个StatefulWidget类会对应一个State类，State表示与其对应的StatefulWidget要维护的状态。State 中的保存的状态信息可以： 在widget构建时可以被同步读取。 可以修改，然后手动调用其setState()方法，重新调用其build方法达到更新UI的目的。 State 中有两个常用属性： widget，重新构建时可能会变化，但State实例只会在第一次插入到树中时被创建。 context，StatefulWidget对应的BuildContext。 2. State生命周期 initState：当widget第一次插入到widget树时会被调用。 didChangeDependencies：当State对象的依赖发生变化时会被调用。 build()会在如下场景被调用： 在调用initState()之后。 在调用didUpdateWidget()之后。 在调用setState()之后。 在调用didChangeDependencies()之后。 在State对象从树中一个位置移除后（会调用deactivate）又重新插入到树的其他位置之后。 reassemble：热重载(hot reload)时会被调用，此回调在Release模式下永远不会被调用。 didUpdateWidget：在widget重新构建时，Flutter 框架会调用widget.canUpdate来检测widget树中同一位置的新旧节点，然后决定是否需要更新，如果widget.canUpdate返回true则会调用此回调。 deactivate：当 State 对象从树中被移除时，会调用此回调。 dispose：当 State 对象从树中被永久移除时调用。 在widget树中获取State对象 1. 通过Context获取 findAncestorStateOfType方法： // 查找父级最近的Scaffold对应的ScaffoldState对象 ScaffoldState _state = context.findAncestorStateOfType()!; // 打开抽屉菜单 _state.openDrawer(); StatefulWidget中提供一个of静态方法来获取其State对象。// 直接通过of静态方法来获取ScaffoldState ScaffoldState _state=Scaffold.of(context); // 打开抽屉菜单 _state.openDrawer(); 2. 通过GlobalKey 给目标StatefulWidget添加GlobalKey。 //定义一个globalKey, 由于GlobalKey要保持全局唯一性，我们使用静态变量存储 static GlobalKey _globalKey= GlobalKey(); ... Scaffold( key: _globalKey , //设置key ... ) 通过GlobalKey来获取State对象 _globalKey.currentState.openDrawer() 通过 RenderObject 自定义 Widget 通过RenderObject定义组件的方式： class CustomWidget extends LeafRenderObjectWidget{ @override RenderObject createRenderObject(BuildContext context) { // 创建 RenderObject return RenderCustomObject(); } @override void updateRenderObject(BuildContext context, RenderCustomObject renderObject) { // 更新 RenderObject super.updateRenderObject(context, renderObject); } } class RenderCustomObject extends RenderBox{ @override void performLayout() { // 实现布局逻辑 } @override void paint(PaintingContext context, Offset offset) { // 实现绘制 } } 如果组件不会包含子组件，则我们可以直接继承自 LeafRenderObjectWidget。 abstract class LeafRenderObjectWidget extends RenderObjectWidget { const LeafRenderObjectWidget({ Key? key }) : super(key: key); @override LeafRenderObjectElement createElement() => LeafRenderObjectElement(this); } 重写了 createRenderObject 方法。该方法被组件对应的 Element 调用（构建渲染树时）用于生成渲染对象。 updateRenderObject 方法是用于在组件树状态发生变化但不需要重新创建 RenderObject 时用于更新组件渲染对象的回调。 RenderCustomObject 类是继承自 RenderBox，而 RenderBox 继承自 RenderObject，我们需要在 RenderCustomObject 中实现布局、绘制、事件响应等逻辑 Flutter SDK内置组件库介绍 1. 基础组件 要使用基础组件库，需要先导入： import 'package:flutter/widgets.dart'; Text：文本。 Row、Column：在水平（Row）和垂直（Column）方向上创建灵活的布局。 Stack：取代线性布局。 Container：矩形视觉元素。 2. Material组件 Android系UI。 要使用 Material 组件，需要先引入它： import 'package:flutter/material.dart'; Material 应用程序以MaterialApp (opens new window) 组件开始。 Material 组件有Scaffold、AppBar、TextButton等。 3. Cupertino组件 iOS系UI。 MaterialPageRoute：在路由切换时，如果是 Android 系统，它将会使用 Android 系统默认的页面切换动画(从底向上)；如果是 iOS 系统，它会使用 iOS 系统默认的页面切换动画（从右向左）。 Cupertino 组件风格的页面： //导入cupertino `widget`库 import 'package:flutter/cupertino.dart'; class CupertinoTestRoute extends StatelessWidget { @override `widget`build(BuildContext context) { return CupertinoPageScaffold( navigationBar: CupertinoNavigationBar( middle: Text(\"Cupertino Demo\"), ), child: Center( child: CupertinoButton( color: CupertinoColors.activeBlue, child: Text(\"Press\"), onPressed: () {} ), ), ); } } 总结 Flutter 的widget类型分为StatefulWidget 和 StatelessWidget 两种。 引入过多组件库会让你的应用安装包变大。 由于 Material 和 Cupertino 都是在基础组件库之上的，所以如果我们的应用中引入了这两者之一，则不需要再引入flutter/ widgets.dart了，因为它们内部已经引入过了。 Widget 简介Flutter中Widget 、Element、RenderObject角色深入分析Flutter State生命周期 "},"pages/fultter/状态管理.html":{"url":"pages/fultter/状态管理.html","title":"状态管理","keywords":"","body":"状态管理 Widget管理自身状态 _TapboxAState 类: 管理TapboxA的状态。 定义_active：确定盒子的当前颜色的布尔值。 定义_handleTap()函数，该函数在点击该盒子时更新_active，并调用setState()更新UI。 实现widget的所有交互式行为。 // TapboxA 管理自身状态. //------------------------- TapboxA ---------------------------------- class TapboxA extends StatefulWidget { TapboxA({Key? key}) : super(key: key); @override _TapboxAState createState() => _TapboxAState(); } class _TapboxAState extends State { bool _active = false; void _handleTap() { setState(() { _active = !_active; }); } @override Widget build(BuildContext context) { return GestureDetector( onTap: _handleTap, child: Container( width: 200.0, height: 200.0, decoration: BoxDecoration( color: _active ? Colors.lightGreen[700] : Colors.grey[600], ), child: Center( child: Text( _active ? 'Active' : 'Inactive', style: const TextStyle(fontSize: 32.0, color: Colors.white), ), ), ), ); } } 父Widget管理子Widget的状态 ParentWidgetState 类: 为TapboxB 管理_active状态。 实现_handleTapboxChanged()，当盒子被点击时调用的方法。 当状态改变时，调用setState()更新UI。 TapboxB 类: 继承StatelessWidget类，因为所有状态都由其父组件处理。 当检测到点击时，它会通知父组件。 // ParentWidget 为 TapboxB 管理状态. //------------------------ ParentWidget -------------------------------- class ParentWidget extends StatefulWidget { @override _ParentWidgetState createState() => _ParentWidgetState(); } class _ParentWidgetState extends State { bool _active = false; void _handleTapboxChanged(bool newValue) { setState(() { _active = newValue; }); } @override Widget build(BuildContext context) { return Container( child: TapboxB( active: _active, onChanged: _handleTapboxChanged, ), ); } } //------------------------- TapboxB ---------------------------------- class TapboxB extends StatelessWidget { TapboxB({Key? key, this.active: false, required this.onChanged}) : super(key: key); final bool active; final ValueChanged onChanged; void _handleTap() { onChanged(!active); } Widget build(BuildContext context) { return GestureDetector( onTap: _handleTap, child: Container( child: Center( child: Text( active ? 'Active' : 'Inactive', style: TextStyle(fontSize: 32.0, color: Colors.white), ), ), width: 200.0, height: 200.0, decoration: BoxDecoration( color: active ? Colors.lightGreen[700] : Colors.grey[600], ), ), ); } } 混合状态管理 _ParentWidgetStateC类: 管理_active 状态。 实现 _handleTapboxChanged() ，当盒子被点击时调用。 当点击盒子并且_active状态改变时调用setState()更新UI。 _TapboxCState 对象: 管理_highlight 状态。 GestureDetector监听所有tap事件。当用户点下时，它添加高亮（深绿色边框）；当用户释放时，会移除高亮。 当按下、抬起、或者取消点击时更新_highlight状态，调用setState()更新UI。 当点击时，将状态的改变传递给父组件。 //---------------------------- ParentWidget ---------------------------- class ParentWidgetC extends StatefulWidget { @override _ParentWidgetCState createState() => _ParentWidgetCState(); } class _ParentWidgetCState extends State { bool _active = false; void _handleTapboxChanged(bool newValue) { setState(() { _active = newValue; }); } @override Widget build(BuildContext context) { return Container( child: TapboxC( active: _active, onChanged: _handleTapboxChanged, ), ); } } //----------------------------- TapboxC ------------------------------ class TapboxC extends StatefulWidget { TapboxC({Key? key, this.active: false, required this.onChanged}) : super(key: key); final bool active; final ValueChanged onChanged; @override _TapboxCState createState() => _TapboxCState(); } class _TapboxCState extends State { bool _highlight = false; void _handleTapDown(TapDownDetails details) { setState(() { _highlight = true; }); } void _handleTapUp(TapUpDetails details) { setState(() { _highlight = false; }); } void _handleTapCancel() { setState(() { _highlight = false; }); } void _handleTap() { widget.onChanged(!widget.active); } @override Widget build(BuildContext context) { // 在按下时添加绿色边框，当抬起时，取消高亮 return GestureDetector( onTapDown: _handleTapDown, // 处理按下事件 onTapUp: _handleTapUp, // 处理抬起事件 onTap: _handleTap, onTapCancel: _handleTapCancel, child: Container( child: Center( child: Text( widget.active ? 'Active' : 'Inactive', style: TextStyle(fontSize: 32.0, color: Colors.white), ), ), width: 200.0, height: 200.0, decoration: BoxDecoration( color: widget.active ? Colors.lightGreen[700] : Colors.grey[600], border: _highlight ? Border.all( color: Colors.black38, width: 10.0, ) : null, ), ), ); } } 全局状态管理 实现一个全局的事件总线，将语言状态改变对应为一个事件，然后在APP中依赖应用语言的组件的initState 方法中订阅语言改变的事件。当用户在设置页切换语言后，我们发布语言改变事件，而订阅了此事件的组件就会收到通知，收到通知后调用setState(...)方法重新build一下自身即可。 这种类似iOS的通知。 使用一些专门用于状态管理的包，如 Provider、Redux，读者可以在 pub 上查看其详细信息。 状态管理 "},"pages/fultter/路由管理.html":{"url":"pages/fultter/路由管理.html","title":"路由管理","keywords":"","body":"路由管理 一个简单示例 创建一个新路由，命名“NewRoute”。 class NewRoute extends StatelessWidget { @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: Text(\"New route\"), ), body: Center( child: Text(\"This is new route\"), ), ); } } 我们添加了一个打开新路由的按钮，点击该按钮后就会打开新的路由页面。 Column( mainAxisAlignment: MainAxisAlignment.center, children: [ ... //省略无关代码 TextButton( child: Text(\"open new route\"), onPressed: () { //导航到新路由 Navigator.push( context, MaterialPageRoute(builder: (context) { return NewRoute(); }), ); }, ), ], ) MaterialPageRoute MaterialPageRoute继承自PageRoute类，PageRoute类是一个抽象类，表示占有整个屏幕空间的一个模态路由页面。 MaterialPageRoute 是 Material组件库提供的组件，它可以针对不同平台，实现与平台页面切换动画风格一致的路由切换动画。 MaterialPageRoute 构造函数的各个参数的意义： MaterialPageRoute({ WidgetBuilder builder, RouteSettings settings, bool maintainState = true, bool fullscreenDialog = false, }) builder：是一个回调函数，返回值是一个widget，也就是我们跳转的页面。 settings：包含路由的配置信息，如路由名称、是否初始路由（首页）。 maintainState：一个已经不可见(被上面的盖住完全看不到啦~)的组件，是否还需要保存状态。 fullscreenDialog：表示新的路由页面是否是一个全屏的模态对话框。 Navigator Navigator是一个路由管理的组件，它提供了打开和退出路由页方法。 Future push(BuildContext context, Route route) 将给定的路由入栈（即打开新的页面），返回值是一个Future对象，用以接收新路由出栈（即关闭）时的返回数据。 bool pop(BuildContext context, [ result ]) 将栈顶路由出栈，result 为页面关闭时返回给上一个页面的数据。 实例方法 Navigator.push(BuildContext context, Route route)等价于Navigator.of(context).push(Route route)。 路由传值 传值： Navigator.pop(context, \"我是返回值\") 获取值： () async { // 打开`TipRoute`，并等待返回结果 var result = await Navigator.push( context, MaterialPageRoute( builder: (context) { return TipRoute( // 路由参数 text: \"我是提示xxxx\", ); }, ), ); //输出`TipRoute`路由返回结果 print(\"路由返回值: $result\"); } 命名路由 1. 路由表 它是一个Map，key为路由的名字，是个字符串；value是个builder回调函数，用于生成相应的路由widget。 Map routes; 2. 注册路由表 MaterialApp添加routes属性: MaterialApp( title: 'Flutter Demo', theme: ThemeData( primarySwatch: Colors.blue, ), //注册路由表 routes:{ \"new_page\":(context) => NewRoute(), ... // 省略其他路由注册信息 } , home: MyHomePage(title: 'Flutter Demo Home Page'), ); 将home注册为命名路由: MaterialApp( title: 'Flutter Demo', initialRoute:\"/\", //名为\"/\"的路由作为应用的home(首页) theme: ThemeData( primarySwatch: Colors.blue, ), //注册路由表 routes:{ \"new_page\":(context) => NewRoute(), \"/\":(context) => MyHomePage(title: 'Flutter Demo Home Page'), //注册首页路由 } ); 3. 打开新路由页 可以使用Navigator 的pushNamed方法： Future pushNamed(BuildContext context, String routeName,{Object arguments}) 调用： onPressed: () { Navigator.pushNamed(context, \"new_page\"); //Navigator.push(context, // MaterialPageRoute(builder: (context) { // return NewRoute(); //})); }, 4. 命名路由参数传递 传递参数: Navigator.of(context).pushNamed(\"new_page\", arguments: \"hi\"); 获取路由参数: @override Widget build(BuildContext context) { //获取路由参数 var args=ModalRoute.of(context).settings.arguments; //...省略无关代码 } 5. 带参数的路由 MaterialApp( ... //省略无关代码 routes: { \"tip2\": (context){ return TipRoute(text: ModalRoute.of(context)!.settings.arguments); }, }, ); 路由生成钩子 MaterialApp有一个onGenerateRoute属性，它在打开命名路由时可能会被调用，之所以说可能，是因为当调用Navigator.pushNamed(...)打开命名路由时，如果指定的路由名在路由表中已注册，则会调用路由表中的builder函数来生成路由组件；如果路由表中没有注册，才会调用onGenerateRoute来生成路由。onGenerateRoute回调签名如下： Route Function(RouteSettings settings) 有了onGenerateRoute回调，要实现上面控制页面权限的功能就非常容易：我们放弃使用路由表，取而代之的是提供一个onGenerateRoute回调，然后在该回调中进行统一的权限控制，如： MaterialApp( ... //省略无关代码 onGenerateRoute:(RouteSettings settings){ return MaterialPageRoute(builder: (context){ String routeName = settings.name; // 如果访问的路由页需要登录，但当前未登录，则直接返回登录页路由， // 引导用户登录；其他情况则正常打开路由。 } ); } ); onGenerateRoute onUnknownRoute区别：onGenerateRoute 无法生成路由，会触发OnUnknownRoute 属性来处理该场景。 注意，onGenerateRoute 只会对命名路由生效。也就是需要调用Navigator.pushNamed。 路由管理Flutter 路由原理解析Flutter 中的 onUnknownRoute 是什么 "},"pages/fultter/包管理.html":{"url":"pages/fultter/包管理.html","title":"包管理","keywords":"","body":"包管理 YAML name: flutter_in_action description: First Flutter Application. version: 1.0.0+1 dependencies: flutter: sdk: flutter cupertino_icons: ^0.1.2 dev_dependencies: flutter_test: sdk: flutter flutter: uses-material-design: true Pub仓库 Pub（https://pub.dev/ ）是 Google 官方的 Dart Packages 仓库，类似于 node 中的 npm仓库、Android中的 jcenter。我们可以在 Pub 上面查找我们需要的包和插件，也可以向 Pub 发布我们的包和插件。 示例 1.将“english_words” 添加到依赖项列表，如下： dependencies: flutter: sdk: flutter # 新添加的依赖 english_words: ^4.0.0 2.下载包。在Android Studio的编辑器视图中查看pubspec.yaml时（图2-13），单击右上角的 Pub get 。3.引入english_words包。 import 'package:english_words/english_words.dart'; 4.使用english_words包来生成随机字符串。 // 生成随机字符串 final wordPair = WordPair.random(); 其他依赖方式 1.依赖本地包 dependencies: pkg1: path: ../../code/pkg1 2.依赖Git dependencies: pkg1: git: url: git://github.com/xxx/pkg1.git 3.使用path参数指定相对位置 dependencies: package1: git: url: git://github.com/flutter/packages.git path: packages/package1 依赖方式:https://www.dartlang.org/tools/pub/dependencies 包管理 "},"pages/fultter/资源管理.html":{"url":"pages/fultter/资源管理.html","title":"资源管理","keywords":"","body":"资源管理 常见类型的 assets 包括静态数据（例如JSON文件）、配置文件、图标和图片等。 指定 assets 在pubspec.yaml中配置： flutter: assets: - assets/my_icon.png - assets/background.png assets指定应包含在应用程序中的文件， 每个 asset 都通过相对于pubspec.yaml文件所在的文件系统路径来标识自身的路径。 Asset 变体（variant） 例如，如果应用程序目录中有以下文件: …/pubspec.yaml …/graphics/my_icon.png …/graphics/background.png …/graphics/dark/background.png …etc. 然后pubspec.yaml文件中只需包含: flutter: assets: - graphics/background.png 那么这两个graphics/background.png和graphics/dark/background.png 都将包含在您的 asset bundle中。前者被认为是main asset （主资源），后者被认为是一种变体（variant）。 在选择匹配当前设备分辨率的图片时，Flutter会使用到 asset 变体（见下文）。 加载 assets 1. 加载文本assets rootBundle：全局静态对象。 Future loadAsset() async { return await rootBundle.loadString('assets/config.json'); } DefaultAssetBundle: DefaultAssetBundle.of(context) 2. 加载图片 1) 声明分辨率相关的图片 assets 2) 加载图片 Widget build(BuildContext context) { return DecoratedBox( decoration: BoxDecoration( image: DecorationImage( image: AssetImage('graphics/background.png'), ), ), ); } AssetImage 并非是一个widget， 它实际上是一个ImageProvider，有些时候你可能期望直接得到一个显示图片的widget，那么你可以使用Image.asset()方法，如： Widget build(BuildContext context) { return Image.asset('graphics/background.png'); } 3) 依赖包中的资源图片 AssetImage('icons/heart.png', package: 'my_icons' 或 Image.asset('icons/heart.png', package: 'my_icons') 4) 打包包中的 assets 3. 特定平台 assets 1）设置APP图标 2）更新启动页 平台共享 assets Flutter 提供了一种Flutter和原生之间共享资源的方式。 资源管理 "},"pages/fultter/文本及样式.html":{"url":"pages/fultter/文本及样式.html","title":"文本及样式","keywords":"","body":"文本及样式 源码 Flutter 中使用自定义字体 文本及样式 Text Text(\"Hello world\", textAlign: TextAlign.left, ); Text(\"Hello world! I'm Jack. \"*4, maxLines: 1, overflow: TextOverflow.ellipsis, ); Text(\"Hello world\", textScaleFactor: 1.5, ); TextStyle Text(\"Hello world\", style: TextStyle( color: Colors.blue, fontSize: 18.0, height: 1.2, fontFamily: \"Courier\", background: Paint()..color=Colors.yellow, decoration:TextDecoration.underline, decorationStyle: TextDecorationStyle.dashed ), ); TextSpan Text.rich(TextSpan( children: [ TextSpan( text: \"Home: \" ), TextSpan( text: \"https://flutterchina.club\", style: TextStyle( color: Colors.blue ), recognizer: _tapRecognizer ), ] )) DefaultTextStyle DefaultTextStyle( //1.设置文本默认样式 style: TextStyle( color:Colors.red, fontSize: 20.0, ), textAlign: TextAlign.start, child: Column( crossAxisAlignment: CrossAxisAlignment.start, children: [ Text(\"hello world\"), Text(\"I am Jack\"), Text(\"I am Jack\", style: TextStyle( inherit: false, //2.不继承默认样式 color: Colors.grey ), ), ], ), ); 字体 "},"pages/fultter/按钮样式.html":{"url":"pages/fultter/按钮样式.html","title":"按钮样式","keywords":"","body":"按钮样式 源码 按钮 flutter TextButton样式 ElevatedButton ElevatedButton( child: Text(\"normal\"), onPressed: () {}, ); TextButton TextButton( child: Text(\"normal\"), onPressed: () {}, ) OutlinedButton OutlineButton( child: Text(\"normal\"), onPressed: () {}, ) IconButton IconButton( icon: Icon(Icons.thumb_up), onPressed: () {}, ) 带图标的按钮 ElevatedButton.icon( icon: Icon(Icons.send), label: Text(\"发送\"), onPressed: _onPressed, ), OutlinedButton.icon( icon: Icon(Icons.add), label: Text(\"添加\"), onPressed: _onPressed, ), TextButton.icon( icon: Icon(Icons.info), label: Text(\"详情\"), onPressed: _onPressed, ), "},"pages/fultter/图片样式.html":{"url":"pages/fultter/图片样式.html","title":"图片样式","keywords":"","body":"图片样式 源码 图片及ICON 图片资源 使用flutter加载本地图片报错 图片 Flutter 中，我们可以通过Image组件来加载并显示图片，Image的数据源可以是asset、文件、内存以及网络。 1. ImageProvider ImageProvider 是一个抽象类，主要定义了图片数据获取的接口load()，从不同的数据源获取图片需要实现不同的ImageProvider ，如AssetImage是实现了从Asset中加载图片的 ImageProvider，而NetworkImage 实现了从网络加载图片的 ImageProvider。 2. Image Image widget 有一个必选的image参数，它对应一个 ImageProvider。下面我们分别演示一下如何从 asset 和网络加载图片。 1）先配置.yaml，再从asset中加载图片： Image( image: AssetImage(\"images/avatar.png\"), width: 100.0 ); 或 Image.asset(\"images/avatar.png\", width: 100.0, ) 2) 从网络加载图片： Image( image: NetworkImage( \"https://avatars2.githubusercontent.com/u/20411648?s=460&v=4\"), width: 100.0, ) 或 Image.network( \"https://avatars2.githubusercontent.com/u/20411648?s=460&v=4\", width: 100.0, ) 3）参数 ICON "},"pages/fultter/单选开关和复选框.html":{"url":"pages/fultter/单选开关和复选框.html","title":"单选开关和复选框","keywords":"","body":"单选开关和复选框 源码 单选开关和复选框 class SwitchAndCheckBoxTestRoute extends StatefulWidget { @override _SwitchAndCheckBoxTestRouteState createState() => _SwitchAndCheckBoxTestRouteState(); } class _SwitchAndCheckBoxTestRouteState extends State { bool _switchSelected=true; //维护单选开关状态 bool _checkboxSelected=true;//维护复选框状态 @override Widget build(BuildContext context) { return Column( children: [ Switch( value: _switchSelected,//当前状态 onChanged:(value){ //重新构建页面 setState(() { _switchSelected=value; }); }, ), Checkbox( value: _checkboxSelected, activeColor: Colors.red, //选中时的颜色 onChanged:(value){ setState(() { _checkboxSelected=value; }); } , ) ], ); } } "},"pages/fultter/输入框及表单.html":{"url":"pages/fultter/输入框及表单.html","title":"输入框及表单","keywords":"","body":"输入框及表单 源码 输入框及表单 TextField参数 controller：编辑框的控制器，通过它可以设置/获取编辑框的内容、选择编辑内容、监听编辑文本改变事件。大多数情况下我们都需要显式提供一个controller来与文本框交互。如果没有提供controller，则TextField内部会自动创建一个。 focusNode：用于控制TextField是否占有当前键盘的输入焦点。它是我们和键盘交互的一个句柄（handle）。 InputDecoration：用于控制TextField的外观显示，如提示文本、背景颜色、边框等。 keyboardType：用于设置该输入框默认的键盘输入类型。 textInputAction：键盘动作按钮图标(即回车键位图标)，它是一个枚举值，有多个可选值。 style：正在编辑的文本样式。 textAlign: 输入框内编辑文本在水平方向的对齐方式。 autofocus: 是否自动获取焦点。 obscureText：是否隐藏正在编辑的文本，如用于输入密码的场景等，文本内容会用“•”替换。 maxLines：输入框的最大行数，默认为1；如果为null，则无行数限制。 maxLength和maxLengthEnforcement ：maxLength代表输入框文本的最大长度，设置后输入框右下角会显示输入的文本计数。maxLengthEnforcement决定当输入文本长度超过maxLength时如何处理，如截断、超出等。 toolbarOptions：长按或鼠标右击时出现的菜单，包括 copy、cut、paste 以及 selectAll。 onChange：输入框内容改变时的回调函数；注：内容改变事件也可以通过controller来监听。 onEditingComplete和onSubmitted：这两个回调都是在输入框输入完成时触发，比如按了键盘的完成键（对号图标）或搜索键（🔍图标）。不同的是两个回调签名不同，onSubmitted回调是ValueChanged类型，它接收当前输入内容做为参数，而onEditingComplete不接收参数。 inputFormatters：用于指定输入格式；当用户输入内容改变时，会根据指定的格式来校验。 enable：如果为false，则输入框会被禁用，禁用状态不接收输入和事件，同时显示禁用态样式（在其decoration中定义）。 cursorWidth、cursorRadius和cursorColor：这三个属性是用于自定义输入框光标宽度、圆角和颜色的。 示例 1. 布局 Column( children: [ TextField( autofocus: true, decoration: InputDecoration( labelText: \"用户名\", hintText: \"用户名或邮箱\", prefixIcon: Icon(Icons.person) ), ), TextField( decoration: InputDecoration( labelText: \"密码\", hintText: \"您的登录密码\", prefixIcon: Icon(Icons.lock) ), obscureText: true, ), ], ); 2. 获取输入内容 //定义一个controller TextEditingController _unameController = TextEditingController(); TextField( autofocus: true, controller: _unameController, //设置controller ... ) print(_unameController.text) 3. 监听文本变化 TextField( autofocus: true, onChanged: (v) { print(\"onChange: $v\"); } ) 或 @override void initState() { //监听输入改变 _unameController.addListener((){ print(_unameController.text); }); } 4. 控制焦点 5. 监听焦点状态改变事件 6. 自定义样式 表单Form 1. Form 2. FormField 3. FormState "},"pages/fultter/进度指示器.html":{"url":"pages/fultter/进度指示器.html","title":"进度指示器","keywords":"","body":"进度指示器 源码 进度指示器 LinearProgressIndicator LinearProgressIndicator是一个线性、条状的进度条，定义如下： LinearProgressIndicator({ double value, Color backgroundColor, Animation valueColor, ... }) value：value表示当前的进度，取值范围为[0,1]；如果value为null时则指示器会执行一个循环动画（模糊进度）；当value不为null时，指示器为一个具体进度的进度条。 backgroundColor：指示器的背景色。 valueColor: 指示器的进度条颜色；值得注意的是，该值类型是Animation，这允许我们对进度条的颜色也可以指定动画。如果我们不需要对进度条颜色执行动画，换言之，我们想对进度条应用一种固定的颜色，此时我们可以通过AlwaysStoppedAnimation来指定。 示例： // 模糊进度条(会执行一个动画) LinearProgressIndicator( backgroundColor: Colors.grey[200], valueColor: AlwaysStoppedAnimation(Colors.blue), ), //进度条显示50% LinearProgressIndicator( backgroundColor: Colors.grey[200], valueColor: AlwaysStoppedAnimation(Colors.blue), value: .5, ) CircularProgressIndicator CircularProgressIndicator是一个圆形进度条，定义如下： CircularProgressIndicator({ double value, Color backgroundColor, Animation valueColor, this.strokeWidth = 4.0, ... }) 前三个参数和LinearProgressIndicator相同，不再赘述。strokeWidth 表示圆形进度条的粗细。示例如下： // 模糊进度条(会执行一个旋转动画) CircularProgressIndicator( backgroundColor: Colors.grey[200], valueColor: AlwaysStoppedAnimation(Colors.blue), ), //进度条显示50%，会显示一个半圆 CircularProgressIndicator( backgroundColor: Colors.grey[200], valueColor: AlwaysStoppedAnimation(Colors.blue), value: .5, ), "},"pages/fultter/布局类组件介绍.html":{"url":"pages/fultter/布局类组件介绍.html","title":"布局类组件介绍","keywords":"","body":"布局类组件介绍 布局类组件介绍 布局类组件都会包含一个或多个子组件，不同的布局类组件对子组件排列（layout）方式不同。 布局类 1. LeafRenderObjectWidget 非容器类组件基类，Widget树的叶子节点，用于没有子节点的widget，通常基础组件都属于这一类，如Image。 2. SingleChildRenderObjectWidget 单子组件基类，包含一个子Widget，如：ConstrainedBox、DecoratedBox等。 3. MultiChildRenderObjectWidget 多子组件基类，包含多个子Widget，一般都有一个children参数，接受一个Widget数组。如Row、Column、Stack等。 4. 继承关系 Widget > RenderObjectWidget > (Leaf/SingleChild/MultiChild)RenderObjectWidget 。 RenderObjectWidget 子类必须实现创建、更新RenderObject的方法。RenderObject是最终布局、渲染UI界面的对象，实现布局算法。Stack（层叠布局）对应的RenderObject对象就是RenderStack，而层叠布局的实现就在RenderStack中。 "},"pages/fultter/布局原理与约束.html":{"url":"pages/fultter/布局原理与约束.html","title":"布局原理与约束","keywords":"","body":"布局原理与约束 源码 布局原理与约束 尺寸限制类容器用于限制容器大小，Flutter中提供了多种这样的容器，如ConstrainedBox、SizedBox、UnconstrainedBox、AspectRatio 等，本节将介绍一些常用的。 任何时候子组件都必须先遵守父组件的约束。 Flutter布局模型 1. 两种布局模型 基于 RenderBox 的盒模型布局。 基于 Sliver ( RenderSliver ) 按需加载列表布局。 2. 布局流程 上层组件向下层组件传递约束（constraints）条件。 下层组件确定自己的大小，然后告诉上层组件。注意下层组件的大小必须符合父组件的约束。 上层组件确定下层组件相对于自身的偏移和确定自身的大小（大多数情况下会根据子组件的大小来确定自身的大小）。 BoxConstraints BoxConstraints 是盒模型布局过程中父渲染对象传递给子渲染对象的约束信息。BoxConstraints.tight(Size size)：固定宽高，BoxConstraints.expand()：尽可能大。 const BoxConstraints({ this.minWidth = 0.0, //最小宽度 this.maxWidth = double.infinity, //最大宽度 this.minHeight = 0.0, //最小高度 this.maxHeight = double.infinity //最大高度 }) ConstrainedBox 对子组件添加额外的约束，可以设置constraints。 ConstrainedBox( constraints: BoxConstraints( minWidth: double.infinity, //宽度尽可能大 minHeight: 50.0 //最小高度为50像素 ), child: Container( height: 5.0, child: redBox , ), ) SizedBox 用于给子元素指定固定的宽高。 SizedBox( width: 80.0, height: 80.0, child: redBox ) ConstrainedBox与SizedBox关系 ConstrainedBox和SizedBox都是通过RenderConstrainedBox来渲染的，我们可以看到ConstrainedBox和SizedBox的createRenderObject()方法都返回的是一个RenderConstrainedBox对象 @override RenderConstrainedBox createRenderObject(BuildContext context) { return RenderConstrainedBox( additionalConstraints: ..., ); } 多重限制 ConstrainedBox( constraints: BoxConstraints(minWidth: 60.0, minHeight: 60.0), //父 child: ConstrainedBox( constraints: BoxConstraints(minWidth: 90.0, minHeight: 20.0),//子 child: redBox, ), ) 或 ConstrainedBox( constraints: BoxConstraints(minWidth: 90.0, minHeight: 20.0), child: ConstrainedBox( constraints: BoxConstraints(minWidth: 60.0, minHeight: 60.0), child: redBox, ) ) 显示效果相同，90x60。我们发现有多重限制时，对于minWidth和minHeight来说，是取父子中相应数值较大的。实际上，只有这样才能保证父限制与子限制不冲突。 UnconstrainedBox ConstrainedBox( constraints: BoxConstraints(minWidth: 60.0, minHeight: 100.0), //父 child: UnconstrainedBox( //“去除”父级限制 child: ConstrainedBox( constraints: BoxConstraints(minWidth: 90.0, minHeight: 20.0),//子 child: redBox, ), ) ) 上面代码中，如果没有中间的UnconstrainedBox，那么根据上面所述的多重限制规则，那么最终将显示一个90×100的红色框。但是由于UnconstrainedBox “去除”了父ConstrainedBox的限制，则最终会按照子ConstrainedBox的限制来绘制redBox，即90×20。 但是，读者请注意，UnconstrainedBox对父组件限制的“去除”并非是真正的去除：上面例子中虽然红色区域大小是90×20，但上方仍然有80的空白空间。也就是说父限制的minHeight(100.0)仍然是生效的，只不过它不影响最终子元素redBox的大小，但仍然还是占有相应的空间，可以认为此时的父ConstrainedBox是作用于子UnconstrainedBox上，而redBox只受子ConstrainedBox限制。 "},"pages/fultter/线性布局.html":{"url":"pages/fultter/线性布局.html","title":"线性布局","keywords":"","body":"线性布局 源码 线性布局 主轴和纵轴 主轴：Row的主轴是水平方向，因为Row是沿水平方向布局；Column的主轴是垂直方向，同理； 纵轴：Row的纵轴是垂直方向，而Column的纵轴是水平方向。 Row Row可以沿水平方向排列其子widget。定义如下： Row({ ... TextDirection textDirection, MainAxisSize mainAxisSize = MainAxisSize.max, MainAxisAlignment mainAxisAlignment = MainAxisAlignment.start, VerticalDirection verticalDirection = VerticalDirection.down, CrossAxisAlignment crossAxisAlignment = CrossAxisAlignment.center, List children = const [], }) textDirection：Row是沿着水平方向布局，那么就存在，从左到右还是从右到左，textDirection就是决定这个布局方向。 mainAxisSize：水平方向上是否占用最大空间。 mainAxisAlignment：水平方向的对齐方式，与textDirection共同决定。 verticalDirection：垂直方向对齐方向。 crossAxisAlignment：垂直方向的对齐方式，与verticalDirection共同决定。 示例： Column( //测试Row对齐方式，排除Column默认居中对齐的干扰 crossAxisAlignment: CrossAxisAlignment.start, children: [ Row( mainAxisAlignment: MainAxisAlignment.center, children: [ Text(\" hello world \"), Text(\" I am Jack \"), ], ), Row( mainAxisSize: MainAxisSize.min, mainAxisAlignment: MainAxisAlignment.center, children: [ Text(\" hello world \"), Text(\" I am Jack \"), ], ), Row( mainAxisAlignment: MainAxisAlignment.end, textDirection: TextDirection.rtl, children: [ Text(\" hello world \"), Text(\" I am Jack \"), ], ), Row( crossAxisAlignment: CrossAxisAlignment.start, verticalDirection: VerticalDirection.up, children: [ Text(\" hello world \", style: TextStyle(fontSize: 30.0),), Text(\" I am Jack \"), ], ), ], ) 解释：第一个Row很简单，默认为居中对齐；第二个Row，由于mainAxisSize值为MainAxisSize.min，Row的宽度等于两个Text的宽度和，所以对齐是无意义的，所以会从左往右显示；第三个Row设置textDirection值为TextDirection.rtl，所以子组件会从右向左的顺序排列，而此时MainAxisAlignment.end表示左对齐，所以最终显示结果就是图中第三行的样子；第四个 Row 测试的是纵轴的对齐方式，由于两个子 Text 字体不一样，所以其高度也不同，我们指定了verticalDirection值为VerticalDirection.up，即从低向顶排列，而此时crossAxisAlignment值为CrossAxisAlignment.start表示底对齐。 Column Column可以在垂直方向排列其子组件。 "},"pages/fultter/弹性布局.html":{"url":"pages/fultter/弹性布局.html","title":"弹性布局","keywords":"","body":"弹性布局 源码 弹性布局 弹性布局允许子组件按照一定比例来分配父容器空间。 Flex Flex继承自MultiChildRenderObjectWidget，对应的RenderObject为RenderFlex，RenderFlex中实现了其布局算法。 Flex({ ... required this.direction, //弹性布局的方向, Row默认为水平方向，Column默认为垂直方向 List children = const [], }) Expanded Expanded 只能作为 Flex 的孩子（否则会报错），它可以按比例“扩伸”Flex子组件所占用的空间。因为 Row和Column 都继承自 Flex，所以 Expanded 也可以作为它们的孩子。 const Expanded({ int flex = 1, required Widget child, }) flex参数为弹性系数，如果为 0 或null，则child是没有弹性的，即不会被扩伸占用的空间。如果大于0，所有的Expanded按照其 flex 的比例来分割主轴的全部空闲空间。下面我们看一个例子： class FlexLayoutTestRoute extends StatelessWidget { @override Widget build(BuildContext context) { return Column( children: [ //Flex的两个子widget按1：2来占据水平空间 Flex( direction: Axis.horizontal, children: [ Expanded( flex: 1, child: Container( height: 30.0, color: Colors.red, ), ), Expanded( flex: 2, child: Container( height: 30.0, color: Colors.green, ), ), ], ), Padding( padding: const EdgeInsets.only(top: 20.0), child: SizedBox( height: 100.0, //Flex的三个子widget，在垂直方向按2：1：1来占用100像素的空间 child: Flex( direction: Axis.vertical, children: [ Expanded( flex: 2, child: Container( height: 30.0, color: Colors.red, ), ), Spacer( flex: 1, ), Expanded( flex: 1, child: Container( height: 30.0, color: Colors.green, ), ), ], ), ), ), ], ); } } 示例中的Spacer的功能是占用指定比例的空间，实际上它只是Expanded的一个包装类，Spacer的源码如下： class Spacer extends StatelessWidget { const Spacer({Key? key, this.flex = 1}) : assert(flex != null), assert(flex > 0), super(key: key); final int flex; @override Widget build(BuildContext context) { return Expanded( flex: flex, child: const SizedBox.shrink(), ); } } "},"pages/fultter/流式布局.html":{"url":"pages/fultter/流式布局.html","title":"流式布局","keywords":"","body":"流式布局 源码 流式布局 超出屏幕显示范围会自动折行的布局称为流式布局。 Wrap 下面是Wrap的定义: Wrap({ ... this.direction = Axis.horizontal, this.alignment = WrapAlignment.start, this.spacing = 0.0, this.runAlignment = WrapAlignment.start, this.runSpacing = 0.0, this.crossAxisAlignment = WrapCrossAlignment.start, this.textDirection, this.verticalDirection = VerticalDirection.down, List children = const [], }) spacing：主轴方向子widget的间距 runSpacing：纵轴方向的间距 runAlignment：纵轴方向的对齐方式 下面看一个示例子： Wrap( spacing: 8.0, // 主轴(水平)方向间距 runSpacing: 4.0, // 纵轴（垂直）方向间距 alignment: WrapAlignment.center, //沿主轴方向居中 children: [ Chip( avatar: CircleAvatar(backgroundColor: Colors.blue, child: Text('A')), label: Text('Hamilton'), ), Chip( avatar: CircleAvatar(backgroundColor: Colors.blue, child: Text('M')), label: Text('Lafayette'), ), Chip( avatar: CircleAvatar(backgroundColor: Colors.blue, child: Text('H')), label: Text('Mulligan'), ), Chip( avatar: CircleAvatar(backgroundColor: Colors.blue, child: Text('J')), label: Text('Laurens'), ), ], ) Flow 主要用于一些需要自定义布局策略或性能要求较高(如动画中)的场景。 "},"pages/fultter/层叠布局.html":{"url":"pages/fultter/层叠布局.html","title":"层叠布局","keywords":"","body":"层叠布局 源码 层叠布局 子组件可以根据距父容器四个角的位置来确定自身的位置。层叠布局允许子组件按照代码中声明的顺序堆叠起来。 Stack Stack({ this.alignment = AlignmentDirectional.topStart, this.textDirection, this.fit = StackFit.loose, this.clipBehavior = Clip.hardEdge, List children = const [], }) alignment：决定如何去对齐没有定位（没有使用Positioned）或部分定位的子组件。 textDirection：和Row、Wrap的textDirection功能一样，都用于确定alignment对齐的参考系。 fit：此参数用于确定没有定位的子组件如何去适应Stack的大小。 clipBehavior：此属性决定对超出Stack显示空间的部分如何剪裁。 Positioned const Positioned({ Key? key, this.left, this.top, this.right, this.bottom, this.width, this.height, required Widget child, }) left、top 、right、 bottom分别代表离Stack左、上、右、底四边的距离。width和height用于指定需要定位元素的宽度和高度。注意，Positioned的width、height 和其他地方的意义稍微有点区别，此处用于配合left、top 、right、 bottom来定位组件，举个例子，在水平方向时，你只能指定left、right、width三个属性中的两个，如指定left和width后，right会自动算出(left+width)，如果同时指定三个属性则会报错，垂直方向同理。 示例 //通过ConstrainedBox来确保Stack占满屏幕 ConstrainedBox( constraints: BoxConstraints.expand(), child: Stack( alignment:Alignment.center , //指定未定位或部分定位widget的对齐方式 children: [ Container( child: Text(\"Hello world\",style: TextStyle(color: Colors.white)), color: Colors.red, ), Positioned( left: 18.0, child: Text(\"I am Jack\"), ), Positioned( top: 18.0, child: Text(\"Your friend\"), ) ], ), ); 由于第一个子文本组件Text(\"Hello world\")没有指定定位，并且alignment值为Alignment.center，所以它会居中显示。第二个子文本组件Text(\"I am Jack\")只指定了水平方向的定位(left)，所以属于部分定位，即垂直方向上没有定位，那么它在垂直方向的对齐方式则会按照alignment指定的对齐方式对齐，即垂直方向居中。对于第三个子文本组件Text(\"Your friend\")，和第二个Text原理一样，只不过是水平方向没有定位，则水平方向居中。 我们给上例中的Stack指定一个fit属性，然后将三个子文本组件的顺序调整一下： Stack( alignment:Alignment.center , fit: StackFit.expand, //未定位widget占满Stack整个空间 children: [ Positioned( left: 18.0, child: Text(\"I am Jack\"), ), Container(child: Text(\"Hello world\",style: TextStyle(color: Colors.white)), color: Colors.red, ), Positioned( top: 18.0, child: Text(\"Your friend\"), ) ], ), 可以看到，由于第二个子文本组件没有定位，所以fit属性会对它起作用，就会占满Stack。由于Stack子元素是堆叠的，所以第一个子文本组件被第二个遮住了，而第三个在最上层，所以可以正常显示。 "},"pages/fultter/对齐与相对定位.html":{"url":"pages/fultter/对齐与相对定位.html","title":"对齐与相对定位","keywords":"","body":"对齐与相对定位 源码 对齐与相对定位 Align Align 组件可以调整子组件的位置，定义如下： Align({ Key key, this.alignment = Alignment.center, this.widthFactor, this.heightFactor, Widget child, }) alignment：需要一个AlignmentGeometry类型的值，表示子组件在父组件中的起始位置。 AlignmentGeometry 是一个抽象类，它有两个常用的子类：Alignment和 FractionalOffset。 widthFactor和heightFactor是用于确定Align 组件本身宽高的属性；它们是两个缩放因子，会分别乘以子元素的宽、高，最终的结果就是Align 组件的宽高。如果值为null，则组件的宽高将会占用尽可能多的空间。 示例 Container( height: 120.0, width: 120.0, color: Colors.blue.shade50, child: Align( alignment: Alignment.topRight, child: FlutterLogo( size: 60, ), ), ) 或 Align( widthFactor: 2, heightFactor: 2, alignment: Alignment.topRight, child: FlutterLogo( size: 60, ), ), 效果一样，FlutterLogo的宽高为 60，则Align的最终宽高都为2*60=120。 Alignment.topRight： //右上角 static const Alignment topRight = Alignment(1.0, -1.0); 可以看到它只是Alignment的一个实例。 Alignment Alignment继承自AlignmentGeometry，表示矩形内的一个点，他有两个属性x、y，分别表示在水平和垂直方向的偏移，Alignment定义如下： Alignment(this.x, this.y) Alignment Widget会以矩形的中心点作为坐标原点。 Alignment可以通过其坐标转换公式将其坐标转为子元素的具体偏移坐标： (Alignment.x*childWidth/2+childWidth/2, Alignment.y*childHeight/2+childHeight/2) FractionalOffset FractionalOffset 继承自 Alignment，它和 Alignment唯一的区别就是坐标原点不同！FractionalOffset 的坐标原点为矩形的左侧顶点，这和布局系统的一致。FractionalOffset的坐标转换公式为： 实际偏移 = (FractionalOffse.x * childWidth, FractionalOffse.y * childHeight) Center Center组件其实是对齐方式确定（Alignment.center）了的Align。 class Center extends Align { const Center({ Key? key, double widthFactor, double heightFactor, Widget? child }) : super(key: key, widthFactor: widthFactor, heightFactor: heightFactor, child: child); } "},"pages/fultter/LayoutBuilder.html":{"url":"pages/fultter/LayoutBuilder.html","title":"LayoutBuilder","keywords":"","body":"LayoutBuilder 源码 LayoutBuilder 通过 LayoutBuilder，我们可以在布局过程中拿到父组件传递的约束信息，然后我们可以根据约束信息动态的构建不同的布局。 示例 比如我们实现一个响应式的 Column 组件 ResponsiveColumn，它的功能是当当前可用的宽度小于 200 时，将子组件显示为一列，否则显示为两列。简单来实现一下： class ResponsiveColumn extends StatelessWidget { const ResponsiveColumn({Key? key, required this.children}) : super(key: key); final List children; @override Widget build(BuildContext context) { // 通过 LayoutBuilder 拿到父组件传递的约束，然后判断 maxWidth 是否小于200 return LayoutBuilder( builder: (BuildContext context, BoxConstraints constraints) { if (constraints.maxWidth []; for (var i = 0; i 可以发现 LayoutBuilder 的使用很简单，但是不要小看它，因为它非常实用且重要，它主要有两个使用场景： 可以使用 LayoutBuilder 来根据设备的尺寸来实现响应式布局。 LayoutBuilder 可以帮我们高效排查问题。比如我们在遇到布局问题或者想调试组件树中某一个节点布局的约束时 LayoutBuilder 就很有用。 打印布局时的约束信息 class LayoutLogPrint extends StatelessWidget { const LayoutLogPrint({ Key? key, this.tag, required this.child, }) : super(key: key); final Widget child; final T? tag; //指定日志tag @override Widget build(BuildContext context) { return LayoutBuilder(builder: (_, constraints) { // assert在编译release版本时会被去除 assert(() { print('${tag ?? key ?? child}: $constraints'); return true; }()); return child; }); } } 控制台输出： flutter: Text(\"xfsdjlfsx\"): BoxConstraints(0.0 注意！我们的大前提是盒模型布局，如果是Sliver 布局，可以使用 SliverLayoutBuiler 来打印。 AfterLayout "},"pages/fultter/Padding.html":{"url":"pages/fultter/Padding.html","title":"Padding","keywords":"","body":"Padding 源码 填充（Padding） Padding Padding可以给其子节点添加填充（留白），和边距效果类似。我们在前面很多示例中都已经使用过它了，现在来看看它的定义： Padding({ ... EdgeInsetsGeometry padding, Widget child, }) EdgeInsetsGeometry是一个抽象类，开发中，我们一般都使用EdgeInsets类，它是EdgeInsetsGeometry的一个子类，定义了一些设置填充的便捷方法。 EdgeInsets 我们看看EdgeInsets提供的便捷方法： fromLTRB(double left, double top, double right, double bottom)：分别指定四个方向的填充。 all(double value) : 所有方向均使用相同数值的填充。 only({left, top, right ,bottom })：可以设置具体某个方向的填充(可以同时指定多个方向)。 symmetric({ vertical, horizontal })：用于设置对称方向的填充，vertical指top和bottom，horizontal指left和right。 示例 下面的示例主要展示了EdgeInsets的不同用法，比较简单，源码如下： class PaddingTestRoute extends StatelessWidget { const PaddingTestRoute({Key? key}) : super(key: key); @override Widget build(BuildContext context) { return Padding( //上下左右各添加16像素补白 padding: const EdgeInsets.all(16), child: Column( //显式指定对齐方式为左对齐，排除对齐干扰 crossAxisAlignment: CrossAxisAlignment.start, mainAxisSize: MainAxisSize.min, children: const [ Padding( //左边添加8像素补白 padding: EdgeInsets.only(left: 8), child: Text(\"Hello world\"), ), Padding( //上下各添加8像素补白 padding: EdgeInsets.symmetric(vertical: 8), child: Text(\"I am Jack\"), ), Padding( // 分别指定四个方向的补白 padding: EdgeInsets.fromLTRB(20, 0, 20, 20), child: Text(\"Your friend\"), ) ], ), ); } } "},"pages/fultter/DecoratedBox.html":{"url":"pages/fultter/DecoratedBox.html","title":"DecoratedBox","keywords":"","body":"DecoratedBox 源码 装饰容器（DecoratedBox） DecoratedBox DecoratedBox可以在其子组件绘制前(或后)绘制一些装饰（Decoration），如背景、边框、渐变等。DecoratedBox定义如下： const DecoratedBox({ Decoration decoration, DecorationPosition position = DecorationPosition.background, Widget? child }) decoration：代表将要绘制的装饰，它的类型为Decoration。Decoration是一个抽象类，它定义了一个接口 createBoxPainter()，子类的主要职责是需要通过实现它来创建一个画笔，该画笔用于绘制装饰。 position：此属性决定在哪里绘制Decoration，它接收DecorationPosition的枚举类型，该枚举类有两个值： background：在子组件之后绘制，即背景装饰。 foreground：在子组件之上绘制，即前景。BoxDecoration 我们通常会直接使用BoxDecoration类，它是一个Decoration的子类，实现了常用的装饰元素的绘制。 BoxDecoration({ Color color, //颜色 DecorationImage image,//图片 BoxBorder border, //边框 BorderRadiusGeometry borderRadius, //圆角 List boxShadow, //阴影,可以指定多个 Gradient gradient, //渐变 BlendMode backgroundBlendMode, //背景混合模式 BoxShape shape = BoxShape.rectangle, //形状 }) 示例 DecoratedBox( decoration: BoxDecoration( gradient: LinearGradient(colors:[Colors.red,Colors.orange.shade700]), //背景渐变 borderRadius: BorderRadius.circular(3.0), //3像素圆角 boxShadow: [ //阴影 BoxShadow( color:Colors.black54, offset: Offset(2.0,2.0), blurRadius: 4.0 ) ] ), child: Padding( padding: EdgeInsets.symmetric(horizontal: 80.0, vertical: 18.0), child: Text(\"Login\", style: TextStyle(color: Colors.white),), ) ) LinearGradient、RadialGradient、SweepGradient "},"pages/fultter/Transform.html":{"url":"pages/fultter/Transform.html","title":"Transform","keywords":"","body":"Transform 源码 变换（Transform） Transform可以在其子组件绘制时对其应用一些矩阵变换来实现一些特效。Matrix4是一个4D矩阵，通过它我们可以实现各种矩阵操作，下面是一个例子： Container( color: Colors.black, child: Transform( alignment: Alignment.topRight, //相对于坐标系原点的对齐方式 transform: Matrix4.skewY(0.3), //沿Y轴倾斜0.3弧度 child: Container( padding: const EdgeInsets.all(8.0), color: Colors.deepOrange, child: const Text('Apartment for rent!'), ), ), ) 平移 Transform.translate接收一个offset参数，可以在绘制时沿x、y轴对子组件平移指定的距离。 DecoratedBox( decoration:BoxDecoration(color: Colors.red), //默认原点为左上角，左移20像素，向上平移5像素 child: Transform.translate( offset: Offset(-20.0, -5.0), child: Text(\"Hello world\"), ), ) 旋转 DecoratedBox( decoration:BoxDecoration(color: Colors.red), child: Transform.rotate( //旋转90度 angle:math.pi/2 , child: Text(\"Hello world\"), ), ) 注意：要使用math.pi需先进行如下导包。 import 'dart:math' as math; 缩放 Transform.scale可以对子组件进行缩小或放大，如： DecoratedBox( decoration:BoxDecoration(color: Colors.red), child: Transform.scale( scale: 1.5, //放大到1.5倍 child: Text(\"Hello world\") ) ); Transform 注意事项 Transform的变换是应用在绘制阶段，而并不是应用在布局(layout)阶段，所以无论对子组件应用何种变化，其占用空间的大小和在屏幕上的位置都是固定不变的，因为这些是在布局阶段就确定的。下面我们具体说明： Row( mainAxisAlignment: MainAxisAlignment.center, children: [ DecoratedBox( decoration:BoxDecoration(color: Colors.red), child: Transform.scale(scale: 1.5, child: Text(\"Hello world\") ) ), Text(\"你好\", style: TextStyle(color: Colors.green, fontSize: 18.0),) ], ) RotatedBox RotatedBox和Transform.rotate功能相似，它们都可以对子组件进行旋转变换，但是有一点不同：RotatedBox的变换是在layout阶段，会影响在子组件的位置和大小。我们将上面介绍Transform.rotate时的示例改一下： Row( mainAxisAlignment: MainAxisAlignment.center, children: [ DecoratedBox( decoration: BoxDecoration(color: Colors.red), //将Transform.rotate换成RotatedBox child: RotatedBox( quarterTurns: 1, //旋转90度(1/4圈) child: Text(\"Hello world\"), ), ), Text(\"你好\", style: TextStyle(color: Colors.green, fontSize: 18.0),) ], ), "},"pages/fultter/Container.html":{"url":"pages/fultter/Container.html","title":"Container","keywords":"","body":"Container 源码 容器组件（Container） Container是一个组合类容器，它本身不对应具体的RenderObject，它是DecoratedBox、ConstrainedBox、Transform、Padding、Align等组件组合的一个多功能容器，所以我们只需通过一个Container组件可以实现同时需要装饰、变换、限制的场景。下面是Container的定义： Container({ this.alignment, this.padding, //容器内补白，属于decoration的装饰范围 Color color, // 背景色 Decoration decoration, // 背景装饰 Decoration foregroundDecoration, //前景装饰 double width,//容器的宽度 double height, //容器的高度 BoxConstraints constraints, //容器大小的限制条件 this.margin,//容器外补白，不属于decoration的装饰范围 this.transform, //变换 this.child, ... }) 容器的大小可以通过width、height属性来指定，也可以通过constraints来指定；如果它们同时存在时，width、height优先。实际上Container内部会根据width、height来生成一个constraints。 color和decoration是互斥的，如果同时设置它们则会报错！实际上，当指定color时，Container内会自动创建一个decoration。 示例 Container( margin: EdgeInsets.only(top: 50.0, left: 120.0), constraints: BoxConstraints.tightFor(width: 200.0, height: 150.0),//卡片大小 decoration: BoxDecoration( //背景装饰 gradient: RadialGradient( //背景径向渐变 colors: [Colors.red, Colors.orange], center: Alignment.topLeft, radius: .98, ), boxShadow: [ //卡片阴影 BoxShadow( color: Colors.black54, offset: Offset(2.0, 2.0), blurRadius: 4.0, ) ], ), transform: Matrix4.rotationZ(.2),//卡片倾斜变换 alignment: Alignment.center, //卡片内文字居中 child: Text( //卡片文字 \"5.20\", style: TextStyle(color: Colors.white, fontSize: 40.0), ), ) Padding和Margin ... Container( margin: EdgeInsets.all(20.0), //容器外补白 color: Colors.orange, child: Text(\"Hello world!\"), ), Container( padding: EdgeInsets.all(20.0), //容器内补白 color: Colors.orange, child: Text(\"Hello world!\"), ), ... 等价于： ... Padding( padding: EdgeInsets.all(20.0), child: DecoratedBox( decoration: BoxDecoration(color: Colors.orange), child: Text(\"Hello world!\"), ), ), DecoratedBox( decoration: BoxDecoration(color: Colors.orange), child: Padding( padding: const EdgeInsets.all(20.0), child: Text(\"Hello world!\"), ), ), ... "},"pages/fultter/Clip.html":{"url":"pages/fultter/Clip.html","title":"Clip","keywords":"","body":"Clip 源码 剪裁（Clip） 剪裁类组件 剪裁Widget 默认行为 ClipOval 子组件为正方形时剪裁成内贴圆形；为矩形时，剪裁成内贴椭圆 ClipRRect 将子组件剪裁为圆角矩形 ClipRect 默认剪裁掉子组件布局空间之外的绘制内容（溢出部分剪裁） ClipPath 按照自定义的路径剪裁 示例 import 'package:flutter/material.dart'; class ClipTestRoute extends StatelessWidget { @override Widget build(BuildContext context) { // 头像 Widget avatar = Image.asset(\"imgs/avatar.png\", width: 60.0); return Center( child: Column( children: [ avatar, //不剪裁 ClipOval(child: avatar), //剪裁为圆形 ClipRRect( //剪裁为圆角矩形 borderRadius: BorderRadius.circular(5.0), child: avatar, ), Row( mainAxisAlignment: MainAxisAlignment.center, children: [ Align( alignment: Alignment.topLeft, widthFactor: .5,//宽度设为原来宽度一半，另一半会溢出 child: avatar, ), Text(\"你好世界\", style: TextStyle(color: Colors.green),) ], ), Row( mainAxisAlignment: MainAxisAlignment.center, children: [ ClipRect(//将溢出部分剪裁 child: Align( alignment: Alignment.topLeft, widthFactor: .5,//宽度设为原来宽度一半 child: avatar, ), ), Text(\"你好世界\",style: TextStyle(color: Colors.green)) ], ), ], ), ); } } 自定义裁剪（CustomClipper） 首先，自定义一个CustomClipper： class MyClipper extends CustomClipper { @override Rect getClip(Size size) => Rect.fromLTWH(10.0, 15.0, 40.0, 30.0); @override bool shouldReclip(CustomClipper oldClipper) => false; } getClip()是用于获取剪裁区域的接口，由于图片大小是60×60，我们返回剪裁区域为Rect.fromLTWH(10.0, 15.0, 40.0, 30.0)，即图片中部40×30像素的范围。 shouldReclip() 接口决定是否重新剪裁。如果在应用中，剪裁区域始终不会发生变化时应该返回false，这样就不会触发重新剪裁，避免不必要的性能开销。如果剪裁区域会发生变化（比如在对剪裁区域执行一个动画），那么变化后应该返回true来重新执行剪裁。DecoratedBox( decoration: BoxDecoration( color: Colors.red ), child: ClipRect( clipper: MyClipper(), //使用自定义的clipper child: avatar ), ) ClipPath 可以按照自定义的路径实现剪裁，它需要自定义一个CustomClipper 类型的 Clipper，定义方式和 MyClipper 类似，只不过 getClip 需要返回一个 Path。 "},"pages/fultter/FittedBox.html":{"url":"pages/fultter/FittedBox.html","title":"FittedBox","keywords":"","body":"FittedBox 源码 空间适配（FittedBox） FittedBox const FittedBox({ Key? key, this.fit = BoxFit.contain, // 适配方式 this.alignment = Alignment.center, //对齐方式 this.clipBehavior = Clip.none, //是否剪裁 Widget? child, }) 适配原理 FittedBox允许子组件无限大(0 FittedBox 对子组件布局结束后就可以获得子组件真实的大小。 FittedBox 知道子组件的真实大小也知道他父组件的约束，那么FittedBox 就可以通过指定的适配方式（BoxFit 枚举中指定），让起子组件在 FittedBox 父组件的约束范围内按照指定的方式显示。 示例 Widget build(BuildContext context) { return Center( child: Column( children: [ wContainer(BoxFit.none), Text('Wendux'), wContainer(BoxFit.contain), Text('Flutter中国'), ], ), ); } Widget wContainer(BoxFit boxFit) { return Container( width: 50, height: 50, color: Colors.red, child: FittedBox( fit: boxFit, // 子容器超过父容器大小 child: Container(width: 60, height: 70, color: Colors.blue), ), ); } 单行缩放布局 @override Widget build(BuildContext context) { return Center( child: Column( children: [ wRow(' 90000000000000000 '), FittedBox(child: wRow(' 90000000000000000 ')), wRow(' 800 '), FittedBox(child: wRow(' 800 ')), ] .map((e) => Padding( padding: EdgeInsets.symmetric(vertical: 20), child: e, )) .toList();, ), ); } // 直接使用Row Widget wRow(String text) { Widget child = Text(text); child = Row( mainAxisAlignment: MainAxisAlignment.spaceEvenly, children: [child, child, child], ); return child; } "},"pages/fultter/Scaffold.html":{"url":"pages/fultter/Scaffold.html","title":"Scaffold","keywords":"","body":"Scaffold 源码 页面骨架（Scaffold） Scaffold 是一个路由页的骨架，我们使用它可以很容易地拼装出一个完整的页面。 示例 我们实现一个页面，它包含： 一个导航栏 导航栏右边有一个分享按钮 有一个抽屉菜单 有一个底部导航 右下角有一个悬浮的动作按钮 class ScaffoldRoute extends StatefulWidget { @override _ScaffoldRouteState createState() => _ScaffoldRouteState(); } class _ScaffoldRouteState extends State { int _selectedIndex = 1; @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( //导航栏 title: Text(\"App Name\"), actions: [ //导航栏右侧菜单 IconButton(icon: Icon(Icons.share), onPressed: () {}), ], ), drawer: MyDrawer(), //抽屉 bottomNavigationBar: BottomNavigationBar( // 底部导航 items: [ BottomNavigationBarItem(icon: Icon(Icons.home), title: Text('Home')), BottomNavigationBarItem(icon: Icon(Icons.business), title: Text('Business')), BottomNavigationBarItem(icon: Icon(Icons.school), title: Text('School')), ], currentIndex: _selectedIndex, fixedColor: Colors.blue, onTap: _onItemTapped, ), floatingActionButton: FloatingActionButton( //悬浮按钮 child: Icon(Icons.add), onPressed:_onAdd ), ); } void _onItemTapped(int index) { setState(() { _selectedIndex = index; }); } void _onAdd(){ } } 组件名称 解释 AppBar 一个导航栏骨架 MyDrawer 抽屉菜单 BottomNavigationBar 底部导航栏 FloatingActionButton 漂浮按钮 AppBar AppBar是一个Material风格的导航栏，通过它可以设置导航栏标题、导航栏菜单、导航栏底部的Tab标题等。下面我们看看AppBar的定义： AppBar({ Key? key, this.leading, //导航栏最左侧Widget，常见为抽屉菜单按钮或返回按钮。 this.automaticallyImplyLeading = true, //如果leading为null，是否自动实现默认的leading按钮 this.title,// 页面标题 this.actions, // 导航栏右侧菜单 this.bottom, // 导航栏底部菜单，通常为Tab按钮组 this.elevation = 4.0, // 导航栏阴影 this.centerTitle, //标题是否居中 this.backgroundColor, ... //其他属性见源码注释 }) 抽屉菜单Drawer class MyDrawer extends StatelessWidget { const MyDrawer({ Key? key, }) : super(key: key); @override Widget build(BuildContext context) { return Drawer( child: MediaQuery.removePadding( context: context, //移除抽屉菜单顶部默认留白 removeTop: true, child: Column( crossAxisAlignment: CrossAxisAlignment.start, children: [ Padding( padding: const EdgeInsets.only(top: 38.0), child: Row( children: [ Padding( padding: const EdgeInsets.symmetric(horizontal: 16.0), child: ClipOval( child: Image.asset( \"imgs/avatar.png\", width: 80, ), ), ), Text( \"Wendux\", style: TextStyle(fontWeight: FontWeight.bold), ) ], ), ), Expanded( child: ListView( children: [ ListTile( leading: const Icon(Icons.add), title: const Text('Add account'), ), ListTile( leading: const Icon(Icons.settings), title: const Text('Manage accounts'), ), ], ), ), ], ), ), ); } } "},"pages/fultter/可滚动组件简介.html":{"url":"pages/fultter/可滚动组件简介.html","title":"可滚动组件简介","keywords":"","body":"可滚动组件简介 可滚动组件简介CustomScrollView和NestedScrollView的详细介绍在Flutter中创建有意思的滚动效果 - Sliver系列 Sliver布局模型 1. 布局模型 基于 RenderBox 的盒模型布局。 基于 Sliver ( RenderSliver ) 按需加载列表布局。 2. 主要作用 Sliver 可以包含一个或多个子组件。加载子组件并确定每一个子组件的布局和绘制信息，实现按需加载模型。 3. 三个角色 Scrollable ：用于处理滑动手势，确定滑动偏移，滑动偏移变化时构建 Viewport 。 Viewport：显示的视窗，即列表的可视区域； Sliver：视窗里显示的元素。 4. 角色关系 三者所占用的空间重合。 Sliver 父组件为 Viewport，Viewport的 父组件为 Scrollable。 5. 布局过程 Scrollable 监听到用户滑动行为后，根据最新的滑动偏移构建 Viewport 。 Viewport 将当前视口信息和配置信息通过 SliverConstraints 传递给 Sliver。 Sliver 中对子组件（RenderBox）按需进行构建和布局，然后确认自身的位置、绘制等信息，保存在 geometry 中（一个 SliverGeometry 类型的对象）。 6. cacheExtent 预渲染的高度，在可视区域之外，如果 RenderBox 进入这个区域内，即使它还未显示在屏幕上，也是要先进行构建，默认值是 250。 Scrollable 用于处理滑动手势，确定滑动偏移，滑动偏移变化时构建 Viewport，我们看一下其关键的属性： Scrollable({ ... this.axisDirection = AxisDirection.down,//滚动方向。 this.controller, this.physics, required this.viewportBuilder, //后面介绍 }) 1. physics ScrollPhysics类型的对象，响应用户操作： 用户滑动完抬起手指后，继续执行动画。 滑动到边界时，如何显示。 ScrollPhysics的子类： ClampingScrollPhysics：列表滑动到边界时将不能继续滑动，通常在Android 中 配合 GlowingOverscrollIndicator（实现微光效果的组件） 使用。 BouncingScrollPhysics：iOS 下弹性效果。 2. controller ScrollController对象，默认的PrimaryScrollController，控制滚动位置和监听滚动事件。 3. viewportBuilder 构建 Viewport 的回调。当用户滑动时，Scrollable 会调用此回调构建新的 Viewport，同时传递一个 ViewportOffset 类型的 offset 参数，该参数描述 Viewport 应该显示那一部分内容。 Viewport 显示 Sliver。 Viewport({ Key? key, this.axisDirection = AxisDirection.down, this.crossAxisDirection, this.anchor = 0.0, required ViewportOffset offset, // 用户的滚动偏移 // 类型为Key，表示从什么地方开始绘制，默认是第一个元素 this.center, this.cacheExtent, // 预渲染区域 //该参数用于配合解释cacheExtent的含义，也可以为主轴长度的乘数 this.cacheExtentStyle = CacheExtentStyle.pixel, this.clipBehavior = Clip.hardEdge, List slivers = const [], // 需要显示的 Sliver 列表 }) 1. offset 该参数为Scrollable 构建 Viewport 时传入，它描述了 Viewport 应该显示那一部分内容。 2. cacheExtent 和 cacheExtentStyle CacheExtentStyle 是一个枚举，有 pixel 和 viewport 两个取值。 当 cacheExtentStyle 值为 pixel 时，cacheExtent 的值为预渲染区域的具体像素长度； 当值为 viewport 时，cacheExtent 的值是一个乘数，表示有几个 viewport 的长度，最终的预渲染区域的像素长度为：cacheExtent * viewport 的积。 Sliver 1. 主要作用 对子组件进行构建和布局，比如 ListView 的 Sliver 需要实现子组件（列表项）按需加载功能，只有当列表项进入预渲染区域时才会去对它进行构建和布局、渲染。 2. RenderSliver Sliver 对应的渲染对象类型是 RenderSliver。 RenderSliver 和 RenderBox 都继承自 RenderObject 类。 RenderSliver 和 RenderBox 约束信息分别是 BoxConstraints 和 SliverConstraints。 通用配置 scrollDirection（滑动的主轴）、reverse（滑动方向是否反向）、controller、physics 、cacheExtent ，这些属性最终会透传给对应的 Scrollable 和 Viewport，这些属性我们可以认为是可滚动组件的通用属性。 reverse表示是否按照阅读方向相反的方向滑动，如：scrollDirection值为Axis.horizontal 时，即滑动发现为水平，如果阅读方向是从左到右（取决于语言环境，阿拉伯语就是从右到左）。reverse为true时，那么滑动方向就是从右往左。 "},"pages/fultter/SingleChildScrollView.html":{"url":"pages/fultter/SingleChildScrollView.html","title":"SingleChildScrollView","keywords":"","body":"SingleChildScrollView 源码 SingleChildScrollView 简介 SingleChildScrollView类似于Android中的ScrollView，它只能接收一个子组件，定义如下： SingleChildScrollView({ this.scrollDirection = Axis.vertical, //滚动方向，默认是垂直方向 this.reverse = false, this.padding, bool primary, this.physics, this.controller, this.child, }) 示例 下面是一个将大写字母 A-Z 沿垂直方向显示的例子，由于垂直方向空间会超过屏幕视口高度，所以我们使用SingleChildScrollView： class SingleChildScrollViewTestRoute extends StatelessWidget { @override Widget build(BuildContext context) { String str = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"; return Scrollbar( // 显示进度条 child: SingleChildScrollView( padding: EdgeInsets.all(16.0), child: Center( child: Column( //动态创建一个List children: str.split(\"\") //每一个字母都用一个Text显示,字体为原来的两倍 .map((c) => Text(c, textScaleFactor: 2.0,)) .toList(), ), ), ), ); } } "},"pages/fultter/ListView.html":{"url":"pages/fultter/ListView.html","title":"ListView","keywords":"","body":"ListView 源码 ListView ListView是最常用的可滚动组件之一，它可以沿一个方向线性排布所有子组件，并且它也支持列表项懒加载（在需要时才会创建）。 默认构造函数 ListView({ ... //可滚动widget公共参数 Axis scrollDirection = Axis.vertical, bool reverse = false, ScrollController? controller, bool? primary, ScrollPhysics? physics, EdgeInsetsGeometry? padding, //ListView各个构造函数的共同参数 double? itemExtent, Widget? prototypeItem, //列表项原型，后面解释 bool shrinkWrap = false, bool addAutomaticKeepAlives = true, bool addRepaintBoundaries = true, double? cacheExtent, // 预渲染区域长度 //子widget列表 List children = const [], }) itemExtent：列表项高度。 prototypeItem：指定一个列表项，用于计算列表项高度。 shrinkWrap：列表高度是否是子组件总长度。 示例： ListView( shrinkWrap: true, padding: const EdgeInsets.all(20.0), children: [ const Text('I\\'m dedicating every day to you'), const Text('Domestic life was never quite my style'), const Text('When you smile, you knock me out, I fall apart'), const Text('And I thought I was so smart'), ], ); ListView.builder ListView.builder适合列表项比较多或者列表项不确定的情况，下面看一下ListView.builder的核心参数列表： ListView.builder({ // ListView公共参数已省略 ... required IndexedWidgetBuilder itemBuilder, int itemCount, ... }) itemBuilder：根据index参数动态构建列表项。 itemCount：列表项的数量，如果为null，则为无限列表。 下面看一个例子： ListView.builder( itemCount: 100, itemExtent: 50.0, //强制高度为50.0 itemBuilder: (BuildContext context, int index) { return ListTile(title: Text(\"$index\")); } ); ListView.separated ListView.separated可以在生成的列表项之间添加一个分割组件，它比ListView.builder多了一个separatorBuilder参数，该参数是一个分割组件生成器。 下面我们看一个例子：奇数行添加一条蓝色下划线，偶数行添加一条绿色下划线。 class ListView3 extends StatelessWidget { @override Widget build(BuildContext context) { //下划线widget预定义以供复用。 Widget divider1=Divider(color: Colors.blue,); Widget divider2=Divider(color: Colors.green); return ListView.separated( itemCount: 100, //列表项构造器 itemBuilder: (BuildContext context, int index) { return ListTile(title: Text(\"$index\")); }, //分割器构造器 separatorBuilder: (BuildContext context, int index) { return index%2==0?divider1:divider2; }, ); } } "},"pages/fultter/滚动监听及控制.html":{"url":"pages/fultter/滚动监听及控制.html","title":"滚动监听及控制","keywords":"","body":"滚动监听及控制 源码 滚动监听及控制 ScrollController ScrollController构造函数如下： ScrollController({ double initialScrollOffset = 0.0, //初始滚动位置 this.keepScrollOffset = true,//是否保存滚动位置 ... }) offset：可滚动组件当前的滚动位置。 jumpTo(double offset)、animateTo(double offset,...)：这两个方法用于跳转到指定的位置，它们不同之处在于，后者在跳转时会执行一个动画，而前者不会。 1. 滚动监听 ScrollController间接继承自Listenable，我们可以根据ScrollController来监听滚动事件，如： controller.addListener(()=>print(controller.offset)) 2. 实例 我们创建一个ListView，当滚动位置发生变化时，我们先打印出当前滚动位置，然后判断当前位置是否超过1000像素，如果超过则在屏幕右下角显示一个“返回顶部”的按钮，该按钮点击后可以使ListView恢复到初始位置；如果没有超过1000像素，则隐藏“返回顶部”按钮。代码如下： class ScrollControllerTestRoute extends StatefulWidget { @override ScrollControllerTestRouteState createState() { return ScrollControllerTestRouteState(); } } class ScrollControllerTestRouteState extends State { ScrollController _controller = ScrollController(); bool showToTopBtn = false; //是否显示“返回到顶部”按钮 @override void initState() { super.initState(); //监听滚动事件，打印滚动位置 _controller.addListener(() { print(_controller.offset); //打印滚动位置 if (_controller.offset = 1000 && showToTopBtn == false) { setState(() { showToTopBtn = true; }); } }); } @override void dispose() { //为了避免内存泄露，需要调用_controller.dispose _controller.dispose(); super.dispose(); } @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar(title: Text(\"滚动控制\")), body: Scrollbar( child: ListView.builder( itemCount: 100, itemExtent: 50.0, //列表项高度固定时，显式指定高度是一个好习惯(性能消耗小) controller: _controller, itemBuilder: (context, index) { return ListTile(title: Text(\"$index\"),); } ), ), floatingActionButton: !showToTopBtn ? null : FloatingActionButton( child: Icon(Icons.arrow_upward), onPressed: () { //返回到顶部时执行动画 _controller.animateTo( .0, duration: Duration(milliseconds: 200), curve: Curves.ease, ); } ), ); } } 3. 滚动位置恢复 PageStorage是一个用于保存页面(路由)相关数据的组件，它并不会影响子树的UI外观，其实，PageStorage是一个功能型组件，它拥有一个存储桶（bucket），子树中的Widget可以通过指定不同的PageStorageKey来存储各自的数据或状态。 使用PageStorage在页面切换时保存状态 4. ScrollPosition ScrollPosition是用来保存可滚动组件的滚动位置的。一个ScrollController对象可以同时被多个可滚动组件使用，ScrollController会为每一个可滚动组件创建一个ScrollPosition对象，这些ScrollPosition保存在ScrollController的positions属性中（List）。ScrollPosition是真正保存滑动位置信息的对象，offset只是一个便捷属性： double get offset => position.pixels; 5. ScrollController控制原理 滚动监听 1. 滚动通知 Flutter Widget树中子Widget可以通过发送通知（Notification）与父(包括祖先)Widget通信。 2. 实例 import 'package:flutter/material.dart'; class ScrollNotificationTestRoute extends StatefulWidget { @override _ScrollNotificationTestRouteState createState() => _ScrollNotificationTestRouteState(); } class _ScrollNotificationTestRouteState extends State { String _progress = \"0%\"; //保存进度百分比 @override Widget build(BuildContext context) { return Scrollbar( //进度条 // 监听滚动通知 child: NotificationListener( onNotification: (ScrollNotification notification) { double progress = notification.metrics.pixels / notification.metrics.maxScrollExtent; //重新构建 setState(() { _progress = \"${(progress * 100).toInt()}%\"; }); print(\"BottomEdge: ${notification.metrics.extentAfter == 0}\"); return false; //return true; //放开此行注释后，进度条将失效 }, child: Stack( alignment: Alignment.center, children: [ ListView.builder( itemCount: 100, itemExtent: 50.0, itemBuilder: (context, index) => ListTile(title: Text(\"$index\")), ), CircleAvatar( //显示进度百分比 radius: 30.0, child: Text(_progress), backgroundColor: Colors.black54, ) ], ), ), ); } } "},"pages/fultter/AnimatedList.html":{"url":"pages/fultter/AnimatedList.html","title":"AnimatedList","keywords":"","body":"AnimatedList 源码 AnimatedList AnimatedList 和 ListView 的功能大体相似，不同的是， AnimatedList 可以在列表中插入或删除节点时执行一个动画，在需要添加或删除列表项的场景中会提高用户体验。 AnimatedList 是一个 StatefulWidget，它对应的 State 类型为 AnimatedListState，添加和删除元素的方法位于 AnimatedListState 中： void insertItem(int index, { Duration duration = _kDuration }); void removeItem(int index, AnimatedListRemovedItemBuilder builder, { Duration duration = _kDuration }) ; 下面我们看一个示例：实现下面这样的一个列表，点击底部 + 按钮时向列表追加一个列表项；点击每个列表项后面的删除按钮时，删除该列表项，添加和删除时分别执行指定的动画： 初始的时候有5个列表项，先点击了 + 号按钮，会添加一个 6，添加过程执行渐显动画。然后点击了 4 后面的删除按钮，删除的时候执行了一个渐隐+收缩的合成动画。 下面是实现代码： class AnimatedListRoute extends StatefulWidget { const AnimatedListRoute({Key? key}) : super(key: key); @override _AnimatedListRouteState createState() => _AnimatedListRouteState(); } class _AnimatedListRouteState extends State { var data = []; int counter = 5; final globalKey = GlobalKey(); @override void initState() { for (var i = 0; i animation, ) { //添加列表项时会执行渐显动画 return FadeTransition( opacity: animation, child: buildItem(context, index), ); }, ), buildAddBtn(), ], ); } // 创建一个 “+” 按钮，点击后会向列表中插入一项 Widget buildAddBtn() { return Positioned( child: FloatingActionButton( child: Icon(Icons.add), onPressed: () { // 添加一个列表项 data.add('${++counter}'); // 告诉列表项有新添加的列表项 globalKey.currentState!.insertItem(data.length - 1); print('添加 $counter'); }, ), bottom: 30, left: 0, right: 0, ); } // 构建列表项 Widget buildItem(context, index) { String char = data[index]; return ListTile( //数字不会重复，所以作为Key key: ValueKey(char), title: Text(char), trailing: IconButton( icon: Icon(Icons.delete), // 点击时删除 onPressed: () => onDelete(context, index), ), ); } void onDelete(context, index) { // 待实现 } } 删除的时候需要我们通过AnimatedListState 的 removeItem 方法来应用删除动画，具体逻辑在 onDelete 中： setState(() { globalKey.currentState!.removeItem( index, (context, animation) { // 删除过程执行的是反向动画，animation.value 会从1变为0 var item = buildItem(context, index); print('删除 ${data[index]}'); data.removeAt(index); // 删除动画是一个合成动画：渐隐 + 缩小列表项告诉 return FadeTransition( opacity: CurvedAnimation( parent: animation, //让透明度变化的更快一些 curve: const Interval(0.5, 1.0), ), // 不断缩小列表项的高度 child: SizeTransition( sizeFactor: animation, axisAlignment: 0.0, child: item, ), ); }, duration: Duration(milliseconds: 200), // 动画时间为 200 ms ); }); 代码很简单，但我们需要注意，我们的数据是单独在 data 中维护的，调用 AnimatedListState 的插入和移除方法知识相当于一个通知：在什么位置执行插入或移除动画，仍然是数据驱动的（响应式并非命令式）。 "},"pages/fultter/GridView.html":{"url":"pages/fultter/GridView.html","title":"GridView","keywords":"","body":"GridView 源码 GridView SliverGridDelegate SliverGridDelegate是一个抽象类，定义了GridView Layout相关接口，子类需要通过实现它们来实现具体的布局算法。Flutter中提供了两个SliverGridDelegate的子类SliverGridDelegateWithFixedCrossAxisCount和SliverGridDelegateWithMaxCrossAxisExtent。 1. SliverGridDelegateWithFixedCrossAxisCount 通过设置横轴子元素的数量和比例的layout算法。 SliverGridDelegateWithFixedCrossAxisCount({ @required double crossAxisCount, double mainAxisSpacing = 0.0, double crossAxisSpacing = 0.0, double childAspectRatio = 1.0, }) 2. SliverGridDelegateWithMaxCrossAxisExtent 设置子元素最大长度的layout算法。 SliverGridDelegateWithMaxCrossAxisExtent({ double maxCrossAxisExtent, double mainAxisSpacing = 0.0, double crossAxisSpacing = 0.0, double childAspectRatio = 1.0, }) GridView.count GridView.count构造函数内部使用了SliverGridDelegateWithFixedCrossAxisCount。 GridView.count( crossAxisCount: 3, childAspectRatio: 1.0, children: [ Icon(Icons.ac_unit), Icon(Icons.airport_shuttle), Icon(Icons.all_inclusive), Icon(Icons.beach_access), Icon(Icons.cake), Icon(Icons.free_breakfast), ], ); GridView.extent GridView.extent构造函数内部使用了SliverGridDelegateWithMaxCrossAxisExtent。 GridView.extent( maxCrossAxisExtent: 120.0, childAspectRatio: 2.0, children: [ Icon(Icons.ac_unit), Icon(Icons.airport_shuttle), Icon(Icons.all_inclusive), Icon(Icons.beach_access), Icon(Icons.cake), Icon(Icons.free_breakfast), ], ); GridView.builder 当子widget比较多时，我们可以通过GridView.builder来动态创建子widget。GridView.builder 必须指定的参数有两个： GridView.builder( ... required SliverGridDelegate gridDelegate, required IndexedWidgetBuilder itemBuilder, ) 示例 假设我们需要从一个异步数据源（如网络）分批获取一些Icon，然后用GridView来展示： class InfiniteGridView extends StatefulWidget { @override _InfiniteGridViewState createState() => _InfiniteGridViewState(); } class _InfiniteGridViewState extends State { List _icons = []; //保存Icon数据 @override void initState() { super.initState(); // 初始化数据 _retrieveIcons(); } @override Widget build(BuildContext context) { return GridView.builder( gridDelegate: SliverGridDelegateWithFixedCrossAxisCount( crossAxisCount: 3, //每行三列 childAspectRatio: 1.0, //显示区域宽高相等 ), itemCount: _icons.length, itemBuilder: (context, index) { //如果显示到最后一个并且Icon总数小于200时继续获取数据 if (index == _icons.length - 1 && _icons.length "},"pages/fultter/PageView.html":{"url":"pages/fultter/PageView.html","title":"PageView","keywords":"","body":"PageView 源码 PageView与页面缓存 构造方法 PageView({ Key? key, this.scrollDirection = Axis.horizontal, // 滑动方向 this.reverse = false, PageController? controller, this.physics, List children = const [], this.onPageChanged, //每次滑动是否强制切换整个页面，如果为false，则会根据实际的滑动距离显示页面 this.pageSnapping = true, //主要是配合辅助功能用的，后面解释 this.allowImplicitScrolling = false, //后面解释 this.padEnds = true, }) 示例 我们看一个 Tab 切换的实例，为了突出重点，我们让每个 Tab 页都只显示一个数字。 // Tab 页面 class Page extends StatefulWidget { const Page({ Key? key, required this.text }) : super(key: key); final String text; @override _PageState createState() => _PageState(); } class _PageState extends State { @override Widget build(BuildContext context) { print(\"build ${widget.text}\"); return Center(child: Text(\"${widget.text}\", textScaleFactor: 5)); } } 我们创建一个 PageView： @override Widget build(BuildContext context) { var children = []; // 生成 6 个 Tab 页 for (int i = 0; i 页面缓存 allowImplicitScrolling设置为true可以缓存前后各一页数据。 "},"pages/fultter/KeepAlive.html":{"url":"pages/fultter/KeepAlive.html","title":"KeepAlive","keywords":"","body":"KeepAlive 源码 KeepAlive AutomaticKeepAlive详解 在列表项Widget的State中混入AutomaticKeepAliveClientMixin。 覆写wantKeepAlive返回true。 在build()内调用super.build(context)。 "},"pages/fultter/TabbarView.html":{"url":"pages/fultter/TabbarView.html","title":"TabbarView","keywords":"","body":"TabbarView 源码 TabbarView TabBarView TabBarView 封装了 PageView，它的构造方法很简单 TabBarView({ Key? key, required this.children, // tab 页 this.controller, // TabController this.physics, this.dragStartBehavior = DragStartBehavior.start, }) TabController 用于监听和控制 TabBarView 的页面切换，通常和 TabBar 联动。如果没有指定，则会在组件树中向上查找并使用最近的一个 DefaultTabController 。 TabBar TabBar 有很多配置参数，通过这些参数我们可以定义 TabBar 的样式，很多属性都是在配置 indicator 和 label，拿上图来举例，Label 是每个Tab 的文本，indicator 指 “历史” 下面的白色下划线。 const TabBar({ Key? key, required this.tabs, // 具体的 Tabs，需要我们创建 this.controller, this.isScrollable = false, // 是否可以滑动 this.padding, this.indicatorColor,// 指示器颜色，默认是高度为2的一条下划线 this.automaticIndicatorColorAdjustment = true, this.indicatorWeight = 2.0,// 指示器高度 this.indicatorPadding = EdgeInsets.zero, //指示器padding this.indicator, // 指示器 this.indicatorSize, // 指示器长度，有两个可选值，一个tab的长度，一个是label长度 this.labelColor, this.labelStyle, this.labelPadding, this.unselectedLabelColor, this.unselectedLabelStyle, this.mouseCursor, this.onTap, ... }) TabBar 通常位于 AppBar 的底部，它也可以接收一个 TabController ，如果需要和 TabBarView 联动， TabBar 和 TabBarView 使用同一个 TabController 即可，注意，联动时 TabBar 和 TabBarView 的孩子数量需要一致。如果没有指定 controller，则会在组件树中向上查找并使用最近的一个 DefaultTabController 。另外我们需要创建需要的 tab 并通过 tabs 传给 TabBar， tab 可以是任何 Widget，不过Material 组件库中已经实现了一个 Tab 组件，我们一般都会直接使用它： const Tab({ Key? key, this.text, //文本 this.icon, // 图标 this.iconMargin = const EdgeInsets.only(bottom: 10.0), this.height, this.child, // 自定义 widget }) 注意，text 和 child 是互斥的，不能同时制定。 实例 class TabViewRoute1 extends StatefulWidget { @override _TabViewRoute1State createState() => _TabViewRoute1State(); } class _TabViewRoute1State extends State with SingleTickerProviderStateMixin { late TabController _tabController; List tabs = [\"新闻\", \"历史\", \"图片\"]; @override void initState() { super.initState(); _tabController = TabController(length: tabs.length, vsync: this); } @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: Text(\"App Name\"), bottom: TabBar( controller: _tabController, tabs: tabs.map((e) => Tab(text: e)).toList(), ), ), body: TabBarView( //构建 controller: _tabController, children: tabs.map((e) { return KeepAliveWrapper( child: Container( alignment: Alignment.center, child: Text(e, textScaleFactor: 5), ), ); }).toList(), ), ); } @override void dispose() { // 释放资源 _tabController.dispose(); super.dispose(); } } 滑动页面时顶部的 Tab 也会跟着动，点击顶部 Tab 时页面也会跟着切换。为了实现 TabBar 和 TabBarView 的联动，我们显式创建了一个 TabController，由于 TabController 又需要一个 TickerProvider （vsync 参数）， 我们又混入了 SingleTickerProviderStateMixin；由于 TabController 中会执行动画，持有一些资源，所以我们在页面销毁时必须得释放资源（dispose）。综上，我们发现创建 TabController 的过程还是比较复杂，实战中，如果需要 TabBar 和 TabBarView 联动，通常会创建一个 DefaultTabController 作为它们共同的父级组件，这样它们在执行时就会从组件树向上查找，都会使用我们指定的这个 DefaultTabController。我们修改后的实现如下： class TabViewRoute2 extends StatelessWidget { @override Widget build(BuildContext context) { List tabs = [\"新闻\", \"历史\", \"图片\"]; return DefaultTabController( length: tabs.length, child: Scaffold( appBar: AppBar( title: Text(\"App Name\"), bottom: TabBar( tabs: tabs.map((e) => Tab(text: e)).toList(), ), ), body: TabBarView( //构建 children: tabs.map((e) { return KeepAliveWrapper( child: Container( alignment: Alignment.center, child: Text(e, textScaleFactor: 5), ), ); }).toList(), ), ), ); } } 可以看到我们无需去手动管理 Controller 的生命周期，也不需要提供 SingleTickerProviderStateMixin，同时也没有其他的状态需要管理，也就不需要用 StatefulWidget 了，这样简单很多。 "},"pages/fultter/CustomScrollView.html":{"url":"pages/fultter/CustomScrollView.html","title":"CustomScrollView","keywords":"","body":"CustomScrollView 源码 CustomScrollView CustomScrollView 前面介绍的 ListView、GridView、PageView 都是一个完整的可滚动组件，所谓完整是指它们都包括Scrollable 、 Viewport 和 Sliver。假如我们想要在一个页面中，同时包含多个可滚动组件，且使它们的滑动效果能统一起来，比如：我们想将已有的两个沿垂直方向滚动的 ListView 成一个 ListView ，这样在第一ListView 滑动到底部时能自动接上第二 ListView，如果尝试写一个 demo： Widget buildTwoListView() { var listView = ListView.builder( itemCount: 20, itemBuilder: (_, index) => ListTile(title: Text('$index')), ); return Column( children: [ Expanded(child: listView), Divider(color: Colors.grey), Expanded(child: listView), ], ); } } 页面中有两个 ListView，各占可视区域一半高度，虽然能够显式出来，但每一个 ListView 只会响应自己可视区域中滑动，实现不了我们想要的效果。之所以会这样的原因是两个 ListView 都有自己独立的 Scrollable 、 Viewport 和 Sliver，既然如此，我们自己创建一个共用的 Scrollable 和 Viewport 对象，然后再将两个 ListView 对应的 Sliver 添加到这个共用的 Viewport 对象中就可以实现我们想要的效果了。如果这个工作让开发者自己来做无疑是比较麻烦的，因此 Flutter 提供了一个 CustomScrollView 组件来帮助我们创建一个公共的 Scrollable 和 Viewport ，然后它的 slivers 参数接受一个 Sliver 数组，这样我们就可以使用CustomScrollView 方面的实现我们期望的功能了： Widget buildTwoSliverList() { // SliverFixedExtentList 是一个 Sliver，它可以生成高度相同的列表项。 // 再次提醒，如果列表项高度相同，我们应该优先使用SliverFixedExtentList // 和 SliverPrototypeExtentList，如果不同，使用 SliverList. var listView = SliverFixedExtentList( itemExtent: 56, //列表项高度固定 delegate: SliverChildBuilderDelegate( (_, index) => ListTile(title: Text('$index')), childCount: 10, ), ); // 使用 return CustomScrollView( slivers: [ listView, listView, ], ); } Sliver Sliver名称 功能 对应的可滚动组件 SliverList 列表 ListView SliverFixedExtentList 高度固定的列表 ListView，指定itemExtent时 SliverAnimatedList 添加/删除列表项可以执行动画 AnimatedList SliverGrid 网格 GridView SliverPrototypeExtentList 根据原型生成高度固定的列表 ListView，指定prototypeItem 时 SliverFillViewport 包含多个子组件，每个都可以填满屏幕 PageView 除了和列表对应的 Sliver 之外还有一些用于对 Sliver 进行布局、装饰的组件，它们的子组件必须是 Sliver，我们列举几个常用的： |Sliver名称|对应RenderBox| |----|----| |SliverPadding|Padding| |SliverVisibility、SliverOpacity |Visibility、Opacity| |SliverFadeTransition|FadeTransition| |SliverLayoutBuilder|LayoutBuilder| 还有一些其他常用的 Sliver： |Sliver名称|说明| |----|----| |SliverAppBar|对应 AppBar，主要是为了在 CustomScrollView 中使用。| |SliverToBoxAdapter|一个适配器，可以将 RenderBox 适配为 Sliver，后面介绍。| |SliverPersistentHeader|滑动到顶部时可以固定住，后面介绍。| 示例 // 因为本路由没有使用 Scaffold，为了让子级Widget(如Text)使用 // Material Design 默认的样式风格,我们使用 Material 作为本路由的根。 Material( child: CustomScrollView( slivers: [ // AppBar，包含一个导航栏. SliverAppBar( pinned: true, // 滑动到顶端时会固定住 expandedHeight: 250.0, flexibleSpace: FlexibleSpaceBar( title: const Text('Demo'), background: Image.asset( \"./imgs/sea.png\", fit: BoxFit.cover, ), ), ), SliverPadding( padding: const EdgeInsets.all(8.0), sliver: SliverGrid( //Grid gridDelegate: SliverGridDelegateWithFixedCrossAxisCount( crossAxisCount: 2, //Grid按两列显示 mainAxisSpacing: 10.0, crossAxisSpacing: 10.0, childAspectRatio: 4.0, ), delegate: SliverChildBuilderDelegate( (BuildContext context, int index) { //创建子widget return Container( alignment: Alignment.center, color: Colors.cyan[100 * (index % 9)], child: Text('grid item $index'), ); }, childCount: 20, ), ), ), SliverFixedExtentList( itemExtent: 50.0, delegate: SliverChildBuilderDelegate( (BuildContext context, int index) { //创建列表项 return Container( alignment: Alignment.center, color: Colors.lightBlue[100 * (index % 9)], child: Text('list item $index'), ); }, childCount: 20, ), ), ], ), ); 头部SliverAppBar：SliverAppBar对应AppBar，两者不同之处在于SliverAppBar可以集成到CustomScrollView。SliverAppBar可以结合FlexibleSpaceBar实现Material Design中头部伸缩的模型，具体效果，读者可以运行该示例查看。 中间的SliverGrid：它用SliverPadding包裹以给SliverGrid添加补白。SliverGrid是一个两列，宽高比为4的网格，它有20个子组件。 底部SliverFixedExtentList：它是一个所有子元素高度都为50像素的列表。 SliverToBoxAdapter 在实际布局中，我们通常需要往 CustomScrollView 中添加一些自定义的组件，而这些组件并非都有 Sliver 版本，为此 Flutter 提供了一个 SliverToBoxAdapter 组件，它是一个适配器：可以将 RenderBox 适配为 Sliver。比如我们想在列表顶部添加一个可以横向滑动的 PageView，可以使用 SliverToBoxAdapter 来配置： CustomScrollView( slivers: [ SliverToBoxAdapter( child: SizedBox( height: 300, child: PageView( children: [Text(\"1\"), Text(\"2\")], ), ), ), buildSliverFixedList(), ], ); 注意，上面的代码是可以正常运行的，但是如果将 PageView 换成一个滑动方向和 CustomScrollView 一致的 ListView 则不会正常工作！原因是：CustomScrollView 组合 Sliver 的原理是为所有子 Sliver 提供一个共享的 Scrollable，然后统一处理指定滑动方向的滑动事件，如果 Sliver 中引入了其他的 Scrollable，则滑动事件便会冲突。上例中 PageView 之所以能正常工作，是因为 PageView 的 Scrollable 只处理水平方向的滑动，而 CustomScrollView 是处理垂直方向的，两者并未冲突，所以不会有问题，但是换一个也是垂直方向的 ListView 时则不能正常工作，最终的效果是，在ListView内滑动时只会对ListView 起作用，原因是滑动事件被 ListView 的 Scrollable 优先消费，CustomScrollView 的 Scrollable 便接收不到滑动事件了。 SliverPersistentHeader SliverPersistentHeader 的功能是当滑动到 CustomScrollView 的顶部时，可以将组件固定在顶部。 "},"pages/fultter/CustomSliver.html":{"url":"pages/fultter/CustomSliver.html","title":"CustomSliver","keywords":"","body":"CustomSliver "},"pages/fultter/NestedScrollView.html":{"url":"pages/fultter/NestedScrollView.html","title":"NestedScrollView","keywords":"","body":"NestedScrollView "},"pages/fultter/Flutter学习记录.html":{"url":"pages/fultter/Flutter学习记录.html","title":"Flutter学习记录","keywords":"","body":"Flutter学习记录 问题记录 \" Error: Member not found: 'packageRoot' \" in Flutter [duplicate] 使用 Flutter 将文本基线与列内的文本对齐 flutter中密码输入如何切换隐藏/显示？ Do not use BuildContexts across async gaps Bad state: add Fetch Article was called without a registered event handler [Discussion] Should Cubit expose Stream API? #1429 Flutter TextField输入框如何优雅的禁止弹出软键盘 入门 Flutter 开发文档 Flutter Gallery 推荐几个优质Flutter 开源项目 Flutter 快速上手 - 4.2 assets导入资源 | 猫哥 Flutter学习记录——28.Flutter 调试及 Android 和 iOS 打包 FLUTTER开发之DART线程与异步 点击空白区域无反应解决办法 Dart语言教程 如何在 Flutter 中添加 ListTile 源码分析系列之InheritedWidget Flutter-状态管理 Flutter学习笔记：Flutter状态管理（使用Provider进行状态管理） Flutter中文网 rxDart RxDart - 使用 map、flatMap、concatMap、switchMap、asyncMap、exhaustMap text截断方式 text截断方式 Text 以字符的方式截断 dart dart.cn Dart | 浅析dart中库的导入与拆分 【-Flutter/Dart 语法补遗-】 sync 和 async 、yield 和yield* 、async 和 await Dart中两个点..和三个点...的用法 Flutter 事件机制 - Future 和 MicroTask 全解析 Dart中的Future及其then、catchError方法 Dart:factory工厂构造函数的使用场景 Dart String转数值int或double GSYGithubApp 「 Flutter 项目实战 」设计企业级项目入口 main.dart 设计与实现 ( GSYGithubApp 源码解读·二 ) GSYGithubApp 源码分析视频 Flutter使用 json_serializable 解析 JSON 最佳方案 环境配置 flutter 配置不同的开发环境(qa/dev/prod) flutter环境配置 Redux Flutter Redux 中的 combineReducers redux_epics 国际化 flutter-Intl国际化、格式化日期、数字 Flutter 多语言国际化配置 -- 使用Intl插件 FunFlutter系列之国际化Intl方案 知识 systemOverlayStyle可以设置状态栏颜色 Flutter ThemeData详解 BaseAppBar Complete Guide To Flutter Drawer Flutter下拉刷新组件CupertinoSliverRefreshControl苹果风格刷新效果 Flutter 基于NestedScrollView+RefreshIndicator完美解决滑动冲突 GraphQL 入门看这篇就够了 获取屏幕尺寸 Flutter-获取屏幕高度、密度、安全区域等 Flutter控制组件显示和隐藏三种方式详解 工具 Flutter中计算文字的宽度/高度 Flutter Utils 全网最齐全的工具类 Flutter-出生日期计算年龄工具类 主题 Flutter 处理主题 Theme 的一些建议 用抽象工厂方法构建 Flutter 主题 Flutter之textTheme FlutterComponent最佳实践之色彩管理 Flutter 小技巧之 Flutter 3 下的 ThemeExtensions 和 Material3 Flutter TextStyle参数解析 如何在FLutter ThemeData中使用ColorScheme？ Flutter 深色模式分析&实践 Flutter: 如何扩展ThemeData 如何在 Flutter 中更改颜色的色调、饱和度或值？ flutter Icons全部图标 iconfont Flutter3.3对Material3设计风格的支持 实战项目 Flutter 手把手写出超漂亮的登录注册 UI - 饮料食谱App - Speed Code flutter-demo 教程 Flutter App Development Tutorial Series' Articles Flutter基础教程 fluttervideo 快捷键 105--Flutter编辑器Android Studio快捷操作 Flutter-开发中常用的快捷键Android Studio（Mac） 按钮 Flutter的button的按钮ElevatedButton 单元测试 使用 Mockito 对 Flutter 代码进行单元测试 Flutter 单元测试 使用 Mockito 模拟依赖关系 状态管理 flutter_bloc [Flutter] Cubit介紹＆Bloc優缺比較 Flutter 入门与实战（八十九）：使用BlocListener处理状态变化 Flutter 入门与实战（九十）：使用 BlocConsumer 同时构建响应式组件和监听状态 Flutter 入门与实战（九十一）：使用 RepositoryProvider简化父子组件的传值 flutter_bloc使用解析---骚年，你还在手搭bloc吗！ slaw-getx Flutter GetX使用---简洁的魅力！ get-cli getx_pattern flutter_ducafecat_news_getx getx-docs 使用组件 Flutter 仿iOS左滑删除，长按拖动 "},"pages/游戏UI设计/平面构成的基本形与骨骼.html":{"url":"pages/游戏UI设计/平面构成的基本形与骨骼.html","title":"平面构成的基本形与骨骼","keywords":"","body":"平面构成的基本形与骨骼 组成画面内容的视觉元素个体称为基本形，基本形排列所依照的规则顺序（构图）称为“骨骼”。 1）基本形 当基本形相同时，会产生以下组合关系，其被广泛运用在游戏UI设计中，如图形、图标与花纹设计，如图3-32所示。 分离 接触 重叠 局部透叠 遮挡 残缺 差叠 包容 分离 接触 遮挡 残缺 包容 （2）骨骼 平面构成组成元素的框架叫作“骨骼”，它让一切基本形产生有秩序的编排顺序，或产生感觉上经过刻意编排的顺序，常在游戏UI构图中使用，使画面内容的排列布局清晰。 比如通过骨骼来检查下面这个界面构图中各个区块内容的问题，如图3-33所示。加上骨骼线后，可以看出很多问题，比如骨骼顶部不居中，并且在画面上部整体骨骼框架下显得货币下的构图太空，下面面板中的右边骨骼区域与左边骨骼区域内容高度不同。 我们还可以通过不同类型的骨骼来设计画面。 骨骼分类 按复杂程度分：单一骨骼、复合骨骼。游戏UI设计中大多数都是单一骨骼，偶尔会用到复合骨骼，如图3-34所示。 按功能分 ：有作用性骨骼、无作用性骨骼。 按结构分：有规律性骨骼、半规律性骨骼、非规律性骨骼。 1.有作用性骨骼 骨骼给基本形准确的空间位置，并可影响基本形的形状，基本形都在骨骼内，并可以改变大小、正负、方向、位置、肌理和颜色，骨骼线起划分空间的作用，并分割背景，超出骨骼线的部分可被骨骼线切掉，不可影响其他单位骨骼内的元素。骨骼线与骨骼线内的基本形不一定全部出现，如果不需要可将部分骨骼线与基本形去掉，如图3-35所示。 2无作用性骨格 骨骼不会分割空间，它只给基本形准确的空间位置，基本形安排在骨格的交点上，基本形可以改变大小、正负、方向、肌理和颜色，如图3-36所示。 3.规律性骨格 规律性骨格有精确严谨的骨格线，有规律，基本形按照骨格排列，有强烈的秩序感，主要有重复、渐变、发射和衍变等形式，如图3-37所示。 "},"pages/面试系列/01_面试问题集锦.html":{"url":"pages/面试系列/01_面试问题集锦.html","title":"面试问题集锦1","keywords":"","body":"1.面试问题集锦 1. weak和assign的区别 一、什么情况使用 weak 关键字？ 在 ARC 中，在有可能出现循环引用的时候，往往要通过让其中一端使用 weak 来解决，比如： delegate 代理属性。 自身已经对它进行一次强引用，没有必要再强引用一次，此时也会使用 weak ，自定义 IBOutlet 控件属性一般也使用 weak ；当然，也可以使用 strong 。 二、区别 2.1. 修饰变量类型的区别 weak 只可以修饰对象。如果修饰基本数据类型，编译器会报错-“Property with ‘weak’ attribute must be of object type”。 assign 可修饰对象，和基本数据类型。当需要修饰对象类型时，MRC时代使用 unsafeunretained 。当然， unsafe_unretained 也可能产生野指针，所以它名字是 unsafe 。 2.2. 是否产生野指针的区别 weak 不会产生野指针问题。因为 weak 修饰的对象释放后（引用计数器值为0），指针会自动被置nil，之后再向该对象发消息也不会崩溃。 weak是安全的。 assign 如果修饰对象，会产生野指针问题；如果修饰基本数据类型则是安全的。修饰的对象释放后，指针不会自动被置空，此时向对象发消息会崩溃。 三、相同 都可以修饰对象类型，但是 assign 修饰对象会存在问题。 四、总结 assign 适用于基本数据类型如 int，float，struct 等值类型，不适用于引用类型。因为值类型会被放入栈中，遵循先进后出原则，由系统负责管理栈内存。而引用类型会被放入堆中，需要我们自己手动管理内存或通过 ARC 管理。weak 适用于 delegate 和 block 等引用类型，不会导致野指针问题，也不会循环引用，非常安全。 五、参考文章 iOS开发中 weak 和 assign 的区别 2. 为什么要用 Copy 修饰 Block 一、栈区和堆区概念 内存的栈区：由编译器自动分配释放，存放函数的参数值，局部变量的值等。 其操作方式类似于数据结构中的栈。 内存的堆区：一般由程序员分配释放，若程序员不释放，程序结束时可能由OS回收。注意它与数据结构中的堆是两回事， 分配方式倒是类似于链表。 二、Block 的三种类型 iOS 内存分布，一般分为：栈区、堆区、全局区、常量区、代码区。其实 Block 也是一个 Objective-C 对象，常见的有以下三种 Block ： NSGlobalBlock：全局的静态 Block 没有访问外部变量。 NSStackBlock：保存在栈中的 Block ，没有用copy去修饰并且访问了外部变量，会在函数调用结束被销毁（需要在MRC）。 NSMallocBlock：保存在堆中的 Block ， 此类型 Block 是用 Copy 修饰出来的 Block ，它会随着对象的销毁而销毁，只要对象不销毁，我们就可以调用的到在堆中的 Block 。 三、回答 Block 引用了普通外部变量，都是创建在栈区的；对于分配在栈区的对象，我们很容易会在释放之后继续调用，导致程序奔溃，所以我们使用的时候需要将栈区的对象移到堆区，来延长该对象的生命周期。对于这个问题，得区分 MRC 环境和 ARC 环境： 对于 MRC 环境，使用 Copy 修饰 Block，会将栈区的 Block 拷贝到堆区。 对于 ARC 环境，使用 Strong、Copy 修饰 Block，都会将栈区的 Block 拷贝到堆区。 所以，Block 不是一定要用 Copy 来修饰的，在 ARC 环境下面 Strong 和 Copy 修饰效果是一样的。 四、参考文章 iOS block 为什么用copy修饰 为什么要用copy修饰Block ·iOS 面试题·Block 的原理，Block 的属性修饰词为什么用 copy，使用 Block 时有哪些要注意的？ 3. 怎么用 Copy 关键字？ NSString、NSArray、NSDictionary 等等经常使用 Copy 关键字。因为他们有对应的可变类型：NSMutableString、NSMutableArray、NSMutableDictionary，他们之间可能进行赋值操作，为确保对象中的字符串值不会无意间变动，应该在设置新属性值时拷贝一份。 Block 也经常使用 Copy 关键字。Block 使用 Copy 是从 MRC 遗留下来的“传统”，在 MRC 中，方法内部的 Block 是在栈区的，使用 Copy 可以把它放到堆区。 4. 这个写法会出什么问题：@property (copy) NSMutableArray *array; 添加，删除，修改数组内的元素的时候，程序会因为找不到对应的方法而崩溃.因为 copy 就是复制一个不可变 NSArray 的对象； 比如下面的代码就会发生崩溃 // .h文件 // http://weibo.com/luohanchenyilong/ // https://github.com/ChenYilong // 下面的代码就会发生崩溃 @property (nonatomic， copy) NSMutableArray *mutableArray; // .m文件 // http://weibo.com/luohanchenyilong/ // https://github.com/ChenYilong // 下面的代码就会发生崩溃 NSMutableArray *array = [NSMutableArray arrayWithObjects:@1，@2，nil]; self.mutableArray = array; [self.mutableArray removeObjectAtIndex:0]; 接下来就会奔溃： -[__NSArrayI removeObjectAtIndex:]: unrecognized selector sent to instance 0x7fcd1bc30460 使用了 atomic 属性会严重影响性能； 该属性使用了互斥锁（atomic 的底层实现，老版本是自旋锁，iOS10开始是互斥锁--spinlock底层实现改变了。），会在创建时生成一些额外的代码用于帮助编写多线程程序，这会带来性能问题，通过声明 nonatomic 可以节省这些虽然很小但是不必要额外开销。 5. 如何让自己的类用 copy 修饰符？如何重写带 copy 关键字的 setter？ 若想令自己所写的对象具有拷贝功能，则需实现 NSCopying 协议。如果自定义的对象分为可变版本与不可变版本，那么就要同时实现 NSCopying 与 NSMutableCopying 协议。 具体步骤： 需声明该类遵从 NSCopying 协议 实现 NSCopying 协议。该协议只有一个方法: - (id)copyWithZone:(NSZone *)zone; 案例： .h文 // 件 // http://weibo.com/luohanchenyilong/ // https://github.com/ChenYilong // 以第一题《风格纠错题》里的代码为例 typedef NS_ENUM(NSInteger， CYLSex) { CYLSexMan， CYLSexWoman }; @interface CYLUser : NSObject @property (nonatomic， readonly， copy) NSString *name; @property (nonatomic， readonly， assign) NSUInteger age; @property (nonatomic， readonly， assign) CYLSex sex; - (instancetype)initWithName:(NSString *)name age:(NSUInteger)age sex:(CYLSex)sex; + (instancetype)userWithName:(NSString *)name age:(NSUInteger)age sex:(CYLSex)sex; - (void)addFriend:(CYLUser *)user; - (void)removeFriend:(CYLUser *)user; @end // .m文件 // http://weibo.com/luohanchenyilong/ // https://github.com/ChenYilong // @implementation CYLUser { NSMutableSet *_friends; } - (void)setName:(NSString *)name { _name = [name copy]; } - (instancetype)initWithName:(NSString *)name age:(NSUInteger)age sex:(CYLSex)sex { if(self = [super init]) { _name = [name copy]; _age = age; _sex = sex; _friends = [[NSMutableSet alloc] init]; } return self; } - (void)addFriend:(CYLUser *)user { [_friends addObject:user]; } - (void)removeFriend:(CYLUser *)user { [_friends removeObject:user]; } - (id)copyWithZone:(NSZone *)zone { CYLUser *copy = [[[self class] allocWithZone:zone] initWithName:_name age:_age sex:_sex]; copy->_friends = [_friends mutableCopy]; return copy; } - (id)deepCopy { CYLUser *copy = [[[self class] alloc] initWithName:_name age:_age sex:_sex]; copy->_friends = [[NSMutableSet alloc] initWithSet:_friends copyItems:YES]; return copy; } @end 至于如何重写带 copy 关键字的 setter这个问题， 如果抛开本例来回答的话，如下： - (void)setName:(NSString *)name { //[_name release]; _name = [name copy]; } 那如何确保 name 被 copy？在初始化方法(initializer)中做： - (instancetype)initWithName:(NSString *)name age:(NSUInteger)age sex:(CYLSex)sex { if(self = [super init]) { _name = [name copy]; _age = age; _sex = sex; _friends = [[NSMutableSet alloc] init]; } return self; } 6. @property 的本质是什么？ivar、getter、setter 是如何生成并添加到这个类中的 @property 的本质是什么？ @property = ivar + getter + setter; 下面解释下： “属性” (property)有两大概念：ivar（实例变量）、存取方法（access method ＝ getter + setter）。 “属性” (property)作为 Objective-C 的一项特性，主要的作用就在于封装对象中的数据。 Objective-C 对象通常会把其所需要的数据保存为各种实例变量。实例变量一般通过“存取方法”(access method)来访问。其中，“获取方法” (getter)用于读取变量值，而“设置方法” (setter)用于写入变量值。这个概念已经定型，并且经由“属性”这一特性而成为 Objective-C 2.0 的一部分。 而在正规的 Objective-C 编码风格中，存取方法有着严格的命名规范。 正因为有了这种严格的命名规范，所以 Objective-C 这门语言才能根据名称自动创建出存取方法。其实也可以把属性当做一种关键字，其表示: 编译器会自动写出一套存取方法，用以访问给定类型中具有给定名称的变量。 所以你也可以这么说： @property = getter + setter; 例如下面这个类： @interface Person : NSObject @property NSString *firstName; @property NSString *lastName; @end 上述代码写出来的类与下面这种写法等效 @interface Person : NSObject - (NSString *)firstName; - (void)setFirstName:(NSString *)firstName; - (NSString *)lastName; - (void)setLastName:(NSString *)lastName; @end 源码分析 property在runtime中是 objc_property_t 定义如下: typedef struct objc_property *objc_property_t; 而 objc_property 是一个结构体，包括name和attributes，定义如下： struct property_t { const char *name; const char *attributes; }; 而attributes本质是 objc_property_attribute_t，定义了property的一些属性，定义如下： /// Defines a property attribute typedef struct { const char *name; /** 而attributes的具体内容是什么呢？其实，包括：类型，原子性，内存语义和对应的实例变量。 例如：我们定义一个string的property@property (nonatomic， copy) NSString *string;，通过 property_getAttributes(property)获取到attributes并打印出来之后的结果为T@\"NSString\"，C，N，V_string 其中T就代表类型，可参阅Type Encodings，C就代表Copy，N代表nonatomic，V就代表对应的实例变量。 ivar、getter、setter 是如何生成并添加到这个类中的? “自动合成”( autosynthesis) 完成属性定义后，编译器会自动编写访问这些属性所需的方法，此过程叫做“自动合成”(autosynthesis)。需要强调的是，这个过程由编译 器在编译期执行，所以编辑器里看不到这些“合成方法”(synthesized method)的源代码。除了生成方法代码 getter、setter 之外，编译器还要自动向类中添加适当类型的实例变量，并且在属性名前面加下划线，以此作为实例变量的名字。在前例中，会生成两个实例变量，其名称分别为 _firstName 与 _lastName。也可以在类的实现代码里通过 @synthesize 语法来指定实例变量的名字. @implementation Person @synthesize firstName = _myFirstName; @synthesize lastName = _myLastName; @end 我为了搞清属性是怎么实现的，曾经反编译过相关的代码，他大致生成了五个东西 OBJCIVAR$类名$属性名称 ：该属性的“偏移量” (offset)，这个偏移量是“硬编码” (hardcode)，表示该变量距离存放对象的内存区域的起始地址有多远。 setter 与 getter 方法对应的实现函数 ivar_list ：成员变量列表 method_list ：方法列表 prop_list ：属性列表 也就是说我们每次在增加一个属性，系统都会在 ivar_list 中添加一个成员变量的描述，在 method_list 中增加 setter 与 getter 方法的描述，在属性列表中增加一个属性的描述，然后计算该属性在对象中的偏移量，然后给出 setter 与 getter 方法对应的实现，在 setter 方法中从偏移量的位置开始赋值，在 getter 方法中从偏移量开始取值，为了能够读取正确字节数，系统对象偏移量的指针类型进行了类型强转。 7. @protocol 和 category 中如何使用 @property 在 protocol 中使用 property 只会生成 setter 和 getter 方法声明,我们使用属性的目的,是希望遵守我协议的对象能实现该属性 category 使用 @property 也是只会生成 setter 和 getter 方法的声明,如果我们真的需要给 category 增加属性的实现,需要借助于运行时的两个函数： objc_setAssociatedObject objc_getAssociatedObject 8. runtime 如何实现 weak 属性 要实现 weak 属性，首先要搞清楚 weak 属性的特点： weak 此特质表明该属性定义了一种“非拥有关系” (nonowning relationship)。为这种属性设置新值时，设置方法既不保留新值，也不释放旧值。此特质同 assign 类似， 然而在属性所指的对象遭到摧毁时，属性值也会清空(nil out)。 那么 runtime 如何实现 weak 变量的自动置nil？ runtime 对注册的类， 会进行布局，对于 weak 对象会放入一个 hash 表中。 用 weak 指向的对象内存地址作为 key，当此对象的引用计数为0的时候会 dealloc，假如 weak 指向的对象内存地址是a，那么就会以a为键， 在这个 weak 表中搜索，找到所有以a为键的 weak 对象，从而设置为 nil。 9. @property中有哪些属性关键字？/ @property 后面可以有哪些修饰符？ 原子性--- nonatomic 特质 读/写权限---readwrite(读写)、readonly (只读) 内存管理语义---assign、strong、 weak、unsafe_unretained、copy 方法名---getter= 、setter= getter=的样式： @property (nonatomic, getter=isOn) BOOL on; setter=一般用在特殊的情境下，比如： 在数据反序列化、转模型的过程中，服务器返回的字段如果以 init 开头，所以你需要定义一个 init 开头的属性，但默认生成的 setter 与 getter 方法也会以 init 开头，而编译器会把所有以 init 开头的方法当成初始化方法，而初始化方法只能返回 self 类型，因此编译器会报错。 这时你就可以使用下面的方式来避免编译器报错： @property(nonatomic, strong, getter=p_initBy, setter=setP_initBy:)NSString *initBy; 另外也可以用关键字进行特殊说明，来避免编译器报错： @property(nonatomic, readwrite, copy, null_resettable) NSString *initBy; - (NSString *)initBy __attribute__((objc_method_family(none))); 不常用的：nonnull,null_resettable,nullable 10. weak属性需要在dealloc中置nil么？ 不需要。 在ARC环境无论是强指针还是弱指针都无需在 dealloc 设置为 nil ， ARC 会自动帮我们处理 11. @synthesize和@dynamic分别有什么作用？ @property有两个对应的词，一个是 @synthesize，一个是 @dynamic。如果 @synthesize和 @dynamic都没写，那么默认的就是@syntheszie var = _var; @synthesize 的语义是如果你没有手动实现 setter 方法和 getter 方法，那么编译器会自动为你加上这两个方法。 @dynamic 告诉编译器：属性的 setter 与 getter 方法由用户自己实现，不自动生成。（当然对于 readonly 的属性只需提供 getter 即可）。假如一个属性被声明为 @dynamic var，然后你没有提供 @setter方法和 @getter 方法，编译的时候没问题，但是当程序运行到 instance.var = someVar，由于缺 setter 方法会导致程序崩溃；或者当运行到 someVar = var 时，由于缺 getter 方法同样会导致崩溃。编译时没问题，运行时才执行相应的方法，这就是所谓的动态绑定。 12. ARC下，不显式指定任何属性关键字时，默认的关键字都有哪些？ 对应基本数据类型默认关键字是 atomic readwrite assign 对于普通的 Objective-C 对象 atomic readwrite strong 13. 用@property声明的NSString（或NSArray，NSDictionary）经常使用copy关键字，为什么？如果改用strong关键字，可能造成什么问题？ 因为父类指针可以指向子类对象,使用 copy 的目的是为了让本对象的属性不受外界影响,使用 copy 无论给我传入是一个可变对象还是不可对象,我本身持有的就是一个不可变的副本. 如果我们使用是 strong ,那么这个属性就有可能指向一个可变对象,如果这个可变对象在外部被修改了,那么会影响该属性. copy 此特质所表达的所属关系与 strong 类似。然而设置方法并不保留新值，而是将其“拷贝” (copy)。 当属性类型为 NSString 时，经常用此特质来保护其封装性，因为传递给设置方法的新值有可能指向一个 NSMutableString 类的实例。这个类是 NSString 的子类，表示一种可修改其值的字符串，此时若是不拷贝字符串，那么设置完属性之后，字符串的值就可能会在对象不知情的情况下遭人更改。所以，这时就要拷贝一份“不可变” (immutable)的字符串，确保对象中的字符串值不会无意间变动 成实例变量的规则是什么？假如property名为foo，存在一个名为_foo的实例变量，那么还会自动合成新变量么？ 如果指定了成员变量的名称,会生成一个指定的名称的成员变量 如果这个成员已经存在了就不再生成了 如果是 @synthesize foo; 还会生成一个名称为foo的成员变量，也就是说 如果没有指定成员变量的名称会自动生成一个属性同名的成员变量 如果是 @synthesize foo = _foo; 就不会生成成员变量了。 14. 在有了自动合成属性实例变量之后，@synthesize还有哪些使用场景？ 回答这个问题前，我们要搞清楚一个问题，什么情况下不会autosynthesis（自动合成）？ 1. 同时重写了 setter 和 getter 时 2. 重写了只读属性的 getter 时 3. 使用了 @dynamic 时 4. 在 @protocol 中定义的所有属性 5. 在 category 中定义的所有属性 6. 重写（overridden）的属性 当你在子类中重写（overridden）了父类中的属性，你必须 使用 @synthesize 来手动合成ivar。 当你同时重写了 setter 和 getter 时，系统就不会生成 ivar（实例变量/成员变量）。这时候有两种选择： 手动创建 ivar 使用@synthesize foo = _foo; ，关联 @property 与 ivar。 15. objc中向一个nil对象发送消息将会发生什么？ 在 Objective-C 中向 nil 发送消息是完全有效的——只是在运行时不会有任何作用: 如果一个方法返回值是一个对象，那么发送给nil的消息将返回0(nil)。例如： Person * motherInlaw = [[aPerson spouse] mother]; 如果 spouse 对象为 nil，那么发送给 nil 的消息 mother 也将返回 nil。 如果方法返回值为指针类型，其指针大小为小于或者等于sizeof(void*)，float，double，long double 或者 long long 的整型标量，发送给 nil 的消息将返回0。 如果方法返回值为结构体,发送给 nil 的消息将返回0。结构体中各个字段的值将都是0。 如果方法的返回值不是上述提到的几种情况，那么发送给 nil 的消息的返回值将是未定义的。 具体原因如下： objc是动态语言，每个方法在运行时会被动态转为消息发送，即：objc_msgSend(receiver, selector)。 objc在向一个对象发送消息时，runtime库会根据对象的isa指针找到该对象实际所属的类，然后在该类中的方法列表以及其父类方法列表中寻找方法运行，然后在发送消息的时候，objc_msgSend方法不会返回值，所谓的返回内容都是具体调用时执行的。 那么，回到本题，如果向一个nil对象发送消息，首先在寻找对象的isa指针时就是0地址返回了，所以不会出现任何错误。 16. objc中向一个对象发送消息[obj foo]和objc_msgSend()函数之间有什么关系？ 该方法编译之后就是objc_msgSend()函数调用. ((void ()(id, SEL))(void )objc_msgSend)((id)obj, sel_registerName(\"foo\")); 17. 什么时候会报unrecognized selector的异常？ 简单来说： 当调用该对象上某个方法,而该对象上没有实现这个方法的时候， 可以通过“消息转发”进行解决。 简单的流程如下，在上一题中也提到过： objc是动态语言，每个方法在运行时会被动态转为消息发送，即：objc_msgSend(receiver, selector)。 objc在向一个对象发送消息时，runtime库会根据对象的isa指针找到该对象实际所属的类，然后在该类中的方法列表以及其父类方法列表中寻找方法运行，如果，在最顶层的父类中依然找不到相应的方法时，程序在运行时会挂掉并抛出异常unrecognized selector sent to XXX 。但是在这之前，objc的运行时会给出三次拯救程序崩溃的机会： Method resolution objc运行时会调用+resolveInstanceMethod:或者 +resolveClassMethod:，让你有机会提供一个函数实现。如果你添加了函数，那运行时系统就会重新启动一次消息发送的过程，否则 ，运行时就会移到下一步，消息转发（Message Forwarding）。 Fast forwarding 如果目标对象实现了 -forwardingTargetForSelector:，Runtime 这时就会调用这个方法，给你把这个消息转发给其他对象的机会。 只要这个方法返回的不是nil和self，整个消息发送的过程就会被重启，当然发送的对象会变成你返回的那个对象。否则，就会继续Normal Fowarding。 这里叫Fast，只是为了区别下一步的转发机制。因为这一步不会创建任何新的对象，但下一步转发会创建一个NSInvocation对象，所以相对更快点。 Normal forwarding 这一步是Runtime最后一次给你挽救的机会。首先它会发送 -methodSignatureForSelector: 消息获得函数的参数和返回值类型。如果 -methodSignatureForSelector: 返回nil，Runtime则会发出 -doesNotRecognizeSelector: 消息，程序这时也就挂掉了。如果返回了一个函数签名，Runtime就会创建一个NSInvocation对象并发送 -forwardInvocation: 消息给目标对象。 18. 一个objc对象如何进行内存布局？（考虑有父类的情况） 所有父类的成员变量和自己的成员变量都会存放在该对象所对应的存储空间中. 每一个对象内部都有一个isa指针,指向他的类对象,类对象中存放着本对象的 对象方法列表（对象能够接收的消息列表，保存在它所对应的类对象中） 成员变量的列表, 属性列表 它内部也有一个isa指针指向元对象(meta class),元对象内部存放的是类方法列表,类对象内部还有一个superclass的指针,指向他的父类对象。 19. 一个objc对象的isa的指针指向什么？有什么作用？ isa 顾名思义 is a 表示对象所属的类。 isa 指向他的类对象，从而可以找到对象上的方法。 同一个类的不同对象，他们的 isa 指针是一样的 20. 下面的代码输出什么？ @implementation Son : Father - (id)init { self = [super init]; if (self) { NSLog(@\"%@\", NSStringFromClass([self class])); NSLog(@\"%@\", NSStringFromClass([super class])); } return self; } @end 都输出 Son 21. runtime如何通过selector找到对应的IMP地址？（分别考虑类方法和实例方法） 每一个类对象中都一个方法列表，方法列表中记录着方法的名称、方法实现、以及参数类型，其实selector 本质就是方法名称，通过这个方法名称就可以在方法列表中找到对应的方法实现。 参考 NSObject 上面的方法： - (IMP)methodForSelector:(SEL)aSelector; + (IMP)instanceMethodForSelector:(SEL)aSelector; 22. 使用runtime Associate方法关联的对象，需要在主对象dealloc的时候释放么？ 无论在MRC下还是ARC下均不需要。 23. objc中的类方法和实例方法有什么本质区别和联系？ 类方法： 类方法是属于类对象的 类方法只能通过类对象调用 类方法中的self是类对象 类方法可以调用其他的类方法 类方法中不能访问成员变量 类方法中不能直接调用对象方法 实例方法： 实例方法是属于实例对象的 实例方法只能通过实例对象调用 实例方法中的self是实例对象 实例方法中可以访问成员变量 实例方法中直接调用实例方法 实例方法中也可以调用类方法(通过类名) "},"pages/面试系列/02_面试问题集锦.html":{"url":"pages/面试系列/02_面试问题集锦.html","title":"面试问题集锦2","keywords":"","body":"1. _objc_msgForward函数是做什么的，直接调用它将会发生什么？ _objc_msgForward是 IMP 类型，用于消息转发的：当向一个对象发送一条消息，但它并没有实现的时候，_objc_msgForward会尝试做消息转发。 2. 能否向编译后得到的类中增加实例变量？能否向运行时创建的类中添加实例变量？为什么？ 不能向编译后得到的类中增加实例变量； 能向运行时创建的类中添加实例变量； 解释下： 因为编译后的类已经注册在 runtime 中，类结构体中的 objc_ivar_list 实例变量的链表 和 instance_size 实例变量的内存大小已经确定，同时runtime 会调用 class_setIvarLayout 或 class_setWeakIvarLayout 来处理 strong weak 引用。所以不能向存在的类中添加实例变量； 运行时创建的类是可以添加实例变量，调用 class_addIvar 函数。但是得在调用 objc_allocateClassPair 之后，objc_registerClassPair 之前，原因同上。 3. runloop和线程有什么关系？ 总的说来，Run loop，正如其名，loop表示某种循环，和run放在一起就表示一直在运行着的循环。实际上，run loop和线程是紧密相连的，可以这样说run loop是为了线程而生，没有线程，它就没有存在的必要。Run loops是线程的基础架构部分， Cocoa 和 CoreFundation 都提供了 run loop 对象方便配置和管理线程的 run loop （以下都以 Cocoa 为例）。每个线程，包括程序的主线程（ main thread ）都有与之相应的 run loop 对象。 主线程的run loop默认是启动的。 iOS的应用程序里面，程序启动后会有一个如下的main()函数 int main(int argc, char * argv[]) { @autoreleasepool { return UIApplicationMain(argc, argv, nil, NSStringFromClass([AppDelegate class])); } } 重点是UIApplicationMain()函数，这个方法会为main thread设置一个NSRunLoop对象，这就解释了：为什么我们的应用可以在无人操作的时候休息，需要让它干活的时候又能立马响应。 对其它线程来说，run loop默认是没有启动的，如果你需要更多的线程交互则可以手动配置和启动，如果线程只是去执行一个长时间的已确定的任务则不需要。 在任何一个 Cocoa 程序的线程中，都可以通过以下代码来获取到当前线程的 run loop NSRunLoop *runloop = [NSRunLoop currentRunLoop]; 4. runloop的mode作用是什么？ model 主要是用来指定事件在运行循环中的优先级的，分为： NSDefaultRunLoopMode（kCFRunLoopDefaultMode）：默认，空闲状态 UITrackingRunLoopMode：ScrollView滑动时 UIInitializationRunLoopMode：启动时 NSRunLoopCommonModes（kCFRunLoopCommonModes）：Mode集合 苹果公开提供的 Mode 有两个： NSDefaultRunLoopMode（kCFRunLoopDefaultMode） NSRunLoopCommonModes（kCFRunLoopCommonModes） 5. 以+ scheduledTimerWithTimeInterval...的方式触发的timer，在滑动页面上的列表时，timer会暂定回调，为什么？如何解决？ RunLoop只能运行在一种mode下，如果要换mode，当前的loop也需要停下重启成新的。利用这个机制，ScrollView滚动过程中NSDefaultRunLoopMode（kCFRunLoopDefaultMode）的mode会切换到UITrackingRunLoopMode来保证ScrollView的流畅滑动：只能在NSDefaultRunLoopMode模式下处理的事件会影响ScrollView的滑动。 如果我们把一个NSTimer对象以NSDefaultRunLoopMode（kCFRunLoopDefaultMode）添加到主运行循环中的时候, ScrollView滚动过程中会因为mode的切换，而导致NSTimer将不再被调度。 同时因为mode还是可定制的，所以： Timer计时会被scrollView的滑动影响的问题可以通过将timer添加到NSRunLoopCommonModes（kCFRunLoopCommonModes）来解决。代码如下： //将timer添加到NSDefaultRunLoopMode中 [NSTimer scheduledTimerWithTimeInterval:1.0 target:self selector:@selector(timerTick:) userInfo:nil repeats:YES]; //然后再添加到NSRunLoopCommonModes里 NSTimer *timer = [NSTimer timerWithTimeInterval:1.0 target:self selector:@selector(timerTick:) userInfo:nil repeats:YES]; [[NSRunLoop currentRunLoop] addTimer:timer forMode:NSRunLoopCommonModes]; 6. 猜想runloop内部是如何实现的？ 一般来讲，一个线程一次只能执行一个任务，执行完成后线程就会退出。如果我们需要一个机制，让线程能随时处理事件但并不退出，通常的代码逻辑 是这样的： function loop() { initialize(); do { var message = get_next_message(); process_message(message); } while (message != quit); } 或使用伪代码来展示下: int main(int argc, char * argv[]) { //程序一直运行状态 while (AppIsRunning) { //睡眠状态，等待唤醒事件 id whoWakesMe = SleepForWakingUp(); //得到唤醒事件 id event = GetEvent(whoWakesMe); //开始处理事件 HandleEvent(event); } return 0; } 7. objc使用什么机制管理对象内存？ 通过 retainCount 的机制来决定对象是否需要释放。 每次 runloop 的时候，都会检查对象的 retainCount，如果 retainCount 为 0，说明该对象没有地方需要继续使用了，可以释放掉了。 8. ARC通过什么方式帮助开发者管理内存？ ARC相对于MRC，不是在编译时添加retain/release/autorelease这么简单。应该是编译期和运行期两部分共同帮助开发者管理内存。 在编译期，ARC用的是更底层的C接口实现的retain/release/autorelease，这样做性能更好，也是为什么不能在ARC环境下手动retain/release/autorelease，同时对同一上下文的同一对象的成对retain/release操作进行优化（即忽略掉不必要的操作）；ARC也包含运行期组件，这个地方做的优化比较复杂，但也不能被忽略。 8. 不手动指定autoreleasepool的前提下，一个autorealese对象在什么时刻释放？（比如在一个vc的viewDidLoad中创建） 手动干预释放时机--指定 autoreleasepool 就是所谓的：当前作用域大括号结束时释放。 系统自动去释放--不手动指定 autoreleasepool Autorelease对象出了作用域之后，会被添加到最近一次创建的自动释放池中，并会在当前的 runloop 迭代结束时释放。 如果在一个vc的viewDidLoad中创建一个 Autorelease对象，那么该对象会在 viewDidAppear 方法执行前就被销毁了。 9. BAD_ACCESS在什么情况下出现？ 访问了悬垂指针，比如对一个已经释放的对象执行了release、访问已经释放对象的成员变量或者发消息。 死循环 10. 苹果是如何实现autoreleasepool的？ autoreleasepool 以一个队列数组的形式实现,主要通过下列三个函数完成. objc_autoreleasepoolPush objc_autoreleasepoolPop objc_autorelease 看函数名就可以知道，对 autorelease 分别执行 push，和 pop 操作。销毁对象时执行release操作。 11. 使用block时什么情况会发生引用循环，如何解决？ 一个对象中强引用了 block，在 block 中又强引用了该对象，就会发生循环引用。 ARC 下的解决方法是： 将该对象使用 weak 修饰符修饰之后再在 block 中使用。 id weak weakSelf = self; 或者 weak typeof(&*self)weakSelf = self 该方法可以设置宏 __weak ：不会产生强引用，指向的对象销毁时，会自动让指针置为 ni1 使用 unsafe_unretained 关键字，用法与 __weak 一致。 unsafe_unretained 不会产生强引用，不安全，指向的对象销毁时，指针存储的地址值不变。 也可以使用 block 来解决循环引用问题，用法为： block id weakSelf = self;，但不推荐使用。因为必须要调用该 block 方案才能生效，因为需要及时的将 __block 变量置为 nii。 12. 在block内如何修改block外部变量？ 先描述下问题： 默认情况下，在block中访问的外部变量是复制过去的，即：写操作不对原变量生效。但是你可以加上 __block 来让其写操作生效，示例代码如下: __block int a = 0; void (^foo)(void) = ^{ a = 1; }; foo(); //这里，a的值被修改为1 \"将 auto 从栈 copy 到堆\" “将 auto 变量封装为结构体(对象)” 13. 使用系统的某些block api（如UIView的block版本写动画时），是否也考虑引用循环问题？ 14. GCD的队列（dispatch_queue_t）分哪两种类型 串行队列Serial Dispatch Queue 并发队列Concurrent Dispatch Queue 15. 如何用GCD同步若干个异步调用？（如根据若干个url异步加载多张图片，然后在都下载完成后合成一张整图） 使用Dispatch Group追加block到Global Group Queue,这些block如果全部执行完毕，就会执行Main Dispatch Queue中的结束处理的block。 16. dispatch_barrier_async的作用是什么？ 在并发队列中，为了保持某些任务的顺序，需要等待一些任务完成后才能继续进行，使用 barrier 来等待之前任务完成，避免数据竞争等问题。 dispatch_barrier_async 函数会等待追加到Concurrent Dispatch Queue并发队列中的操作全部执行完之后，然后再执行 dispatch_barrier_async 函数追加的处理，等 dispatch_barrier_async 追加的处理执行结束之后，Concurrent Dispatch Queue才恢复之前的动作继续执行。 打个比方：比如你们公司周末跟团旅游，高速休息站上，司机说：大家都去上厕所，速战速决，上完厕所就上高速。超大的公共厕所，大家同时去，程序猿很快就结束了，但程序媛就可能会慢一些，即使你第一个回来，司机也不会出发，司机要等待所有人都回来后，才能出发。 dispatch_barrier_async 函数追加的内容就如同 “上完厕所就上高速”这个动作。 （注意：使用 dispatch_barrier_async ，该函数只能搭配自定义并发队列 dispatch_queue_t 使用。不能使用： dispatch_get_global_queue ，否则 dispatch_barrier_async 的作用会和 dispatch_async 的作用一模一样。 ） 17. 苹果为什么要废弃dispatch_get_current_queue？ dispatch_get_current_queue函数的行为常常与开发者所预期的不同。 由于派发队列是按层级来组织的，这意味着排在某条队列中的块会在其上级队列里执行。 队列间的层级关系会导致检查当前队列是否为执行同步派发所用的队列这种方法并不总是奏效。dispatch_get_current_queue函数通常会被用于解决由不可以重入的代码所引发的死锁，然后能用此函数解决的问题，通常也可以用\"队列特定数据\"来解决。 18. 以下代码运行结果如何？ - (void)viewDidLoad { [super viewDidLoad]; NSLog(@\"1\"); dispatch_sync(dispatch_get_main_queue(), ^{ NSLog(@\"2\"); }); NSLog(@\"3\"); } 只输出：1 。发生主线程锁死。 19. 如何手动触发一个value的KVO 所谓的“手动触发”是区别于“自动触发”： 自动触发是指类似这种场景：在注册 KVO 之前设置一个初始值，注册之后，设置一个不一样的值，就可以触发了。 想知道如何手动触发，必须知道自动触发 KVO 的原理： 键值观察通知依赖于 NSObject 的两个方法: willChangeValueForKey: 和 didChangevlueForKey: 。在一个被观察属性发生改变之前， willChangeValueForKey: 一定会被调用，这就 会记录旧的值。而当改变发生后， observeValueForKey:ofObject:change:context: 会被调用，继而 didChangeValueForKey: 也会被调用。如果可以手动实现这些调用，就可以实现“手动触发”了。 那么“手动触发”的使用场景是什么？一般我们只在希望能控制“回调的调用时机”时才会这么做。 20. 若一个类有实例变量 NSString *_foo ，调用setValue:forKey:时，可以以foo还是 _foo 作为key？ 都可以 21. KVC的keyPath中的集合运算符如何使用？ 必须用在集合对象上或普通对象的集合属性上 简单集合运算符有@avg， @count ， @max ， @min ，@sum 格式 @\"@sum.age\"或 @\"集合属性.@max.age\" 22. KVC和KVO的keyPath一定是属性么？ KVC 支持实例变量，KVO 只能手动支持手动设定实例变量的KVO实现监听 23. 如何关闭默认的KVO的默认实现，并进入自定义的KVO实现？ 《如何自己动手实现 KVO》 KVO for manually implemented properties 24. apple用什么方式实现对一个对象的KVO？ 当你观察一个对象时，一个新的类会被动态创建。这个类继承自该对象的原本的类，并重写了被观察属性的 setter 方法。重写的 setter 方法会负责在调用原 setter 方法之前和之后，通知所有观察对象：值的更改。最后通过 isa 混写（isa-swizzling） 把这个对象的 isa 指针 ( isa 指针告诉 Runtime 系统这个对象的类是什么 ) 指向这个新创建的子类，对象就神奇的变成了新创建的子类的实例 25. IBOutlet连出来的视图属性为什么可以被设置成weak? Should IBOutlets be strong or weak under ARC? 文章告诉我们： 因为既然有外链那么视图在xib或者storyboard中肯定存在，视图已经对它有一个强引用了。 不过这个回答漏了个重要知识，使用storyboard（xib不行）创建的vc，会有一个叫_topLevelObjectsToKeepAliveFromStoryboard 的私有数组强引用所有 top level 的对象，所以这时即便outlet声明成weak也没关系 26. IB中User Defined Runtime Attributes如何使用？ 它能够通过KVC的方式配置一些你在interface builder 中不能配置的属性。当你希望在IB中作尽可能多得事情，这个特性能够帮助你编写更加轻量级的viewcontroller 27. 如何调试BAD_ACCESS错误 重写object的respondsToSelector方法，现实出现EXEC_BAD_ACCESS前访问的最后一个object 通过 Zombie 设置全局断点快速定位问题代码所在行 Xcode 7 已经集成了BAD_ACCESS捕获功能：Address Sanitizer。 28. lldb（gdb）常用的调试命令？ breakpoint 设置断点定位到某一个函数 n 断点指针下一步 po打印对象 "},"pages/面试系列/取东西.html":{"url":"pages/面试系列/取东西.html","title":"取东西","keywords":"","body":"取东西 测试一下 // 初始化每行数量 var row1Count = 3; var row2Count = 5; var row3Count = 7; // 获取行剩余数量 function getRemainCount(rowNum) { switch(parseInt(rowNum)) { case 1: return row1Count; case 2: return row2Count; case 3: return row3Count; default: return 0; } } // 更新行数量 // rowNum: 第几行（1,2,3） // getCount: 取数量 function updateRemainCount(rowNum, getCount) { switch(parseInt(rowNum)) { case 1: row1Count -= getCount; break; case 2: row2Count -= getCount; break; case 3: row3Count -= getCount; break; default: break; } }; // 判断是否取完所有 function judgeFinish() { return (row1Count + row2Count + row3Count) 0) { if (getCount > remainCount) { return -3;// 第x行数量不足 } else { // 更新行数量 updateRemainCount(rowNum, getCount); // 判断是否取完 if (judgeFinish()) { return -1;// 你是输家 } return 0;// 成功取出 } } else { return -2;// 第x行空 } } // 执行回合 // getCount: 取数量 // rowNum: 第几行（1,2,3） function round(rowNum, getCount) { var result = getThing(rowNum, getCount); switch(result) { case 0: alert(\"成功取出\"); break; case -1: alert(\"你是输家\"); break; case -2: alert(\"第\"+rowNum+\"行空\"); break; case -3: alert(\"第\"+rowNum+\"行数量不足\"); break; default: break; } }; // 运行 function run() { var rowNum = prompt(\"取第几行:\",\"\"); var getCount = prompt(\"取多少:\",\"\"); round(rowNum, getCount); } 15个任意物品（可以是火柴牙签poker） 以下按牙签为例 将15根牙签 分成三行 每行自上而下（其实方向不限）分别是3、5、7根 安排两个玩家，每人可以在一轮内，在任意行拿任意根牙签，但不能跨行 拿最后一根牙签的人即为输家 题目 请用你最擅长的语言，以你觉得最优雅的方式写一个符合以上游戏规则的程序。完成后把写好的代码和简历同时发到以下邮箱（备注姓名+岗位），并加上一段简短的文字描述一下你的想法 （请使用javascript，typescript或C#的其中一种语言完成测试题） "},"pages/面试系列/sizeof.html":{"url":"pages/面试系列/sizeof.html","title":"sizeof","keywords":"","body":"sizeof 面试官：定义一个空的类型，里面没有任何成员变量和成员函数。对 该类型求sizeof，得到的结果是多少？应聘者：答案是1。 面试官：为什么不是0？应聘者：空类型的实例中不包含任何信息，本来求sizeof应该是0，但是当我们声明该类型的实例的时候，它必须在内存中占有一定的空间，否则无法使用这些实例。至于占用多少内存，由编译器决定。在Visual Studio中，每个空类型的实例占用1字节的空间。 面试官：如果在该类型中添加一个构造函数和析构函数，再对该类型求sizeof,得到的结果又是多少？应聘者：和前面一样，还是1。调用构造函数和析构函数只需要知道函数的地址即可，而这些函数的地址只与类型相关，而与类型的实例无关，编译器也不会因为这两个函数而在实例内添加任何额外的信息。 面试官：那如果把析构函数标记为虚函数呢？应聘者：C+的编译器一旦发现一个类型中有虚函数，就会为该类型生成虚函数表，并在该类型的每一个实例中添加一个指向虚函数表的指针。在32位的机器上，一个指针占4字节的空间，因此求sizeof得到4；如果是64位的机器，则一个指针占8字节的空间，因此求sizeof得到8。 测试代码： #include using namespace std; class X { }; class Y: public virtual X { }; class Z: public virtual X { }; class A: public Y, public Z { }; int main() { int x = 0; x = sizeof(X); cout 在Xcode上的执行结果： x：1 y：8 z：8 a：16 参考链接：https://blog.csdn.net/zhuiqiuzhuoyue583/article/details/92846054 "},"pages/面试系列/复制构造函数.html":{"url":"pages/面试系列/复制构造函数.html","title":"复制构造函数","keywords":"","body":"复制构造函数 比如，面试官递给应聘者一张有如下代码的A4打印纸要求他分析编译运行的结果，并提供3个选项：A.编译错误；B.编译成功，运行时程序崩溃；C.编译运行正常，输出10。 在上述代码中，复制构造函数A(A other)传入的参数是A的一个实例。由于是传值参数，我们把形参复制到实参会调用复制构造函数。因此，如果允许复制构造函数传值，就会在复制构造函数内调用复制构造函数，就会形成永无休止的递归调用从而导致栈溢出。因此，C+的标准不允许复制构造函数传值参数，在Visual Studio和GCC中，都将编译出错。要解决这个问题，我们可以把构造函数修改为A(const A&other),也就是把传值参数改成常量引用。 "},"pages/面试系列/赋值运算符函数.html":{"url":"pages/面试系列/赋值运算符函数.html","title":"赋值运算符函数","keywords":"","body":"赋值运算符函数 题目：如下为类型CMyString的声明，请为该类型添加赋值运算符函数。 class CMyString { public: CMyString(char* pData = nullptr); CMyString(const CMyString& str); ~CMyString(void); CMyString& operator = (const CMyString& str); void Print(); private: char* m_pData; }; 当面试官要求应聘者定义一个赋值运算符函数时，他会在检查应聘者写出的代码时关注如下几点： 是否把返回值的类型声明为该类型的引用，并在函数结束前返回实例自身的引用(*this)。只有返回一个引用，才可以允许连续赋值。否则，如果函数的返回值是void,则应用该赋值运算符将不能进行连续赋值。假设有3个CMyString的对象：strl、str2和str3,在程序中语句str1=str2=str3将不能通过编译。 是否把传入的参数的类型声明为常量引用。如果传入的参数不是引用而是实例，那么从形参到实参会调用一次复制构造函数。把参数声明为引用可以避免这样的无谓消耗，能提高代码的效率。同时，我们在赋值运算符函数内不会改变传入的实例的状态，因此应该为传入的引用参数加上const关键字。 是否释放实例自身已有的内存。如果我们忘记在分配新内存之前释放自身已有的空间，则程序将出现内存泄漏。 判断传入的参数和当前的实例(*this)是不是同一个实例。如果是同一个，则不进行赋值操作，直接返回。如果事先不判断就进行赋值，那么在释放实例自身内存的时候就会导致严重的问题：当*this和传入的参数是同一个实例时，一旦释放了自身的内存，传入的参数的内存也同时被释放了，因此再也找不到需要赋值的内容了。 经典的解法，适用于初级程序员 当我们完整地考虑了上述4个方面之后，可以写出如下的代码： CMyString& CMyString::operator = (const CMyString& str) { if(this == &str) return *this; delete []m_pData; m_pData = nullptr; m_pData = new char[strlen(str.m_pData) + 1]; strcpy(m_pData, str.m_pData); return *this; } 考虑异常安全性的解法，高级程序员必备 在前面的函数中，我们在分配内存之前先用delete释放了实例m_pData的内存。如果此时内存不足导致new char抛出异常，则m_pData将是一个空指针，这样非常容易导致程序崩溃。也就是说，一旦在赋值运算符函数内部抛出一个异常，CMyString的实例不再保持有效的状态，这就违背了异常安全性(Exception Safety)原则。 要想在赋值运算符函数中实现异常安全性，我们有两种方法。一种简单的办法是我们先用new分配新内容，再用delete释放已有的内容。这样只在分配内容成功之后再释放原来的内容，也就是当分配内存失败时我们能确保CMyString的实例不会被修改。我们还有一种更好的办法，即先创建一个临时实例，再交换临时实例和原来的实例。下面是这种思路的参考代码： CMyString& CMyString::operator = (const CMyString& str) { if(this != &str) { CMyString strTemp(str); char*pTemp=strTemp.m_pData; strTemp.m_pData=m_pData; m_pData=pTemp; } return *this; } 在这个函数中，我们先创建一个临时实例strTemp,接着把strTemp.m_pData和实例自身的m_pData进行交换。由于strTemp是一个局部变量，但程序运行到if的外面时也就出了该变量的作用域，就会自动调用strTemp的析构函数，把strTemp.m_pData所指向的内存释放掉。由于strTemp.m_pData指向的内存就是实例之前m_pData的内存，这就相当于自动调用析构函数释放实例的内存。 在新的代码中，我们在CMyString的构造函数里用new分配内存。如果由于内存不足抛出诸如bad alloc等异常，但我们还没有修改原来实例的状态，因此实例的状态还是有效的，这也就保证了异常安全性。 源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/01_AssignmentOperator "},"pages/面试系列/实现Singleton模式.html":{"url":"pages/面试系列/实现Singleton模式.html","title":"实现Singleton模式","keywords":"","body":"实现Singleton模式 题目：设计一个类，我们只能生成该类的一个实例： 不好的解法一：只适用于单线程环境 由于要求只能生成一个实例，因此我们必须把构造函数设为私有函数以禁止他人创建实例。我们可以定义一个静态的实例，在需要的时候创该实例。下面定义类型Singleton1就是基于这个思路的实现： public sealed class Singleton1 { private Singleton1() { } private static Singleton1 instance = null; public static Singleton1 Instance { get { if (instance == null) instance = new Singleton1(); return instance; } } } 上述代码在Singleton1的静态属性instance中，只有在instance为的时候才创建个实例以避免重复创建。同时我们把构造函数定义为私有函数，这样就能确保只创建一个实例。 不好的解法二：虽然在多线程环境中能工作，但效率不高 解法一中的代码在单线程的时候工作正常，但在多线程的情况下就有问题了。设想如果两个线程同时运行到判断instance是否为null的if语句，并且instance的确没有创建时，那么两个线程都会创建一个实例，此时类型Singleton1就不再满足单例模式的要求了。为了保证在多线程环境下我们还是只能得到类型的一个实例，需要加上一个同步锁。把Singleton1稍作修改得到了如下代码： public sealed class Singleton2 { private Singleton2() { } private static readonly object syncObj = new object(); private static Singleton2 instance = null; public static Singleton2 Instance { get { lock (syncObj) { if (instance == null) instance = new Singleton2(); } return instance; } } } 我们还是假设有两个线程同时想创建一个实例。由于在一个时刻只有一个线程能得到同步锁，当第一个线程加上锁时，第二个线程只能等待。当第一个线程发现实例还没有创建时，它创建出一个实例。接着第一个线程释放同步锁，此时第二个线程可以加上同步锁，并运行接下来的代码。这时候由于实例已经被第一个线程创建出来了，第二个线程就不会重复创建实例了，这样就保证了我们在多线程环境中也只能得到一个实例。 但是类型Singleton2还不是很完美。我们每次通过属性Instance得到Singleton2的实例，都会试图加上一个同步锁，而加锁是一个非常耗时的操作，在没有必要的时候我们应该尽量避免。 可行的解法：加同步锁前后两次判断实例是否已存在 我们只是在实例还没有创建之前需要加锁操作，以保证只有一个线程创建出实例。而当实例已经创建之后，我们已经不需要再执行加锁操作了。于是我们可以把解法二中的代码再做进一步的改进： public sealed class Singleton3 { private Singleton3() { } private static object syncObj = new object(); private static Singleton3 instance = null; public static Singleton3 Instance { get { if (instance == null) { lock (syncObj) { if (instance == null) instance = new Singleton3(); } } return instance; } } } Singleton3中只有当instance为null即没有创建时，需要加锁操作。当instance已经创建出来之后，则无须加锁。因为只在第一次的时候instance为null，因此只在第一次试图创建实例的时候需要加锁。这样Singleton3的时间效率比Singleton2要好很多。 Singleton3用加锁机制来确保在多线程环境下只创建一个实例，并且用两个if判断来提高效率。这样的代码实现起来比较复杂，容易出错，我们还有更加优秀的解法。 强烈推荐的解法一：利用静态构造函数 C#的语法中有一个函数能够确保只调用一次，那就是静态构造函数，我们可以利用C#的这个特性实现单例模式。 public sealed class Singleton4 { private Singleton4() { Console.WriteLine(\"An instance of Singleton4 is created.\"); } public static void Print() { Console.WriteLine(\"Singleton4 Print\"); } private static Singleton4 instance = new Singleton4(); public static Singleton4 Instance { get { return instance; } } } Singleton4的实现代码非常简洁。我们在初始化静态变量instance的时候创建一个实例。由于C#是在调用静态构造函数时初始化静态变量，.NET运行时能够确保只调用一次静态构造函数，这样我们就能够保证只初始化一次instance。 C#中调用静态构造函数的时机不是由程序员掌控的，而是当.NET运行时发现第一次使用一个类型的时候自动调用该类型的静态构造函数。因此在Singleton4中，实例instance并不是在第一次调用属性Singleton4.Instance的时候被创建的，而是在第一次用到Singleton4的时候就会被创建。假设我们在Singleton4中添加一个静态方法，调用该静态函数是不需要创建一个实例的，但如果按照Singleton4的方式实现单例模式，则仍然会过早地创建实 例，从而降低内存的使用效率。 强烈推荐的解法二：实现按需创建实例 最后一个实现Singleton5则很好地解决了Singleton4中的实例创建时机过早的问题。 public sealed class Singleton5 { Singleton5() { Console.WriteLine(\"An instance of Singleton5 is created.\"); } public static void Print() { Console.WriteLine(\"Singleton5 Print\"); } public static Singleton5 Instance { get { return Nested.instance; } } class Nested { static Nested() { } internal static readonly Singleton5 instance = new Singleton5(); } } 在上述Singleton5的代码中，我们在内部定义了一个私有类型Nested。当第一次用到这个嵌套类型的时候，会调用静态构造函数创建Singleton5的实例instance。类型Nested只在属性Singleton5.Instance中被用到，由于其私有属性，他人无法使用Nested类型。因此，当我们第一次试图通过属性Singleton5.Instance得到Singleton5的实例时，会自动调用Nested的静态构造函数创建实例instance。如果我们不调用属性Singleton5.Instance，就不会触发.NET运行时调用Nested，也不会创建实例，这样就真正做到了按需创建。 解法比较 在前面的5种实现单例模式的方法中，第一种方法在多线程环境能正常工作，第二种模式虽然能在多线程环境中正常工作，但时间效率很低，都不是面试官期待的解法。在第三种方法中，我们通过两次判断一次加锁确保在多线程环境中能高效率地工作。第四种方法利用C#的静态构造函数的特性，确保只创建一个实例。第五种方法利用私有嵌套类型的特性，做到只在真正需要的时候才会创建实例，提高空间使用效率。如果在面试中给出第四种或者第五种解法，则毫无疑问会得到面试官的青睐。 源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/02_Singleton "},"pages/面试系列/排序算法.html":{"url":"pages/面试系列/排序算法.html","title":"排序算法","keywords":"","body":"排序算法 冒泡排序 import Foundation func bubbleSort (arr: inout [Int]) { for i in 0.. arr[j+1] { arr.swapAt(j, j+1) } } } } // 测试调用 func testSort () { // 生成随机数数组进行排序操作 var list:[Int] = [] for _ in 0...99 { list.append(Int(arc4random_uniform(100))) } print(\"\\(list)\") bubbleSort(arr:&list) print(\"\\(list)\") } testSort() 选择排序 /// 选择排序 /// /// - Parameter list: 需要排序的数组 func selectionSort(_ list: inout [Int]) -> Void { for j in 0.. list[i] { minIndex = i } } list.swapAt(j, minIndex) } } 插入排序 func insertSort(list: inout [Int]) { for i in 1.. temp { list.swapAt(j, j+1) } } } } 希尔排序 public func insertSort(_ list: inout[Int], start: Int, gap: Int) { for i in stride(from: (start + gap), to: list.count, by: gap) { let currentValue = list[i] var pos = i while pos >= gap && list[pos - gap] > currentValue { list[pos] = list[pos - gap] pos -= gap } list[pos] = currentValue } } public func shellSort(_ list: inout [Int]) { var sublistCount = list.count / 2 while sublistCount > 0 { for pos in 0.. 快速排序 func quicksort(_ a: [T]) -> [T] { guard a.count > 1 else { return a } let pivot = a[a.count/2] let less = a.filter { $0 pivot } return quicksort(less) + equal + quicksort(greater) } 归并排序 func mergeSort(_ array: [Int]) -> [Int] { guard array.count > 1 else { return array } // 1 let middleIndex = array.count / 2 // 2 let leftArray = mergeSort(Array(array[0.. 堆排序 func heapSort(_ array : inout Array){ //1、构建大顶堆 //从二叉树的一边的最后一个结点开始 for i in (0...(array.count/2-1)).reversed() { //从第一个非叶子结点从下至上，从右至左调整结构 SortSummary.adjustHeap(&array, i, array.count) } //2、调整堆结构+交换堆顶元素与末尾元素 for j in (1...(array.count-1)).reversed() { //将堆顶元素与末尾元素进行交换 array.swapAt(0, j) //重新对堆进行调整 SortSummary.adjustHeap(&array, 0, j) } } //调整大顶堆（仅是调整过程，建立在大顶堆以构建的基础上） func adjustHeap(_ array : inout Array, _ i : Int, _ length : Int){ var i = i //取出当前元素i let tmp = array[i] var k = 2*i+1 //从i结点的左子节点开始，也就是2i+1处开始 while k tmp { array[i] = array[k] //记录当前结点 i = k }else{ break } //下一个结点 k = k*2+1 } //将tmp值放到最终的位置 array[i] = tmp } 参考 Swift算法俱乐部-希尔排序Swift算法俱乐部-归并排序swift算法之排序：（四）堆排序 十大经典排序算法常用算法面试题Swift算法俱乐部-快速排序Sort "},"pages/面试系列/RSA.html":{"url":"pages/面试系列/RSA.html","title":"RSA","keywords":"","body":"RSA 面试官：如何保证用户模块的数据安全？说说你的解决方案RSA算法介绍如何加密传输和存储用户密码 "},"pages/面试系列/面试题链接.html":{"url":"pages/面试系列/面试题链接.html","title":"面试题链接","keywords":"","body":"面试题链接 iOSInterviewsAndDevNotes iOS开发中 weak和assign的区别 做了快5年iOS，这份面试题让我从15K变成了30K 《招聘一个靠谱的iOS》面试题参考答案 iOS性能优化 iOS]NSHashTable和NSMapTable用法 Swift中的unowned和weak "},"pages/面试系列/数组与指针.html":{"url":"pages/面试系列/数组与指针.html","title":"数组与指针","keywords":"","body":"数组与指针 在C/C++中，数组和指针是既相互关联又有区别的两个概念。当我们声明一个数组时，其数组的名字也是一个指针，该指针指向数组的第一个元素。我们可以用一个指针来访问数组。但值得注意的是，C/C++没有记录数组的大小，因此在用指针访问数组中的元素时，程序员要确保没有超出数组的边界。下面通过一个例子来了解数组和指针的区别。运行下面的代码，请问输出是什么？ int GetSize(int data[]) { return sizeof(data); } int main(int argc, const char * argv[]) { int data1[] = {1,2,3,4,5}; int size1 = sizeof(data1); int* data2 = data1; int size2 = sizeof(data2); int size3 = GetSize(data1); printf(\"%d,%d,%d\",size1,size2,size3); return 0; } 答案是输出“20,4,4”。data1是一个数组，sizeof(data1)是求数组的大小。这个数组包含5个整数，每个整数占4字节，因此共占用20字节。data2声明为指针，尽管它指向了数组data1的第一个数字，但它的本质仍然是一个指针。在32位系统上，对任意指针求sizeof，得到的结果都是4。在C/C++中，当数组作为函数的参数进行传递时，数组就自动退化为同类型的指针。因此，尽管函数GetSize的参数data被声明为数组，但它会退化为指针，size3的结果仍然是4。 剑指 Offer P38 "},"pages/面试系列/数组中重复的数字.html":{"url":"pages/面试系列/数组中重复的数字.html","title":"数组中重复的数字","keywords":"","body":"数组中重复的数字 题目一：找出数组中重复的数字。 在一个长度为n的数组里的所有数字都在0~n-1的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是重复的数字2或者3。 解决这个问题的一个简单的方法是先把输入的数组排序。从排序的数组中找出重复的数字是一件很容易的事情，只需要从头到尾扫描排序后的数组就可以了。排序一个长度为n的数组需要O(nlogn)的时间。 还可以利用哈希表来解决这个问题。从头到尾按顺序扫描数组的每个数字，每扫描到一个数字的时候，都可以用O(1)的时间来判断哈希表里是否已经包含了该数字。如果哈希表里还没有这个数字，就把它加入哈希表。如果哈希表里已经存在该数字，就找到一个重复的数字。这个算法的时间复杂度是O(n)，但它提高时间效率是以一个大小为O(n)的哈希表为代价的。我们再看看有没有空间复杂度是O(1)的算法。 我们注意到数组中的数字都在0~n-1的范围内。如果这个数组中没有重复的数字，那么当数组排序之后数字i将出现在下标为i的位置。由于数组中有重复的数字，有些位置可能存在多个数字，同时有些位置可能没有数字。 现在让我们重排这个数组。从头到尾依次扫描这个数组中的每个数字。当扫描到下标为i的数字时，首先比较这个数字（用m表示）是不是等于i。如果是，则接着扫描下一个数字；如果不是，则再拿它和第m个数字进行比较。如果它和第m个数字相等，就找到了一个重复的数字（该数字在下标为i和m的位置都出现了)；如果它和第m个数字不相等，就把第i个数字和第m个数字交换，把m放到属于它的位置。接下来再重复这个比较、交换的过程，直到我们发现一个重复的数字。 以数组{2,3,1,0,2,5,3}为例来分析找到重复数字的步骤。数组的第0个数字（从0开始计数，和数组的下标保持一致）是2，与它的下标不相等，于是把它和下标为2的数字1交换。交换之后的数组是{1,3,2,0,2,5,3}。此时第0个数字是1，仍然与它的下标不相等，继续把它和下标为1的数字3交换，得到数组{3,1,2,0,2,5,3}。接下来继续交换第0个数字3和第3个数字0，得到数组{0,1,2,3,2,5,3}。此时第0个数字的数值为0，接着扫描下一个数字。在接下来的几个数字中，下标为1、2、3的3个数字分别为1、2、3，它们的下标和数值都分别相等，因此不需要执行任何操作。接下来扫描到下标为4的数字2。由于它的数值与它的下标不相等，再比较它和下标为2的数字。注意到此时数组中下标为2的数字也是2，也就是数字2在下标为2和下标为4的两个位置都出现了，因此找到一个重复的数字。 上述思路可以用如下代码实现： // 参数: // numbers: 一个整数数组 // length: 数组的长度 // duplication: (输出) 数组中的一个重复的数字 // 返回值: // true - 输入有效，并且数组中存在重复的数字 // false - 输入无效，或者数组中没有重复的数字 bool duplicate(int numbers[], int length, int* duplication) { if(numbers == nullptr || length length - 1) return false; } for(int i = 0; i 在上述代码中，找到的重复数字通过参数duplication传给函数的调用者，而函数的返回值表示数组中是否有重复的数字。当输入的数组中存在 重复的数字时，返回true；否则返回false。代码中尽管有一个两重循环，但每个数字最多只要交换两次就能找到属于它自己的位置，因此总的时间复杂度是O(n)。另外，所有的操作步骤都是在输入数组上进行的，不需要额外分配内存，因此空间复杂度为O(1)。 剑指 Offer P39，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/03_01_DuplicationInArray "},"pages/面试系列/数组中重复的数字2.html":{"url":"pages/面试系列/数组中重复的数字2.html","title":"数组中重复的数字2","keywords":"","body":"数组中重复的数字2 题目二，不修改数组找出重复的数字。 在一个长度为n+1的数组里的所有数字部在1~n的范面内，所以数组中至少有一个数字是重复的，请找出数组中任意一个重复的数字，但不能修改输入的数组。例如，如果输入长度为8的数组{2,3,5,4,3,2,6,7}，那么对应的输出是重复的数字2或者3。 这一题看起来和上面的面试题类似。由于题目要求不能修改输入的数组，我们可以创建一个长度为n+1的辅助数组，然后逐一把原数组的每个数字复制到辅助数组。如果原数组中被复制的数字是m，则把它复制到辅助数组中下标为m的位置。这样很容易就能发现哪个数字是重复的。由于需要创建一个数组，该方案需要O(n)的辅助空间。 接下来我们尝试避免使用O(n)的辅助空间。为什么数组中会有重复的数字？假如没有重复的数字，那么在从1~n的范围里只有n个数字。由于数组里包含超过n个数字，所以一定包含了重复的数字。看起来在某范围里数字的个数对解决这个问题很重要。 我们把从1~n的数字从中间的数字m分为两部分，前面一半为1~m，后面一半为m+1~n。如果1~m的数字的数目超过m,那么这一半的区间里一定包含重复的数字；否则，另一半m+1~n的区间里一定包含重复的数字。我们可以继续把包含重复数字的区间一分为二，直到找到一个重复的数字。这个过程和二分查找算法很类似，只是多了一步统计区间里数字的数目。 我们以长度为8的数组{2,3,5,4,3,2,6,7}为例分析查找的过程。根据题目要求，这个长度为8的所有数字都在1~7的范围内。中间的数字4把1~7的范围分为两段，一段是1~4，另一段是5~7。接下来我们统计1～4这4个数字在数组中出现的次数，它们一共出现了5次，因此这4个数字中一定有重复的数字。 接下来我们再把1~4的范围一分为二，一段是1、2两个数字，另一段是3、4两个数字。数字1或者2在数组中一共出现了两次。我们再统计数字3或者4在数组中出现的次数，它们一共出现了三次。这意味着3、4两个数字中一定有一个重复了。我们再分别统计这两个数字在数组中出现的次数。接着我们发现数字3出现了两次，是一个重复的数字。 上述思路可以用如下代码实现： int countRange(const int* numbers, int length, int start, int end); // 参数: // numbers: 一个整数数组 // length: 数组的长度 // 返回值: // 正数 - 输入有效，并且数组中存在重复的数字，返回值为重复的数字 // 负数 - 输入无效，或者数组中没有重复的数字 int getDuplication(const int* numbers, int length) { if(numbers == nullptr || length = start) { // 0000 0010 >> 左移1位 0000 0101 int middle = ((end - start) >> 1) + start; int count = countRange(numbers, length, start, middle);// 查找落在二分左区间内个数 //cout 1) return start; else break; } if(count > (middle - start + 1))// 如果落在左区间的个数大于区间范围，则这里面一定有重复，否则就去右区间看看 end = middle; else start = middle + 1; } return -1; } int countRange(const int* numbers, int length, int start, int end) { if(numbers == nullptr) return 0; int count = 0; for(int i = 0; i = start && numbers[i] 上述代码按照二分查找的思路，如果输入长度为n的数组，那么函数countRange将被调用O(logn)次，每次需要O(n)的时间，因此总的时间复杂度是O(nlogn)，空间复杂度为O(1)。和最前面提到的需要O(n)的辅助空间的算法相比，这种算法相当于以时间换空间。 需要指出的是，这种算法不能保证找出所有重复的数字。例如，该算法不能找出数组{2,3,5,4,3,2,6,7}中重复的数字2。这是因为在1~2的范围里有1和2两个数字，这个范围的数字也出现2次，此时我们用该算法不能确定是每个数字各出现一次还是某个数字出现了两次。 从上述分析中我们可以看出，如果面试官提出不同的功能要求（找出任意一个重复的数字、找出所有重复的数字)或者性能要求（时间效率优先、空间效率优先)，那么我们最终选取的算法也将不同。这也说明在面试中和面试官交流的重要性，我们一定要在动手写代码之前弄清楚面试官的需求。 剑指 Offer P41，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/03_02_DuplicationInArrayNoEdit "},"pages/面试系列/二维数组中的查找.html":{"url":"pages/面试系列/二维数组中的查找.html","title":"二维数组中的查找","keywords":"","body":"二维数组中的查找 题目：在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的个二维数组和一个整数，判断数组中是否含有该整数。例如下面的二维数组就是每行、每列都递增排序。 如果在这个数组中查找数字7，则返回true;如果查找数字5，由于数组不含有该数字，则返回false。 1 2 8 9 2 4 9 1 4 7 9 12 6 8 11 15 在分析这个问题的时候，很多应聘者都会把二维数组画成矩形，然后从数组中选取一个数字，分3种情况来分析查找的过程。当数组中选取的数字刚好和要查找的数字相等时，就结束查找过程。如果选取的数字小于要查找的数字，那么根据数组排序的规则，要查找的数字应该在当前选取位置的右边或者下边，如图2.1(a)所示。同样，如果选取的数字大于要查找的数字，那么要查找的数字应该在当前选取位置的上边或者左边，如图2.1(b)所示。 图2.1二维数组中的查找 注：在数组中间选择一个数（深色方格），根据它的大小判断要查找的数字可能出现的区域（阴影部分）。 在上面的分析中，由于要查找的数字相对于当前选取的位置有可能在两个区域中出现，而且这两个区域还有重叠，这问题看起来就复杂了，于是很多人就卡在这里束手无策了。 当我们需要解决一个复杂的问题时，一个很有效的办法就是从一个具体的问题入手，通过分析简单具体的例子，试图寻找普遍的规律。针对这个问题，我们不妨也从一个具体的例子入手。下面我们以在题目中给出的数组中查找数字7为例来一步步分析查找的过程。 前面我们之所以遇到难题，是因为我们在二维数组的中间选取一个数字来和要查找的数字进行比较，这就导致下一次要查找的是两个相互重叠的区域。如果我们从数组的一个角上选取数字来和要查找的数字进行比较，那么情况会不会变简单呢？ 首先我们选取数组右上角的数字9。由于9大于7，并且9还是第4列的第一个（也是最小的）数字，因此7不可能出现在数字9所在的列。于是我们把这一列从需要考虑的区域内剔除，之后只需要分析剩下的3列，如图2.2(a)所示。在剩下的矩阵中，位于右上角的数字是8。同样8大于7，因此8所在的列我们也可以剔除。接下来我们只要分析剩下的两列即可，如图2.2(b)所示。 在由剩余的两列组成的数组中，数字2位于数组的右上角。2小于7，那么要查找的7可能在2的右边，也可能在2的下边。在前面的步骤中，我们已经发现2右边的列都已经被剔除了，也就是说7不可能出现在2的右边，因此7只有可能出现在2的下边。于是我们把数字2所在的行也剔除，只分析剩下的三行两列数字，如图2.2(c)所示。在剩下的数字中，数字4位于右上角，和前面一样，我们把数字4所在的行也删除，最后剩下两行两列数字，如图2.2（d)所示。 在剩下的两行两列4个数字中，位于右上角的刚好就是我们要查找的数字7，于是查找过程就可以结束了。 注：矩阵中加阴影的区域是下一步查找的范围。 总结上述杏找的讨程，我们发现如下规律：首先选取数组中右上角的数字。如果该数字等于要查找的数字，则查找过程结束；如果该数字大于要查找的数字，则剔除这个数字所在的列；如果该数字小于要查找的数字，则剔除这个数字所在的行。也就是说，如果要查找的数字不在数组的右上角，则每一次都在数组的查找范围中剔除一行或者一列，这样每一步都可以缩小查找的范围，直到找到要查找的数字，或者查找范围为空。 把整个查找过程分析清楚之后，我们再写代码就不是一件很难的事情了。下面是上述思路对应的参考代码： bool Find(int* matrix, int rows, int columns, int number) { bool found = false; if(matrix != nullptr && rows > 0 && columns > 0) { int row = 0; int column = columns - 1; while(row =0) { if(matrix[row * columns + column] == number) { found = true; break; } else if(matrix[row * columns + column] > number) -- column; else ++ row; } } return found; } 在前面的分析中，我们每次都选取数组查找范围内的右上角数字。同样，我们也可以选取左下角的数字。感兴趣的读者不妨自己分析一下每次都选取左下角数字的查找过程。但我们不能选择左上角数字或者右下角数字。以左上角数字为例，最初数字1位于初始数组的左上角，由于1小于7，那么7应该位于1的右边或者下边。此时我们既不能从查找范围内剔除1所在的行，也不能剔除1所在的列，这样我们就无法缩小查找的范围。 剑指 Offer P44，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/04_FindInPartiallySortedMatrix "},"pages/面试系列/替换空格.html":{"url":"pages/面试系列/替换空格.html","title":"替换空格","keywords":"","body":"替换空格 题目：请实现一个函数，把字符串中的每个空格替换成\"%20\"。例如，输入“We are happy.”，则输出“We%20are%20happy.”。 在网络编程中，如果URL参数中含有特殊字符，如空格、'#'等，则可能导致服务器端无法获得正确的参数值。我们需要将这些特殊符号转换成服务器可以识别的字符。转换的规则是在'%'后面跟上ASCII码的两位十六进制的表示。比如空格的ASCI码是32，即十六进制的0x20,因此空格被替换成\"%20\"。再比如'#'的ASCⅡ码为35，即十六进制的0x23，它在URL中被替换为\"%23\"。 看到这个题目，我们首先应该想到的是原来一个空格字符，替换之后变成'%'、'2'和'0'这3个字符，因此字符串会变长。如果是在原来的字符串上进行替换，就有可能覆盖修改在该字符串后面的内存。如果是创建新的字符串并在新的字符串上进行替换，那么我们可以自己分配足够多的内存。由于有两种不同的解决方案，我们应该向面试官问清楚，让他明确告诉我们他的需求。假设面试官让我们在原来的字符串上进行替换，并且保证输入的字符串后面有足够多的空余内存。 时间复杂度为O(n²)的解法，不足以拿到Offer 现在我们考虑怎么执行替换操作。最直观的做法是从头到尾扫描字符串，每次碰到空格字符的时候进行替换。由于是把1个字符替换成3个字符，我们必须要把空格后面所有的字符都后移2字节，否则就有两个字符被覆盖了。 举个例子，我们从头到尾把\"We are happy.\"中的每个空格替换成\"%20\"。为了形象起见，我们可以用一个表格来表示字符串，表格中的每个格子表示一个字符，如图2.3(a)所示。 注：(a)字符串\"We are happy.\"。(b)把字符串中的第一个空格替换成%20。灰色背景表示需要移动的字符。(c)把字符串中的第二个空格替换成%20。浅灰色背景表示需要移动一次的字符，深灰色背景表示需要移动两次的字符。 我们替换第一个空格，这个字符串变成图2.3（b）中的内容，表格中灰色背景的格子表示需要进行移动的区域。接着我们替换第二个空格，替换之后的内容如图2.3（c）所示。同时，我们注意到用深灰色背景标注的“happy”部分被移动了两次。 假设字符串的长度是n。对每个空格字符，需要移动后面O(n)个字符，因此对于含有O(n)个空格字符的字符串而言，总的时间效率是O(n²)。 当我们把这种思路阐述给面试官后，他不会就此满意，他将让我们寻找更快的方法。在前面的分析中，我们发现数组中很多字符都移动了很多次，能不能减少移动次数呢？答案是肯定的。我们换一种思路，把从前向后替换改成从后向前替换。 时间复杂度为O(n)的解法，搞定Offer就靠它了 我们可以先遍历一次字符串，这样就能统计出字符串中空格的总并可以由此计算出替换之后的字符串的总长度。每替换一个空格，长度增加2，因此替换以后字符串的长度等于原来的长度加上2乘以空格数目。我们还是以前面的字符串\"We are happy.\"为例。\"We are happy.\"这个字符串的长度是14（包括结尾符号0），里面有两个空格，因此替换之后字符串的长度是18。 我们从字符串的后面开始复制和替换。首先准备两个指针：P1和P2。P1指向原始字符串的末尾，而P2指向替换之后的字符串的末尾，如图2.4(a)所示。接下来我们向前移动指针P1，逐个把它指向的字符复制到P2指向的位置，直到碰到第一个空格为止。此时字符串如图2.4(b)所示，灰色背景的区域是进行了字符复制（移动）的区域。碰到第一个空格之后，把P1向前移动1格，在P2之前插入字符串\"%20\"。由于\"%20\"的长度为3，同时也要把P2向前移动3格，如图2.4(c)所示。 我们接着向前复制，直到碰到第二个空格，如图2.4(d)所示。和上一次一样，我们再把P1向前移动1格，并把P2向前移动3格插入\"%20”，如图2.4（e）所示。此时P1和P2指向同一位置，表明所有空格都已经替换完毕。 从上面的分析中我们可以看出，所有的字符都只复制（移动）一次，因此这个算法的时间效率是O(n)，比第一个思路要快。 注：图中带有阴影的区域表示被移动的字符。(a)把第一个指针指向字符串的末尾，把第二个指针指向替换之后的字符串的末尾。(b)依次复制字符串的内容，直至第一个指针碰到第一个空格。(c)把第一个空格替换成\"%20”，把第一个指针向前移动1格，把第二个指针向前移动3格。(d)依次向前复制字符串中的字符，直至碰到空格。（e）替换字符串中的倒数第二个空格，把第一个指针向前移动1格，把第二个指针向前移动3格。 在面试过程中，我们也可以和前面的分析一样画一两个示意图解释自己的思路，这样既能帮助我们厘清思路，也能使我们和面试官的交流变得更加高效。在面试官肯定我们的思路之后，就可以开始写代码了。下面是参考代码： /*length 为字符数组str的总容量，大于或等于字符串str的实际长度*/ void ReplaceBlank(char str[], int length) { if(str == nullptr && length length) return; int indexOfOriginal = originalLength; int indexOfNew = newLength; while(indexOfOriginal >= 0 && indexOfNew > indexOfOriginal) { if(str[indexOfOriginal] == ' ') { str[indexOfNew --] = '0'; str[indexOfNew --] = '2'; str[indexOfNew --] = '%'; } else { str[indexOfNew --] = str[indexOfOriginal]; } -- indexOfOriginal; } } 剑指 Offer P51，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/05_ReplaceSpaces "},"pages/面试系列/链表.html":{"url":"pages/面试系列/链表.html","title":"链表","keywords":"","body":"链表 链表应该是面试时被提及最频繁的数据结构。链表的结构很简单，它由指针把若干个节点连接成链状结构。链表的创建、插入节点、删除节点等操作都只需要20行左右的代码就能实现，其代码量比较适合面试。而像哈希表、有向图等复杂数据结构，实现它们的一个操作需要的代码量都较大，很难在几十分钟的面试中完成。另外，由于链表是一种动态的数据结构，其需要对指针进行操作，因此应聘者需要有较好的编程功底才能写出完整的操作链表的代码。而且链表这种数据结构很灵活，面试官可以用链表来设计具有挑战性的面试题。基于上述几个原因，很多面试官都特别青睐与链表相关的题目。 我们说链表是一种动态数据结构，是因为在创建链表时，无须知道链表的长度。当插入一个节点时，我们只需要为新节点分配内存，然后调整指针的指向来确保新节点被链接到链表当中。内存分配不是在创建链表时一次性完成的，而是每添加一个节点分配一次内存。由于没有闲置的内存，链表的空间效率比数组高。如果单向链表的节点定义如下： struct ListNode { int m_nValue; ListNode* m_pNext; }; 那么往该链表的末尾添加一个节点的C++代码如下： void AddToTail(ListNode**pHead, int value) { ListNode* pNew = new ListNode(); pNew->m_nValue = value; pNew->m_pNext = nullptr; if(*pHead == nullptr) { *pHead = pNew; } else { ListNode* pNode = *pHead; while(pNode->m_pNext != nullptr) pNode = pNode->m_pNext; pNode->m_pNext = pNew; } } 在上面的代码中，我们要特别注意函数的第一个参数pHead是一个指向指针的指针。当我们往一个空链表中插入一个节点时，新插入的节点就是链表的头指针。由于此时会改动头指针，因此必须把pHead参数设为指向指针的指针，否则出了这个函数pHead仍然是一个空指针。 由于链表中的内存不是一次性分配的，因而我们无法保证链表的内存和数组一样是连续的。因此，如果想在链表中找到它的第i个节点，那么我们只能从头节点开始，沿着指向下一个节点的指针遍历链表，它的时间效率为O(n)。而在数组中，我们可以根据下标在O(1)时间内找到第i个元素。下面是在链表中找到第一个含有某值的节点并删除该节点的代码： void RemoveNode(ListNode** pHead, int value) { if(pHead == nullptr || *pHead == nullptr) return; ListNode* pToBeDeleted == nullptr; if((*pHead)->m_nValue == value) { pToBeDeleted = *pHead; *pHead = (*pHead)->m_pNext; } else { ListNode* pNode = *pHead; while(pNode->m_pNext != nullptr && pNode->m_pNext->m_nValue != value) pNode = pNode->m_pNext; if(pNode->m_pNext != nullptr && pNode->m_pNext->m_nValue == value) { pToBeDeleted = pNode->m_pNext; pNode->m_pNext = pNode->m_pNext->m_pNext; } } if(pToBeDeleted != nullptr) { delete pToBeDeleted; pToBeDeleted = nullptr; } } 剑指 Offer P56 "},"pages/面试系列/从尾到头打印链表.html":{"url":"pages/面试系列/从尾到头打印链表.html","title":"从尾到头打印链表","keywords":"","body":"从尾到头打印链表 题目：输入一个链表的头节点，从尾到头反过来打印出每个节点的值链表节点定义如下： struct ListNode { int m_nKey; ListNode* m_pNext; }; 看到这道题后，很多人的第一反应是从头到尾输出将会比较简单，于是我们很自然地想到把链表中链接节点的指针反转过来，改变链表的方向，然后就可以从头到尾输出了。但该方法会改变原来链表的结构。是否允许在打印链表的时候修改链表的结构？这取决于面试官的要求，因此在面试的时候我们要询问清楚面试官的要求。 面试小提示：在面试中，如果我们打算修改输入的数据，则最好先问面试官是不是允许修改。 通常打印是一个只读操作，我们不希望打印时修改内容。假设面试官也要求这个题目不能改变链表的结构。 接下来我们想到解决这个问题肯定要遍历链表。遍历的顺序是从头到尾，可输出的顺序却是从尾到头。也就是说，第一个遍历到的节点最后一个输出，而最后一个遍历到的节点第一个输出。这就是典型的“后进先出”，我们可以用栈实现这种顺序。每经过一个节点的时候，把该节点放到一个栈中。当遍历完整个链表后，再从栈顶开始逐个输出节点的值，此时输出的节点的顺序己经反转过来了。这种思路的实现代码如下： #include \"..\\Utilities\\List.h\" #include void PrintListReversingly_Iteratively(ListNode* pHead) { std::stack nodes; ListNode* pNode = pHead; while(pNode != nullptr) { nodes.push(pNode); pNode = pNode->m_pNext; } while(!nodes.empty()) { pNode = nodes.top(); printf(\"%d\\t\", pNode->m_nValue); nodes.pop(); } } 既然想到了用栈来实现这个函数，而递归在本质上就是一个栈结构，于是很自然地又想到了用递归来实现。要实现反过来输出链表，我们每访问到一个节点的时候，先递归输出它后面的节点，再输出该节点自身，这样链表的输出结果就反过来了。 基于这样的思路，不难写出如下代码： void PrintListReversingly_Recursively(ListNode* pHead) { if(pHead != nullptr) { if (pHead->m_pNext != nullptr) { PrintListReversingly_Recursively(pHead->m_pNext); } printf(\"%d\\t\", pHead->m_nValue); } } 上面的基于递归的代码看起来很简洁，但有一个问题：当链表非常长的时候，就会导致函数调用的层级很深，从而有可能导致函数调用栈溢出。显然用栈基于循环实现的代码的鲁棒性要好一些。更多关于循环和递归的讨论，详见本书的2.4.1节。 剑指 Offer P58，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/06_PrintListInReversedOrder "},"pages/面试系列/面试准备.html":{"url":"pages/面试系列/面试准备.html","title":"面试准备","keywords":"","body":"面试准备 自我介绍 谢谢您今天给我的这次机会，我是Green，我的专业是计算机通信，9年iOS开发经验，3年Java开发经验，担任过iOS团队负责人，有团队管理经验，有组件化开发经验，具备独立开发能力，具备文档编写能力，有良好的代码习惯。 Thank you for giving me the opportunity to be interviewed for this position today. I'm Green. My major is computer communication. I have 9 years of experience in iOS development, 3 years of experience in Java development, and served as the leader of the iOS team. I have experience in team management, component development, independent development, document writing, and good code habits. 我大学的专业是计算机通信，10年来到上海从事Java后端方面的工作，做了几个OA功能的系统，主要使用的技术有JavaWeb相关的技术，像JAVA、SpringMVC、HTMl、JS等。从事Java大概3年多，我自学进入到了iOS行业，期间做过多媒体拍摄项目，RTC直播项目，对自研播放器有所了解。我掌握的技术章有OC、Swift，熟悉C、C++编程，掌握组件化编程，能够独立开发iOS项目。最近也学习了Flutter相关知识，通过阅读开源项目，对Flutter技术栈了解。 擅长技术 熟练OC、Swift、Java、RxSwift、MVVM、MVC、Cocoapods、GCD、Git Flow。熟悉C、C++、Maven、HTML、JS、CSS、SQL、Mysql、Tomcat、JDBC、SSH。了解Android、Flutter、ReactNative、libRTMP、OpenGLES、FFmpeg。 视频播放器 组件化开发 设计模式 排序算法 数据结构 RSA CICD SwiftUI 敏捷开发 如何学习 在工作中，如果遇到自己不会的问题，通过Google、百度相关资料，然后总结记录。学习一门新技术，我会通过查看其官方教程或找学习视频，同时买一本书籍阅读，然后实践，再根据自己的理解总结写文章记录。 职业规划 结合自身还有目前的职业环境，我有认真想过这个问题。在工作方面，我想通过积极完成工作任务，积累各方面经验，让自己成为这个领域的专业人士，也希望有机会能够带领团队，成为优秀的管理者，为单位作出更大的贡献，实现双赢。在学习方面，打算在iOS专业领域做进一步学习和研究，同时也学习Android、H5等技术，为以后自己成为管理者做下铺垫。 提问环节 项目中会使用SwiftUI吗？ 项目是针对海外客户吗？ 一个功能需求下来，在开发人员开发着前，您最希望它做好哪些开发准备？ 项目中难免存在一些不得不进行重构优化的代码，您是如何看待这个问题的？ 面试问题 函数式编程：函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！ 线程间通信：内存共享、通知、等待、锁。 swift特性：元组、可选、解包、扩展、泛型、枚举、泛型关联、命名空间、权限关键字、协议、闭包。 内存管理：Swift使用自动引用计数（ARC）来简化内存管理，与OC一致。 swift语言、架构能力、block原理、swift特性、项目管理、代码规范。 内存管理 OC 和 Swift 的弱引用源码分析iOS内存分配-栈和堆 多线程 关于iOS多线程，你看我就够了 参考链接 Swift 语言的一些功能特性为何面试时都会问你的职业规划呢？该如何回答呢？IOS面试题(其他) --- 英文自我介绍 凯捷 代码加固** https://zhuanlan.zhihu.com/p/33109826 1.字符串混淆 对应用程序中使用到的字符串进行加密，保证源码被逆向后不能看出字符串的直观含义。 2.类名、方法名混淆 对应用程序的方法名和方法体进行混淆，保证源码被逆向后很难明白它的真正功能。 3.程序结构混淆加密 对应用程序逻辑结构进行打乱混排，保证源码可读性降到最低。 4.反调试、反注入等一些主动保护策略 这是一些主动保护策略，增大破解者调试、分析App的门槛。 文件名重复会有什么影响 https://blog.csdn.net/weixin_33994429/article/details/93696758 duplicate symbol问题 swift和oc的区别 1.Swift和Objective-C共用一套运行时环境，Swift的类型可以桥接到Objective-C。 2.swift是静态语言，有类型推断，更加安全，OC是动态语言。 3.swift支持泛型，OC只支持轻量泛型 4.Swift速度更快，运算性能更高。 5.Swift的访问权限变更。 7.Swift便捷的函数式编程。 8.swift有元组类型、支持运算符重载 9.swift引入了命名空间。 10.swift支持默认参数。 11.swift比oc代码更加简洁。 struct和class的区别 https://blog.csdn.net/baidu_40537062/article/details/108349757 1.struct是值类型（Value Type）,深拷贝。class是引用类型（Reference Type），浅拷贝。 2.类允许被继承，结构体不允许被继承。 3.类中的每一个成员变量都必须被初始化，否则编译器会报错，而结构体不需要，编译器会自动帮我们生成init函数，给变量赋一个默认值。 4.NSUserDefaults：Struct 不能被序列化成 NSData 对象,无法归解档。 5.当你的项目的代码是 Swift 和 Objective-C 混合开发时，你会发现在 Objective-C 的代码里无法调用 Swift 的 Struct。因为要在 Objective-C 里调用 Swift 代码的话，对象需要继承于 NSObject。 6.class像oc的类一样，可以用kvo,kvc,runtime的相关方法，适用runtime系统。这些struct都不具备。 7.内存分配：struct分配在栈中，class分配在堆中。struct比class更“轻量级”（struct是跑车跑得快，class是SUV可以载更多的人和货物）。 验证HTTPS证书 客户端向服务器发送支持的SSL/TSL的协议版本号，以及客户端支持的加密方法，和一个客户端生成的随机数。 服务器确认协议版本和加密方法，向客户端发送一个由服务器生成的随机数，以及数字证书。 客户端验证证书是否有效，有效则从证书中取出公钥，生成一个随机数，然后用公钥加密这个随机数，发给服务器。 服务器用私钥解密，获取发来的随机数。 客户端和服务器根据约定好的加密方法，使用前面生成的三个随机数，生成对话密钥，用来加密接下来的整个对话过程。 作者：阿拉斯加大狗 链接：https://juejin.cn/post/6844903892765900814 来源：稀土掘金 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 H5信息存储在哪 localStorage与？区别 cookie 图片宽高未知的情况下怎么自适应高度 Model层异步下载图片，然后缓存图片及图片宽高。 图片显示先占位，按需加在图片，缓存图片及图片宽高，reload指定Cell。 上传图片告诉服务器图片尺寸。 通过约束让Cell高度随图片自适应。 push很多页面后怎么控制导航栏里面的子控制器 push到相同页面产生递归。 内存回收处理。 灵动岛 https://www.51cto.com/article/742613.html 展示： 紧凑(Compact)、最小化(Minimal)、扩展(Expanded) 开发框架：ActivityKit（实时活动）、SwiftUI（UI）、WidgetKit（小组件） 实时活动权限 静态库制作 https://www.jianshu.com/p/8ea45370a20d XCFramework H5的http拦截 RSA 1024 2048区别 Git分之管理 支付 "},"pages/面试系列/视频播放器.html":{"url":"pages/面试系列/视频播放器.html","title":"视频播放器","keywords":"","body":"视频播放器 实现一个播放器SDK，要求播放控制界面可定制，提供播放、暂停、停止、Seek等功能。 I帧、P帧、B帧 I帧表示关键帧，帧画面的完整保留，解码时只需要本帧数据就可以完成。 P帧表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。 B帧是双向差别帧，也就是B帧记录的是本帧与前后帧的差别，换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面，B帧压缩率高，但是解码时CPU会比较累。 视频如何播放？ 视频画面其实是由视频帧组成，分别为I帧、P帧、B帧。也就是说，显示视频画面需要对视频压缩格式进行遍历解码帧数据，遍历的同时对每一帧数据按照播放时间进行显示，每一帧可以理解为一张图片，其实就是对图片数据的显示。 视频如何显示？ 不仅是视频，显示问题无处不在，我们所有在手机端看到的画面都和视频一样，是一个不断刷新绘制的过程，底层都是通过使用OpenGL的API，对GPU硬件发出指令，通过图形渲染管线程序，最终在屏幕的每个像素点显示。OpenGL是跨平台的，iOS上是通过GLKView进行渲染显示。 什么是图形渲染管线？ 顶点着色器 —— 图元装配 —— 几何着色器 —— 光栅化 —— 片段着色器 —— 测试与混合。 着色器是一段运行在GPU中的程序；顶点着色器确定绘制图形的形状；图元装配是将顶点着色器传来的顶点数据组装为图元；光栅化是将一个图元转化为一张二维的图片，而这张图片由若干个片段（fragment）组成；片段着色器计算片段的颜色；测试和混合丢弃一些不需要显示的片段。 如何播放全景视频？ 在图形渲染管线中，顶点着色器和片元着色器是可编程的，也就是说我们可以通过顶点着色器构建任意事物模型顶点，然后通过过GPU进行绘制，再通过片元着色器给事物上色（纹理贴图）。普通的视频播放可以理解为就是在一个二维的面显示视频画面，二维的面就是顶点着色器构建的，而画面的显示则交给了片元着色器，那么全景视频的显示，其实就是在顶点着色器环节构建一个球面即可实现播放全景视频。 iOS上如何对MP4文件播放？ AVFoundation中的AVPlayer提供对视频压缩文件的播放。以下是播放步骤： 1）构建AVPlayerItem实例。2）通过KVO监听AVPlayerItem实例加载的状态，加载成功，否则播放失败，最后移除监听。3）构建播放器AVPlayer实例。4）为AVPlayerItem实例增加AVPlayerItemVideoOutput实例，加入异步队列。5）构建屏幕定时器，通过AVPlayerItemVideoOutput实例获取当前视频帧数据，交给OpengGL渲染。6）监听播放进度，播放结束。7）开始播放。 如何使用FFmpeg对MP4文件播放？ FFmpeg提供ffplay命令可以对几乎所有的视频压缩格式进行播放，包括yuv格式。 播放器设计 播发器类：CPVideoPlayer，提供播放、暂停、停止、Seek等方法。协议一：CPVideoPlayerDelegate，回调播放状态及视频数据。协议二：CPVideoPlayerDataSource，获取视频地址。协议三：CPVideoControlViewProtocol，播放控制界面。 播放器weak引用上面三个协议。 参考链接 一看就懂的OpenGL ES教程——图形渲染管线的那些事YUV图解I帧、P帧、B帧、GOP、IDR 和PTS, DTS之间的关系 "},"pages/面试系列/组件化开发.html":{"url":"pages/面试系列/组件化开发.html","title":"组件化开发","keywords":"","body":"组件化开发 如何优雅的使用CTMeditor？ 我们项目会有多个组件，比如：个人中心（Me），登录（Login）等，现在我想实现如下的写法： Mediator.shared.me Mediator.shared.login 以me为例子，建立Me的结构体，为Me增加扩展，如果Base为Mediator类型，那么可以拥有debugEnvironment方法。 public struct Me { public let base: Base public init(_ base: Base) { self.base = base } } extension Me where Base: Mediator { public func debugEnvironment(_ env: String) -> Bool { var deliverParams: [String: Any] = [\"env\": env] let result = base.performTarget(\"Me\", action: \"DebugEnvironment\", params: deliverParams) as? [String: Any] return (result?[\"result\"] as? Bool) ?? false } } 扩展Mediator，为Mediator增加me的实例 public protocol MeProtocol {} extension Mediator: MeProtocol {} extension MeProtocol { public var me: Me { return Me(self) } public static var me: Me.Type { return Me.self } } 通过以上操作就可以实现Mediator.shared.me.debugEnvironment(\"dev\")的调用。 CTMeditor实现原理？ 以Mediator.shared.me.debugEnvironment(\"dev\")为例分析，通过这段代码Mediator可以找到Target是类名为Target_Me的实例，方法名是debugEnvironment，参数\"dev\"封装成了字典，通过Runtime的perform方法就可以调用到目标逻辑。 蘑菇街组件化方案问题？ 1）蘑菇街没有拆分远程调用和本地间调用。2）蘑菇街以远程调用的方式为本地间调用提供服务。3）蘑菇街的本地间调用无法传递非常规参数，复杂参数的传递方式非常丑陋。4）蘑菇街必须要在app启动时注册URL响应者。5）新增组件化的调用路径时，蘑菇街的操作相对复杂。6）蘑菇街没有针对target层做封装。 组建化方案如何加载xib及其他资源文件？ 通过Class找到Bundle，然后构建Nib的时候传入制定的bundle。 参考链接 iOS应用架构谈 组件化方案 "},"pages/面试系列/设计模式.html":{"url":"pages/面试系列/设计模式.html","title":"设计模式","keywords":"","body":"设计模式 1) 状态模式 在状态模式（State Pattern）中，类的行为是基于它的状态改变的。这种类型的设计模式属于行为型模式。 public class StatePatternDemo { public static void main(String[] args) { Context context = new Context(); StartState startState = new StartState(); startState.doAction(context); System.out.println(context.getState().toString()); StopState stopState = new StopState(); stopState.doAction(context); System.out.println(context.getState().toString()); } } 参考链接 菜鸟·设计模式[译] 使用 Swift 的 iOS 设计模式（第一部分）MVC，MVP 和 MVVM 的图示 "},"pages/面试系列/数据结构.html":{"url":"pages/面试系列/数据结构.html","title":"数据结构","keywords":"","body":"数据结构 "},"pages/面试系列/SwiftUI.html":{"url":"pages/面试系列/SwiftUI.html","title":"SwiftUI","keywords":"","body":"SwiftUI 苹果官方教程swiftui SwiftUI-api SwiftUI-tutorials SwiftUI-互动教程 SwiftUI 基础之05 list 和 searchbar (2020) SwiftUI教程与源码 SwiftUI Search Bar in the Navigation Bar UISearchController Tutorial 弄清 SwiftUI，才看得懂苹果的强大 "},"pages/面试系列/敏捷开发.html":{"url":"pages/面试系列/敏捷开发.html","title":"敏捷开发","keywords":"","body":"敏捷开发 在传统开发中，如果变更需求，需有走需求变更流程，当频繁出现需求时，这个流程走起来时非常辛苦，很有可能导致变更不控制，项目就容易失控，这样是谁都不愿意看到的，新产品开发流程--敏捷开发就这样产生了。 敏捷方法Vs传统方法 传统的写作方式 敏捷的写作方式 确定主题 与读者微博、论坛互动 整理大纲、搭建框架 确定主题 书写内容 与读者互动、收集反馈 设计、排版、校对 试写第一章 出版 与读者互动、收集反馈 与读者见面 试写第二章 收集反馈，在版本 ... 初步设计、排版 与读者互动、收集反馈 出版 传统方法的问题：需求不是一尘不变的，等做完了发现不是产品想要的。 敏捷方法的好处：早期交付，较低成本；增加与客户的交互，降低产品不适用的风险；传统方法开发到一半的时候，代码不可用，而敏捷已交付的可用。 生命周期类型 敏捷型：交付频率高、变更程度高。技术不确定、需求确定或者技术确定、需求不确定的项目适用于敏捷开发。 敏捷思维 4大价值观，12大原则，2000多实践。 敏捷思维模式由价值观定义，以原则为指导，并在许多不同的实践中来体现。敏捷实践者根据自身需求选择不同的实践。 敏捷宣言 通过运用此法及帮助他人运用此法，我们正在探寻更好的软件开发方法。在这项工作中，我们看重“个体以及互动”胜过“流程和工具”，“可工作的软件”胜过“完整的文档”，“客户合作”胜过“合同谈判”，“响应变化”胜过“遵循计划”。 4大价值观 以人为本（个体以及互动）、以价值为导向（可工作的软件）、合作共赢（客户合作）、拥抱变化（响应变化）。 12条原则 1.通过早期和持续交付有价值的软件，实现客户满意度。2.欢迎不断变化的需求，即使是在项目开发的后期。要善于利用需求变更，帮助客户获得竞争优势。3.不断交付可用的软件，周期通常是几周，越短越好。4.项目过程中，业务人员与开发人员必须在一起工作。5.项目必须围绕那些有内在动力的个人而建立，他们应该受到信任。6.面对面交谈是最好的沟通方式。7.可用性是衡量进度的主要指标。8.提倡可持续的开发，保持稳定的进展速度。9.不断关注技术是否优秀，设计是否良好。10.简单性至关重要，尽最大可能减少不必要的工作。11.最好的架构、要求和设计，来自团队内部自发的认识。12.团队要定期反思如何更有效，并相应地进行调整。 Scrum 3个支柱：透明性、检验、适应。3个角色：产品负责人、敏捷教练、敏捷团队。 3个工件：产品代办事项列表、冲刺待办事项列表、可交付产品增量。5个事件：冲刺、冲刺规划会议、每日站会、迭代评审会议、迭代回顾会议。 参考链接 敏捷开发入门教程抖音@希赛项目管理 "},"pages/面试系列/重建二叉树.html":{"url":"pages/面试系列/重建二叉树.html","title":"重建二叉树","keywords":"","body":"重建二叉树 题目：输入某二叉树的前序遍历和中序遍历的结果，请重建该二又树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如，输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建如图2.6所示的二叉树并输出它的头节点。二叉树节点的定义如下： struct BinaryTreeNode { int m_nValue; BinaryTreeNode* m_pLeft; BinaryTreeNode* m_pRight; }; 在二叉树的前序遍历序列中，第一个数字总是树的根节点的值。但在中序遍历序列中，根节点的值在序列的中间，左子树的节点的值位于根节点的值的左边，而右子树的节点的值位于根节点的值的右边。因此我们需要扫描中序遍历序列，才能找到根节点的值。 图2.6 根据前序遍历序列(1,2,4,7,3,5,6,8}和中序遍历序列4,7,2,1,5,3,8,6}重建的二叉树 如图2.7所示，前序遍历序列的第一个数字1就是根节点的值。扫描中序遍历序列，就能确定根节点的值的位置。根据中序遍历的特点，在根节点的值1前面的3个数字都是左子树节点的值，位于1后面的数字都是右子树节点的值。 由于在中序遍历序列中，有3个数字是左子树节点的值，因此左子树共有3个左子节点。同样，在前序遍历序列中，根节点后面的3个数字就是3个左子树节点的值，再后面的所有数字都是右子树节点的值。这样我们就在前序遍历和中序遍历两个序列中分别找到了左、右子树对应的子序列。 图2.7 在二叉树的前序遍历和中序遍历序列中确定根节点的值、左子树节点的值和右子树节点的值 既然我们已经分别找到了左、右子树的前序遍历序列和中序遍历序列，我们可以用同样的方法分别构建左、右子树。也就是说，接下来的事情可以用递归的方法去完成。 在想清楚如何在前序遍历和中序遍历序列中确定左、右子树的子序列之后，我们可以写出如下的递归代码： BinaryTreeNode* ConstructCore(int* startPreorder, int* endPreorder, int* startInorder, int* endInorder); BinaryTreeNode* Construct(int* preorder, int* inorder, int length) { if(preorder == nullptr || inorder == nullptr || length m_nValue = rootValue; root->m_pLeft = root->m_pRight = nullptr; if(startPreorder == endPreorder) { if(startInorder == endInorder && *startPreorder == *startInorder) return root; else throw std::exception(\"Invalid input.\"); } // 在中序遍历中找到根结点的值 int* rootInorder = startInorder; while(rootInorder 0) { // 构建左子树 root->m_pLeft = ConstructCore(startPreorder + 1, leftPreorderEnd, startInorder, rootInorder - 1); } if(leftLength m_pRight = ConstructCore(leftPreorderEnd + 1, endPreorder, rootInorder + 1, endInorder); } return root; } 在函数ConstructCore中，我们先根据前序遍历序列的第一个数字创建根节点，接下来在中序遍历序列中找到根节点的位置，这样就能确定左、右子树节点的数量。在前序遍历和中序遍历序列中划分了左、右子树节点的值之后，我们就可以递归地调用函数ConstructCore去分别构建它的左、右子树。 剑指 Offer P62，本题完整的源代码： https://github.com/zhedahht/CodingInterviewChinese2/tree/master/07_ConstructBinaryTree "},"pages/面试系列/二叉树的下一个节点.html":{"url":"pages/面试系列/二叉树的下一个节点.html","title":"二叉树的下一个节点","keywords":"","body":"二叉树的下一个节点 题目：给定一棵二叉树和其中的一个节点，如何找出中序遍历序列的下一个节点？树中的节点除了有两个分别指向左、右子节点的指针，还有一个指向父节点的指针。 在图2.8中的二叉树的中序遍历序列是{d,b,h,e,i,a,f,c,g}。我们将以这棵树为例来分析如何找出二叉树的下一个节点。 图2.8 一棵有9个节点的二叉树。树中从父节点指向子节点的指针用实线表示，从子节点指向父节点的指针用虚线表示 如果一个节点有右子树，那么它的下一个节点就是它的右子树中的最左子节点。也就是说，从右子节点出发一直沿着指向左子节点的指针，我们就能找到它的下一个节点。例如，图2.8中节点b的下一个节点是h，节点a的下一个节点是f。 接着我们分析一个节点没有右子树的情形。如果节点是它父节点的左子节点，那么它的下一个节点就是它的父节点。例如，图2.8中节点d的下一个节点是b，节点f的下一个节点是c。 如果一个节点既没有右子树，并且它还是它父节点的右子节点，那么这种情形就比较复杂。我们可以沿着指向父节点的指针一直向上遍历，直到找到一个是它父节点的左子节点的节点。如果这样的节点存在，那么这个节点的父节点就是我们要找的下一个节点。 为了找到图2.8中节点i的下一个节点，我们沿着指向父节点的指针向上遍历，先到达节点e。由于节点e是它父节点b的右节点，我们继续向上遍历到达节点b。节点b是它父节点a的左子节点，因此节点b的父节点a就是节点i的下一个节点。 找出节点g的下一个节点的步骤类似。我们先沿着指向父节点的指针到达节点c。由于节点c是它父节点a的右子节点，我们继续向上遍历到达节点a。由于节点a是树的根节点，它没有父节点，因此节点g没有下一个节点。 我们用如下的C++代码从二叉树中找出一个节点的下一个节点： BinaryTreeNode* GetNext(BinaryTreeNode* pNode) { if(pNode == nullptr) return nullptr; BinaryTreeNode* pNext = nullptr; if(pNode->m_pRight != nullptr) { BinaryTreeNode* pRight = pNode->m_pRight; while(pRight->m_pLeft != nullptr) pRight = pRight->m_pLeft; pNext = pRight; } else if(pNode->m_pParent != nullptr) { BinaryTreeNode* pCurrent = pNode; BinaryTreeNode* pParent = pNode->m_pParent; while(pParent != nullptr && pCurrent == pParent->m_pRight) { pCurrent = pParent; pParent = pParent->m_pParent; } pNext = pParent; } return pNext; } 剑指 Offer P65，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/08_NextNodeInBinaryTrees "},"pages/面试系列/用两个栈实现队列.html":{"url":"pages/面试系列/用两个栈实现队列.html","title":"用两个栈实现队列","keywords":"","body":"用两个栈实现队列 题目：用两个栈实现一个队列。队列的声明如下，请实现它的两个函数appendTail和deleteHead，分别完成在队列尾部插入节点和在队列头部删除节点的功能。 template class CQueue { public: CQueue(void); ~CQueue(void); void appendTail(const T& node); T deleteHead(); private: stackstack1; stackstack2; }; 从上述队列的声明中可以看出，一个队列包含了两个栈stack1和stack2，因此这道题的意图是要求我们操作这两个“先进后出”的栈实现一个“先进先出”的队列CQueue。 我们通过一个具体的例子来分析往该队列插入和删除元素的过程。首先插入一个元素a，不妨先把它插入stack1，此时stack1中的元素有{a}，stack2为空。再压入两个元素b和c，还是插入stack1，此时stack1中的元素有{a,b,c}，其中c位于栈顶，而stack2仍然是空的，如图2.9(a)所示。 这时候我们试着从队列中删除一个元素。按照队列先入先出的规则，由于a比b、c先插入队列中，最先被删除的元素应该是a。元素a存储在stack1中，但并不在栈顶上，因此不能直接进行删除。注意到stack2一直没有被使用过，现在是让stack2发挥作用的时候了。如果我们把stack1中的元素逐个弹出并压入stack2，则元素在stack2中的顺序正好和原来在stack1中的顺序相反。因此经过3次弹出stack1和压入stack2的操作之后，stack1为空，而stack2中的元素是{c,b,a}，这时候就可以弹出stack2的栈顶a了。此时的stack1为空，而stack2的元素为{c,b}，其中b在栈顶，如图2.9(b)所示。 如果我们还想继续删除队列的头部应该怎么办呢？剩下的两个元素是b和c，b比c早进入队列，因此b应该先删除。而此时b恰好又在栈顶上，因此直接弹出stack2的栈顶即可。在这次弹出操作之后，stackl仍然为空，而stack2中的元素为{c}，如图2.9(c)所示。 从上面的分析中我们可以总结出删除一个元素的步骤：当stack2不为空时，在stack2中的栈顶元素是最先进入队列的元素，可以弹出。当stack2为空时，我们把stack1中的元素逐个弹出并压入stack2。由于先进入队列的元素被压到stack1的底端，经过弹出和压入操作之后就处于stack2的顶端，又可以直接弹出。 接下来再插入一个元素d。我们还是把它压入stack1,如图2.9(d)所示，这样会不会有问题呢？我们考虑下一次删除队列的头部stack2不为空，直接弹出它的栈顶元素c，如图2.9(e)所示。而c的确比d先进入队列，应该在d之前从队列中删除，因此不会出现任何矛盾。 总结完每一次在队列中插入和删除操作的过程之后，我们就可以开始动手写代码了。参考代码如下： template void CQueue::appendTail(const T& element) { stack1.push(element); } template T CQueue::deleteHead() { if(stack2.size()0) { T& data = stack1.top(); stack1.pop(); stack2.push(data); } } if(stack2.size() == 0) throw new exception(\"queue is empty\"); T head = stack2.top(); stack2.pop(); return head; } 剑指 Offer P68，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/09_QueueWithTwoStacks "},"pages/数据结构导论/第二章_线性表.html":{"url":"pages/数据结构导论/第二章_线性表.html","title":"第二章 线性表","keywords":"","body":"第二章 线性表 顺序表实现算法的分析 插入 时间复杂度：O(n) 移动次数：n-i+1 平均移动次数：n/2 删除 时间复杂度：O(n) 移动次数：n-i，最坏移动次数：n-1 平均移动次数：(n-1)/2 定位 时间复杂度：O(n) 读表长和读元素 时间复杂度：O(1) 顺序表的插入、删除算法在时间性能方面是不理想的。 手把手教你数据结构c语言实现 数据结构 C语言数据结构-顺序栈 "},"pages/操作系统概论/第一章_操作系统简介.html":{"url":"pages/操作系统概论/第一章_操作系统简介.html","title":"第一章 操作系统简介","keywords":"","body":"第一章 操作系统简介 第一节 什么是操作系统 操作系统（Operating System，OS）是一种复杂的系统软件，是不同程序代码、数据结构、数据初始化文件的集合，可执行。 操作系统提供计算机用户与计算机硬件之间的接口，并管理计算机软件和硬件资源。 操作系统是覆盖在裸机上的第一层软件，编译程序、数据库管理系统及其他应用程序都运行在操作系统之上，操作系统为这些软件提供运行环境。 一、用户与硬件之间的接口 接口是两个不同组成部分的交接面，分为硬件接口和软件接口。 计算机的所有功能最终都是由硬件的操作实现的。 应用程序是在操作系统上运行的，而对硬件的控制过程都封装在操作系统的核心程序中了。 有操作系统对硬件的抽象，而且操作系统为应用程序提供了运行环境，所以在操作系统上编写和运行应用程序就简单多了。 操作系统屏蔽了对硬件操作的细节，提供了计算机用户与计算机硬件之间的接口，并且通过这个接口使应用程序的开发变得简单、高效。 操作系统必须完成的两个主要目标如下： 与硬件部分相互作用，为包含在硬件平台上的所有底层可编程部分提供服务。 为运行在计算机系统上的应用程序（即所谓用户程序）提供执行环境。 二、资源的管理者 现代计算机系统的一个重要特点是支持多任务，即允许在同一个系统内同时驻留多个应用程序。 多个应用程序共同使用计算机硬件和软件资源时，就需要操作系统对这些资源进行有效的管理。 操作系统的主要功能： 1. 处理机管理 程序的执行必须依靠处理机，任意时刻处理机都只能执行一个程序流。 在单处理机系统中执行多个程序流，必须由操作系统的处理机调度程序来管理处理机的分配，以使多个程序共享处理机，从宏观上看多个程序能同时顺利执行。 2. 内存管理 在多任务系统中，内存不再是独占资源，而是可能被多个应用程序共同占用。如何为应用程序分配内存并使不同应用程序的地址空间互不干扰，如何在程序执行完成后回收其所占内存，以及完成逻辑地址到物理地址的转换，都是操作系统内存管理程序要完成的功能。 3. 设备管理 设备管理主要完成接受用户的I/O请求、为用户分配I/O设备、管理I/O缓存和驱动I/O设备等功能。 4. 文件管理 计算机系统把大量需要长时间保留的数据信息以文件的形式存放在外存储设备中（如硬盘、光盘、磁带和U盘），操作系统通过自己的文件管理程序完成外存空间的分配、回收、文件的按名存取、文件的组织、共享与保护等功能。 第二节 操作系统的发展 操作系统的发展从时间顺序上经历了从无操作系统到单道批处理系统、多道程序系统（多道批处理系统、分时系统）的发展过程。 一、无操作系统 第一代计算机（1945～1955年）使用电子管作为主要的电子器件，用插件板上的硬连线或穿孔卡片表示程序，没有用来存储程序的内存，无操作系统。 二、单道批处理系统 第二代计算机（1955～1965年）使用的主要电子器件是晶体管，开始使用磁性存储设备，内外存容量增加，计算机运行速度提高，出现了早起的单道批处理系统。 三、多道程序系统 早期多道程序系统不具有交互功能，被称为多道批处理系统。程序员提交作业后，在作业运行结束、输出结果之前，无法观察和控制作业的运行。 解决这一问题的需求非常迫切，于是出现了分时操作系统。 在分时操作系统的支持下，多个用户可以同时通过不同的终端使用主机，主机可以快速响应常用命令。 使终端用户感觉自己独占计算机资源，并且实现用户与主机的及时交互。 在分时操作系统中同时登录系统的多个用户提交的作业轮流执行，每个作业都是运行一小段时间就把主机资源让给另一个作业运行一段时间，多个作业交替执行，分时使用主机资源。 四、微机操作系统 随着个人计算机的出现，微机操作系统应运而生。第一个微机操作系统是Intel公司的CP/M系统，用于Intel 8080。 20世纪80年代形成了新DOS版本MS-DOS。 1985年微软开始构建Windows操作系统。 另，还有Linux、Mac OS等一些支持个人计算机的操作系统。 五、实时操作系统 随着计算机的广泛应用，出现了各种实时操作系统。实时操作系统是支持实时计算的系统。 实时操作系统不仅要求系统正确地计算出结果，而且要求必须在规定的时间内计算出正确结果。 如果计算结果正确，但时间超过了规定时间，依然被认为计算出错。 六、批处理系统、分时系统、实时系统的特点 1. 单道批处理系统的特点 1）自动性 单道批处理系统是计算机能够在操作系统控制下，自动地将作业从外存装入内存运行。当作业运行完毕后，自动撤销已运行完毕的作业，并依次从外存装入下一个作业，使之运行。 2）顺序性 存放在外存中的作业按顺序依次装入内存运行，先进入内存的作业先运行完毕。 3）单道性 任何时刻内存中只有一道作业。 2. 多道批处理系统的特点 在多道批处理系统中，用户所提交的作业都先存放在外存中并排成一个队列，该队列被称为“后备作业队列”。由操作系统的作业调度程序按一定策略从后备作业队列中选择若干个作业调入内存，使它们共享CPU和系统中的各种资源，以达到提高资源利用率和系统吞吐量的目的。 1）多道性 在内存中可以同时驻留多道程序，当在CPU上运行的作业提出I/O请求后，该CPU可以执行其他作业，从而有效地提高系统资源的利用率和吞吐量。 2）无序性 多个作业完成的先后顺序与它们进入内存的顺序之间没有严格的对应关系。先进入内存的作业不一定被调度，也不一定先被执行完。 3）调度性 多道程序系统必须具有作业调度和进程调度功能。作业调度用来从后备作业队列中选择一个或多个要被装入内存的作业。进程调度程序用来从内存中选择一个（单CPU系统）或多个（多CPU系统）进程，使其在CPU上运行。 4）复杂性 3. 分时系统的特点 分时操作系统允许多个用户通过终端同时使用计算机。 分时操作系统是多道程序系统的自然延伸，支持多用用户任务同时驻留内存，每个用户通过终端与主机交互时都能得到快速响应。 分时系统的特点是多路行、独立性、及时性和交互性。 分时系统的优点是向用户提供了人机交互的方便性，使多个用户可以通过不同的终端共享主机。 4. 实时系统的特点 实时系统主要用于实时控制和实时信息处理领域。 实时系统必须能及时响应外部事件的请求，在规定时间内完成对该事件的处理，并控制所有实时任务协调一致地运行。 1）多路性 2）独立性 每个终端用户在向实时系统提出服务请求时，是彼此独立操作，互不干扰。 3）及时性 实时信息处理系统对实时性的要求是以人所能够接受的等待时间来确定的。 4）交互性 5）可靠性 七、操作系统产品现状 主机操作系统 服务器操作系统 微机操作系统 嵌入式操作系统 第三节 操作系统的特征 现代操作系统都支持多任务，具有并发、共享、虚拟和异步性特征。 1. 并发 并发是指两个或多个事件在同一时间间隔内发生。并发强调“同一时间间隔”，与并行是有区别的两个不同的概念，并行是指多个事件同时发生。 2. 共享 共享是指系统中的资源可供内存中多个并发执行的进程共同使用。资源共享有两种方式，即互斥共享和同时共享。 3. 虚拟 虚拟是指通过某种技术把一个物理实体变成若干逻辑上的对应物。 4. 异步性 进程以不可预知的速度向前推进。内存中的每个程序何时执行、何时暂停、以怎样的速度向前推进，以及每道程序总共需要多少时间才能完成等，都是不可预知的。 第四节 操作系统的功能 一、内存管理 内存管理的主要任务是为多道程序的运行提供良好的环境，方便用户使用内存，提高内存的利用率，以及从逻辑上扩充内存以实现虚拟存储。 1. 内存分配 内存分配的主要任务是为每道程序分配内存空间。 有静态分配和动态分配两种方式： 静态方式：内存划分成固定大小和数量一定的区域，再系统运行过程中各分区的大小和数量不再变化。 动态方式：根据进程的请求分配内存，内存中分区的大小和数量都是动态变化的。 2. 内存保护 一是是操作系统内核的空间不会被用户随意访问，以保证系统的安全和稳定； 二是确保每道用户程序都在自己的内存空间中运行，互不干扰。 3. 地址映射 CPU执行程序过程中访问内存时，需要把程序的逻辑地址转变为物理地址，这个转换的过程称为地址映射。 1）逻辑地址与物理地址 目标程序中的地址称为逻辑地址，从0开始；内存中的单元地址称为物理地址。 CPU访问内存需要将物理地址送入地址总线，以选中要访问的内存单元。 2）地址映射 将逻辑地址转换为对应的物理地址。 4. 内存扩充 内存扩充的任务时借助于虚拟存储技术，从逻辑上扩充内存容量，使系统能够向用户提供比物理内存大的存储容量。 1）请求调入功能 2）置换功能 二、进程管理 进程可以被认为是程序的执行实体。 三、设备管理 主要完成用户的I/O请求，为用户分配I/O设备。 设备管理的功能： 1）缓冲管理 2）设备分配 3）设备处理 由设备驱动程序来实现CPU与设备控制器之间的通信。 4）设备独立性和虚拟设备 设备独立性功能使应用程序独立于物理设备。 虚拟设备的功能是把一个物理设备变换为多个对应的逻辑设备，使一个物理设备能供多个用户共享。 四、文件管理 1. 文件存储空间的管理 为每个文件分配必要的外存空间，提高外存利用率，并能有助于提高访问文件的速度。 2. 目录管理 为每个文件建立目录项并对众多目录项进行有效组织。 3. 文件的读、写管理和存取控制 根据用户的请求，从外存中读取数据或将数据写入外存。防止未经核准的用户存取文件，防止冒名顶替存取文件，防止以不正确的方式使用文件。 五、提供用户接口 为了方便用户使用操作系统，操作系统向用户提供了用户与操作系统之间的接口。 1. 命令接口 为了便于用户与计算机系统的交互 1）联机用户接口 这是为联机用户设计的接口，它由一组键盘操作命令和命令解释程序组成。 2）脱机用户接口 脱机用户接口是为批处理作业的用户提供的，也称为批处理用户接口。 2. 图形用户接口 20世纪90年代，在操作系统中开始引入图形化用户接口。 3. 程序接口 操作系统提供给程序员的接口是系统调用。 第五节 操作系统的体系结构 一、软件体系结构简介 软件体系结构是一个复杂软件系统的高层结构，为软件系统提供了一个结构、行为和属性的高级抽象。 明确清晰的软件体系结构是一条贯穿软件系统整个生命周期的主线，是软件设计成功的基础和关键，也是对软件大型化、复杂化趋势的一种很好的应对决策。 1. 简单的监控程序模型 任意时刻系统中只能运行一个任务，这样保证了对系统信息的互斥访问，保护了系统数据的安全。 2. 单体结构模型 单体结构模型是软件工程出现以前的早期操作系统及目前一些小型操作系统采用的体系结构。在单体结构模型，所有的软件和数据结构都放置在一个逻辑模块中，对外层的用户程序提供一个完整的内核界面-系统调用。 具有单体内核结构的典型操作系统由UNIX系统、MS-DOS、Linux、Mac OS X 和 BSD等系统。 3. 层次结构模型 层次结构的基本思想是将操作系统分解为多个小的、容易理解的层，系统功能被隔离在不同层中，每一层提供对系统功能的部分抽象，然后采用单向调用的顺序，形成一连串彼此连续的对系统功能的“抽象串”，最终形成对整个系统的完成抽象。 4. 客户/服务器模型与微内核结构 把传统操作系统内核中的一些组成部分放到内核之外作为一个独立的服务器进程来实现，在微内核中只保留了操作系统最基本的功能。 目前微内核的操作系统一个是想通用操作系统的方向发展，如Windows NT；另一个就是嵌入式操作系统。 5. 动态可扩展结构模型 动态可扩展结构模型的基本思想是在运行过程中，能够动态实现系统行为扩展的结构，也可称为弹性结构。 第六节 指令的执行 程序是指令的集合，程序的执行就是按照某种控制流执行指令的过程。 1. 指令周期 一个单一指令需要的处理称为指令周期，一个指令周期可以划分成两个步骤，分别是取指周期和执行周期。 2. 取指令和执行指令 1）取指令 在每个指令周期开始时，处理器从存储器中取一条指令。 2）执行指令 取到的指令被放置在处理器的指令寄存器IR中。 3. 一个实例 1）程序计数器（PC），存指令地址。 2）指令寄存器（IR），存正在执行的指令。 3）累加器（AC），临时存储体和累加操作。 部分操作码的二进制表示与其对应的操作如下。 1）0001:从内存中读取数据并送入AC。 2）0010:把AC的内容存储到内存中。 3）0101:把从内存中取得的值与AC中的值相加，结果仍存放在AC中。 程序功能 把内存940单元的内容与941单元的内容相加，结果送入941单元。程序的3条指令： move AC,(940); 1940 add AC,(941); 5941 move(941),AC; 2941 其中，move表示传送，move AC,(940)指令的功能是将(940)内存单元的值传入累加器AC。对应的操作码为0001。 “;”后面内容为注释信息，1940是move AC,(940)指令对应的机器码。 add AC,(941)指令的功能是将(941)存储单元的内容与累加器AC中的内容相加，再送入AC；对应的操作码为0101. move(941),AC指令的功能是将AC累加器的内容传送到941号存储单元。对应的操作码为0010。 分析：第一条指令的执行过程 1）程序开始执行时，程序计数器PC的值为move AC,(940)指令对应的机器码1940所在的存储单元的地址300。 2）CPU将1940送入指令寄存器IR，PC值自动加1，变为301。 3）CPU对1940进行译码。 4）操作码为0001时，执行从内存940单元取数送入AC。 5）指令执行结果是把地址为940的内存单元的值3送入AC。 例题 将内存100单元的内容与101单元的内容相加，并将相加的结果传入101单元，请根据程序的功能写出程序对应的指令。（提示：期间需要用到累加器AC） move AC,(100) add AC,(101) move (101),AC 解析：本题通过累加器AC求和，并将结果送入指定的单元。 4. 指令的执行小结 程序执行的过程是反复取指令和执行指令的过程。 本章小节 第一章主要介绍了操作系统的基础知识，从操作系统的概念、作用、功能、特征、发展及软件体系结构等方面对操作系统进行了介绍，最后一节介绍了指令的执行过程。 "},"pages/操作系统概论/第二章_进程管理/第一节_进程的描述.html":{"url":"pages/操作系统概论/第二章_进程管理/第一节_进程的描述.html","title":"第一节 进程的描述","keywords":"","body":"第一节 进程的描述 一、程序的并发执行 1. 程序的顺序执行 早期无操作系统及单道批处理系统的计算机中，程序的执行方式都是经典的顺序执行。 1）顺序性 处理机的操作，严格按照程序所规定的顺序执行，即只有前一个操作结束后，才能执行后继操作。 2）封闭性 程序是在封闭的环境下运行的。即程序运行时独占全机资源，因而各资源的状态（除初始状态外）只有本程序才能改变。 3）可再现性 只要程序执行时的环境和初始条件相同，当程序多次重复执行时，其执行结果相同。 2. 程序的并发执行 程序并发执行是指在同一时间间隔内运行多个程序。一个程序执行结束之前，可以运行其他程序。 程序并发执行的确切含义是从宏观上，用户看到多个程序同时向前不间断地推进。而从微观上，任意时刻一个CPU上只有一个程序在执行。 1）间断性 程序在并发执行时，由于它们共享资源，而资源数量又往往少于并发执行的程序数量，系统不能保证每个程序不受限制地占用资源。因而，每个程序在CPU上运行，都是时断时续的。 当请求某个资源的程序数量大于被请求的资源数量时，就必然有因申请不到资源而暂停执行的程序。 当其他程序释放资源后，该程序才可能继续执行。资源的有限使并发执行的程序呈现执行过程的间断性。 2）失去封闭性 程序在并发执行时，由于它们共享资源或者合作完成同一项任务，系统的状态不再是有正在执行的某一个程序可以“看见”和改变。 3）不可再现性 程序在并发执行时，由于失去了封闭性，也将导致其失去执行结果的可再现性。同一个程序在输入完全相同的情况下多次运行，可能出现不同的运行结果。 二、进程的概念 当操作系统支持程序并发执行时，并发执行的程序可能是同一个程序在不同数据集合上的执行，也可能是不同的程序在不同数据集合上的执行，它们共享系统资源。用程序已不能描述程序的并发执行，所以引入了进程的概念。 1. 进程的定义 定义1：进程是允许并发执行的程序在某个数据集合上的运行过程。 定义2：进程是由正文段、用户数据段及进程控制块共同组成的执行环境。正文段存放被执行的机器指令，用户数据段存放进程在执行时直接进行操作的用户数据。进程控制块存放程序的运行环境，操作系统通过这些数据描述和管理进程。 程序装入内存后就可以运行了，根据指令计数器PC的值，不断将指令从内存取到CPU的指令寄存器中，经过译码后完成各种操作。这些指令控制的对象不外乎各种存储器（内存、外存和各种CPU寄存器），这些存储器中有待运行的指令和待处理的数据，指令只有得到CPU才能发挥作用。 可见，在计算机内部，程序的执行过程实际对应了一个执行环境的总和。这个执行环境包括程序中的各种指令和数据，还有一些额外数据，如寄存器的值、用来保存临时数据的堆栈、被打开文件的数量及输入/输出设备的状态等。这个执行环境的动态变化表征程序的运行，一个进程对应了一个这样的环境。 进程代表了程序的执行过程，是一个动态的实体，它随着程序中指令的执行而不断变化，在某个特定时刻的进程内容被称为进程映像。 2. 进程的特征 1）并发性 多个进程实体能在一段时间间隔内同时运行。 2）动态性 进程是进程实体的执行过程。 3）独立性 在没有引入线程概念的操作系统中，进程是独立运行和资源调度的基本单位。 4）异步性 是指进程的执行时断时续，进程什么时候执行、什么时候暂停都无法预知，呈现一种随机的特征。 5）结构特征 进程实体包括用户正文段，用户数据段和进程控制块。 3. 进程与程序的比较 1）进程与程序的区别 程序是静态的，进程是动态的。程序是存储在某种介质上的二进制代码，进程对应了程序执行的过程。 程序是永久的，进程是暂时存在的。 进程因程序的执行而被创建，因程序执行的结果而被撤回，有一个相对短暂的生命期。 2）进程与程序的联系 进程是程序的一次执行，进程总是对应至少一个特定的程序，执行程序的代码。 一个程序可以对应多个进程。同一个进程可以在不同的数据集合上运行，因而构成若干个不同的进程。几个进程能并发地执行相同的程序代码，而同一个进程能顺序地执行几个程序。 例题 请解释进程的概念，并说明进程和程序的区别和联系。 进程是允许并发执行的程序在某个数据集合的运行过程；是由正文段、用户数据段及进程控制块共同组成的执行环境。 进程和程序的区别： 程序是静态的，进程是动态。 程序是永久的，进程是暂时存在的。 进程和程序的联系： 进程是程序的一次执行，执行程序的代码。 一个程序可以对应多个进程。 三、进程控制块 进程实体存在的标志是操作系统管理进程所使用的数据结构--进程控制块。 1. 什么是进程控制块 进程控制块是进程实体的一部分，是操作系统中最重要的数据结构。 操作系统在创建进程时，首先要为进程创建进程控制块，也就是生成一个进程控制块类型的变量，以存储所创建进程的描述信息。每个进程由唯一的进程控制块，进程控制块时操作系统感知进程存在的唯一标志。 2. 进程控制块中的信息 一般操作系统中的进程控制块中通常包含一下信息： 1）进程标识符信息 进程标识符用户唯一标识一个进程。 2）处理机状态信息 1）通用寄存器。用户程序可以访问的寄存器，用于暂存信息。 2）指令计数器。其中存放了CPU要访问的下一条指令的地址。 3）程序状态字PSW。其中包含状态信息，如条件码、执行方式和中断屏蔽标志等。 4）用户栈指针。每个用户进程都有一个与之相关的系统栈，用于存放过程和系统调用参数及调用地址，栈指针指向该栈的栈顶。 3）进程调度信息 进程调度信息包括进程状态信息、进程优先级和进程调度所需的其他信息。 4）进程控制信息 进程控制信息包括程序和数据的地址、进程同步和通信机制、资源清单，以及链接指针。 本书中的数据结构就是为程序的需要而定义的简单类型或复杂类型的变量，变量用来存放随着程序的执行不断发生变化的数据。进程控制块就是复杂类型的变量。 四、进程的状态 进程控制块的状态字短描述了进程当前所处的状态。 1. 进程的3中基本状态 1）就绪态 就绪态时进程一旦获得CPU就可以投入运行的状态。在多任务系统中，可以有多个处于就绪态的进程，这些进程被组成成一个或多个就绪进程队列。 2）执行态 执行态是进程获得CPU正在运行的状态。 3）阻塞态 阻塞态是进程由于等待资源或某个事件的发生而暂停执行的状态，系统不会为阻塞态的进程分配CPU。阻塞态进程在获得其等待的资源或其等待的事件发生之后，转变为就绪状态。 2. 进程状态的转换 新创建进程的状态一般被设置为就绪态，当操作系统为处于就绪态的进程分配CPU时，进程开始在CPU上运行，进程的状态由就绪态变为执行态。 在多任务系统中，CPU被多个进程共享的资源，操作系统通常会为普通进程规定一个在CPU上连续运行的时间长度，称这个时间长度为时间片。当进程在CPU上运行的时间片长度递减为0时，系统把CPU分配给其他就绪态进程。如果进程在CPU上运行的时间递减为0，系统将该进程的状态由执行态变为就绪态。 如果执行态进程在运行过程中因为申请某种资源或等待某时间的发生而不宜继续占用CPU，系统将把该进程的状态改变为阻塞态，并暂停该进程的执行，将CPU分配给就绪态进程。阻塞态进程在其等待的时间发生或申请到所需资源后，系统将其状态转变为就绪态。 五、进程的组织 在操作系统中任意时刻都可能存在很多进程，每个进程对应一个进程控制块，操作系统组织和管理进程是通过管理和组织进程控制块来实现的。 实际上对进程的组织是通过定义数据结构来实现的。 1. 链接方式 把系统中具有相同状态的进程的进程控制块（PCB）用其中的链接字链接成一个队列。 2. 索引方式 系统根据所有进程的状态，建立几张索引表，索引表的每一个表项指向一个PCB的物理块。 3. 进程队列 当系统中有很多进程时，可以把进程控制块用队列组织起来，形成进程队列。把具有相同状态的进程放在同一个队列中，具有不同状态的进程就形成了不同的进程队列。 处于就绪态的进程构成的进程队列称为就绪队列，处于阻塞态的进程构成的进程队列称为阻塞队列。 "},"pages/操作系统概论/第二章_进程管理/第二节_进程的控制.html":{"url":"pages/操作系统概论/第二章_进程管理/第二节_进程的控制.html","title":"第二节 进程的控制","keywords":"","body":"第二节 进程的控制 一、进程的创建 创建进程包括为为进程分配必要的资源，建立操作系统用于管理进程的数据结构（如进程控制块）等操作。通常在下列情况下需要创建新进程。 1）用户登录。 2）作业调度。 3）提供服务。 4）应用请求。 在Linux系统中，除了0号（swapper 进程）进程外的其他进程都是由父进程创建的。在操作系统启动并正常工作后，系统中的已有进程在执行过程中都可以通过系统调用创建新进程。被创建的新进程成为创建该新进程的进程的子进程，创建者进程和被创建新进程成为父子进程。一个进程可以创建多个子进程，由同一个进程创建的多个子进程之间的关系成为兄弟进程。 在一个进程创建子进程时，子进程可以从操作系统哪里直接获得资源，也可能从父进程资源子集那里获得资源。 当新进程被创建时，有两种执行的可能。 1）父进程与子进程并发执行。 2）父进程等待，直到某个或全部子进程执行完毕。 新进程的地址空间也有两种可能。 1）子进程共享父进程的地址空间。 2）子进程拥有独立的地址空间。 调用创建新进程的系统调用来创建进程的一般步骤如下。 1）申请空白PCB。 2）为新进程分配资源。 3）初始化进程控制块。 4）将新进程插入就绪队列。 二、进程的阻塞 操作系统在下列情况下可能进行进程的阻塞和唤醒操作。 1）请求系统服务。 2）启动某种操作。 3）新数据尚未到达。 4）无心工作可做。 完成进程阻塞的简化过程如下。 1）将进程状态改为阻塞态。 2）将进程插入相应的阻塞队列。 3）转进程调度程序，从就绪进程中选择进程为其分配CPU。 处于执行态的进程被阻塞后，CPU空闲，需要执行进程调度程序，从就绪进程中为CPU选择一个进程运行。 三、进程的唤醒 操作系统通过下列过程将阻塞态进程唤醒，使其变成就绪态进程。进程唤醒的过程如下。 1）将进程从阻塞队列中移除。 2）将进程状态由阻塞态改为就绪态。 3）将进程插入就绪队列。 四、进程的终止 进程的终止也称为进程的撤销，在下列情况下，进程会被终止。 1）当进程正常执行完毕，调用终止进程的系统调用，请求操作系统删除该进程。 2）一个进程调用适当的系统调用，终止另外一个进程。 父进程终止其子进程的原因可能有以下几个。 1）子进程使用了超过它所分配到的一些资源。 2）分配给子进程的任务不再需要。 3）父进程退出，如果父进程终止，那么操作系统不允许子进程继续。 操作系统通过系统调用终止进程的一般过程如下。 1）从进程PCB中读取进程状态。 2）若进程正在执行，则终止进程的执行。 3）若进程有子孙进程，在大多数情况下需要终止子孙进程。 4）释放资源。 5）将终止进程的PCB移出。 五、操作系统的启动和系统中进程的出现 当打开计算机电源后，计算机会先进行加电自检，然后寻找启动盘。如果选择硬盘启动，计算机会检查硬盘的0柱面0磁道1扇区。 当硬盘被划分为多个分区，同时安装了多个操作系统，每个分区都有自己的引导扇区，但整个硬盘有一个主引导扇区，主引导扇区就是硬盘的0柱面0磁道1扇区。通过执行主引导扇区的代码，判断当前被激活的分区，然后加载被激活分区的引导扇区，通过该引导扇区代码的执行加载该激活分区的操作系统系统。 "},"pages/操作系统概论/第二章_进程管理/第三节_操作系统内核.html":{"url":"pages/操作系统概论/第二章_进程管理/第三节_操作系统内核.html","title":"第三节 操作系统内核","keywords":"","body":"第三节 操作系统内核 擦操作系统内核是计算机硬件的第一次扩充，内核执行操作系统与硬件关系密切，执行频率高的模块，常驻内存。 1. 支撑功能 支撑功能包括中断处理、时钟管理和原语操作。原语操作也称原子操作，是一组在执行过程中不能被中断的操作。 2. 资源管理功能 资源管理包括进程管理、存储器管理和设备管理。 一、中断 1. 什么是中断 中断是改变计算机执行指令顺序的一种事件，这样的事件与CPU芯片内外部硬件电路产生的电信号相对应。 计算机在执行程序的过程中，当出现中断时，计算机停止现行程序的运行，转向对这些中断事件的处理，处理结束后再返回到现行程序的间断处。 2. 为什么要中断 引入中断机制后，使CPU可以与其他设备并行工作，能有效得提高CPU的利用率，改善系统性能，支持系统的异步性。 3. 中断的类型 中断分为同步中断（也称内部中断或异常）和异步中断（也称外部中断）两种。 1）同步中断（内部中断或异常） 同步中断是指指令执行时由CPU控制单元产生的，之所以称为同步，是因为只有在一条指令终止执行（注意：此时指令不一定执行完毕）后CPU才会发出中断，如除法出错、调试、溢出和浮点出错等。 2）异步中断（外部中断） 异步中断是由其他硬件设备随机产生的。 外部中断又可分为外部可屏蔽中断和外部不可屏蔽中断。 1）外部可屏蔽中断。外部可屏蔽中断是由I/O设备产生的中断，有两种方式可屏蔽中断。 一是如在 Intel 80x86 的CPU上，把 EFLAGS 寄存器的 IF 标志置0，表示关中断。此时CPU忽略所有的可屏蔽中断。 二是可以对可编程中断控制器（Programmable Interrupt Controller，PIC）编程来禁止中断请求（Interrupt Request，IRQ）。也就是说，可以告诉PIC停止对给定的IRQ线发布中断。 2）外部不可屏蔽中断。外部不可屏蔽中断是由紧急事件引起的中断，如硬件故障。 4. 引起中断的原因 1）人为设置中断。在程序中人为设置中断。 2）程序性事故。例如，计算机出现除数为0. 3）硬件故障。插件接触不良、电源掉电等。 4）I/O设备。I/O设备被启动以后，一旦其准备就绪或完成一次输入/输出，便向CPU发出中断请求。 5）外部事件。如用户通过键盘和鼠标来中断现行程序。 5. 中断响应 1）响应中断的条件 对于可屏蔽中断，开中断是响应中断的前提。例如，在 Intel 80x86 CPU 上，当EFLAGS寄存器的IF标志为1表示开中断。 2）响应中断的时机 对于外部中断，CPU每执行完一条指令都会检测是否有外部中断信号的到来。若有，则转中断处理过程。 6. 单重中断的处理过程 CPU在反复执行指令的过程中，每执行完一条指令，都会检测是否有外部中断信号的到来。如果检测到有中断信号，则转中断处理过程。 1）系统关闭中断，保护断点，把当前要执行的下一条指令的地址保存到内存中，以便中断返回时，能把这个地址恢复到程序计数器PC中，使被中断的程序从断点处开始继续执行。 2）转中断处理程序。在中断处理程序中完成保护现场的工作，就是把相关的硬件上下文信息保存到内存中。硬件上下文就是中断返回恢复被中断程序的执行时，需要写回CPU寄存器的值。 3）保护现场后，要根据中断向量到中断向量表中（在 Linux 中是到中断描述符表中）找到与中断处理子例程入口地址相关的信息，由这些信息得到中断处理子例程的入口地址，以执行中断处理子例程，完成本次中断处理的特殊处理工作。 4）最后，恢复现场，开中断，CPU返回断点处继续执行被中断的程序。 7. 如何找到中断服务子程序 1）中断向量。中断向量是对不同中断源到来的信号编号，该编号是一个无符号整数（0～255），成为中断向量。 2）中断描述符表（Interrupt Descriptor Table，IDT）是一个系统表，每一个中断或异常与向量相联系。每个向量在表中有唯一对应的表项，其中存有与中断或异常处理子例程入口地址相关的信息。 当发生中断时，CPU根据中断号获取中断向量值，再根据中断描述符表的起始地址和中断向量值，得到形成中断服务子程序（中断服务例程）入口地址的相关信息。 二、时钟管理 1. 时钟的重要性 操作系统的内核可以利用时钟机制防止一个进程垄断CPU或其他资源。系统可以利用时钟机制限制一个用户进程在CPU上的连续执行的时间，当进程一次连续运行的时间超过限定的时间时，由系统将CPU分配给其他进程。 2. 计算机系统中的时钟 大部分PC中有两个时钟源，分别称为实时时钟（Real Timer Clocker，RTC）和 OS 时钟。RTC时钟也称为 CMOS时钟，是一块时钟芯片，靠电池供电，为计算机提供计时标准，是最原始、最底层的数据。OS时钟产生于PC主板上的定时/计数芯片，在开机时有效，由系统控制。 3. 操作系统的时钟机制 操作系统需要完成的两种系统测量，一是保存当前的日期和时间，以便能通过系统调用把它们返回给用户程序，让用户程序获得当前的日期和时间，也可以由内核本身把当前时间作为文件和网络包的时间戳。二是维持定时器，这种机制能够告诉内核或用户程序某一时间间隔已经过去了。 操作系统依靠时钟硬件（可编程间隔定时器PIT）和时钟驱动程序完成上述两种定时测量功能。 1）OS 时钟管理硬件（可编程间隔定时器PIT）。 2）时钟软件--时钟驱动程序 时钟驱动程序也称为时钟中断处理程序，每产生一次时钟中断信号，操作系统内核要执行时钟驱动程序，时钟驱动程序完成下列功能。 1）维护日期和时间。 2）递减当前进程在一个时间片内剩余执行时间，并检查是否为零，防止进程运行超时。 3）对CPU的使用情况记账。 4）递减报警计数器。 三、系统调用 1. 什么是系统调用 系统调用是一群预先定义好的模块，它们提供一条管道让应用程序或一般用户能由此得到核心程序的服务。 系统调用是系统程序与用户程序之间的接口。 2. 系统调用与一般函数的区别 1）用户态执行 用户空间是指用户进程所处的地址空间。一个用户进程不能访问其他用户进程的地址空间，只有系统进程才能访问其他用户地址空间。当CPU执行用户空间的代码时，称该进程在用户态执行。 2）系统态执行 系统空间是指含有一切系统核心代码的地址空间，当CPU执行系统核心代码时，称进程处于系统态执行。 系统调用与一般函数调用的区别如下。 1）系统调用运行在系统态（核心态），而一般函数运行在用户态。 2）系统调用与一般函数调用的执行过程不同。系统调用执行时，当前进程被中断，由系统找相应的系统调用子程序，并在系统态下执行，执行结果返回进程。 3）系统调用要进行“中断处理”，比一般函数调用多了一些系统开销。 5. 系统调用的类型 根据系统调用的功能把系统总结为以下几种类型。 1）进程控制类系统调用。创建、撤销进程；获得、改变进程属性。 2）文件操作类系统调用。创建文件、删除文件、打开文件、关闭文件和读/写文件。 3）设备管理类系统调用。请求、释放设备。 4）通信类系统调用。打开、关闭连接，交换信息。 5）信息维护类系统调用。返回系统当前日期、时间、版本号、用户数、空闲内存和磁盘空间大小等信息。 "},"pages/操作系统概论/第二章_进程管理/第四节_进程同步.html":{"url":"pages/操作系统概论/第二章_进程管理/第四节_进程同步.html","title":"第四、五节 进程同步、通信","keywords":"","body":"第四节 进程同步 多任务操作系统支持多个进程并发执行，并发执行的进程共享系统的软件和硬件资源。 操作系统同步机制的主要任务就是要保证多任务共享系统资源的情况下，程序执行能得到正确的结果。 一、进程同步的基本概念 进程同步有两个任务： 一是对具有资源共享关系的进程，保证诸进程以互斥的方式访问临界资源。临界资源是必须以互斥方式访问的共享资源。 二是对具有相互合作关系的进程，保证互相合作的诸进程协调执行。相互合作的进程可能同时存在资源共享的关系。 为了说明什么是临界资源，先回忆前面p1、p2两个进程并发执行，counter是全局变量，初始值为0，p1、p2两个进程分别对counter做加1操作，代码如下。 p1 { ... counter = counter + 1 ... } p2 { ... counter = counter + 1 ... } 若当前counter=0，p1和p2各执行一次后，counter的正确值应该是2，但是当p1和p2在经过编译后，必须分别经过下列指令的执行，才能完成counter=counter+1的功能。 p1 { ... register1 = counter; register1 = register1 + 1; counter = register1; ... } p2 { ... register2 = counter; register2 = register2 + 1; counter = register2; ... } p1、p2并发执行时，指令的执行序列可能出现各种组合，当按下列顺序执行时，会发生counter计数错误。 register1 = counter; // 执行结果：register1 = 0 register1 = register1 + 1; // 执行结果：register1 = 1 register2 = counter; // 执行结果：register2 = 0 register2 = register2 + 1; // 执行结果：register2 = 1 counter = register1; // 执行结果：counter = 1 counter = register2; // 执行结果：counter = 1 执行结果是counter=1，而正确结果应该是counter=2。 如果p1和p2以互斥的方式去访问counter，也就是说，如果p1先开始对counter的访问，p2就必须等待p1对counter的访问完全结束，再开始对counter的访问。反之，如果p2先开始对counter的访问，p1就必须等待p2对counter的访问完全结束，再开始对counter的访问。那么p1和p2对counter访问指令的交错执行的情况就不会出现，计算结果的错误也就不会出现了。p1和p2以互斥的方式去访问counter的含义是在p1和p2中的任意一个进程执行counter+1操作的一系列指令的过程中，不允许另一个进程执行访问counter的操作。类似counter这样必须以互斥方式访问的共享资源称为临界资源。 在设计操作系统时，设计人员必须严格界定系统中的哪些资源是临界资源，通过同步机 制保证系统运行过程中，对临界资源的访问进行有效控制。 临界区是进程中访问临界资源的那段代码。访问临界资源是通过执行临界区代码来实现的，如果能使程序以互斥的方式进入临界区，就能够实现对临界资源的互斥访问。这一点可以通过在临界区前面加进入区代码，在临界区后面加退出区代码来实现。进入区代码在临界区代码之前执行，检查进程是否可以进入临界区并对临界区“加锁”。退出区代码在临界区代码之后执行，完成释放临界区访问权的功能。 在进程中，可以采用下列模式的代码来实现对临界资源的互斥访问。 { ... 进入区（Entry Section）； 临界区（Critical Section）； 退出区（Exit Section）； } 二、同步机制应遵循的准则 当存在多种可选的同步方案时，可能是硬件或软件的不同，也可能是算法的不同，根据什么来权衡利弊，做出怡当的选择呢？对于一种同步技术，如何对它收确性和性能做出评 价呢？同步机制应遵循的准则可以为人们提供判断、选择和评价的参考依据。 1. 空闲让进 当没有进程处于临界区时，表明临界资源处于空闲状态，应允许一个请求进入临界区的进程立即进入自己的临界区，以有效地利用临界资源。 2. 忙则等待 当已有进程进入临界区时，表明临界资源正在被访问，因而其他试图进入临界区的进程 必须等待，以保证对临界资源的互斥访问。 3. 有限等待 对要求访问临界资源的进程，应保证在有限时间内能进人自己的临界区，以免进程陷人 无限等待的状态。 4. 让权等待 当进程申请不到共享资源的访回权时，应立即释放处理机，以免进程哈人“忙等”状态， 浪费CPU资源。 三、信号量机制 在信号量机制中，用某种类型的变量，即信号量的取值来表示资源的使用状况，或某种事件是否发生，以此为基础实现进程的同步。本节将介绍整型信号量机制、记录型信号量机 制、AND型信号量机制。 交通信号灯的作用是通过灯的不同颜色告知车辆应如何行驶，信号量机制类似这种信号灯的作用。对不同的共享资源设置被称为信号量的变量，用信号量的取值来表示资源的使用 状况，或某种事件是否发生。通过信号量的取值来判断进程是否能访问与信号量对应的共享资源。 1. 整型信号量机制 整型信号量是表示共享资源状态且只能由特殊的原子操作改变的整型量。其完成同步功能的原理是定义一个整型变量，用整型变量值来标记资源的使用情况。如果整型量>0，说明有可用资源：如果整型量≤0，说明资源忙，进程必须等待。对于一次只允许一个进程访问的临界资源，可定义一个用于互斥的整型信号量，并将其初始化为1。整型信号量的值只能通过两个特定的原子操作wait和signal来改变。 (1) 整型信号量的wait和signal操作 s定义为整型信号量。 Var s integer; wait(s)//用于申请资源 { while s≤0 do no-op; //整型信号量值≤0时循环执行空操作 s=s-1 } signal(s)//用于释放资源 { s=s+1; } (2) 用整型信号量实现进程互斥 用整型信号量实现进程互斥的思想是：为必须互斥访问的临界资源CS定义一个互斥信号量mutex,将初始值置为1，然后将CS放入wait(mutex)和signal(mutex)之间。当CS可访问时，wait(mutex)才能正常结束使进程进入CS。 (3) 用整型信号量实现进程的协调 (4) Linux中的整型信号量 (5) 对整型信号量机制的总结 1）整型信号量的值只能由wait和signal操作改变。 2）wait和signal操作都是原子操作，即在这两个操作中对信号量的访问是不能被中断的。 3）原子操作可以通过关中断来实现。 4）整型信号量机制的实例：Linux中的自旋锁Spin Lock。 5）不同的资源对应不同的信号量，并不是系统中所有的资源都用同一个信号量表示。 2. 记录型信号量机制 (1) 记录型信号量的数据类型 Type semaphore = record Value: integer; // 资源数量 L: list of process; // 阻塞队列 end (2) 记录型信号量的wait(s)和signal(s)操作 procedure wait(s) var s: semaphore; begin s.value = s.value -1; if s.value (3) 对记录型信号量wait(s)和signal(s)的说明 1) 当s.value>=0时，s.value的值表示资源数量。当s.value 2) 每次的wait(s)操作，意味着进程请求一个单位的资源，描述为s.value=s.value-1。当s.value 3) 每次的signal(s)操作，意味着进程释放一个资源，故s.value=s.value+1操作表示系统可用的资源数目加1.若加1后s.value 4) 如果s.value的初值为1，表示只允许一个进程访问临界资源，此时的信号量转化为互斥信号量。 5) 记录型信号量机制的优点是不存在“忙等”，采取了“让权等待”的策略。 (4) 利用记录型信号量实现互斥 利用记录型信号量实现互斥的模型从实现互斥的代码片段上看与整型信号量一致，都是在访问临界资源的进程中定位临界区。在临界区前面加wait(s)，在临界区后面加signal(s)，不同的是信号量的类型和wait(s)、signal(s)实现代码不同。实现进程互斥的代码模型描述如下。 var s: semaphore; s.value = 1; Begin Repeat wait(s); Critical Section; signal(s); Remainder section; Until false; End (5) 利用记录型信号量实现“协调”的应用举例 3. AND型信号量机制 (1) AND型信号量机制的引入 AND信号量机制的基本思想是将进程在整个运行过程中所需要的所有资源一次性地全部分配给进程，待该进程使用完后再一起释放。只要还有一个资源不能分配给该进程，其他所有可能为之分配的资源也不分配给它。 (2) AND型信号量机制的实现 四、经典的进程同步问题 本节将介绍如何用记录型信号量机制解决两个经典的进程同步问题。 1. 生产者一消费者问题的描述 生产者一消费者问题是相互合作进程关系的一种抽象。例如，输人进程和计算进程的关系中，输入进程是生产者进程，计算进程是消费者进程。计算进程和输出进程的关系中，计算进程是生产者进程，输出进程是消费者进程。 (1) 问题描述 生产者进程生产消息，并将消息提供给消费者进程消费。在生产者进程和消费者进程之间设置了一个具有个缓冲区的缓冲池，生产者进程可以将它所生产的消息放人缓冲池的一个缓冲区中，消费者进程可以从一个缓冲区中取得一个消息消费。任意两个进程必须以互斥的方式边问公共缓冲池。当缓冲池空，没有可供消费的消息时，消费者进程必须阻塞等待。当缓冲池装满消息，没有空闲缓冲区时，生产者进程必须阻塞等待。 如图2-18所示，out指针指向装有消息的缓冲区，消费者进程从out指向的缓冲区中取 消息。n指针指向空缓冲区，生产者进程总是把新生产的消息放入in指向的空缓冲区。 (2) 需要解决的问题 1) 实现任意两个进程对缓冲池的互斥访问。 2) 实现对生产者进程和消费者进程的“协调”，即缓冲池中有消息时消费者进程才能执行取消息的操作。无消息时，阻塞消费者进程。缓冲池中有空闲缓冲区时，生产者进程才能执行放消息的操作。无空间缓冲区时，阻塞生产者进程。 (3) 信号量的设置 1) 设置一个互斥信号量mutex,用于实现对公共缓冲池的互斥访问，初值为1。 2) 设置两个资源信号量，分别表示可用资源数 empty：表示缓冲池中的空缓冲区数，初值为n。 full：表示装有消息的缓冲区数，初值为0（一个缓冲区中放一个消息） (4) 同步程序 利用记录型信号量机制实现生产者一消费者问题的同步代码描述如下。 1) 生产者进程同步代码的描述。 Producer: begin repeat ... produce an item in nextp; wait(empty); //申请空缓冲区 wait(mutex); //申请公共缓冲池的互斥访问权 buffer(in)=nextp; //将消息放入in指针指向的缓冲区 in (in +1)mod n; //in指针指向下一个空缓冲区 signal(mutex); //释放对公共缓冲池的互斥访问权 signal(full); //释放消息资源 until false; end 2) 消费者进程同步代码的描述。 Consumer: begin repeat … wait(full); //申请消息 wait(mutex); //申请公共缓冲池的互斥访问权 nexte buffer(out); //从out指针指向的缓冲区中取消息 out (out +1)mod n; //out指针指向下一个装有消息的缓冲区 signal(mutex); //释放对公共缓冲池的互斥访问权 signal(empty); //释放空缓冲区 consume item in nextc; until false; end (5) 说明 1) wait和signal操作必须成对出现。 2) wait操作的顺序不能颠倒。必须先对资源信号量（即empty和full)进行wait操作，然 后再对互斥信号量进行wait操作。 3) 用记录型信号量机制解决生产者一消费者问题，对具有相互合作关系的进程，提供 了解决问题的模型。 2. 读者一写者问题 (1) 问题描述 D是多个进程共享的数据区，允许多个进程同时读D区，仅允许一个进程写D区，且有进程写D区时，不能有任何其他进程读或写D区。 数据库管理中存在这种同步问题的实例，系统允许多个用户同时读一个数据库表，但是任意时刻只允许一个用户修改它，当数据库表被用户（通常是有特殊权限的数据库管理员）修改时，任何其他用户不能读或者写这一数据库表。 (2) 信号量的设置 1) 全局变量readcount用于对进入共享区的读进程计数。 2) 互斥信号量mutex用于对多个进程共享的全局变量readcount的互斥访问。 3) 互斥信号量wmutex用于实现读操作与写操作的互斥，以及写操作与写操作的互斥。 (3) 同步程序 1) 写进程同步代码的描述。 writer; begin: wait(wmutex); … writing operation; … signal(wmutex); end; 2) 读进程同步代码的描述。 reader: begin: wait(rmutex); if readcount =0 then wait(wmutex); readcount++; signal(rmutex); … reading file from D: wait(rmutex); readcount--; if readcount =0 then signal(wmutex); signal(rmutex) end; 五、管程 信号量机制的缺陷是每个访问共享资源的进程都必须自备同步操作wait(s)和signal(s)。这就使大量的同步操作分散在各个进程中，这不仅给系统的管理带来麻烦，而且还会因同步操作的使用不当而导致系统出错，因此引入了管程的概念。 1. 管程的基本概念 (1) 管程的定义 管程是描述共享资源的数据结构和在数据结构上的共享资源管理程序的集合。其中包括变量的定义、变量的初始化代码，以及管理共享资源的过程。管程的语法描述如下。 type moniter-name=moniter variable declarations; procedure entrypl(…) begin…end; … procedure entrypn(…) begin…end; begin initialization code; end (2) 对管程的说明 1) 管程是可供程序猿调用的软件包。 2) 每次只有一个进程调用管程执行，任意时刻管程中只能有一个活跃进程。 3) 管程是一种编程语言的构建，所有编译器知道它们很特殊，并可以调用与其他过程不同的方法处理它们。 (3) 条件变量 第五节 进程通信 操作系统提供进程通信功能，以支持进程之间的信息交换。进程之间的高级通信机制分为共享存储器系统、消息传递系统和管道通信系统。 1. 共享存储器系统 在共享存储器系统中，相互通信的进程共享其些数据结构或共享存储区，进程之间能通过这些空间进行通信。共享存储系统可分为两种类型。 1) 基于共享数据结构的通信方式。在这种通信方式中，要求诸进程公用某些数据结构，以实现进程间的信息交换。例如在生产者一消费者问题中，使用有界缓冲区这种数据结构来实现进程间的通信。 2) 基于共享存储区的通信方式。为了传输大量数据，在存储器中划出一块共享存储区， 进程可以通过对共享存储区中的数据的读或写来实现通信。 2. 消息传递系统 在消息传递系统中，进程间通过操作系统提供的一组通信程序传递格式化的消息。这种方式对应用程序隐藏了通信实现的细节，使通信过程对用户是透明的。 消息传递系统中，根据源进程向目标进程传递消息方式的不同，可分为直接通信方式和间接通信方式。 1) 直接通信方式。操作系统利用发送程序直接把消息发送给目标进程。 2) 间接通信方式。进程之间的通信需要通过用于暂存消息的共享数据结构来实现，如信箱。该方式既可以实现实时通信，又可以实现非实时通信。 3. 管道通信 管道(Pipeline)是连接读写进程的一个特殊文件，也被称为管道文件。管道文件存在于外存中，其中的消息没有固定长度，能用于进程间大量的信息通信。向管道提供输人的发送进程以字符流的形式将大量的数据送入管道（写）。接受管道输出的接收进程，从管道中接收数据（读）。 4. 消息缓冲队列 消息缓冲队列机制广泛用于本地进程之间的通信。该机制包括数据结构、发送原语和接收原语，每个进程都有自己的消息缓冲队列和消息缓冲区。发送进程发送消息时，先申清一个消息缓冲区将要发送的消息从发送进程的发送区放入消息缓冲区。然后，调用发送原语将消息发送给接收进程，发送原语将发送缓冲区插入接收进程的消息缓冲队列。接收消息的进程通过调用接收原语将该进程消息缓冲队列中的消息复制到自己的消息接收区。 消息缓冲区是一个结构型数据结构。通常包括发送进程标志符、消息长度、消息正文和指向下一个消息缓冲区的指针。在采用消息缓冲队列的系统中，进程控制块中要增加指向消息缓冲队列的指针、消息队列的互斥信号量和消息队列的资源信号量等字段。消息缓冲队列需要被当作临界资源，在发送原语和接收原语中对消息缓冲队列的访问需要进行互斥与同步。 "},"pages/操作系统概论/第二章_进程管理/第六节_线程.html":{"url":"pages/操作系统概论/第二章_进程管理/第六节_线程.html","title":"第六节 线程","keywords":"","body":"第六节 线程 在传统的操作系统中，进程是进行资源分配和独立执行的基本单位。为了进一步提高程序的并发性，减少系统开销，在操作系统中引人了线程的概念。 由于进程既是独立执行的基本单位，又是资源拥有者，在进程创建、撤销和切换时需要较大的时空开销。所以，系统中所设置的进程数和进程切换的频率都受到了限制，影响了操作系统并发程度的提高。引入线程作为独立调度和分派的单位，不独立拥有资源（仅有少量基本资源)，而与其他线程共享同一进程的资源，减小了系统的时空开销。 线程的实质是把进程的任务划分成更小、具有独立功能的单位，以线程的形式来并发执行，以提高程序并发执行的程度。 一、线程的描述 1. 线程的概念和分类 (1) 线程的概念 线程是进程中的一个实体，是被系统独立调度和分派的基本单位。线程只拥有在运行中必需的资源，包括程序计数器、一组寄存器和栈，但它可与同属一个进程的其他线程共享进程所拥有的全部资源。一个线程可以创建和撤销另一个线程。同一进程中的多个线程可以并发执行。线程在运行中呈现间断性，也有就绪、阻塞和执行3种基本状态。 (2)线程的分类 线程的实现可以分为两类，即用户级线程和内核级线程。内核级线程依赖于内核，用户进程和系统进程中的线程，它们的创建、撤销和切换都由内核实现。在内核中为线程创建线程控制块，内核根据该控制块感知线程的存在并对线程进行控制。 用户级线程不依赖于内核，用户级线程的创建、撤销和切换都与内核无关。 下面从不同方面对内核级线程和用户级线程进行比较。 1) 线程的调度与切换速度。 内核级线程的调度由内核的线程调度程序完成，用户级线程的调度则由用户线程包中的一个过程来完成。内核级线程的调度程序运行在系统态，用户级线程的调度程序运行在用户态。 内核级线程的调度规则与进程调度相似，用户级线程的调度规则相对简单。内核级线程 切换慢，用户级线程切换快。 2) 系统调用。 内核级线程进行系统调用，只阻塞该线程。用户级线程的系统调用，要阻塞线程所属的 进程。 3) 线程执行时间的分配。 内核级线程的CPU时间以线程为单位分配，每一个线程都可以独享一个CPU时间片。用户级线程的CPU时间以进程为单位，同一进程的多个线程共享一个CPU时间片。 2. 线程的3种基本状态 与进程一样，线程的主要状态有就绪、运行和阻塞3种。就绪态是线程一旦获得CPU就可以投入运行的状态。运行态是线程获得CPU正在运行的状态。阻塞态是线程由于等待某个事件的发生而暂停执行的状态。 线程的3种状态转换如图2-19所示。 3. 线程控制块 (1) 线程控制块的定义 每个线程都由一个数据结构表示，包括它的基本状态、标识及记账信息。这个数据结构就是线程控制块(Thread Control Block,TCB)。TCB记录了操作系统所需要的、用于描述线程情况及控制线程运行所需的全部信息。 (2) 线程控制块中的信息 线程控制块中包含的信息有线程标识符信息、处理机状态信息、线程调度信息和线程控制信息。 (3) 线程控制块的组织方式 线程控制块通常采用链接方式来组织，把同一进程中具有相同状态的TCB用指针链接 成队列，如图2-20所示。 4. 线程与进程的关系 由于线程与进程密切相关，可从以下几个角度来说明线程与进程的关系。 1) 资源和调度。线程是程序执行的基本单位，进程是拥有资源的基本单位。 2) 地址空间资源。不同进程的地址空间是相互独立的，而同一进程中的各线程共享同一地址空间。 3) 通信关系。进程之间的通信必须使用操作系统提供的进程间通信机制、而同一进程中的各线程间可以通过直接读或写全局变量来进行通信，其至无需操作系统的参与。 4) 并发性。多进程之间可以并发执行，多线程之间也可以并发执行，而且同一进程中的多个线程之间也可并发执行。 5) 系统开销。由于创建进程或撤销进程时，系统都要为之分配或回收资源，操作系统所付出 的开销远大于创建或撤销线程时的开销。在进行进程上下文切换时，涉及整个当前进程的CPU环境的保存及新调度到进程的CPU环境的设置。而线程上下文切换时，只需保存和设置少量寄存器内容，因此开销很小。另外，由于同一进程内的多个线程共享进程的地址空间，因此，同一进程中的线程的上下文切换要更快。 图2-21和图2-22分别从执行的角度和进程管理的角度来描述线程和进程的关系。 二、线程的控制 线程控制是线程实现中最基本的功能。它包括创建新线程、终止线程、线程调度和线程切换，以及线程由于等待某个事件的发生而被阻塞与该事件发生后线程被唤醒。 1. 线程创建 (1) 用户线程的创建 用户线程的创建是通过调用线程库中的实用程序完成的。创建线程的实用程序为新线程申请空白线程控制块，并初始化线程控制块，然后将新线程插入其所属进程的就绪线程队列。 (2) 内核线程的创建 内核线程的创建是由内核完成的。内核为新线程申请空白线程控制块，并初始化线程控制块，然后将新线程插入其所属进程的就绪线程队列。 2.线程的终止 (1) 引起线程终止的原因 1) 正常结束。 2) 异常结束。 3) 外界干预 (2)线程的终止过程 如果系统发生了上述要求终止线程的事件，用户线程由线程库的实用程序来调用线程终止原语，而内核线程由内核来调用线程终止原语，然后按下列过程终止指定的线程。 1) 根据被终止线程的标识符，从TCB集合中检索出该线程的TCB,从中读出该线程的状态。 2) 若被终止线程正处于运行状态，应立即终止该线程的执行，并置调度标志为真，用于指示该线程被终止后应重新执行线程调度程序。 3) 将被终止线程的TCB从所在队列（或链表）中移出，等待其他程序来搜集信息。 3. 线程的调度与切换 (1) 用户线程的调度与切换 由于内核没有意识到用户线程的存在，所以内核继续以进程为单位进行调度，并且给该进程指定一个状态（就绪、运行或阻塞等）。用户线程的调度在应用程序内部进行，通常采用非抢占式和更简单的规则，如时间片轮转规则，也无需用户模式和内核模式之间的切换，所以速度特别快。调度算法可以是应用程序专用的，可以去适应应用程序而不会扰乱底层的操作系统调度程序。CPU时间片分配给进程，当进程内有多个线程时，每个线程的执行时间相对较少。 一个多用户线程的应用程序不能充分利用多CPU技术。内核一次只把一个CPU分配给一个进程，因此一个进程中只有一个用户线程可以执行，无法享用多CPU带来的好处。 由于所有线程管理数据结构都在一个进程的用户地址空间中，线程切换不需要内核模式特权，所以也不需要用户模式和内核模式之间的切换，节省了两种模式间切换的开销。 (2)内核线程的调度与切换 内核线程由内核来维护其上下文信息，调度是由内核以线程为单位进行的。内核线程的调度和切换都需要用户模式和内核模式之间的切换。 调度可以为一个进程中的多个内核线程分配多个CPU,使多个内核线程达到并行。 4. 线程的阻塞与唤醒 (1) 引起线程阻塞的事件 1) 请求系统服务。当运行态线程请求操作系统提供服务时，由于某种原因，操作系统并不立即满足该线程的要求，该线程只能被阻塞。 2) 启动某种操作。当线程启动某种操作后，如果该线程必须在该操作完成之后才能继续执行，则必须先将该线程阻塞，以等待该操作完成。 3) 新数据尚未到达。对于相互合作的线程，如果其中一个线程先获得另一个线程提供 的数据才能运行以对数据进行处理，则只要其所需数据尚未到达，该线程只能被阻塞。 (2) 用户线程的阻塞与唤醒 如果进程中的一个用户线程被阻塞，则整个进程都必须等待，即使还有其他用户线程可 以在应用程序内运行。 正在执行的用户线程，当需要等待某事件的发生时，该线程便调用线程阻塞原语把自己 阻塞，该线程所属的进程也调用进程阻塞原语把自己阻塞。用户线程的阻塞过程如下。 1) 停止该线程的执行，将该线程的状态改为阻塞态。 2) 将该线程控制块插入相应的线程阻塞队列。 3) 将该线程所属进程的状态改为阻塞态。 4) 将该线程所属进程的进程控制块插入相应的进程阻塞队列。 5) 将控制传递给进程调度程序，重新进行进程调度。 当被阻塞的线程所等待的事件发生时，则由有关进程唤醒原语将该线程所属的进程唤 醒，然后由有关线程唤醒原语将该线程唤醒。用户线程的唤醒过程如下。 1) 将该线程所属进程的状态由阻塞改为就绪。 2) 将该线程所属进程的进程控制块从进程阻塞队列中移出。 3) 将该线程所属进程的进程控制块插入进程就绪队列。 4) 将该线程状态由阻塞改为就绪。 5) 将该线程的线程控制块从线程阻塞队列中移出。 6) 将该线程的线程控制块插人线程就绪队列。 (3) 内核线程的阻塞与唤醒 如果进程中的一个内核线程被阻塞，内核可以调度同一个进程中的另一个内核线程运行。 正在执行的内核线程，当需要等待某事件的发生时，该线程便调用线程阻塞原语进行自我阻塞。内核线程的阻塞过程如下。 1) 停止该线程的执行，将该线程的状态改为阻塞态。 2) 将该线程控制块插入相应的线程阻塞队列。 3) 将控制传递给线程调度程序，重新进行线程调度。 当被阻塞的线程所等待的事件发生时，则由有关线程唤醒原语将该线程唤醒。内核线程 的唤醒过程如下。 1) 将该线程状态由阻塞态改为就绪态。 2) 将该线程的线程控制块从线程阻塞队列中移出。 3) 将该线程的线程控制块插人线程就绪队列。 阻塞和唤醒是一对作用相反的原语，因此，如果在某线程中调用了阻塞原语，则必须在与之相合作的另一个线程或其他相关的线程中安排唤醒原语，用于唤醒阻塞线程。 三、线程的同步 一个进程中的所有线程共享同一个地址空间和诸如打开的文件之类的其他资源。一个线程对资源的任何修改都会影响同一个进程中其他线程的环境。因此，需要对各种线程的活动进行同步，保证诸线程以互斥的方式访问临界资源，以便它们互不干扰且不破坏数据结构、线程同步的机制有原语操作和信号量机制。 四、线程通信 线程通信是指线程之间的信息交换。由于同一进程中线程间共享内存和文件资源，各线程间可以通过直接读/写全局变量来进行通信，甚至无需操作系统内核的参与。对于不同进程的线程间通信，则必须使用操作系统提供的线程间通信机制。 "},"pages/操作系统概论/第三章_进程调度与死锁/第一节_进程调度的功能与时机.html":{"url":"pages/操作系统概论/第三章_进程调度与死锁/第一节_进程调度的功能与时机.html","title":"第一、二节 进程调度的功能与时机、算法","keywords":"","body":"第三章 进程调度与死锁 当多个进程在系统中并发执行时，所有进程共享CPU。当某一CPU上运行的进程因阻塞或进程运行结束而使CPU可以分配给其他进程使用时，如何从众多就绪的可运行进程中选择一个进程，将C PU分配给该进程，使系统有效运行，是多任务操作系统必须解决的问题之一。本章介绍进程调度的算法，以及选择进程调度算法的原则。 在多任务系统中，进程共享系统资源，当多个进程竞争有限的共享资源时会发生进程无限阻塞，不能继续运行的僵死状态，即本章所述的进程死锁问题。死锁发生的原因和解决死锁的方式是本章阐述的另一个重要知识点。 第一节 进程调度的功能与时机 一、进程调度的功能 进程调度功能由操作系统内核的进程调度程序完成，在Liux内核中，进程调度功能的 实现从调用内核函数schedule()开始。进程调度的功能是按照某种策略和算法从就绪态进程 (在Linux中是可执行进程)中为当前空闲的CPU选择在其上运行的新进程。在后续章节中将阐述进程调度的策略和算法。 二、进程调度的时机 当一个进程运行结束（包括正常结束和异常结束）、进程阻塞、中断返回、在支持抢占式调度的系统中有比当前运行进程优先级更高的进程到来、当前运行进程的时间片用完时，系统都会通过执行进程调度程序重新进行进程调度。 第二节 进程调度算法 进程调度算法是指从就绪态进程中选择一个或几个进程为其分配CPU，使其进人执行态的算法。也就是说进程调度算法要“决定”把CPU分配给就绪队列中的哪个进程。进程调度由操作系统内核中的进程调度程序完成。 一、选择调度方式和算法的若干准则 不同的调度算法有不同的特点，为了根据特殊应用领域的要求选择合适的调度算法，可以下面的准则作为选择依据。 1. 周转时间短 周转时间是指从作业被提交给系统开始到作业完成为止的这段时回间隔。它包括4部分时间：作业在外存后备队列上等待调度的时间，进程在就绪队列上等待进程调度的时间，进程在CPU上执行的时间，以及进程等待I/O操作完成的时间。 对每个用户而言，都希望自己的作业周转时间尽可能短。而计算机系统的设计者和管理者使用平均周转时间和带权平均周转时间来衡量系统的时间性能。 如果系统中有n个作业，系统的平均周转时间T等于n个作业的周转时间之和除以n。其表达式如式(3-1)所示。 作业的周转时间T与系统为它提供的服务时间Ts之比为W，W=T/Ts，被称为带权周转时间，n个作业的平均带权周转时间表达式如式(3-2)所示。 服务时间Ts,是一个作业在CPU上执行的总时间。 2. 响应时间快 响应时间是指从用户提交一个请求开始直至系统首次产生响应的时间为止的一段时间，它包括3部分时间：从输入设备（如键盘、鼠标等）输入的请求信息传送到处理机的时间、处理机对请求信息进行处理的时间，以及将所形成的响应信息回送到终端显示器的时间。对于交互式系统，响应时间是衡量系统时间性能的重要指标。进程调度算法和输入/输出设备的速度都会影响系统的响应时间。 3. 截止时间的保证 截止时间是指某个任务必须开始执行的最迟时间，或必须完成的最迟时间。截止时间是评价实时系统性能的重要指标。在实时系统中，若实时任务的开始截止时间或完成截止时间不能得到保障，实时系统的可靠性就无法保证。因为实时系统计算的正确性不仅取决于计算的逻辑结果的正确性，而且取决于得到计算结果的时间。实时系统必须采取特殊的调度策略和调度算法来满足对截止时间的要求。 4. 系统吞吐量高 吞吐量是指单位时间内完成的作业数。系统的吞吐量是评价系统性能的重要指标之一：调度算法影响系统的吞吐量。 5. 处理机利用率好 CPU是计算机系统中影响时间性能的最重要的硬件资源，在多任务系统中，进程调度算法对CPU的利用率有很大影响。因此，在选择和设计进程调度算法时应该考虑使CPU的利用率尽可能高。 二、调度算法 1. 先来先服务调度算法(First-Come,First-Served,FCFS) (1) 调度算法 在进程调度中，FCFS就是从就绪队列的队首选择最先到达就绪队列的进程，为该进程分配CPU。 (2) 性能分析 FCFS适合长进程，不利于短进程，短进程等待时间相对运行时间而言太长。FCFS使短进程的周转时间过长，系统的平均周转时间也比较长。FCFS有利手CPU繁忙型进程(如科学计算)，不利于I/O繁忙型进程（如多数的事务处理），考虑以下实例。 有3个进程p1、p2和p3，分别在0、1、2时刻进入系统，需要的运行时间长度分别为24、3、3，如果按FCFS调度算法，3个进程的等待时间和周转时间如表3-1所示。 进程名 进入系统时间 开始运行时间 服务时间 等待时间 周转时间 p1 0 0 24 0 24 p2 1 24 3 23 26 p3 2 27 3 23 28 系统的平均周转时间T=(24+26+28)/3=26。 平均带权周转时间W=(24/24+26/3+28/3)/3≈6.33。 如果进程进入系统的顺序是p3、p2和p1，3个进程进入系统的时间分别是0、1、2，那么3个进程的等待时间和周转时间则如表3-2所示。 进程名 进入系统时间 开始运行时间 服务时间 等待时间 周转时间 p1 2 6 24 4 28 p2 1 3 3 2 5 p3 0 0 3 0 3 此种情况下系统的平均周转时间为T=(28+5+3)/3=12。 平均带权周转时间W=(28/24+5/3+33)/3≈1.28。 对比可见，让短进程先运行能显著降低系统的平均周转时间和平均带权周转时间。 2. 短进程优先调度算法(Shortest-Process-First,.SPF) (1) 调度算法 短进程优先(SPF)的调度算法是从就绪队列中选择估计运行时间最短的进程，将处理机分配给它，使它立即执行并一直执行完成，或发生某事件而被阻塞放弃处理机时，再重新 调度。 (2) 算法优点 与FCFS算法相比，短进程优先的算法能有效降低进程的平均等待时间，提高系统的吞吐量。 (3) 算法的缺陷 1) 对长进程不利。如果系统中不断有短进程到来，长进程可能长时间得不到调度。 2) 不能保证紧迫进程的及时处理，因为该算法不考虑进程的紧迫程度。 3) 进程的长短根据用户的估计而定，并不一定能真正做到短进程优先。 (4) 性能分析 从表3-1和表3-2的比较中可以看出短进程优先的调度算法与先来先服务调度算法相比，能降低系统的平均周转时间和带权平均周转时间。 3. 优先权调度算法(Priority-Scheduling Lgorithm) (1) 调度算法 在使用优先权调度的系统中，每个进程都有一个与之关联的优先权。优先权值通常是固定区间的数字，如0~127中的一个数。系统可能设计为优先权值越大，优先权越高；也可能设计为优先权值越小，优先权越低。优先权可以通过内部或者外部方式来定义。内部定义优先权可使用一些可测量数据以计算进程的优先权值。例如，时间极限、内存要求、打开文件的数量、平均I/O服务时间与平均CPU服务时间之比都可以用于计算优先权。外部优先权是通过操作系统之外的准则来设置的，如进程的重要性、用于支付使用计算机的费用和数量等。 当使用优先权调度算法进行进程调度时，系统将CPU分配给就绪队列中优先权值最高的进程。 (2) 优先权调度算法的类型 用于进程调度的优先权调度算法可以分为下列两种类型。 1) 非抢占式(Nonpreemptive)优先权调度算法。 在支持非抢占式调度的系统中，高优先权进程一旦得到处理机，则该进程便一直运行下去，直到完成或由于某事件使该进程主动放弃处理机。在进程运行期间，即使有高优先权进程到来，系统也不能剥夺当前进程（即正在运行的进程）的CPU使用权。高优先权进程到来时如果有别的进程正在运行，高优先权进程只能先进入就绪队列。在对截止时间要求严格的实时系统中，非抢占式调度难以保证高优先权进程得到及时调度。 2) 抢占式(Preemptive)优先权调度算法。 在支持抢占式调度的系统中，如果新到达进程的优先权高于当前正在运行进程的优先 权，那么系统会抢占CPU，把它分配给新到达的高优先权进程，而正在执行的低优先权进程暂停执行。在抢占式调度算法中，每当系统中出现一个新的就绪进程，就将其优先权与正 在执行的进程的优先权相比较，如果其优先权大于正在执行的进程的优先权，可采用立即抢占策略，立刻进行进程切换，使其投入运行。也可以采用基于时钟中断的抢占策略，在最近一次时钟中断到来时进行进程切换。 (3) 优先权的类型 如何确定进程的优先权是实现优先权调度算法的一个关键问题，优先权的计算方法对优先权调度算法的性能有很大的影响。根据优先权的变化特点可以将优先权的类型分为静态优先权和动态优先权。 1) 静态优先权。静态优先权在创建时确定，在进程的整个运行期间保持不变。静态优 先权值通常可以根据进程的类型、进程需要的资源数量和用户的要求来设定。 2) 动态优先权。进程创建时被赋予的优先权，随进程的推进或随其等待时间的增加而 改变。动态优先权调度算法可以使系统获得更好的调度性能。 (4) 优先权调度算法存在的问题和解决方案 1) 问题。优先权调度算法的一个主要问题是无穷阻塞，或称饥饿(Starving)问题。这里的阻塞是指就绪态进程因得不到CPU而等待的状态。优先权调度算法会使某个低优先权进程无穷等待CPU，在一个重负载的计算机系统中，平稳的高优先权进程流可以阻止低优先权权进程获得CPU。据说，M IT在1973年关闭 IBM 7094 时，发现有一个低优先权进程是于1967年提交但一直未运行的。 2) 解决方案。低优先权进程无穷等待间题的解决方案之一是老化(Aging)技术。老化技术以逐渐增加车系统中等待时间很长的进程的优先权，使低优先权进程在等待时间很长的情况下，优先权变高而获得CPU执行。 4. 时间片轮转调度算法(Round--Robin,RR) 时间片轮转调度算法是在现代分时系统中广泛使用的进程调度算法，UNIX、Liux和Windows操作系统郑采用基于时间片轮转、支持优先权和抢占式调度的混合式进程调度算法。 (1) 时间片轮转调度算法 在早期的时间片轮转调度算法中，系统将所有的就绪进程按先来先服务的原则，排成一个队列，每次调度时把CPU分配给队首进程，并令其执行一个时间片。当时间片用完时，调度程序终止当前进程的执行，并将它送到就绪队列的队尾。时间片是一个较小的时间单元，通常为10~100ms。Linux2.4内核给用户进程分配的时间片大小一般为50ms。 在采用时间片轮转调度算法的系统中，进程需要在CPU上运行的时间总数（这里称之为时间区间)可能小于一个时间片，也可能大于一个时间片。对于进程的时间区间小于一个时间片的情况，进程在CPU上运行结束后，进程本身会自动释放CPU。然后，由操作系统执行进程调度程序为另一个就绪进程分配CPU。对于进程的时间区间大于一个时间片的情况，进程可能需要执行若干个时间片。每当进程在CPU上连续运行的时间等于一个时间片长度时，操作系统在时钟中断处理过程中会抢占CPU，进行进程切换，用新的就绪进程替代当前进程，被替换的当前进程重新回到就绪队列中。 (2) 时间片大小的确定 在设计时间片轮转调度算法时，时间片大小的确定是一个重要的环节。时间片太长，可以使多数进程在一个时间片内处理完，能够降低进程的周转时间，但是可能造成交互用户的响应时间过长。时间片太短，一个进程要经过多次调度运行才能执行完毕，进程切换和进程调度的开销会增加，系统的平均周转时间也会比较长。例如，假定进程切换需要10s，如果时间片也是10s，那么，CPU至少把50%的时间花费在进程切换上。因此，要根据用户和系统的需要选择大小合适的时间片。选择时间片时既要考虑交互用户的需求，也要平衡系统的效率。在为调度程序确定时间片的大小时，通常要考虑以下几个因素。 1) 系统对响应时间的要求。响应时间是分时系统性能的重要指标，设计时间片大小时首先要考虑其对响应时间的影响和系统对响应时间的要求。系统响应时间为T,进程数目为N,时间片为q，有T=Nq,也就是说响应时间与进程数和时间片成比例。因此在系统允许的最大进程数一定的情况下，时间片的长短取决王系统要求的响应时间。响应时间越短，时间片取值应该越小。 2) 就绪队列中进程的数目。在分时系统中，系统允许的最大进程数往往是给定的，系统中的进程数会影响系统的响应时间，进程越多，响应时间越长。但是，操作系统的设计者希望系统的响应时间有一个上限值，不能出现因为进程多而使系统响应时间很长的情况。当设定了系统的最长响应时间值后，一时间片的大小就与系统允许的最大进程数成反比。 3) 系统的处理能力。系统的处理能力是必须保证系统支持的基本命令能在一个时间片内执行完，否则将无法保证得到满意的响应时间，而且会使平均周转时间及带权周转时间都很长。 (3) 时间片轮转调度算法的性能评价 时间片轮转调度算法的性能很大程度上依赖于时间片的大小。在极端情况下，如果时间片很大，那么时间片轮转调度算法就与先来先服务算法一样（如果就绪队列按先进先出对进程排序)。如果时间片很小，进程需要经过多次上下文切换和进程调度，会大大增加CPU用于进程切换和进程调度的开销。 5. 多级队列调度 在现代操作系统中，进程可以根据不同的特点进行分类，每一类进程属于一个就绪队列。因此，系统中的就绪队列往往不止一个而是多个，这种系统通常根据不同进程对响应时间要求的不同，采用多级队列调度算法。 (1) 多级队列调度算法(Multilevel Queue-Scheduling Algorithm) 将就绪队列分成多个独立队列，根据进程的某些属性，如需要占用的内存大小、进程优先权或进程类型，进程会被永久地分配到一个队列。每个队列有自己的调度算法。不同的队列优先权不同，调度算法也可能不同。 (2) 多级队列调度算法应用举例 多级队列调度举例：Minⅸ操作系统的进程调度。 Minx把进程按类型不同分为3类，分别对应3个优先权不同的就绪队列：优先权最高的任务进程队列、优先权次之的服务进程队列和优先权最低的用户进程队列。对任务进程和服务进程采用基于优先权的非抢占式调度，优先权高的进程一旦获得处理机将一直运行下去，直到阻塞。对优先权最低的用户进程采用时间片轮转的抢占式调度。 调度程序按如图3-1所示的流程完成进程调度。 6. 多级反馈队列调度 采用多级队列调度，一旦进程进入系统，就被固定地分配到一个就绪队列中，进程在被撤销前不会在不同队列之间移动。这种算法的优点是降低了进程调度的开销，但是不够灵活，对低优先权进程会存在无穷阻塞（饥饿）问题。而多级反馈队列调度算法(Ml-tilevel Feedback Queue Scheduling)可以弥补这些不足。 在采用多级反馈队列调度的系统中建立多个优先权不同的就绪队列，为每个队列赋予大小不同的时间片。有一种反馈策略可以规定：队列优先权越高，时间片越短，时间片通常成倍增长。新进程被创建后，先插人优先权最高的队列，仅当高优先权队列空时，才调度优先权次之的队列。同一队列中，采用时间片轮转调度算法。使用CPU时间过多的进程会被移到优先权较低的队列中，在较低优先权队列中等待时间过长的进程会被转移到较高的优先权队列中。这样就通过使用老化技术阻止了饥饿的发生。但是，不可的操作系统在设计多级反馈队列调度时采取的策略可能是不同的。多级反馈队列算法的设计要考虑以下几个方面的问题。 1) 就绪队列的数量。 2) 根据进程优先权确定进程应该进入哪个就绪队列的算法。 3) 用以确定进程何时转移到较高优先权队列的算法。 4) 用以确定进程何时转移到较低优先权队列的算法。 5) 用以确定进程在需要眼务时应该进人哪个队列的算法。 多级反馈认列调度算法的一个典型实例是Linux 2.6.11 内核的进程调度算法。 "},"pages/操作系统概论/第三章_进程调度与死锁/第三节_实时系统中的调度.html":{"url":"pages/操作系统概论/第三章_进程调度与死锁/第三节_实时系统中的调度.html","title":"第三、四、五节 实时系统中的调度、进程切换、多处理器调度","keywords":"","body":"第三节 实时系统中的调度 在第一章介绍了实时系统的特点，实时系统对处理器操作或者数据流动有着严格的时间限制，实时系统的进程调度对保证时间的要求具有重要作用。 一、实现实时调度的基本条件 为了满足实时系统对截止时间的要求，实现实时调度应具备下列几个条件。 1. 提供必要的调度信息 为了实现实时调度，系统可能需要为调度程序提供下列一些信息。 1) 就绪时间。是一个实时任务成为就绪态的起始时间。 2) 开始截止时间和完成截止时间。 3) 处理时间。指一个实时任务从开始执行直至完成所需要的时间。 4) 资源要求。关于任务执行所需要的资源信息。 5) 优先级。根据实时任务紧迫程度的不同，可以给实时任务赋予不同的优先权，使高优先权任务能优先获得系统资源，尽快得到执行。系统中实时进程的优先权不能动态降低。 2. 系统处理能力强 在实时系统中，通常有多个实时进程（通常也称其为实时任务），若处理机的处理能力不够强，处理速度不够快，从而使某些实时进程不能得到及时处理，可能发生难以预料的 后果。 (1) 单处理机情况下必须满足的限制条件 假定系统中有m个周期性的硬实时进程，它们的处理时间可表示为Ci，周期时间表示为Pi,则在单处理机情况下，必须满足如式(3-3)所示的限制条件。 例如，有6个实时进程，它们的周期时间都是50ms，也就是说每个进程都必须每隔50ms完成一次运行。处理机的处理能力为处理个进程的时间具10ms，所以该系统是无法保证所有的6个实时进程都能在截止时间内完成的，这种情况称为系统不可调度。要保证该系统是可调度的，可以提高处理机的处理能方以缩短每个实时进程的处理时间，也可以增加处理机数量。 (2) n个处理机情况下必须满足的限制条件 采用多处理机系统可以提高实时系统的处理能力，若系统的处理机个数为n，处理能力的限制条件改变如式(3-4)所示。 操作系统在提高处理能力方面是可以有所作为的，比如，设计适合特定实时应用的进程调度算法，规定实时进程具有最高的优先权，以及使调度程序和进程切换的延迟时间尽可能短。 3. 采用抢占式调度机制 在实时系统中广泛采用抢占式调度。当一个优先权更高的进程到达时，允许将当前进程暂时挂起，而令高优先权进程立即投人运行。这样可以满足实时进程对截止时间的要求。 为了使调度过程更简单，调度开销更小，对于支持需要服务时间很短的小实时进程的系统，也可以采用非抢占式调度。 抢占式调度算法根据抢占CPU的时机不同，可以分为基于时钟中断的抢占和立即抢占。 (1) 基于时钟中断的抢占式优先权调度算法 在高优先权的实时进程到达后，虽然该进程的优先权大于正在执行的进程，系统也并不立即抢占当前进程的处理机，而是等到最近一次时钟中断到来时，系统才剥夺当前进程的CPU，将CPU分配给新到来的优先权更高的实时进程。 (2) 立即抢占的优先权调度算法 在这种调度策略中，一旦接收到触发实时进程运行的信号，这通常是一个外部中断信号，系统立即剥夺当前进程的CPU，把它分配给请求中断的新的实时进程。这种算法能获得比基于时钟中断的抢占式优先算法更快的响应速度。 4. 具有快速切换机制 为保证对截止时间要求较高的实时进程能及时运行，在实时系统中还应具有快速切换机制，以保证进程的快速切换。该机制应具有以下两个方面的能力。 (1) 对外部中断的快速响应能力 为使在紧迫的外部事件请求中断时系统能及时响应，要求系统具有快速的硬件中断机构，还应使禁止中断的时间间隔尽可能短。 (2) 快速的进程切换能力 在完成进程调度后进行进程切换。为了提高进程切换时的速度，应使系统中的每个运行功能单位适当地小，以减少进程切换的时间开销。 二、常用的几种实时调度算法 1. 最早截止时间优先EDF(Earliest Deadline First,EDF)算法 该算法是根据进程的开始截止时间确定进程的优先级。截止时间越早，进程的优先级越高，越优先获得处理机。该算法要求在系统中保持一个实时进程的就绪队列，该队列按各进程截止时间的早晚排序，具有最早截止时间的进程排在队列的最前面。调度程序在选择进程时，总是选择就绪队列中的第一个进程，为之分配处理机，使之投入运行。最早截止时间优先的算法既可用于抢占式调度，也可用于非抢占式调度。 2. 最低松弛度优先LLF(Least Laxity First,LLF)算法 松弛度用来表示一个实时进程的紧迫程度，如果一个进程的完成截止时间为T,当前时间为Tc，处理完该任务还需要的时间为Ts，则松驰度L的计算式表示为 L=T-Tc-Ts 在使用最低松弛度优先算法时，调度程序在调度时机到来时，每次选择松弛度L最小的进程，把CPU分配给该进程。该算法在实现时，把进程按松弛度排序，让松驰度最小的进程处于就绪队列队首，这样调度程序执行时只需选择就绪队列队首的进程执行即可。 第四节 进程切换 当进程调度程序选择到一个新的进程后，要进行进程切换，用新选择的进程替换原来的执行进程，也就是把CPU的控制权交给由调度程序所选择的进程。 进程切换使当前正在执行的进程成为被替换进程，出让其所使用的CPU，以运行被进程调度程序选中的新进程。进程切换通常包括以下几个步骤。 1) 保存包括程序计数器和其他寄存器在内的CPU上下文环境。 2) 更新被替换进程的进程控制块。 3) 修改进程状态，把执行态改为就绪态或者阻塞态。 4) 将被替换进程的进程控制块移到就绪队列或阻塞队列。 5) 执行通过进程调度程序选择的新进程，并更新该进程的进程控制块。 6) 更新内存管理的数据结构。 7) 恢复被调度程序选中的进程的硬件上下文。 第五节 多处理器调度 随着对计算机性能的要求，尤其是对计算速度、处理能力的要求越来越高，多处理器系统得到更加广泛的应用，各类大、中、小型主机系统，网络服务器，以及高端的工作站都采用多处理器结构，甚至个人微机都使用双CPU。操作系统与计算机体系结构有着天然的联系，面对多处理器系统，必须解决多处理器的进程调度问题。 一、多处理器系统(MultiProcessor Systems,MPS)的类型 对处理器系统有多种不同的分类方式，根据处理器的耦合程度，可以把多处理器系统分为紧密耦合多处理器系统和松弛耦合多处理器系统；根据处理器结构是否相同，可以把多处理器系统分为对称多处理器系统和非对称多处理器系统。 1. 紧密耦合的多处理器系统和松弛耦合的多处理器系统 (1) 紧密耦合的多处理器系统 紧密耦合的多处理器系统通常通过高速总线或高速交叉开关实现多个处理器之间的互连，它们共享主存储器系统和I/O设备，并要求将主存储器划分为若干个独立访问的存储器模块，以便多个处理器能同时对主存进行访问。系统中的所有资源和进程都由操作系统实施统一的控制和管理。 (2) 松弛耦合的多处理器系统 松弛耦合的多处理器系统通常通过通道或通信线路来实现多台计算机之间的互连。每台计算机都有自己的存储器和I/O设备，并配置了操作系统来管理本地资源和在本地运行的进程。因此，每一台计算机都能独立工作，必要时可通过通信线路与其他计算机交换信息，以及协调它们之间的工作。 2. 对称多处理器系统和非对称多处理器系统 对称多处理器系统属于同构的多处理器系统，其中所包含的各处理单元，在功能和结构上都是相同的，当前的绝大多数多处理器系统都是对称多处理器系统 非对称多处理器系统中有多种类型的处理单元，它们的功能和结构各不相同。其中只有一个主处理器，有多个从处理器。 二、多处理器系统中的进程分配方式 1. 对称多处理器系统中的进程分配方式 在对称多处理器系统中，进程到处理器的分配可以采用以下两种方式。 (1) 静态分配 在采用这种分配方式时，操作系统为每个处理器建立一个专门的就绪队列，该就绪队列的每个进程都只能在与就绪队列对应的处理器上运行。一个进程无论经过多少次调度，操作系统都把同一个处理器分配给该进程。静态分配方式的优点是进程调度的开销小，缺点是不 能动态地平衡各处理器的负载，使系统存在各处理器忙闲不均的情况。 (2) 动态分配 动态分配的基本特征就是每个进程经过多次调度，每次获得的不一定是同一个处理器。为实现动态分配，可以在系统中设置一个公共的就绪队列，系统中的所有就绪进程都被放在 该队列中。可将进程分配到任何一个处理器上，这样，对一个进程的整个运行过程而言，在每次被调度执行时，都是随机地被分配到当时处于空闲状态的某一处理器上去执行。例如，某进程一开始被分配到处理器A上执行，后来因为阻塞或时间片用完而暂停执行，当再次回到公共就绪队列中，重新被调度时，该进程可能被分配到B处理器或C处理器上运行。动态分配的优点是可以在每次调度时考虑处理器的负载平衡问题，总是把进程分配给当前空 闲的处理器。此外，对于共享存储器的紧密耦合系统，所有的处理器可以共享保存在内存中 的进程信息。 2. 非对称多处理器系统(MPS)中的进程分配方式 对于非对称多处理器系统，大多采用主一从式操作系统，即操作系统的核心部分驻留在一台主机上，而从机上只运行用户程序，只有主机执行调度程序，所有丛机的进程都是由主机分配的。每当从机空闲时，便向主机发送一个请求分配进程的信号，然后便等待主机为它分配进程。在主机中保持有一个就绪队列，只要就绪队列不为空，主机便从其队首摘下一个进程分配给请求分配进程的从机。从机接收到分配的进程后便运行该进程，该进程结束后从机又向主机发出请求。 在非对称多处理系统中，主、从式的进程分配方式的主要优点是系统处理比较简单。这是因为所有的进程分配都由一台主机独自处理，使进程间的同步问题得以简化，并且进程调度程序也很易于从单处理器的进程调度程序演化而来。但由一台主机控制一切，也存在着不可靠性，主机一旦出现故障，会导致整个系统瘫痪，而且也容易因主机太忙而形成系统瓶 颈。克服这些缺点的有效方法是利用多台处理器来管理整个系统。这样，当其中一台处理器出现故障时，可由其他处理器来接替其完成任务，不影响系统正常运行。而且用多合处理器还具有更强的执行管理任务的能力，也不容易形成系统瓶颈。 三、进程（线程）调度方式 多处理器的调度比单处理器调度机制要复杂，下面简要介绍几种比较典型的多处理器调度方式。 1. 自调度 自调度算法是当前多处理器系统中最常用的调度方式之一，也是最简单的一种调度方式。采用自调度的系统中设置有一个公共的就绪队列，任何一个空闲的处理器都可以自行从该就绪队列中选取一个进程或者一个线程运行。在自调度方式中，可采用在单处理器环境下所用的调度算法，如先来先服务(First-Come,First-Service,FCFS)调度算法、最高优先权调度算法等。在多处理器环境下，FCFS是一种较好的自调度算法，算法简单，开销小。 (1) 自调度算法的优点 1) 易移植。在采用自调度方式的系统中，公共就绪队列可按照单处理器系统中所采用的各种组织方式加以组织，其调度算法也可沿用单处理器系统所用的算法，因此，很容易将单处理器环境下的调度机制移植到多处理器系统中。 2) 有利于提高CPU的利用率。只要系统中有任务，或者说只要公共就绪队列不为空，就不会出现处理器空闲的情况，也不会发生处理器忙闲不均的现象，因而有利于提高处理器 的利用率。 (2) 自调度方式的缺点 1) 瓶颈问题。采用自调度，在整个系统中只有一个必须互斥访问的公共就绪队列。在系统中有多个处理器的情况下，对公共就绪队列的访问很容易形成瓶颈。当系统中处理器的数目在数十个，甚至数百个时，采用自调度算法瓶颈问题会非常严重。 2) 低效性。采用自调度算法，CPU上的高速缓存的命中率较低。当进程阻塞后再重新就绪时，它只能进入唯一的公共就绪队列，经过进程调度后，不一定能在阻塞前的处理器上运行。如果在每台处理器上都配有高速缓存，则这时在其中保留的该进程的数据已经失效，而在该进程新获得的处理器上又需要重新建立这些数据的备份。由于一个进程的整个生命期中，可能要多次更换处理器，因而使高速缓存的命中率很低。 3) 线程切换频繁。在多线程系统中，通常一个应用中的多个线程是相互合作的关系。而采用自调度时，相互合作的线程很难同时获得处理器运行，这将会使某些线程因其合作线程未获得处理器运行而阻塞，进而被切换下来。 2. 成组调度 成组调度方式是由系统将一组相互合作的进程或线程同时分配到一组处理器上运行，进 程或线程与处理器一一对应。 (1) 成组调度的优点 1) 减少线程切换，改善系统性能。如果一组相互合作的线程或进程能并行执行，则能 有效减少进程（线程）阻塞情况的发生，从而减少线程的切换，使系统性能得到改善。 2) 减少调度开销。因为每次调度都可以解决一组线程的处理器分配问题，因而可以显 著减少调度频率，从而减少调度开销。 (2) 成组调度中的时间分配 在成组调度中可以采用两种方式为应用程序分配处理器时间：一是面向所有的应用程序平均分配处理器时间；二是面向所有的线程平均分配处理器时间。 3. 专用处理器分配 1989年由Tucker提出了专用处理器分配方式。该方式是在一个应用程序执行期间，专门为该应用程序分配一组处理器，每个线程一个，这组处理器供该应用程序专用，直至应用 程序完成。这种方式会造成处理器资源的严重浪费。因为当一个处理器上的线程被阻塞时，处理器不能分配给别的线程使用，只能空等。例如，有一个线程为了与另一个线程同步而被 阻塞起来，则该线程所分配到的处理器就会空闲。 之所以可以将这种专用处理器调度方式用于并发程度高得多的处理器环境，是因为在具有数十个乃至上百个处理器的并行系统中，每个处理器的投资费用在整个系统中仅占很小的比重。对整个系统的性能和效率而言，单个处理器的使用效率并不像在单处理器系统中那么重要。其次，在一个应用程序的整个运行过程中，由于每个进程或线程专用一台处理器，因此可以完全避免进程或线程的切换，从而可以大大加速程序的完成。总的来说，专用处理器的优点一是加速了应用程序的运行速度，二是避免了进程切换。 "},"pages/操作系统概论/第三章_进程调度与死锁/第六节_死锁.html":{"url":"pages/操作系统概论/第三章_进程调度与死锁/第六节_死锁.html","title":"第六节 死锁","keywords":"","body":"第六节 死锁 一、产生死锁的原因和必要条件 在多道程序系统中，多个进程可能竞争数量有限的资源。如果一个进程所申请的资源被其他处于阻塞状态的进程占有，该进程就会因为不能获得所申请的资源而被阻塞。若此时该进程恰好又占有了前述其他进程所需要的资源，那么这一组进程就可能因为等待释放自己所需要但被其他进程已占有的资源而无法向前推进。这种由于多个进程竞争共享资源而引起的进程不能向前推进的僵死状态称为死锁。 1. 产生死锁的原因 进程访问资源是通过执行程序实现的。一般来说，进程按照申请资源、访问资源和释放资源的顺序使用资源。产生死锁的原因为：竞争共享资源且分配资源的顺序不当。 2. 产生死锁的必要条件 死锁产生时，必须同时满足下列4个条件。 (1) 互斥条件 指一个进程在访问资源的过程中，其他进程不能访问该资源。如果一个资源正在被访问时，有其他进程也提出对该资源的访问请求，必须把请求该资源的进程阻塞起来，直到资源被进程释放。 (2) 请求和保持条件 进程已经保持了至少一个资源，又提出了新的资源要求，而新请求的资源已经被其他进程占有，此时进程阻塞，但又对已经获得的资源保持不放，使得其他进程无法使用被保持的资源。举例说明：进程P1获得了资源A的访间权，然后又请求资源B。但是资源B因为被别的进程占用而无法分配给进程p1。通常，操作系统会把p阻塞起来，插入资源B的等待队列中。p1被阻塞却没有释放对资源A的访问权。这时就出现了p1请求资源B而保持资源A的条件。 (3) 不剥夺条件 进程已经获得的资源不能被剥夺，只能由进程自己释放。例如，进程p1获得了资源A的访问权，进程p2再申请访间资源A，即使进程p1因为某种原因被阻塞，暂时不需要使用资源A,进程p2也不能通过系统剥夺资源A的访问权，只能等待进程p1释放对资源A的访问权。 (4) 环路等待条件 在发生死锁时，必然存在一个进程申请资源的环形链。即进程集合p0,p1,p2···，pn中的pO正在等待一个p1占用的资源；p1正在等待p2占用的资源，···，pn正在等待已经被pO占用的资源。 注意：只有当上述4个条件同时满足时才会发生死锁。 3. 实例 下面以生产者和消费者问题为例，说明产生死锁的必要条件，如图3-4所示。 如果生产者和消费者进程都先通过执行wait(mutex)申请公共缓冲池的互斥访问权，当缓冲池满且生产者和消费者进程几乎同时访问公共缓冲池时，可能发生死锁，出现4个必要条件同时满足的情况。 1) 互斥条件。若生产者进程先占用公共缓冲池访问权，消费者请求缓冲池访问权时会被阻塞。即缓冲池访问权只能被任意两个进程中的一个所占有。所有的生产者进程和消费者进程对公共缓冲池的访问是互斥的。 2) 请求和保持条件。生产者进程获得缓冲池使用权后，又提出申请空缓冲区的请求。因为缓冲池装满了消息，没有空缓冲区。因此，生产者进程无法获得空缓冲区资源而被阻塞，但生产者进程仍保持缓冲池的访问权。 3) 不剥夺条件。虽然缓冲池中有消费者进程需要的消息资源，只要消费者进程能够获得缓冲池的互斥访问权就可以执行取消息的操作，但消费者进程不能剥夺生产者已获得的缓冲池访问权。 4) 环路等待。存在生产进程等待消费者进程释放空缓冲区，消费者进程等待生产者进 程释放公共缓冲池互斥访问权的资源申请环路，如图3-5所示。 二、处理死锁的基本方法 处理死锁的基本方法有预防死锁、避免死锁、检测并解除死锁和忽略死锁问题（即假定死锁不可能在系统内发生而忽略死锁)。 为确保不发生死锁，操作系统可以采用死锁预防或死锁避免方案。 1. 死锁的预防 死锁预防是根据前面讨论的死锁必要条件，通过保证至少其中一个条件不成立来达到预防发生死锁的目的。 由于有些共享资源必须被定义为临界资源，对于这些资源的访问必须是互乐的。因此，对这类资源的访问不能通过摒弃互斥条件来预防死锁的发生，如对打印机的访问。但是，有些共享资源是允许进程同时访问的， 如允许进程同时读一个共享文件。如果进程访问的都是这些资源，是不会出现死锁问题的。在操作系统中无法预知进程是否一定不访问临界资源，所以通常不能采用摒弃互斥条件来预防死锁的发生。 预防死锁可以通过摒弃下列三个必要条件之一来实现 (1) 摒弃请求和保持条件 可以通过摒弃请求和保持条件来预防死锁的发生。摒弃请求和保持条件的一种方法是系统要求所有进程执行前要一次性地申请在整个运行过程中所需要的全部资源，只要有一个资 源申请不成功，其他所有资源也不分配给该进程、并阻塞该进程。例如生产者一消费者问题中，生产者进程执行前必须申请到公共缓冲池的互斥访问权和空缓冲区资源，只要有一种资源申请不到，就阻塞生产者进程。还有一种方法是对某些进程在申请其他资源前要求该进程必须释放已经分配给它的所有其他资源。例如，一个进程要从磁盘读数据送打印机，可以规定，必须在进程释放磁盘驱动器之后，才能申请并获得打印机。 (2) 摒弃不剥夺条件 摒弃不剥夺条件的方法是一个已保持了某些资源的进程，当它再提出新的资源要求而不能立即得到满足时，必须释放它已经保持的所有资源。具体实现时可以采取这样的策略：当一个进程p1申请某资源R1时，如果R1可用，就将R1分配给p1。如果R1不可用，系统就检查是否有其他进程占用了R1，而又申请其他资源不成功而被阻塞。如果有这样的阻塞进程，系统抢占被该进程占用的R1资源分配给p1进程。这种方法的缺点是实现复杂而且代价高。 (3) 摒弃环路等待条件 摒弃环路等待的方法是指进程必须按规定的顺序申请资源。对所有不同类型的资源排序，要求每个进程按规定的顺序申请资源。 打印机和磁盘驱动器是两种不同类型的资源，一个系统中可能存在多个打印机和多个磁盘驱动器。把某台打印机称为打印机这种类型资源的一个实例，把某个磁盘驱动器称为磁盘驱动器类型资源的一个实例。 设R={R1,R2,…,Rn为资源类型的集合。为每个资源类型分配一个唯一的整数，以比较资源的先后顺序。可定义一个函数F:R->N，其中N是自然数集合。例如，R的集合包括打印机和磁盘驱动器，那么函数F可以按以下来定义。 F(disk drive)=1 F(printer)=2 用这样的方法来摒弃环路等待的策略是：每个进程按资源排序的递增顺序申请资源。即一个进程开始可申请任何数量的资源类型R的实例。之后，当且仅当F(Rj)>F(Ri)时，该进程可以申请资源类型为Rj的实例。例如，对于以上给定的函数，一个进程需要同时使用磁盘驱动器和打印机，那么就必须先申请磁盘驱动器，后申请打印机，因为F(printer))>F(disk drive)。换而言之，当一个进程申请资源类型Rj的实例时，它必须先释放所有资源Ri，其中Ri使F(Ri) 1) 限制了新设备的增加。 2) 系统为资源分配的序号与进程实际使用资源的顺序不同，造成资源浪费。 3) 给用户编程带来了麻烦。 2. 死锁的避免 避免死锁的方法是把系统的资源分配状态分为安全状态和不安全状态，只要资源分配使系统资源分配状态处于安全状态，死锁就不会发生。在避免死锁的方法中，允许进程动态地申请资源。系统在资源分配之前，先计算资源分配的安全性。若本次资源分配不会导致系统进入不安全状态，便将资源分配给进程。否则拒绝进程的资源请求，将进程阻塞起来。 (1) 系统的安全状态 当系统能找到一个进程执行序列，使系统只要按此序列为每个进程分配资源，就可以保证进程的资源分配和执行顺利完成，不会发生死锁时，称系统处于安全状态。若系统不存在这样的安全序列，则称系统处于不安全状态。 不安全状态不一定是死锁状态，但当系统进入不安全状态之后，便可能进人死锁状态。反之，只要系统处于安全状态，系统可避免进入死锁状态。因此，避免进程死锁的实质在于使系统处于安全状态。 (2) 安全状态举例 假定系统有3个进程p1、p2、p3，共有12个某类资源R。进程p1总共要求10个R类资源，p2和p3分别要求4个和9个R类资源。设在T0时刻，系统分配资源情况如表3-3所示。 经分析可以发现在T0时刻系统是安全的。因为T0时刻存在一个安全序列，即只要系统按此进程序列分配资源，每个进程都可以顺利完成。T0时刻系统资源R的可用数量为3，如果先将其中的2个资源分配给进程p2，p2就可以顺利执行完，然后释放p2已经占用的全部4个资源。此时，系统的可用资源数量变为5。把5个R类资源全部分配给进 程p1，p1也可以顺利执行完毕。然后释放p1占用的全部资源，使系统可分配的资源数量达 到10个。将其中的7个分配给进程p3，p3即可顺利执行完毕。 (3) 不安全状态举例 设在T1,时刻，系统分配资源情况如表3-4所示，则系统处于不安全状态，因为当系统处于表3-4所示的状态时，不存在安全序列，无论进程按什么顺序推进，都无法避免死锁。 (4) 安全状态可以向不安全状态转换 处于安全状态的系统，可能由于一次资源分配，由安全状态进人不安全状态。 安全状态向不安全状态转换举例如表3-5所示。 T0时刻处于安全状态的系统，在进程p3提出申请一个R类资源后，把系统可用的3个资源中的1个分配给了p3，系统状态由T0时刻的安全状态转换为T1时刻的不安全状态。 三、银行家算法 1965年Dijkstra提出了一种能够避免死锁的资源分配算法。其基本思想是一个进程提出资源请求后，系统先进行资源的试分配。然后检测本次的试分配是否使系统处于安全状态，若安全则按试分配方案分配资源，否则不分配资源。 1. 数据结构 为了实现银行家算法，需要数据结构的支持，用m表示系统中资源的种类数，n表示系统中的进程数。需要的数据结构如下。 1) available[]是一个一维数组。表示系统中某种资源的可用数量，也就是这种资源可分配的数量，available[j]=k表示j类资源的可用数量为k，系统还可以为进程分配的j类资源为k个。 2) max[]是个n行m列的二维数组。表示各进程需要各类资源的最大数量。max[i,j]=k表示进程pi需要j类资源的最大数量为k个。 3) allocation[]是二维数组，表示某时刻已分配给进程的某类资源数。allocation[i,j]=k表示进程pi已经占有i类资源k个。 4) need[]是二维数组，表示某个进程还需要某类资源的数量。eed[i,j]=k表示进程pi还需要i类资源k个。 2. 银行家算法的说明 银行家算法分为两个过程，一是进行资源试分配的过程；二是对试分配后系统的状态做安全性检测的过程。经安全性检测，若试分配后系统状态是安全的，则分配资源。若不安全，则阻塞申请资源的进程，暂不为它分配资源。 四、死锁的检测和解除 操作系统可以不采取事先预防和避免的方法来解决死锁问题，而是检测是否有死锁发生。如果检测到系统中有死锁的进程，则解除死锁。 应用检测死锁算法必须面对的问题是：何时调用检测算法？如何检测死锁？下面先对第一个问题进行说明，然后介绍用于检测和解除死锁的资源分配图及死锁定理。 1. 何时调用检测算法 关于何时调用检测算法？答案取决于两个因素：一是死锁可能发生的频率，二是当死锁发生时受影响的进程数量。如果死锁经常发生，就应该经常调用检测算法。只有当某个进程提出资源请求且得不到满足时，才会出现死锁。在极端情况下，当进程请求资源分配而不能立即被满足的情况下，就调用死锁检测算法。但是，对于每个请求都调用死锁检测算法会引起相当大的计算开销。另一种开销相对较小的方法是只在一个不频繁的时间间隔里调用检测算法，如每小时一次，或当CPU使用率低于40%时。因为死锁最终会使系统性能下降，并造成CPU使用率下降。 2. 资源分配图 系统死锁可利用资源分配图来描述，该图由一组结点和一组边组成。如图3-8所示，用圆圈代表一个进程，用方框代表一类资源。由于一种类型的资源可能有多个，用方框中的一个小圆圈代表某一类资源中的一个资源。此图3-8资源分配图示例时，请求边由进程指向方框中的R。而分配边则应始于方框中的一个点。图3-8所示的资源分配图中，p1进程已分得了两个R1资源，并又请求一个R2资源。p2进程分得了一个R1和一个R2资源，并请求一个R1资源。 3. 死锁定理 死锁定理用于检测系统所处的资源分配状态S是否为死锁状态。 死锁定理为：S为死锁状态的充分条件是当且仅当S状态的资源分配图是不可完全简化的。 下面用一个实例阐述资源分配图的简化问题，说明什么是不可完全简化的。 实例图如3-9a所示，为了判断该图所表示的资源分配状态是否是死锁状态，对该图进行简化，简化方法如下。 在资源分配图中，找出一个既不阻塞又非独立的进程结点pi。在顺利的情况下，pi可获得所需资源而继续执行，直至运行完毕，再释放其所占有的全部资源。这相当于消去pi所有的请求边和分配边，使之成为孤立的结点。在图3-9a中，将p1的两个分配边和一个请求边消去，便形成如图3-9b所示的情况。 p1释放资源后，便可以使p2获得资源而继续运行，直到p2完成后又释放它所占有的全部资源，而形成如图3-9c所示的情况。 在进行一系列的简化后，若能消去图中所有的边，使所有的进程都成为孤立结点，则称该图是可完全简化的。若不能通过任何过程使该图完全简化，则称该图是不可完全简化的。 对于较复杂的资源分配图，可能有多个既未阻塞又非孤立的进程结点，不同的简化顺序是否会得到不同的简化图呢？有关文献已经证明，所有的简化顺序都将得到相同的不可简化图。 4. 死锁的解除 检测到系统出现死锁后，可以由系统管理员人工处理死锁，也可以让系统自动解除死锁。解除死锁的途径有两个：一是终止处于死锁状态的进程，二是抢占死锁进程占有资源。 (1) 进程终止 终止死锁的进程后，系统回收进程占有的资源。可以采用下面两种方式终止进程。 1) 终止所有死锁进程。 2) 一次只终止一个处于死锁的进程，直到死锁解除。 在采用终止部分进程的方法时，每次应选择终止哪个进程呢？原则上应该选择终止代价最小的进程。需要考虑的因素包括以下几个。 1) 进程的优先级是什么？ 2) 进程已执行了多久？进程在完成其指定任务之前还需要多长时间？ 3) 进程使用了多少资源？分别是什么类型的资源？这些资源是否容易抢占？ 4) 进程需要多少资源才能完成？ 5) 需要终止多少进程才能解除死锁？ 6) 进程是交互的还是批处理的？ (2) 资源抢占 逐步从进程中抢占资源给其他进程使用，直到死锁环被打破为止。如果要求使用抢占来处理死锁，那么有3个问题需要处理。 1) 抢占哪个进程和哪些资源？必须确定抢占的顺序以使代价最小。影响因素有：死锁进程所拥有的资源数量、死锁进程到目前为止已经消耗的执行时间等。 2) 回滚。如果从一个进程那里抢占一个资源，那么该进程因缺少资源不能从抢占点正常执行，必须将进程回滚到某个安全状态，以便从该状态重启进程。最简单的方法是完全回滚，就是终止进程并重启进程。 3) 饥饿。饥饿是进程因长时间不能获得所需要的资源而无限等待的状态。比如系统采取基于静态优先权的进程调度算法，当系统不断有高优先权进程到来时，优先权低的进程就可能长时间得不到CPU，也就不能运行，这种状态被称为饥饿状态。 处于饥饿状态的进程不影响其他进程获得资源，死锁的进程与饥饿进程一样无法继续获得资源。但是死锁一且发生，便涉及竞争资源的一组进程。系统中不会出现单个进程死锁的现象，但是可能会只有一个进程饥饿。 在抢占资源的过程中如何保证不会总是从同一个进程抢占资源而使该进程处于饥饿状态？一个解决方案是限制对同一进程进行资源抢占的次数。最常用的方法是在影响代价的因素中加上回滚次数。 "},"pages/操作系统概论/第四章_内存管理/第一_二_三节_存储器的层次结构.html":{"url":"pages/操作系统概论/第四章_内存管理/第一_二_三节_存储器的层次结构.html","title":"第一、二、三节 存储器的层次结构","keywords":"","body":"第四章 内存管理 内存是计算机系统的重要组成部分、当操作系统接收到运行某程序的命令后，要为该程序的运行分配内存资源，创建进程，并把进程的全部或部分调入内存。进程运行结束，系统要回收被撤销进程的内存空间。内存管理的目标一方面是实现内存分配、内存回收等基本内存管理功能，另一方面是要提高内存空间的利用率和内存的访问速度。每个程序员都希望自己的程序能运行在一个独占的、容量充分大、访问速度足够快的存储器上。然而，事实上计算机的内存是有限的，而且是被许多应用程序共享的。处于同一内存中的不同进程之间必然会存在竞争与相互影响。操作系统的内存管理正是解决这一问题的，其目标是充分利用现有的内存资源，为应用程序提供方便的内存使用方式和一个快速、安全且充分大的存储器。 关于内存管理，需要掌握：①计算机硬件为程序的运行提供了什么样的存储器使用方式。②操作系统为应用程序的运行提供了什么样的存储服务。③操作系统怎样将多个不同的用户进程放在一个内存中，并使它们不相互干扰地并发执行。④操作系统如何为进程提供远比物理内存大得多的虚拟内存空间。带着这些问题来学习内存管理，就会理解计算机硬件、操作系统和应用程序在内存的使用上各自的角色，以及如何相互合作，从而有一个内存管理的大局观。本章将详细介绍内存管理的基本原理和实现技术。 第一节 存储器的层次结构 内存是计算机存储系统的一部分，为了更好地理解后续章节的内容，有必要先了解存储器系统的层次结构。 存储器系统是一个具有不同容量、成本和访问时间的存储设备的层次结构，如图4-1所示。 在这个层次系统中，从高层到低层(L0~L5),较低层的存储设备速度更慢、容量更大、价格更便宜。在最高层(L0层)，是少量的快速CPU寄存器，CPU可以在一个时钟周期内访问它们。接下来是一个或多个小型或中型的基于SRAM的高速缓存存储器，可以在几个CPU时钟周期内访问它们。然后L3层是一个大的基于DRAM的主存，可以在几十或几百个时钟周期内访问它们。L3的下层L4是慢速但容量很大的本地磁盘。L5表示有些系统可能还包括一层附加的远程服务器上的磁盘，需要通过网络来访问它们。例如NTF网络文件系统这样的分布式文件系统，允许程序访问存储在远程网络服务器上的文件。 CPU寄存器保存最常用的数据。靠近CPU的容量小、速度快的高速缓存存储器作为速度相对较慢、容量较大的主存中数据和指令子集的缓冲区。主存暂时存放存储容量更大、速度更慢的磁盘上的数据。而这些磁盘常常又作为存储在通过网络连接的其他机器的磁盘或磁带上的数据的缓冲区。 如果程序需要的数据是存放在CPU寄存器中的，程序执行期间在零个周期内就可以访问到它们。如果存储在高速缓存中、需要1~10个周期，如果这些数据存放在主存中，访问它们就需要50~100个周期。而如果这些数据是存放在磁盘中的，要访问它们需要大约2000万个周期。因此，在编写程序、设计算法时要尽可能把最近将要访问的指令或数据存储在层次较高的地方，以便让CPU更快地访问到它们。 程序的执行遵循局部性原理。程序执行的局部性原理指出，程序在执行时呈现出局部性规律，即在一段较短的时间内，程序的执行仅局限于某个部分，相应地，它所访问的存储空间也局限于某个区域。关于程序执行的局部性原理有以下几个论点。 1) 程序在执行时，除了少部分的转移和过程调用指令以外，在大多数情况下是顺序执行的。 2) 过程调用将会使程序的执行轨迹由一部分内存区域转到另一部分内存区域。但研究表明，在大多数情况下，过程调用的深度都不超过5。这就是说，程序将会在一段时间内局限在这些过程的范围内运行。 3) 程序中存在很多循环结构，它们虽然由少数指令构成，但多次执行。 4) 程序中往往包括许多对数据结构的处理。例如对数组进行操作，它们往往都局限在 很小的范围内。 总的来说，局部性原理表现为时间和空间的局部性。 1) 时间局部性。如果程序中的某条指令一旦执行，则不久后该指令可能再次执行。如 果某个数据结构被访回，不久以后该数据结构可能被再次访。 2) 空间局部性。一旦程序访问了某个单元，在不久之后，其附近的存储单元也将被访问。 具有良好局部性的程序会经常访问相同的数据集合或相邻的数据集合。具有良好局部性的程序比局部性差的程序能更好地利用处于高层次的存储器，因此运行速度更快。例如，不同的矩阵初始化程序，执行相同数量的赋值操作，由于程序的不同，具有不同的局部性，其运行速度可能相差很大。 第二节 程序的链接和装入 高级语言程序必须经过编译、链接才能成为可执行程序，操作系统需要为程序的执行分配内存空间。下面介绍链接程序的功能和程序被装人内存的几种方式。 一、程序的链接 链接程序不属于操作系统的构成部分，但是它为操作系统提供可装入的程序模块。链接程序要解决的问题是将编译后的目标模块装配成一个可执行的程序。根据链接进行的时间和实现方式的不同，可以把链接分为静态链接和动态链接。 1. 静态链接 静态链接(Static Linking)是在程序运行前，用链接程序将目标模块链接成一个完整的装入模块。静态链接程序的任务一是对逻辑地址进行修改，二是变换外部调用符号。 (1) 对逻辑地址进行修改 如图4-2所示，链接前模块A、B、C的相对地址范围分别是0~L-1、0~M-1、0~N -1，通过链接程序把三个目标模块链接成一个可执行程序后，模块B和C的逻辑地址范围变成了L~L+M-1、L+M~L+M+N-1。三个独立目标模块的逻辑地址空间链接成了一个连续的地址空间。 (2) 变换外部调用符号 将每个模块中所用的外部调用符号都变换为逻辑地址。在图4-2中，经过链接后，j将模块B的外部调用CALL B变成了一条跳转到模块B在相对地址空间中的起始地址L处的指令JSR”L\"。对模块B的外部调用CALL C变成了一条跳转到模块C在逻辑地址空间中的起始地址L+M处的指令JSR\" L+M\"。 静态链接相对于动态链接而言，程序运行速度较快。但是无论程序在本次运行中会不会被执行，都将全部被链接到一个可执行文件中，使可执行文件比较大，占用的内外存空间较大，使存储开销较大。另外，使用静态链接的方式，程序开发不够灵活、方便，修改某一个模块会导致整个程序的重新链接。 2. 动态链接 采用动态链接(Run--time Dynamic Linking)，可将某些目标模块的链接推迟到这些模块中的函数被调用执行时才进行。即在程序执行时，若发现一个被调用模块尚未链接，再把它链接到调用者模块上。采用动态链接的优点是节省内存和外存空间，方便了程序开发。例如开发一个图形控件，若让图形控件以动态链接库的形式存在，这个控件就可以独立开发、独立编译和链接。但由于动态链接是在程序运行过程中从外存将被调用的模块调入内存并链接到调用者模块上，这需要运行时的时间开销，会使程序运行时的速度变慢。 二、程序的装入 在多道程序环境下，程序要运行必须为之创建进程，而创建进程后，不可避免地要为进程分配内存，并将进程的程序和数据装人内存。将一个用户的源程序变为一个可在内存中执行的程序，通常要经过编译、链接和装人3个阶段。 通常，可执行程序以二进制可执行文件的形式存储在磁盘上，为执行程序，操作系统需要把程序调人内存。 多数系统允许操作系统将用户进程放在物理内存的任意位置。因此，虽然计算机的地址空间从0开始，但用户进程的起始地址不一定是0。在绝大多数情况下，源程序需要经过编译、链接和装入几个阶段才能执行。在不同阶段，程序地址有不同的表示形式。源程序中的地址通常是符号表示，如一个整型变量counter。编译器将这些符号地址变成可重定位地址，通常是相对于本模块开始位置的地址。例如，被编译模块的起始地址为0，所有符号地址都转变成相对于0开始的一个逻辑地址。链接程序把几个目标模块的逻辑地址转变为相对于整个可执行程序的起始地址的逻辑地址。每一次的地址变化都是从一个地址空间到另一个地址 空间的映射。根据形成在内存中物理地址的时机不同，把程序的装入方式分为绝对装入方式、可重定位装入方式（静态重定位）和动态运行时装入方式。 1. 绝对装入方式 编译程序事先已知程序在内存中的驻留位置，编译时产生物理地址的目标代码，绝对装入程序按照装入模块的物理地址将程序和数据装入内存。因此装入模块被装入内存后，无需对程序和数据的地址进行修改。 2. 可重定位装入方式（静态重定位） 在多道程序系统中，众多用户进程共享内存空间，什么时候空闲、内存的哪一块区域空闲，可以让操作系统在此装入一个新进程是无法预知的。因此，编译器在编译程序时无法形成程序的物理地址。如果编译时不知道目标程序将驻留在内存的什么位置，那么编译时就必须生成可重定位的代码，其中的地址都是逻辑地址，在程序被装入内存时，再把这些逻辑地址映射为物理地址。在程序装入时对目标程序中的指令和数据地址的修改过程称为重定位。 可重定位方式的两个特点如下。 1) 编译程序使目标模块的起始地址从0开始。 2) 程序装入时，装入程序根据内存的使用情况将装入模块装入到内存的某个位置，并对模块进行重定位。 物理地址=有效逻辑地址+程序在内存中的起始地址。 在采用可重定位装入方式将程序装入内存后，程序在内存中的实际物理地址与逻辑地址通常是不相同的。如图4-3所示，程序在逻辑地址（相对地址）为1000的单元中有一条指 令LOAD 1，2500。该指令的语义是将逻辑地址为2500处的内容取出装入1号寄存器，逻辑地址为2500的单元装有数据365。若采用重定位装人方式将该用户程序装入内存实际物理地址从10000开始的内存单元，就必须根据逻辑地址和用户程序在内存中的起始地址计算并修改用户程序在物理内存中的实际地址。指令LOAD 1，2500的实际物理地址改为1000+10000，即11000。数据365所在的实际物理内存单元地址改为2500+10000，即12500。用户程序装入内存后，指令中的逻辑地址也需要修改为实际物理地址，指令L OAD 1，2500要修改为LOAD1，12500。 3. 动态运行时装入（动态重定位） 进程在装入内存后，还可能从内存的一个区域移动到另一个区域，这种情况可能发生在支持虚拟存储的系统中。一个进程在被换出之前所在的内存位置与后来被从外存重新调入内存时所在的内存位置不同，在这种情况下，地址映射必须延迟到进程执行时再进行，把这种装人方式称为动态运行时装入。 在采用动态运行时装入方式的系统中，系统将进程装入内存后，由于进程在内存中的位置可能发生移动，所以此时并不计算物理地址，而是在进程运行访存的过程中才进行地址转换，这种方式需要重定位寄存器的支持。当进程获得CPU运行时，系统把该进程在内存的起始地址存入重定位寄存器，进程在运行过程中访存时，通过重定位寄存器与被访问单元的逻辑地址计算出被访问单元的物理地址，如图4-4所示。 进程获得CPU运行时，操作系统将进程在内存中的起始地址10000存入重定位寄存器，作业运行的过程中，当要执行逻辑地址为1000的单元中的指令时，用逻辑地址1000与重定位寄存器中的地址10000相加，得到实际物理内存地址11000。CPU从11000物理单元取到指令LOAD 1，2500。当执行该指令访问逻辑地址为2500的单元取365时，先将2500与重定位寄存器的值10000相加，得到365所在的物理单元地址12500，然后从12500物理单元读取数据365 重定位寄存器是每CPU一个的，当发生进程切换时，要用获得CPU的进程在内存的起始地址更新重定位寄存器。 第三节 连续分配存储管理方式 连续分配是指操作系统分配内存时，为每个进程分配一块物理地址连续的内存空间。连 续分配方式有3种类型。 1. 单一连续区分配方式 内存中只有一个用户区，任意时刻内存中只能装人一道程序，这种分配方式仅适用于单用户、单任务的系统。 2. 固定分区分配方式 将内存用户区划分成若干个固定大小的区域，每个区域中驻留一道程序。 3. 动态分区分配方式 系统动态地对内存进行划分，根据进程需要的空间大小分配内存。内存中分区的大小和数量是变化的。动态分区方式比固定分区方式显著地提高了内存利用率。 一、单一连续分配 单一连续分配方式适用于单用户、单任务的操作系统，它把内存分为系统区和用户区，系统区仅供操作系统使用，用户区供用户使用，如图4-5所示。 系统区用于驻留操作系统，用户区用于分配给用户进程使用。为了防止用户程序对操作系统的破坏，保证系统的安全可靠，在操作系统中应该考虑设置存储器保护机制。在单用户单任务操作系统中较常用的方法是设置一个基址寄存器和一个 界限寄存器。基址寄存器中存放程序在物理内存中的最小地址，界限寄存器中存放装入用户区程序的地址范围。在CPU访问内存时，检查CPU要访问的内存单元的地址是否大于界限寄存器的值。如果大于界限寄存器中的地址范围，则是非法地址，表 示用户进程访存越界，程序执行会被中断。 在有些单用户、单任务的操作系统，如CP/M和MS-DOS中，没有设置存储器保护机制。其理由为：一是节省硬件；二是单任务单用户系统中，用户独占机器，对系统的破坏只可能是用户自己造成的，后果也不严重，不会影响其他用户程序的执行，而且操作系统很容易重装和再次启动。 二、固定分区分配 固定分区分配将用户内存空间划分为若干个固定大小的区域，在每个用户区中可以装入一道用户程序，如图4-6所示。 内存的用户区被划分成几个分区，便允许几个进程并发运行。当有一个空闲分区时，可从外存的后备队列中选择一个大小适当的作业装入该分区。当该作业结束时，释放所占用的分区，系统又可从后备作业队列中找出另一个作业调人该分区。 1. 划分分区的方法 固定分区分配的用户分区数量是固定的，每个分区的大小也是固定的。但是每个分区的大小可以相等、也可以不相等。 (1) 分区大小相等 在这种设计中，把用户区划分成大小相等的若干个分区。这种设计的缺点是内存利用率比较低。当程序太小时，该程序所占用的分区有很大一部分空间是空闲的。而程序较大时，可能找不到一个分区足以装下该程序。这种设计主要用于利用一台计算机去控制多个相同对象的场合，因为这种情况下，各用户进程需要的空间大小相同，例如，用一台计算机控制多台相同的冶炼炉。 (2) 分区大小不等 为了更好地利用内存，可以将用户区划分成大小不同、数量固定的若干个分区。为用户进程分配空间时，把大小最接近进程大小的空闲分区分配给申请内存空间的进程。使小进程占小分区，大进程占大分区，减少内存浪费。 2. 支持固定分区分配的数据结构 操作系统为了完成对固定分区的管理，必须定义一个记录用户分区大小和使用情况的数 据结构。可以使用一个如表4-1所示的记录内存状况的内存分区说明表来管理内存分配。 用C语言来描述这个分区表，可以定义一个有4个元素的结构数组。每个元素对应一个分区。结构体包括4个字段：分区编号、分区大小、分区起始地址和分区状态。数据结构定义示例如下。 struct { int num;//分区编号 int length;//分区大小 int addr;//分区起始地址 int state;//分区状态，该值为0表示分区空闲；该值为1表示分区被占用 } mem_block[4]; 写程序时数据结构的定义根据具体系统的需要和算法的需要来调整。 3. 固定分区分配的过程 需要为进程分配内存时，操作系统执行内存分配程序，搜索内存分区使用表。当找到个大小大于或等于进程需要的内存空间而且处于室闲状态的用户分区时，将该分区分配给进程，并将该分区状态改为“已占用”。在上面的数据结构中，就是将相应分区的state字段置1。 4. 固定分区的回收 当进程运行结束后，系统要回收进程占用的分区。通过执行内存回收程序完成回收操作，只要把回收分区的使用状态改为“空闲”即可，即把上面数据结构中相应分区的state字段置0。 固定分区分配实现简单，但是由于每个分区的大小固定，必然造成存储空间的浪费，使内存利用率低下。现在的多道程序系统很少使用固定分区分配的内存管理方式。但是，在一些实时控制系统中，使用固定分区分配还是简单而有效的。 三、动态分区分配 由于固定分区分配将内存空间划分成大小固定的分区，当运行需要的内存空间比分区大的进程时，固定分区无法满足要求。而当运行只需要很小空间的进程时，内存空间浪费大。使用固定分区分配内存利用率低，难以提高系统的多道程序度。动态分区分配是根据进程的实际需要，为进程分配大小合适的内存区域。系统中用户分区的数量和大小都是动态变化的。 本节主要介绍下列内容 1) 动态分区分配使用的数据结构。 2) 动态分区分配算法。 3) 动态分区的分配和回收操作。 1. 动态分区分配原理 动态分区分配算法的原理：系统初始只有一个大空闲区，当进程请求空间时，由系统根据进程需要的空间大小划分出一片空闲区分配给进程。系统运行一段时间后，内存的空闲区 可能散布在不连续的区域。系统维护一个记录当前空闲分区情况的数据结构，当进程请求内 存时，系统从所有空闲区中找到大小合适的空闲分区进行分配。系统中分区的大小和数量都 是变化的，空闲区的大小和数量也是变化的。 2. 动态分区分配中的数据结构 为实现动态分区分配，系统需要建立并维护记录空闲分区情况的数据结构。常用的数据 结构有空闲分区表和空闲分区链。 (1) 空闲分区表 如表4-2所示，系统在空闲分区表中为每一个空闲分区建立一个表项，每个表项中包括分区编号、分区大小和分区起始地址。在具体程序实现时，可以用结构数组来实现空闲分区表。数组的每一个元素对应一个表项，记录一个空闲分区的情况。每个表项包含3个字段，即分区编号、分区大小和分区起始地址。使用空闲分区表的缺点是，若设置太多表项，会浪费内存空间；设置太少的表项，当空闲分区较多时，无法记录所有空闲分区的情况。在实现时，结构数组的大小不容易确定。 在程序中可以用下列结构数组表示空闲分区表。 struct { int num;//分区编号 int length;//分区大小 int addr;//分区起始地址 } FreeMem_block[N] 数组大小根据系统管理的最大分区数确定。 (2) 空闲分区链 使用空闲分区链可以动态地为每一个空闲分区建立一个结点，每个结点包括分区大小、分区起始地址、指向前一个空闲分区结点的指针，以及指向后一个空闲分区结点的指针。空闲分区链中的每个结点占用的内存可以动态分配、动态回收。使用空闲分区链可以克服空闲分区表存在的缺点。 空闲分区链数据结构如图4-7所示。图中有5个分区，其中有3个分区是已占用分区。两个大小分别为10KB和20KB的分区是空闲分区，其起始地址分别为100KB和130KB。系统建立的空闲链表如图4-7所示，包括两个与空闲分区对应的结点。每个结点中包含了空闲分区的大小、空闲分区的起始地址和指向前一个空闲分区结点的指针、指向后一个空闲分区结点的指针。在实际系统中，也可以采用更复杂的双向链表或者双向循环链表。 使用双向空闲链表的数据结构如下所示。 Struct FreeMem { int length;//空闲分区大小 int addr;//空闲分区起始地址 TFreeMem*prcv,*next;//指向前一个结点的指针和指向后一个结点的指针 } 3. 动态分区分配算法 当进程需要内存空间时，操作系统通过执行动态分区分配算法从多个空闲分区中选定一个合适的分区分配给进程。不同的分区分配算法有不同的特点，需要不同数据结构的支持。 下面介绍3种动态分区分配算法，即首次适应算法、循环首次适应算法和最佳适应算法。 (1)首次适应算法FF(First Fit,FF) 在采用空闲分区链作为数据结构时，首次适应算法要求空闲分区链以地址递增的顺序链接。在进行内存分配时，从链首开始顺序查找，直至找到一个能满足进程大小要求的空闲分区为止。然后，再按照进程请求内存的大小，从该分区中划出一块内存空间分配给请求者，余下的空闲分区仍留在空闲链中。 该算法总是先分配低地址部分的内存空间，容易使低地址部分留下小分区，而高地址部分大空闲区较多。当进程请求大内存空间时，要找到合适的空闲分区，搜索空闲分区链需要的时间开销比较大。 此外，由于低地址部分的空闲分区反复被划分，可能留下许多难以利用的很小的空闲分区，这种难以被利用的小空闲区也被称为外部碎片或外碎片。分配给进程的分区若大于进程请求的分区，分区内会存在一部分不被利用的空间，这部分被浪费的空间称为内部碎片或内碎片。 (2) 循环首次适应算法NF(Next Fit,NF) 该算法是由首次适应算法演变而形成的。在为进程分配内存空间时，不再每次从链首开始查找合适的空闲分区，而是从上次找到的空闲分区的下一个空闲分区开始查找，直至找到第一个能满足要求的空闲分区，并从中划出一块与请求的大小相等的内存空间分配给进程。为实现该算法，应设置一个起始查找指针，以指示下一次起始查找的空闲分区，并采用循环查找方式，如图4-8所示。 循环首次适应算法的优点是：空闲区分布均匀、查找开销较小，缺点是容易使系统缺乏大空闲区。 (3) 最佳适应算法BF(Best Fit,BF) 该算法每次为作业分配内存，总是把大小与进程所请求的内存空间大小最接近的空闲分区分配给进程，避免了“大材小用”。为了加速寻找，该算法要求将所有的空闲区按分区大小递增的顺序形成一个空闲区链。这样，第一次找到的满足要求的空闲区必然是大小最接近进程需要的内存空间大小的。 最佳适应算法的优点是避免了大材小用，能提高内存利用率。但是，采用最佳适应算法容易留下难以利用的小空闲区。 下面通过一个例题来说明首次适应算法、循环首次适应算法和最佳适应算法。 例4-1：当前空闲链如图4-9所示。 空闲链图示的含义如下。 假设系统中现有3个空闲分区，第一个空闲分区起始地址为20KB,大小为120KB；第二个空闲分区起始地址为200KB,大小为100KB；第三个空闲分区起始地址为400KB,大小为60KB。若某进程p1先请求大小为30KB的内存空间，随后进程p2再请求大小为20KB的内存空间。画出分别采用首次适应算法、循环首次适应算法和最佳适应算法时，为进程p1分配完空间之后系统的空闲链和在此之后为进程p2分配完空间后的空闲链（当空闲分区大小m.size-进程请求空间的大小u.size>1KB时，从空闲分区中为进程分配u.size大小的空间，将m.size-u.size大小的空闲空间作为新的空闲区使用)。 解答： 1) 若系统采用首次适应算法，空闲区应该按地址递增顺序排列，空闲区链如题所示。按照首次适应算法，从第一个大小合适的空闲区中划分一块给进程p1,从起始地址为20KB、大小为120KB的空闲区中划分30KB空间分配给进程p1。为进程p1分配完空间之后，空闲链变为如图4-10所示。 从这个链表中可以看到，为进程p1分配30KB空间后，第一个起始地址为20KB、大小为120KB的分区因给进程p1划分了30KB空间，变为起始地址为50KB，大小为90KB的空闲分区。若在此后，进程p2申请20KB空间，则空闲分区链中的第一个分区大小也满足p2的需要，从起始地址为50KB、大小为90KB的空闲分区中再划分20KB空间分配给p2后，系统空闲区链变为如图4-11所示。 2) 若采用循环首次适应算法。系统维护的空闲区链表中要增加一个循环指针，每次为进程分配内存都从循环指针指向的结点对应的空闲分区开始。 初始空闲区链表为如图4-12所示。 进程p1申请30KB空间后，循环指针移到下一个结点，下一次分配空闲区时，从循环指针指向的结点开始搜索适合进程请求的空闲分区。系统为进程p1分配30KB空间后，系统空闲分区链表变为如图4-13所示。 此后，p2再申请20KB空间，由于循环指针指向起始地址为200KB、大小为100KB的分区对应的结点，因此系统从该结点开始搜索大小大于或等于20KB的空闲分区，从中划分出20KB空间给进程p2，为进程p2分配20KB空间后，循环指针后移，系统空闲分区链变为如图4-14所示。 3) 若采用最佳适应算法。系统维护的空闲区链表中的结点要按照空闲区的大小由小到大排序，因此在进程p1、p2申请内存空间之前，空闲区链如图4-15所示。 进程p1申请30KB空间后，空闲链变为如图4-16所示。 该链表在每一次分配空间后都要重新排序，以保证空闲分区链是按分区大小递增的顺序排序的。 此后，系统再为p2分配20KB空间后，系统空闲分区链表变为如图4-17所示。 为了更好地说明问题，假设进程p1申请30KB空间后，p2申请大小为110KB的空间，在图4-16所示的链表中查找大小合适的空闲区，只有起始地址为20KB、大小为120KB的分区能满足进程p2的请求。系统从该分区中划分110KB的空间给进程p2，之后该分区剩余10KB的小分区作为新的空闲分区。空闲分区链变为如图4-18所示。 对空闲分区链重新排序，得到的空闲分区链如图4-19所示。 4. 动态分区分配的流程 采取动态分区分配的内存管理方式，内存分配功能由内存分配程序完成，内存分配程序采用某种内存分配算法为进程分配内存。内存不再被应用程序需要时，由系统调用内存回收程序回收原来被占用的内存分区。下面说明采用动态分区分配时内存分配的流程和内存回收的流程。 (1) 内存分配流程 如果进程申请的内存空间大小为u.size，当前结点对应的空闲分区大小为m.size，size是系统规 定的一个阈值。当系统接收到申请内存的请求后，按照如图4-20所示的流程分配空闲分区。 1) 检索空闲分区链。找到满足条件m.size≥u.size的空闲区(可以采用首次适应算法、循环首次适应算法或最佳适应算法)。 2) 如果m.size-u.size≤size，则直接把该空闲分区分配给进程。否则，从m.size中划出大小为u，size的空间分配给进程，把剩余的大小为m.size-u.size的空闲空间作为新的空闲分区。 3) 将分配给进程的分区起始地址返回给内存分配程序的调用者 4) 修改空闲分区链表。 (2) 内存回收流程 内存回收的任务是释放被占用的内存区域，如果被释放的内存空间与其他空闲分区在地址上相邻接，还需要进行空间合并。 下面以采用动态分区链管理空闲分区的情况为例，说明内存回收的流程。 1) 释放一块连续的内存区域。 2) 如果被释放区域与其他空闲区间相邻，则合并空闲区。 3) 修改空闲分区链。 如果被释放的内存区域（回收区）与其他任何的空闲区都不相邻，则为该回收区建立一个空闲区链的结点。使新建结点的起始地址字段等于回收区起始地址，空闲分区大小字段等于回收区大小。根据内存分配程序使用的算法要求（按地址递增顺序或按空闲分区大小由小到大排序)，把新建结点插入空闲分区链的适当位置。 如果被释放区域与其他空闲区间相邻，需要进行空间合并，在进行空间合并时需要考虑 以下3种情况。 1) 仅回收区的前面有相邻的空闲分区，如图4-21a所示。在这种情况下，把回收区与 空闲分区R1合并成一个空闲分区，把空闲链中与R1对应的结点的分区起始地址作为新空 闲区的起始地址，将该结点的分区大小字段修改为空闲分区R1与回收区大小之和。 2) 仅回收区的后面有相邻的空闲分区，如图4-21b所示。在这种情况下，把回收区与空闲分区R2合并成一个空闲分区。把空闲链中与R2对应的结点的分区起始地址改为回收区起始地址，将该结点的分区大小字段修改为空闲分区R2与回收区大小之和。 3) 回收区的前、后都有相邻的空闲分区，如图4-21c所示。在这种情况下，把回收区与空闲分区R1、R2合并成一个空闲分区。把空闲链中与R1对应的结点的分区起始地址作为合并后新空闲分区的起始地址，将该结点的分区大小字段修改为空闲分区R1、R2与回收区三者大小之和，删去与R2分区对应的空闲分区结点。当然，也可以修改分区R2对应的结点，而删去R1对应的结点。还可以为新合并的空闲分区建立一个新的结点，插入空闲分区链表，删除R1和R2对应的分区结点。 为了进一步说明回收流程来看下面的例题。 例4-2：当前空闲链如图4-22所示。 空闲链结点分别对应起始地址为30KB，大小为10KB；起始地址为60KB，大小为100KB；起始地址为500KB，大小为200KB的空闲分区。 假如系统采用首次适应算法分配内存，请画出分别回收如表4-3中所示的空闲区之后的空闲链。 解答： 当回收区是F1时，F1前面的空闲区起始地址为30KB，大小为10KB，地址范围是30~40KB。F1后面（高地址部分）的空闲区起始地址为60KB，大小为100KB，地址范围是60~160KB。而F1的起始地址为50KB，大小为5KB，F1的地址范围为50~55KB。可见，回收区F1前后都没有相邻的空闲分区，因此，要为回收F1创建新的空闲分区链表结点，结点的起始地址字段为F1的起始地址50KB，结点的空闲分区大小字段为F1的大小5KB。按首次适应算法的要求，将该结点按空闲分区起始地址递增的顺序排序插入，得到回收F1后的系统空闲分区链表如图4-23所示。 当回收区是F2时，F2前面的空闲区起始地址为30KB，大小为10KB，地址范围是30~40KB。F2后面（高地址部分）的空闲区起始地址为60KB，大小为100KB，地址范围是60~160KB。而F2的起始地址为40KB，大小为10KB，F2的地址范围为40~50KB。可见，回收区F2与前面起始地址为30KB，大小为10KB，地址范围是30~40KB的空闲分区相邻。因此，将该空闲分区与F2合并，合并后的空闲区起始地址为30KB，大小为20KB，得到回收F2后的系统空闲分区链表如图4-24所示。 当回收区是F3时，F3前面的空闲区起始地址为30KB，大小为10KB，地址范围是30~40KB。F3后面（高地址部分）的空闲区起始地址为60KB，大小为1O0KB，地址范围是60~160KB。而F3的起始地址为50KB，大小为10KB，F3的地址范围为50~60KB。可见，回收区F3与后面起始地址为60KB，大小为100KB，地址范围是60~160KB的空闲分区相邻。因此，将该空闲分区与F3合并，合并后的空闲区起始地址为50KB，大小为110KB。得到回收3后的系统空闲分区链表如图4-25所示。 当回收区是F4时，F4前面的空闲区起始地址为30KB，大小为10KB，地址范围是30~40KB。F4后面（高地址部分）的空闲区起始地址为60KB，大小为100KB，地址范围是60~160KB。而F4的起始地址为40KB，大小为20KB，F4的地址范围为40~60KB。可见，回收区F4与前面起始地址为30KB，大小为10KB，地址范围是30~40KB的空闲分区相邻。与后面起始地址为60KB，大小为100KB，地址范围是60~160KB的空闲分区也相邻。因此，将F4前后两个空闲分区与F4合并，合并后的空闲区起始地址为30KB，大小为130KB。得到回收F4后的系统空闲分区链表如图4-26所示。 "},"pages/操作系统概论/第四章_内存管理/第四节_基本分页存储管理方式.html":{"url":"pages/操作系统概论/第四章_内存管理/第四节_基本分页存储管理方式.html","title":"第四节 基本分页存储管理方式","keywords":"","body":"第四节 基本分页存储管理方式 把进程离散地存储在内存中物理地址不连续的区域中，这种内存管理方式称为离散内存管理方式。为了支持虚拟内存管理，需要引入离散内存管理方式。根据离散内存管理分配内存空间的基本单位的不同，将其分为3种不同的管理方式：分页存储管理、分段存储管理和段页式在储管理。 本节详细说明分页存储管理的基本原理。 一、分页存储管理的基本原理 1.基本概念 (1) 页(Page) 将一个进程的逻辑地址空间分成若于个大小相等的片，称为页。 (2) 页框(Page Frame) 将物理内存空间分成与页大小相同的若干个存储块，称为页框或页帧。 (3) 分页存储 在为进程分配内存时，以页框为单位将进程中的若干页分别装入多个可以不相邻接的页框中。如图4-27所示，一个进程的逻辑地址空间包括两个页：0号页和1号页。0号页存放在0号页框中，1号页存放在2号页框中。 (4) 页内碎片 进程的最后一页一般装不满一个页框，而形成了不可利用的碎片，称为“页内碎片”是一种内部碎片。 (5) 页表(Page Table) 页表是系统为进程建立的数据结构，页表的作用是实现从页号到页框号的映射，在进程地址空间内的所有页(0~n)，依次在页表中有一个页表项，其中记录了相应页在内存中对应的页框号。如图4-27所示，一个进程的逻辑地址空间划分成了两个页：0号页和1号页。0号页存放在0号页框中，1号页存放在2号页框中。该进程的页表中有两个表项，0号表项中存放了页框号0，指示0号页存放在0号页框中。1号表项中存了页框号2，指示1号页存放在2号页框中。在基本的分页机制中，每个进程有一个页表，进程的每一个页在页表中有一个对应的页表项。页表在内存中连续存放。 2. 基本分页存储管理方式中的地址结构 基本分页的逻辑地址结构包含两部分：页号P和页内偏移量W。若用m位表示逻辑地址，页大小为2^n字节（如512字节，1024字节），则用低n位表示页内偏移量W,用高m-n位表示页号P。 以32位地址为例：可用0~11位表示页内偏移，n=12，页大小=页框大小=4KB。12~31位(20位)表示页号，共可有2^20个页即1M个页。这种地址结构可以表示4G的逻辑地址空间，地址结构如图4-28所示。 若A为逻辑地址，L为页大小，P为页号，W为页内偏移量，则有以下计算关系。 P=INT(A/L) W=MOD (A/L) 例4-3：逻辑地址为十进制的5236，系统页大小为4KB,该逻辑地址所在的页号P和页内偏移地址W计算如下。 P=INT(5236/4096)=1 W=M0D(5236/4096)=1140 在实际系统中，是根据系统设计好的页大小和地址结构，由硬件从逻辑地址A中分离出页号和页内偏移地址。若系统的地址结构为32位，页大小为4KB，则硬件将逻辑地址A的高20位解释为页号，低12位解释为页内偏移，如图4-29所示。 例4-4：已知逻辑地址A为0x503200A0，若地址结构如图4-28所示，请问该地址对应的页号和页内偏移地址分别是什么？ 逻辑地址A在题中以十六进制表示，其对应的二进制地址为0101 0000 0011 0010 0000 0000 1010 0000,高20位为0101 0000 0011 0010 0000，此为页号。低12位为0000 1010 0000，此为页内偏移地址。页号的十六进制数为0x50320，页内偏移地址的十六进制数为0xA0。 3. 分页地址变换 为了能将用户地址空间中的逻辑地址变换为内存空间中的物理地址，在系统中必须设置地址变换机构，该机构的基本任务是实现逻辑地址到物理地址的变换。 分页地址变换原理如图4-31所示。 图4-31所示的地址变换过程如下。 1) 进程执行，PCB中页表起始地址和页表长度送CPU的页表寄存器 2) CPU访问逻辑单元A。 3) 由分页地址变换硬件自动将A分为页号和页内偏移两部分 4) 由硬件检索页表，得到A所在的页对应的页框号 页号对应的页表项起始地址=页表起始地址+页表项长度×页号（页表项中存有页框号)。从该地址指示的内存单元中读取页框号 5) 页框号和页内偏移地址送物理地址寄存器，计算物理地址。物理地址=页框大小×页框号+页内偏移量。 例4-5：在这个实例中假定页大小为1KB，进程的逻辑地址空间有1026个单元，逻辑地址为0~1025。将逻辑地址空间划分为两个页，即0号页和1号页。逻辑单元1025落在1号页中。0号页存放在0号页框中，1号页存放在2号页框中。此时页表存储了这一对应关系。可以很显然地看出，逻辑单元1025中的内容实际上存放在物理地址为2049的物理单元内，如图4- 32所示。 当执行汇编指令Move eax，(1025)时，如何根据程序中给出的逻辑单元1025计算出对应的物理地址呢？ 1) 计算页号p。 由题意可知，页大小为1KB,即1024个字节。 P=INT(A/L)=INT(1025/1024)=1。 2) 搜索页表可知，1号页存放在2号页框中。 3) 页内偏移地址W=M0D(A/L)=M0D(1025/1024)=1。 4) 逻辑单元1025所对应的物理单元地址=页框号×页框大小+页内偏移地址=2×1024+1=2049。 由图4-32可见逻辑单元1025内存放整数200，逻辑单元1025相应的物理内存地址为2049。 例4-6：某计算机系统按字节编址，采用一级页表的分页存储管理方式，分页逻辑地址格式如图4-33所示。 页表如图4-34所示。 请问逻辑地址0x01000001经过地址转换后的物理地址是什么？ 解答： 由题中给出的地址结构可知：硬件将逻辑地址的高20位解释为页号，低12位解释为页内偏移。逻辑地址0x 0100 0001的二进制表示为0000 0001 0000 0000 0000 0000 0000 0001。该逻辑地址的高20位为0000 0001 0000 0000 0000，此为页号，其十六进制表示为0x01000。该逻辑地址的低12位为0000 0000 0001，此为页内偏移，其十六进制表示为0x001。由页号检索页表可知该逻辑单元所在的页对应的页框号为0x00110。 物理地址=0x00110×0x1000+0x001=0x00110001 4.页大小的选择 在分页系统中，页的大小是由机器的体系结构和操作系统共同决定的。对于某一种机器只能支持几种固定的页大小，一旦操作系统选定了一种页大小，系统中所有进程的页大小就固定不变了（例如，分页单元把低12位逻辑地址解释为页内偏移地址，则页大小就是4KB)。 在设计系统时，如何选择页大小呢？选择较小的页，可以减小页内碎片及所有进程页内碎片的总和。但较小的页会使进程的页数较多，页表较长，页表所需占用的连续内存空间比较大。而且较小、较多的页会导致进程的缺页率高，页换入、换出频繁。若选择较大的页，页内碎片会增大。为了保证系统有较好的时间和空间性能，要慎重选择大小合适的页，一般页的大小为2的整数次幂。在目前的计算机系统中，大多选择4KB大小的页。 归纳影响页大小设计的因素如下。 (1)管理内存的开销 选择较小的页，会导致进程被划分为较多的页，页表过长，占用大量内存空间。同时，进程的缺页率和置换率都会比较高，内存管理的时间开销相对大。 (2)内存的利用率 选择较小的页，有利于提高内存的利用率，但存在(1)所述的缺点。选择较大的页，可克服(1)所述的缺点，但页内碎片大，空间利用率降低。 页大小一般是2的整数次幂，大小为512B~4KB。现在硬件可以支持多种不同的页大小，页大小可以选择4KB、16KB、2MB、4MB、8MB和16MB等。如果系统支持的通常是大作业，可以选择较大的页。而支持较小的交互式应用的系统可以选择较小的页，现在的交互式系统大多选择4KB的页。 二、快表 页表的硬件实现有多种方法，最为简单的一种方法就是将页表存放在一组专用寄存器中，进程调度程序选中新的进程执行时，更新这组寄存器的值，把当前进程的页表装入寄存器。操作系统使用特权级指令装入或修改页表寄存器的值。早期的DEC PDP-11就采用这种结构。但是，目前使用的计算机系统都允许非常大的进程页表，一个页表中的表项可能多达上百万个，把这么大的页表存放在寄存器中显然是不合适的，所以，现在的计算机系统基本上都把页表存放在内存中。在这种情况下，CPU要访问内存读写数据或读取指令，必须访问两次内存。第一次访问内存，从内存页表中获取访存单元所在的页框号，以形成访存单元的物理地址。第二次访存是根据计算出的物理地址实现对内存单元的访问，读写数据或读取指令。每一次的有效访存都必须要访问页表，因此访问页表的效率很重要。而两次访存都有时间开销，为了减少CPU在有效访存上的时间开销，提高访存速度，在硬件上引入了快表机制。 1. 什么是快表 快表也称转换后援缓冲(Translation Look-aside Buffer,TLB)，是为了提高CPU访存速度而采用的专用缓存，用来存放最近被访问过的页表项。TLB是关联的快速闪存。TLB的条目由两部分组成：键和值。键部分对应页号，值部分对应页所在的页框号。当关联内存查找TLB中的页表项时，会同时与所有键进行比较，如果找到条目，就得到相应的值域，从而得到页的页框号。这种查找方式比较快，但是硬件比较昂贵。通常，TLB的条目数很有限，在64~1024个之间。 2. 引入TLB之后的地址变换过程 引入TLB之后的地址变换过程可以概括为以下3个步骤。 1) CPU产生分页的逻辑地址页号和页内偏移后，将该逻辑地址的页号提交给TLB。 2) 查找TLB,如果找到页号，则把该页所在的页框号用于形成物理地址。否则(TLB失效)查找内存页表，从内存页表中找到相应的页表项，读取页所在的页框号，以形成物理地址。 3) 如果所查找的页表项不在TLB中，在访问完内存页表后，要把找到的页表项中的页号和页框号写到TLB中。如果TLB中的条目已满，系统会根据某种策略（如最近最少使用替换)选择一个TLB中的条目，用刚访问的页表项信息替换选中的这个TLB条目。 有些系统中允许TLB中的某些条目是固定不变的，也就是说这些条目是永远不会被更新和替换的。这些被固定的TLB条目通常是与操作系统内核代码相关的条目。 带TLB的分页硬件如图4-35所示。 使用TLB还有一个很重要的问题是，TLB是系统中所有进程公用的一套硬件缓存，如何保证在发生进程切换后，新的当前进程不会错用了TLB中保存的旧进程的页表项？下面用一个实例来说明这个问题。系统中有两个进程p1和p2，进程p1的1号页存放在第100号页框中，在进程p1的页表中有一个相应的页表项如下。 页号 页框号 1 100 进程p2的1号页存放在200号页框中，在进程2的页表中有一个相应的页表项如下。 页号 页框号 1 200 当进程p1在CPU上运行时，页表项【1,1O0】在进程访问过1号页后会存入TLB。当TLB中还存留有进程p1的页表项【1,1O0】时，如果发生进程切换，p2开始在CPU上运行。当CPU访问进程p2的1号页时，如何保证不会错误地找到【1,100】这个条目，而能正确地找到【1,200】这个条目？ 解决途径如下。 1) 每当进程切换时就刷新一次TLB,保证TLB中只有当前进程的页表项。 2) 在每个TLB页表项中保存地址空间标识符(Address-Space Identifier,ASID)。ASID用来唯一标识进程。当TLB试图命中一个页表项时，为了确保这个页表项属于当前进程，TLB会确保当前进程的ASID与被查找的TLB页表项的ASID相同。 3. 引入TLB的性能分析 在TLB中找到某一个页号对应的页表项的百分比称为TLB命中率。当能在TLB中找到所需要的页表项时，有效访存时间等于一次访问TLB的时间加上一次访问内存的时间。当没有在TLB中找到所需要的页表项时，访存时间等于一次访问TLB的时间加上两次访问内存（一次访问内存页表，一次访问内存读写数据或指令）的时间。 下面以一个实例来说明引入TLB对改善有效访存速度所起的作用。 例4-7：若CPU访问内存的速度为120ns，访问TLB的速度为20ns，试比较有TLB和无TLB系统的平均有效访存时间。假定TLB的命中率为90%。 1) 有TLB系统的有效访存时间=(120+120+20)×10%+(120+20)×90%=152ns 2) 无TLB系统的有效访存时间=两次访问内存的时间=120+120=240ns 无TLB系统的有效访问时间比有TLB系统的有效访问时间慢(240-152)/152=57.9%。 三、两级和多级页表 现代计算机都支持很大的逻辑地址空间，如32位系统的4GB，64位系统的224TB。使用前面所讲的基本分页存储，回忆以下3个知识点。 1) 每个进程有一个页表。 2) 每个页表至少包含与进程页数相同的页表项数 3) 页表必须连续存放。 当系统支持的逻辑空间很大时，意味着系统允许每个进程的逻辑地址空间很大。大进程包含的页数多，其页表中的页表项数就多。因为一个系统的页表项长度是固定的，所以，页表项数越多，页表的长度越大，存放页表所需要的连续地址空间也越大。大多数系统都支持32位逻辑地址空间，即4GB的逻辑地址空间。随着计算机硬件的更新换代，逻辑地址空间和物理内存空间都会更大。如果系统支持的页大小为4KB，那么在32位系统中每个进程可以拥有4GB/4KB=1M个页。相应的，页表中有至少需要1M个页表项，如果每个页表项的长度为4个字节（存放一个页表项需要4个字节的物理内存空间），那么一个进程的页表需要1M×4B=4MB的连续物理空间来存放。从系统性能考虑，不希望用这么大的连续地址空间存放页表，解决的办法就是把页表再分页，形成两级或多级页表。由此，可以将页表离散地存放在物理内存中。页表离散存放，也可以在支持虚拟存储管理时，将一部分页表存放在外存中。 1. 两级页表 两级页表是将页表再进行分页，使每个页表分页的大小与内存页框的大小相同，并为它们编号。将这些页表分页分别放入不同的、不一定相邻的页框中为离散分配的页表再建立一张外层页表，本书称之为页目录表，页目录表中的每个表项中记录了页表分页所在的页框号。 以前面系统支持大小为4GB的逻辑地址空间、页大小为4KB的情况为例，一个逻辑地址空间为4GB的进程，拥有1M个页，页表中有1M个页表项，如果每个页表项的长度为4个字节，那么一个进程需要1M×4B=4MB的连续物理空间来存放。使用二级页表时，把4M大小的页表分成1024个页表分页，这1024个页表分页中包含的是页表项，本书中把这种页表分页称为页表。每个页表占4KB空间，连续存放。不同的页表可以离散地存放在页框号不连续的物理空间内。总结一下，大小为4GB的进程，其页表有1K个，每个页表大小为4KB，1K个页表离散存放，如图4-36所示。 (1) 两级页表的逻辑地址结构 在二级分页系统中，为了能在地址映射时得到离散存放的页表在物理内存中的地址，需要为页表再建立一个连续存放的外层页表，本书也称之为页目录表。页目录表的表项 中存放了每一个页表在物理内存中所在的页框号。为了能实现逻辑地址到物理地址的映射，必须把要访问的逻辑地址解析成如图4-37所示的结构。 页目录号p1实际上是一个索引值，根据p1从页目录表中找到页表所在的页框号。页号p2是页表中的偏移量，根据p2可以知道应该从页表的第p2项中找到进程页所在的页框号。 根据如图4-38所示的实例做简单说明：如果由逻辑地址A得到p1=0，p2=2，那么A所在的进程页存放在哪个页框中？ p1=0时，根据p1值在页目录表中找到0号表项，该表项中存放的页框号是1011，说明0号页表存放在1011号页框中。系统根据这个数据，可以在内存中找到0号页表。然后根据p2=2，找到0号页表的第2个表项。其中存放的页框号是6，说明A所在的进程页存放在6号页框中。得到这个页框号后就可以计算A对应的物理地址了。物理地址=A所在的页框号×页框大小+页内偏移地址d。 (2) 两级页表的寻址 使用两级页表的系统，当进程切换时，要运行的进程的页目录表起始地址被写入CPU寄存器，可以称之为页表寄存器，地址映射的过程如下。 1) 对于给定的逻辑地址A，由硬件从中分离出页目录号p1、页号p2和页内地址d。 2) 由页表寄存器的值和页目录号p1，从存放页目录的页框中找到页表所在的页框号。页表所在的页框号在内存中的地址=页目录起始地址+p1×页表项长度，从该地址指示的物理内存单元中读取页表所在的页框号。 3) 由页表所在的页框号和页号p2，从存放页表的页框中找到进程页所在的页框号。进程页所在的页框号在内存中的地址=页表的起始地址+p2×页表项长度。 页表的起始地址=页表所在的页框块号×页框大小。 4) A的物理地址=进程页所在的页框号×页框大小+页内地址d。 可以结合图4-39理解上述寻址过程。 例4-8：某系统逻辑地址和物理地址均为32位，采用两级分页，页大小为4KB，地址 结构如图4-40所示。 已知逻辑地址为0x00801004，2号页表如图4-41所示。求其对应的物理地址。 答案：逻辑地址0x00801004的二进制地址为0000 0000 1000 0000 0001 0000 0000 0100 由图中给出的地址结构可知，取二进制逻辑地址的高10位作为页目录号，得到页目录号为0x2。 取二进制逻辑地址的中间10位作为页号，得到页号为1。 取二进制逻辑地址的低12位作为页内偏移，得到页内偏移值为0x4。 由2号页表可得到页号为1的页表项中存放的页框号为0x208,即进程页所在的页框号。 逻辑地址0x00801004对应的物理地址为：0x208×0x1000+0x4=0x208004。 (3) 减少页表占用内存的方法 为了减少页表所占内存，可以将当前所需要的页目录表和页表存放在内存中，其余页表存放在外存中，当所需页表不在内存中时，产生中断，将请求的页表调入内存。 2. 多级页表结构 对于64位的机器，使用二级页表，仍存在连续占用大量内存的问题，可以采用多级页表结构，将外层页表再分若干页，然后为外层页表建立连续存放的索引表。 例4-9：64位机器若仍然采用二级页表，页表项占4个字节，页大小为4KB,低12位表示页内偏移，中间10位表示页表项在页表中的表项号，那么外层页号就可以使用42位。表示系统中可以有2^42个页表分页，外层页表就有4096G个页表项，需要4×4096G字节的连续物理内存来存放外层页表。因此，需要对外层页表再进行分页，建立更外层的页表，形成多级页表。 四、反置页表 1. 反置页表 在前面所介绍的分页系统中，每个进程都有一个页表，进程逻辑地址空间的每个页在页表中都有一个相应的页表项用来存放页所在的页框号。现代系统中可能存在大量进程，每个进程都允许很大的逻辑地址空间，因而进程可能拥有一个很大的页表，这些页表会占用大量的物理内存空间。为了解决这个问题，可以使用反置页表，为每一个页框设一个表项，表项中存放进程号和页号，系统只维护一张反置页表即可。由于物理存储空间小于逻辑存储空间，所以使用反置页表减少了页表占用的内存空间。 2. 反置页表的地址映射 在利用反置页表进行地址变换时，是用进程标志符（进程号）和页号去检索反置页表以获取页框号。地址映射过程如下。 1) 根据进程号和页号找到页框号。 2) 物理地址=页框号×页框大小+页内偏移地址。 五、空闲页框的管理 操作系统需要跟踪和记录空闲页框的信息，下面介绍两种管理空闲页框的方式。 1. 使用位图管理空闲页框 使用位图管理空闲页框时，使位图中的每一位对应一个页框，具有N个页框的内存需要至少有N个二进制位的位图。当某个二进制位对应的页框被占用时，将该位置1。当该页框空闲时，该位置0。当操作系统为进程分配页框时，检索位图，找对应位为0的页框分配给进程（具体实现时用0表示空闲，1表示被占用，反之也可）。 2. 使用空闲页框的链表 使用空闲页框链表时，系统维护记录空闲页框信息的链表。空闲页框链表可以按地址递增的顺序排序，每个结点中包含页框的地址信息、指向后面结点的指针和指向前面结点的指针。 "},"pages/操作系统概论/第四章_内存管理/第五节_基于分页的虚拟存储系统.html":{"url":"pages/操作系统概论/第四章_内存管理/第五节_基于分页的虚拟存储系统.html","title":"第五节 基于分页的虚拟存储系统","keywords":"","body":"第五节 基于分页的虚拟存储系统 虚拟存储器是指具有请求调入功能和置换功能，能从逻辑上对内存容量进行扩充的一种存储器系统。在虚拟存储器系统中，进程无需全部装入，只要装入一部分即可运行。 虚拟存储技术实现的基本思想是，只把进程的一部分装入内存。进程执行过程中，CPU访问内存时如果发现所访问的内容不在内存中，则通过异常处理将所需要的内容从外存调入内存。也就是说先将进程的一部分装入内存，其余的部分什么时候需要，什么时候请求系统装入，这就是请求调入。如果在请求系统装入进程在外存中的某一部分时，没有足够的内存，则由操作系统选择一部分内存中的进程内容换出到外存，以腾出内存空间把当前需要装入的内容调入内存，这就是置换。 虚拟存储技术至少能带来以下3点好处。 1) 提高内存利用率。因为虚拟存储技术允许只把进程的一部分装入内存，原则上尽量把必须或常用的部分装入内存。 2) 提高多道程序度。因为只把每个进程的一部分装人内存，因此可以在内存中装人更多的进程。 3) 把逻辑地址空间和物理地址空间分开，使程序员不用关心物理内存的容量对编程的限制。 虚拟存储系统具有以下几个主要特征。 1) 离散性。离散性是指进程可以分散地存储在物理内存中。分页、分段和段页式存储都属于离散的内存管理方式。离散性是实现虚拟存储管理的基础。 2) 多次性。多次性是指不必把进程一次性全部装入内存，可以先将执行当前进程所必需的部分代码和数据装入内存，其余部分等进程运行需要时再装入，可以将进程分多次装入内存。 3) 对换性。对换性是指在内存中的进程可以换出，以腾出内存空间换入外存中的进程。为了提高内存的利用率，为程序员提供足够大的虚拟空间，在进程运行期间，系统可以将内存中暂时不用的程序代码或数据换出到外存，以后需要这些程序和代码时再由系统调入内存。 4) 虚拟性。虚拟性是指虚拟存储系统为用户提供了比实际物理内存大的逻辑内存空间，使程序员不必在编程时受物理内存空间大小的限制。虚拟性是实现虚拟存储系统的最重要的目标。 请求分页系统是最基本、最常用的虚拟存储系统的实现方式。其基本原理是，把进程的 逻辑地址空间分成大小相同的页，操作系统创建进程时只把进程的一部分页调入内存。进程运行过程中访问内存，若发现所访问的页不在内存中，则产生一个缺页异常信号，系统响应缺页异常，请求调入缺页。若调入缺页时内存已满，则需要先从内存中选择一个或若干个页换出到外存空间，以腾出内存空间容纳请求调入的缺页。本节将详细说明请求分页存储管理的实现原理。 一、请求分页中的硬件支持 为了实现请求分页，需要特殊的页表（相对于基本分页存储的页表而言）、缺页异常机构和支持请求分页的地址变换机构。 1. 页表 页表是支持请求分页系统最重要的数据结构，其作用是记录描述页的各种数据、包括在实现逻辑地址到物理地址映射时需要的页号与页框号的对应关系。除了页号和页框号之外，页表中增加了请求换入和页置换时需要的数据。例如，表示页是否在内存中的状态位，以及记录页被访问情况的字段。不同的操作系统可能为页表项定义不同的字段，在支持请求分页的系统中一般包含以下基本字段，页表项的构成如图4-42所示。 1) 页框号：存放页所在的页框号。 2) 状态位P：用来标识页是否在内存中。例如，可以规定P为0时表示页不在内存中，需要请求调页；P为1时表示页在内存中，可以获取该页所在的页框号。 3) 访问字段A：用于记录页最近被访问的情况。A中可以存放页最近被访问的次数，也可以存放最近未被访问的时间长度。也可以简单地用A=1表示页最近被访问过，用A=0表示页最近没有被访问过。操作系统实现置换程序时需要根据A的值来选择被换出的页，出于效率的考虑，系统总是希望根据A的值把最近、最久未访问的页换出到外存。 4) 修改位M：用于标识页最近是否被修改过。由于内存中的每个页在外存中都保存有一个副本，如果页没有被修改过，则外存中的页副本和内存中的页完全一致。因此，换出页时就不用把页信息写回外存，只需要把该页所占用的页框标识为空闲可用即可。如果页最近被修改过，那么内存中的页和外存中该页的副本就不一致了。在换出页时，必须把最近修改过的页写回外存。往外存中写信息，需要请求磁盘操作，为了减少系统开销和启动磁盘的次数，在进行页置换时，尽量选择最近没有被修改过的页。 5) 保护位：用于标识页的访问权限，比如，该位值为1，表示页是既可读又可写的；该位为0，表示页是只读的。 2. 缺页异常机构 缺页异常机构的主要作用是在访问内存过程中发现缺页时产生缺页异常信号，使CP中断当前控制流的执行，转去执行操作系统的缺页异常处理程序，完成请求调页。具体过程 如下。 1) 分页硬件通过页表完成逻辑地址到物理地址的映射时，通过检查页表中的状态位P，判断当前被访问的页是否在内存中。如果不在，则产生缺页异常信号。 2) 执行操作系统的缺页异常处理过程。先在内存中为请求调入的页找一个空闲页框。然后，调度磁盘操作，把需要的页装入找到的空闲页框中。 3) 修改页表，更新已经调入页的存在位、在内存中的页框号、访问位和保护位等字段的值。 4) 重新开始执行因缺页而被中断的指令。 缺页异常发生时，系统保存了被中断的进程状态，包括寄存器和程序计数器的内容。所以，缺页异常处理完成返回时，可以按照与中断前完全相同的位置和地址重新开始执行进程。 由于单条指令可能涉及多个不同的页，所以，从理论上说，执行单条指令的过程中可能发生多次缺页异常。例如指令：copy A to B，指令本身可能涉及两个不同的页，A、B两个变量也可能分别涉及不同的页，如图4-43所示。在这种情况下，如果该指令和A、B涉及的所有页都不在内存中，执行这条指令就会发生6次缺页异常。 3. 地址变换 请求分页系统中的地址变化过程如下 1) 由分页地址变换机构从逻辑地址中分离出页号和页内偏移地址。 2) 以页号为索引查找快表，若快表中有该页的页表项，则读出页框号，计算物理地址。 3) 若快表中无该页信息，转到内存页表中查找。若页表中的状态位P显示该页已调入内存，则从相应的页表项读出页所在的页框号，并计算物理地址，然后把该页表项写入快表。 4) 若该页尚未调入内存，则产生缺页异常，请求操作系统从外存中把该页调入内存然后修改页表，重新执行被中断的指令。该过程可以用图4-44来表示。 二、页分配策略 本节要解决的问题是： 1) 只要装入进程的一部分页就可以运行进程，那么究竟为进程分配多少个页框，在内存中装入进程的多少个页呢？我们通过阐述最小页框数，也就是至少要为进程分配多少个页框才能使进程正常运行，回答上述问题。 2) 操作系统在需要请求调入并进行页置换时，是从缺页进程本身的页中选择淘汰页，还是从系统中所有的进程页中选择被淘汰的页？ 3) 采用什么样的算法为不同的进程分配页框？系统如何在多个进程之间合理分配页框数？ 下面对上述问题进行详细阐述 1. 最少页框数 最少页框数，是指能保证进程正常运行所需要的最少的页框数。如果系统为进程分配的页框数少于这个值，进程将无法正常运行。保证进程正常运行所需要的最少页框数与进程的大小没有关系，它与计算机的硬件结构有关，取决于指令的格式、功能和寻址方式。 例如，一个支持16位指令格式的机器，用高8位表示操作码，低8位表示操作数，如图4-45所示。 如果内存单元以一个8位字节作为一个编址单元，当采用直接寻址方式时，上面的指令本身可能涉及两个页，操作数部分的地址可能涉及第三个页，这种情况下，至少要为进程分配3个页框，才能保证进程正常执行。 如果采用间接寻址方式，操作数部分的地址指向的内存单元中存放的是另外一个地址，这时指令涉及的页数最多可能达到4个（假设地址和数据都只占一个字节），即指令本身可能涉及两个页，操作数部分的地址可能涉及第三个页，操作数地址中存放的内存地址可能涉及第四个页。这时，系统必须为进程分配至少4个页框才能保证进程正常执行。因为缺页异常发生在执行指令的过程中，缺页异常处理完毕，调入所缺页后，进程要重新执行原来被中断的指令。因此，必须保证这条指令涉及的全部页都在物理内存中，该指令才能被顺利执行完毕。为了保证进程顺利执行，操作系统为进程分配的初始页数应该大于或等于最少页框数。 2. 页分配和置换策略 在请求分页系统中，从分配给进程的页框数量上来看，可以采用固定分配和可变分配策略。固定分配策略是指在进程从创建到撤销的过程中，为进程分配的页框数保持不变，而可变分配是指为进程分配的页框数是可变的。从选择淘汰页的候选页是请求调入页的进程页还是系统中的所有用户进程页来看，可以采用局部置换和全局置换两种策略。局部置换是指发生置换时，只从请求调页进程本身的内存页中选择一个被淘汰的页，以腾出内存页框，装入请求调入的页。全局置换是指置换发生时，从系统中所有进程的内存页中选择被淘汰的页。固定分配和可变分配策略、局部置换和全局置换两种策略在实际系统中可以组合成以下3种页分配和置换策略。 (1) 固定分配局部置换 在进程创建时为每个进程分配一定数量的页框，在进程运行期间，进程拥有的页框数不再改变。当进程发生缺页时，系统从该进程在内存中的页中选择一页换出，然后再调入请求的页，以保证分配给该进程的内存空间保持不变。 (2) 可变分配全局置换 这是在操作系统中被广泛使用的策略。在采用这种策略时，先为系统中的每个进程分配一定数量的页框，同时，操作系统保持一个空闲页框队列。当某进程发生缺页时，由系统从 空闲页框队列中取出一个页框分配给该进程，并将欲调入的缺页装入其中。任何产生缺页的进程都可以由系统获得新的页框，以增加本进程在物理内存中的页数。当系统，总空闲页框数小于一个规定的阀值时，操作系统会从内存中选择一些页调出，以增加系统的空闲页框数，调出的页可能是系统中任何一个进程的页。 (3) 可变分配局部置换 进程创建时，为进程分配一定数目的页框。当进程发生缺页时，只允许从该进程在内存中的页中选出一页换出。只有当进程频繁发生缺页时，操作系统才会为该进程追加页框，以装入更多的进程页，直到该进程的缺页率降低到适当程度。反之，若一个进程在运行过程中的缺页率特别低，则可在不引起进程缺页率明显增加的前提下，适当减少分配给该进程的页框数。 3. 分配算法 操作系统为进程分配的页框数通常都是大于最少页框数的，究竟为每个进程分配多少个页框，可以通过执行页框的分配算法来确定。可采用的页框分配算法如下。 (1) 平均分配算法 采用平均分配算法，如果系统中有n个进程，m个可供分配的内存页框，则为每个进程分配INT[m/n]个页框，其余的MOD[m/n]个页框可以放入空闲页框缓冲池中。例如，系统中有26个空闲页框，有3个进程，那么为每个进程分配8个页框，即共为3个进程分配24个页框，剩下的2个页框可以作为系统可用的空闲页框放在空闲页框缓冲池中。 (2) 按比例分配算法 采用平均分配算法为进程分配页框的缺点是算法不考虑进程规模，可能使大进程分配到的页框与小进程一样多。大进程能进入内存的页数占本进程页总数的比例远远小于小进程，大进程可能因此频繁缺页。为了解决这个问题，可以采用按比例分配的算法。 为进程分配的页框数=进程页数/所有进程页数的总和×页框数。 例4-10：在两个进程之间按比例分配40个页框，进程p1大小为80页，另一个进程p2大小为240页，则为p1分配10个页框，为p2分配30个页框。计算式如下 为p1分配的页框数=80/(80+240)×40=10。 为p2分配的页框数=240/(80+240)×40=30。 (3) 考虑优先权的分配算法。 这种分配算法的思想是为优先权高的进程分配较多的页框数，为优先权低的进程分配较少的页框数。 三、页调入策略 实现请求分页的虚拟存储系统，还需要解决系统应在何时、从何处调入所需页的问题。当系统产生缺页异常，调入请求页的同时可以只把该页调入内存，也可以在调入该页的同时，把与该页相邻的页也调入内存。外存中的页可以存放于对换区，也可能在文件区。 1. 何时调入页 系统可以设计成只有在进程需要时才将页调入内存。在一种极端的情况下，进程的所有页都不在内存中就开始执行进程。当操作系统将指令指针指向进程的第一条指令时，由于所在的页不在内存中，立即产生缺页异常。当完成缺页异常处理，指令所在的页调入内存后，进程继续执行，执行过程中不断发生缺页异常，并不断将所缺页调人内存。当调入足够多的页后，多数需要的页都在内存中了，进程执行将不再出现缺页。这种策略有利于提高内存的 利用率，但是对系统的时间性能不利。 大多数系统都采用预先调入页的策略，将预计不久之后会被访问的页预先调入内存，而不是缺哪一页时再调入该页。在实际系统中，通常是在调入缺页时，把与所缺页相邻的若干页也调入内存。 2. 从何处调入页 (1) 从对换区调入 当系统拥有足够的对换空间时，若发生缺页请求，则从对换区调入页。从对换区调入页比从文件区调入页的速度快。对换区中的页是进程运行前从文件区复制到对换区的。 (2) UNIX方式 进程运行前，与进程有关的程序代码、数据等都存放在文件区，所以，没有被访问过的页都直接从文件区调入。换出页都存放在对换区，曾经运行过而又被换出的页从对换区中调入。对于可以共享的页，如果已经由于其他进程的需要而被调入内存，就无需再次从外存调入。 3. 页调入过程 请求调页和页调入的过程如图4-46所示。 四、页置换算法 本小节主要阐述页置换算法，同时介绍抖动及缺页率的相关问题。页置换算法是从内存页中选择换出页的算法。 1. 最佳置换算法和先进先出置换算法 (1) 最佳置换算法(Optimal page-Replacement Algorithm) 最佳置换算法是Belady于1966年提出的一种页置换算法，该算法选择以后永远不会被访问的页或者在未来最长时间内不再被访问的页作为换出页。因此，该算法主要用于理论研究。例如，如果知道一个算法不是最优，但是与最优相比最坏情况下的缺页率比最佳置换算法最多高10%，平均缺页率与最佳置换算法相比最多高5%，可以此说明该置换算法的性能。 假定系统为某进程分配了3个页框，并考虑有以下的页引用序列。 7,0,1,2,0,3,0,4,2,3,0,3,2,1,2,0,1,7,0,1 进程运行时，先将7、0、1这3个页装入内存。当进程访问2号页时，由于2号页不在内存中，将产生缺页异常，请求调入2号页。由于3个页框已满，需要选择一个换出页，以腾出空闲页框装入2号页。若操作系统根据最佳置换算法选择被换出的页，由于7号页是未来最长时间不会被访问的页，所以选择7号页作为换出页，然后将2号页调入内存。访问完2号页后，访问0号页，由于该页已在内存中，所以不会产生缺页异常。当进程访问3号页时，因为3号页不在内存中，又要产生缺页异常，并且由于内存满，又要执行页置换算法选择换出页。根据最佳置换算法，这次选择2、0、1这3个页中的1号页换出，因为1号页是未来最长时间不会被访问的页，所以换出1号页，调入3号页。图4-47所示是采用最佳置换算法的置换图。由图可见，在此例中，采用最佳置换算法发生了6次页置换。 (2) 先进先出页置换算法(First In First Out,FIFO) FIFO是最简单的页置换算法。实现这种算法的一种方式是为每个页记录该页调入内存的时间，当选择换出页时，选择进入内存时间最早的页。最简单的实现方法是创建一个FIFO的队列来管理内存中的所有页，选择队首的页作为换出页。新调入的页被加入到队尾。 FIFO算法实现简单，但是效率较低，会导致较高的缺页率。因为FIFO算法不考虑页被引用的频繁程度，例如有的页刚被换出，可能立刻又要被访问。在进程中有些页经常被访问，如包含全局变量、常用函数等的页，FIFO算法不能保证这些经常被访问的页不被换出。 仍然采用上面的例子来说明采用FIFO算法进行页置换的过程。如图4-48所示，当进程第一次访问2号页时，将把7号页换出，因为它是最早被调入内存的。访问0号页时，因为0号页已经在内存中，所以不会产生缺页异常，也不会发生页置换。当第一次访问3号页时，因为内存中的3个页2、0、1中，0号页是最早被调入内存中的，所以选择0号页作为淘汰页。由图4-48可见，在该例中，利用FIFO算法时，进行了12次页置换，比最佳置换算法的置换次数高一倍。 2. 最近最久未使用LRU置换算法 FIFO算法实现简单，但是导致很高的缺页率和置换次数，性能较差。而最佳置换算法有最低的页置换次数，性能很好，却难以实现。因此，这里试图用“最近的历史”来预测“最近的将来”，实现最佳置换算法的近似算法。LU算法是广泛使用的性能较好的算法。 (1) LRU(Least Recently Used,LRU)算法的描述 LRU置换算法是选择最近最久未使用的页换出（最近最久未使用的页在最近的将来被访问的可能性也比较小)。该算法赋于每个页一个访问字段，用来记录一个页自上次被访问以来所经历的时间。当需要淘汰一个页时，选择现有页中t值最大的页换出。 仍然采用上面的例子来说明采用LU算法进行页置换的过程。当进程第一次访问2号页时，由于内存满，需要从7、0、1这3个页中选择一页换出，由于7号页是最近最久没有被访问的页，所以选择7号页换出。当进程第二次访问0号页时，由于0号页已在内存中，所以不会发生缺页异常和页置换。当进程第一次访问3号页时，内存满，要从在内存中的3个页2、0、1中选择一个页换出，由于1号页是这3个页中最近最久没有被访问的页，所以换出1号页，以装入3号页。由图4-49所示的结果可见，对该访问序列采用LRU置换算法发生了9次置换，性能优于FIFO算法，但较最佳置换算法差。 (2)LRU算法的实现 LRU算法是常用的页置换算法，性能较好，也有较好的实现方法。其实现主要解决两个问题，一是内存中的各个页各有多久时间未被进程访问；二是如何快速地知道哪一页是最近最久未使用的页。其可行的实现方法如下。 1) 寄存器。 为每个在内存中的页配置一个移位寄存器，可表示为R=Rn-1...R2R1R0。当进程访问某页框时，要将相应的寄存器的最高位置成1。此时，每隔一定时间将寄存器右移一位。如果把位寄存器的数看作是一个页对应的整数，那么具有最小数值的寄存器所对应的页就是最近最久未使用的页。图4-50所示的例子说明了一个具有8个页的进程，通过为每个页配备一个8位寄存器实现LRU算法的情况。在这个例子中，3号页的8位寄存器值最小。所以发生缺页、需要置换时，选择3号页换出。图中的实页是指在内存中的页。 2) 栈。 可利用一个特殊的栈来保存当前使用的各个页的页号。每当进程访问某页时，便将该页的页号从栈中移出，将它压入栈顶。因此，栈顶始终是最新被访问页的编号，而栈底则是最近最久未使用的页号。 图4-51所示的例子中，页访问序列为4,7,0,7,1,0,1,2,1,2,6，在访间6号页时发生缺页。此时栈底的页为4，是最近最久没有被访问的页，将4号页换出，调入6号页。 3) 计数器。 为每个页表项增加一个时间字段，并为CPU增加一个逻辑时钟或计数器。每次访问内存中的某个页时，就增加这个页对应的页表项的时间字段的值，每次置换时选择时间字段值最小的页作为换出页。 3. 其他置换算法 计算机系统要提供足够的硬件来支持LRU算法是比较困难的，许多系统在实现时都采用LRU的近似算法，如附加引用位算法、简单Clok算法和改进型Clok算法等。 (1) 附加引用位算法 采用这种算法时，为每个内存中的页增加一个8位的字节，每隔一段时间（如100ms)，操作系统把每个页自己的访问位（请求分页管理的页表项中的A字段）移到8字节附加引用位的最高位，将其他位向右移，并抛弃最低位。进行页置换时，选择附加引用位值最小的页换出。因为具有最小附加引用位值的页可能不止一个，既可以换出所有这些页，也可以按FIFO选择部分具有最小附加引用位值的页换出。 (2) 简单Clock置换算法 利用简单Clok算法时，为每一页设置一位访问位，再将内存中的所有页都通过链接指针链接成一个循环队列。当某页被访问时，其访问位被置1。置换算法在选择一页换出时、按照FIFO算法，检查各页的访问位。如果是0，就选择该页换出；若为1，则重新将它置O，暂不换出而给该页第二次驻留内存的机会，再按照FIFO算法检查下一个页。 当检查到队列中的最后一个页时，若其访问位仍为1，则再返回到队首去重新从第一个页开始检查。 (3) 改进型Clock算法 简单Clock算法在选择一个换出页时，不考虑该页被修改的情况，而选择最近既没有被访问过又没有被修改过的页换出，能大大提高页置换的效率。在将一个页换出时，如果该页已被修改过，便应将它重新写到磁盘上，但如果该页未被修改过，则不必将它写回磁盘。换而言之，修改过的页在换出时所付出的时间开销将比未修改过的页开销大。在改进型C1ock算法中，除了必须考虑页的使用情况外，还考虑了置换代价这一因素，这样选择换出页时，既要是未使用过的页，又要是未被修改过的页。把同时满足这两个条件的页作为首选的淘汰页。改进型Clock算法描述如下。 在该算法中，由访问位A和修改位M可以组合成下面4种类型的页。 1) 第1类(A=0,M=0)。表示该页最近既未被访问，又未被修改，是最佳换出页。 2) 第2类(A=0,M=1)。表示该页最近未被访问，但已被修改，并不是很好的换出页。 3) 第3类(A=1,M=0)。最近已被访问，但未被修改，该页有可能再被访问。 4) 第4类(A=1,M=1)。最近已被访问且被修改，该页可能再被访问。 改进型Clok置换算法可分为以下3步。 1) 从指针所指示的当前位置开始，扫描循环队列，寻找A=0且M=0的第一类页。将所遇到的第一个页作为所选中的换出页。在第一次扫描期间不改变访问位A。 2) 如果第一步失败，即遍历页的链表后未遇到第一类页，则开始第二轮扫描。寻找A=0且M=1的第二类页，将所遇到的第一个这类页作为换出页。在第二轮扫描期间，将所有经过的页的访问位A置0。 3) 如果第二步也失败，即未找到第二类页，则将指针返回到开始的位置。然后再次寻找A=0且M=0的第一类页，如果仍失败，再寻找A=0且M=1的页，此时就一定能找到被换出的页。 改进型Clok置换算法最坏情况下，需4次扫描页队列，才能选中被换出的页。 (4) 最少使用置换算法(Least Frequently Used Page--Replacement Algorithm,LFU) 这种算法选择最近时期内使用次数最少的页作为淘汰页。这种算法为每个页保留一个用于记录页被访问次数的计数器，每次选择其访问计数器值最小的页换出。这种算法可能存在的问题是：有些页在进程开始时被访问的次数很多，但以后这些页不再被访问，这些页不应该长时间留在内存中，而应该在不再被访问时被换出。解决这个问题的方法之一是定期地将计数器右移，以形成指数衰减的平均使用次数。 (5) 页缓冲算法 页缓冲算法采用FIFO算法选择换出页，并为换出页建立两个链表，被换出的页要放入两个链表中的一个。如果页没有被修改过，则将其放入空闲页链表中。如果页被修改过，则将其放入已修改页的链表中。页在内存中并不做物理上的移动，而只是将页表中的表项移到上述两个链表之一中。 空闲页链表实际上是一个空闲页框链表，其中的每个页框都是空闲的，因此可以在其中装入程序或者数据。当需要读入一个页时，便可利用空闲页框链表中的第一个页框来装入该页。当有一个没有被修改的页要换出时，实际上并不将它换出内存，而是把该页所在的页框挂在空闲页链表的末尾。在置换一个已修改的页时，将其所在的页框挂在修改页链表的末尾。采用这种方式可以使没有被修改的页和已经被修改的页都留在内存中。当进程以后再次需要访问这些页时，就不用从磁盘而只需从内存中再次使这些页回到进程在内存的驻留集中，使这些页所在的页框成为“被占用”状态即可，由此减小了请求调入和页置换的开销。 已修改页的链表周期性地或者当已修改页的数量达到一定阈值时被延迟写人磁盘，减少了换 出时写磁盘的时间开销。 五、请求分页系统的性能分析 前面已经阐述了基于请求分页的存储管理的基本原理，在设计和选择虚拟存储管理的策略和算法时，不可避免地要考虑访存的性能。请求调入和置换技术都是以时间换空间的技术，如何在应用这种技术时尽量提高时间性能是在设计系统时需要考虑的问题。 1. 缺页率对有效访问时间的影响 进程执行中访存发生缺页时，需要请求从外存调入缺页。如果内存中没有空闲页框，还需要进行页置换，调入缺页后，指令需要重新执行。因此，一旦发生缺页，进程会存在因为访存而带来的时间延迟。下面分析缺页率对有效访问时间的影响。 有效访问时间是成功访存所用的时间。假设P为缺页率，ma为存储器访问时间。这里根据实际系统的可能性，取ma=100 ns=0.1μs。缺页异常服务时间、缺页读入时间及进程重新执行时间近似地取25ms是合理的。由此： 有效访问时间=（1-P)×ma+P×缺页异常时间。 缺页异常时间=缺页异常服务时间+缺页读人时间+进程重新执行时间≈25ms(毫秒)=25000μs(微秒) 所以，有效访问时间=0.1×(1-P)+25000×P=0.1+24999.9×P 由上式可见，有效访问时间正比于缺页率。下面以实例数据说明缺页率对有效访问时间的影响。 例4-11： P=0时 有效访问时间=0.1us P=1/1000时 有效访问时间=0.1+24999.9×1/1000=0.1+24.9999≈25μs 可见，缺页率P=1/1000时，有效访问时间延长为P=0时的250倍。 例4-12：要使有效访问时间延长不超过10%，缺页率P为多少？ 因为P=0时，有效访问时间=0.1μs。 P不为0时，有效访问时间为(0.1+24999.9P)μs。 P不为0时比P为0延长的时间为：(0.1+24999.9P)-0.1=24999.9P 要使 24999.9p/0.1 即要使有效访问时间延长不超过10%，缺页率应该小于250万分之一。 2. 工作集 (1) 基本原理 引入工作集机制是为了能有效降低缺页率，从而提高访存的时间效率。程序在运行时对页的访问是不均匀的，即往往在某段时间内的访问仅局限于较少的若干个页，而在另一段时间内，则又可能仅局限于访问另一些较少的页。若能预知程序在某段时间间隔内要访问哪些页，并能将它们提前调入内存，将会大大降低缺页率，从而减少置换次数，提高CPU的利用率。工作集就是在某段时间间隔△里，进程实际要访问的页的集合。要使缺页少发生，必须使程序的工作集全部在内存中。 用W(t,△)表示当时间间隔为△时t时刻的工作集。 1) △太大，工作集中的页太多，影响存储器利用率。 2) △太小，工作集中的页太少，缺页率高。 3) W(t,△)与t相关，不同的t对应不同的工作集。即程序运行的过程中，运行到不同的时刻，需要访问的页不同。 (2) Windows NT的工作集机制 1) 进程被第一次创建时，为进程指定了一个最小工作集和一个最大工作集。最小工作集是进程正常运行需要调入内存的最小页数。当内存有足够的空间时，可以为进程分配足够的空间以装入它的最大工作集。 2) 当某进程在内存中的页数小于最大工作集时，若发生缺页，系统从空闲页框队列中取一页框分配给该进程。 3) 当某进程在内存中的页数等于最大工作集时，若发生缺页，系统从该进程的页中按FIFO的原则，换出一个该进程的页。 4) 当空闲页框队列中空闲页的数量减到一个最低值时，系统检查所有进程，对工作集大于最小工作集的进程，淘汰该进程的一些页，使该进程的工作集等于最小工作集。 3. 抖动产生的原因和预防方法 (1) 抖动 多道程序度太高，使运行进程的大部分时间都用于进行页的换入、换出，而几乎不能完成任何有效工作的状态称为抖动。引起系统抖动的主要原因是系统中的进程数量太多，每个进程能分配到的页框太少，以至于进程运行过程中频繁请求调页。 (2) 抖动的预防 可以采用以下方法预防抖动的发生。 1) 采取局部置换策略。当进程发现缺页后，仅在进程自己的内存空间范围内置换页，不允许从其他进程获得新的页框。 2) 在CPU调度程序中引入工作集算法。只有当每个进程在内存中都有足够大的驻留集时，才能再从外存中调入新的作业。 3) 挂起若干进程。为了预防抖动，挂起若干进程，腾出进程占用的空间。 "},"pages/操作系统概论/第五章_文件系统/第一节_文件.html":{"url":"pages/操作系统概论/第五章_文件系统/第一节_文件.html","title":"第一节 文件","keywords":"","body":"第五章 文件系统 文件系统管理是操作系统的重要功能之一，它为用户提供了在计算机系统中对数据信息 进行长期、大量存储和访问的功能。文件的结构，以及文件的命名、访问、存储、保护和实现方法都是文件系统设计的主要内容。从总体上看，操作系统中处理文件的部分称为文件系统(File System)，文件系统包括了文件及管理文件的软件集合。 本章分两部分来讨论文件系统，一部分是文件系统的用户接口，另一部分是文件系统的实现，包括文件的按名访问和存储空间管理的实现。 第一节 文件 本节讨论文件系统的用户接口，即用户可以“看见”和使用的文件系统部分，包括文件的命名、类型、属性和对文件的操作。 一、文件命名 文件命名向用户提供了简单、直观的文件访问方式，使用户在访问文件时不必了解信息存储的方法、位置及磁盘实际动作方式等细节，而只需给出要访问的文件名。在创建一个文件时，进程给出文件名。进程终止后，文件仍然存在，其他进程使用该文件名可以对它进行存取。 不同的文件系统为文件命名制定的规则略有不同，但所有操作系统都允许用1~8个字母组成的字符串作为文件名。例如，hello、bruce和oenw都是合法文件名。一般情况下，文件名中也允许有数字和一些特殊字符，如1、error!和Tab.6-1都是合法的。许多文件系统支持长达255个字符的文件名。文件的命名规则和文件名的长度在文件系统设计时确定。 有的文件系统区分大小写字母，如UNIX的文件系统；有的不区分，如MS-DOS。因此，在UNIX中，hello、Helo和HELLO是3个不同的文件名；而在MS-DOS中，它们是同一个文件名。 多数操作系统都支持文件名用圆点隔开分为两部分，如hello.c。圆点后面的部分称为文件扩展名。文件名info.txt用于表示这个文件的类型，给用户一个提示，而不是表示给计算机传送什么信息。但是，编译器、链接程序等利用扩展名区分哪些是C文件，哪些是汇编文件，哪些是其他文件。 在Windows系统中，扩展名被赋予了含义。用户可以在操作系统中注册扩展名，并且规定哪个程序与该扩展名关联，当用户双击某个文件名时，关联该扩展名的程序就会启动并访问该文件。例如，双击“孔子.rmvb”就会启动关联的电影播放软件，并播放该视频。 二、文件结构 本节介绍如图5-1所示的3种文件结构。 1.无结构字节序列 无结构字节序列文件也称为流式文件，操作系统不知道也不关心文件内容是什么，操作系统所见到的就是字节，其任何含义由使用该文件的程序自行理解，在UNIX和Windows系统中都采用这种方式。 把文件看成字节序列为操作系统提供了最大的灵活性。用户程序可以向文件中加人任何 内容，并以方便的形式对文件进行命名。 2.固定长度记录序列 在该模型中，构成文件的基本单位是具有固定长度的记录，每个记录都有其内部结构。把文件作为记录序列的中心思想是：读操返回一个记录，而写操作重写或追加一个记录。 3.树形结构 文件由一棵记录树构成，记录长度不定，在记录的固定位置包含一个关键字域，记录树 按关键字域排序。在这种文件结构中，基本操作是获取具有特定关键字的记录（而不是获取下一条记录)。增加记录时，由操作系统决定记录在文件中的存放位置。这类文件结构与UNIX和Windows系统中采用的无结构字节序列有明显不同，它在一些处理商业数据的大型计算机中获得了广泛使用。 三、文件类型 在设计操作系统时要考虑系统支持哪些类型的文件，并为每种类型的文件定义合法的操作。文件的类型有正规文件、目录文件、字符设备文件和块设备文件等。正规文件包含用户信息，一般分为ASCII文件和二进制文件。目录文件是用于管理文件的系统文件。字符设备文件和输入/输出有关，用于串行I/O类设备，如终端、打印机和网络等。块设备文件用于磁盘类设备。本节主要介绍正规文件。 1.ASCII(American Standard Code for Information Interchange) ASCII文件由多行正文组成，在某些系统中每行用回车符结束，某些则用换行符结束，而有些系统还同时采用回车符和换行符，如MS-D05。各行的长度不必相同。 ASCII文件的明显优势是可以显示和打印，也可以用通常的文本编辑器进行编辑。另外，如果程序以ASCII文件作为输入和输出，就很容易把一个程序的输出作为另一个程序的输入。 2.二进制文件 二进制文件具有一定的内部结构，如可执行的.exe文件。用通常的文本编辑器不能直接显示或打印二进制文件，必须使用专门的二进制文件编辑器。不同的操作系统可以识别不同的二进制文件，比如把某一种结构（如ELF格式）的二进制文件作为系统中的可执行文件。 四、文件存取 用户通过对文件的存取来完成对文件的各种操作，文件的存取方式是由文件的性质和用户使用文件的情况确定的。常用的文件存取方式有两种：顺序存取和随机存取。 1.顺序存取 早期的操作系统只有顺序存取这一种文件存取方式。进程可以从文件开始处读取文件中的所有字节或者记录，但不能跳过某些内容，也不能不按顺序存取。在存储介质是磁带而不是磁盘时，顺序存取文件是很方便的。 2.随机存取 随机存取又称直接存取，即可以以任意顺序读取文件中的字节或记录。现代操作系统的文件一旦被创建，所有文件自动成为随机存取文件。定长记录的文件能很好地支持随机存取，而变长记录虽然可以随机存取，但实现起来复杂而且存取速度慢。 五、文件属性 为方便管理，除了文件名和文件数据外，文件系统还会保存其他与文件相关的信息，如文件的创建日期、文件大小和修改时间等，这些附加信息称为文件属性。文件属性在不同的操作系统中差别很大，表5-1列出了一些常用的文件属性。 表5-1一些常用的文件属性 六、文件操作 使用文件的目的是存储信息并方便以后检索。不同的系统提供了不同的操作来存储和检索信息，以下是常用的文件操作。 1.CREATE 该操作完成创建文件的功能，并设置文件的一些属性。 2.DELETE 当不再需要某个文件时，删除该文件并释放磁盘空间。 3.OPEN 在使用文件之前，必须先打开文件。OPEN调用的目的是将文件属性和文件的地址信息装入主存，便于在对文件的后续访问中能快速存取文件信息。 4.CLOSE 当存取结束后，不再需要文件属性和地址信息，这时应该关闭文件以释放内部表空间。很多系统限制进程打开文件的个数，以鼓励用户关闭不再使用的文件。 5.READ 从文件中读取数据。一般地，用户可以指定从哪个文件的第几个字节开始读取多少个字节的内容。此外，调用读文件的函数时，还需要给出存放被读取内容的内存缓冲区。 6.WRITE 往文件中写数据，写操作一般从写函数的参数指定的文件位置开始。如果当前位置是文件末尾，文件长度增加。如果当前位置在文件中间，则现有数据被覆盖，并且永远丢失。 7.APPEND 该操作是WRITE调用的限制形式，它只能在文件末尾添加数据。 8.SEEK 对于随机存取文件，要指定从何处开始取数据。通常的做法是用SEEK系统调用把当前 位置指针指向文件中特定的位置。SEEK调用结束后，就可以从该位置开始读写数据了。 9.GETATTRIBUTES 该操作用于获取文件属性。 10.SETATTRIBUTES 某些属性是可由用户设置的，文件创建以后，用户还可以通过系统调用SETAT-TRIBUTES来修改它们。 11.RENAME 该操作用于修改已有文件的名件名。 "},"pages/操作系统概论/第五章_文件系统/第二节_目录.html":{"url":"pages/操作系统概论/第五章_文件系统/第二节_目录.html","title":"第二节 目录","keywords":"","body":"第二节 目录 文件系统通常提供目录或文件夹用于记录文件，很多系统中目录本身也是文件，目录是文件系统中实现按名访问文件的重要数据结构。本节讨论目录、目录的组成、目录的特性，以及可以对目录进行的操作。 一、层次目录系统 1.目录文件的结构 目录文件有两种常见的结构：属性放在目录项中和放在i结点中。 目录文件包含许多目录项，每个目录项用于描述一个文件。在第一种结构中，每个目录项长度相同，每个文件对应一个目录项。其中包含文件名、文件属性和文件的地址。在第二种结构中，目录项中有一个文件名和i结点号。i结点是一种数据结构，文件属性和文件的地址信息存放在i结点中。这两种结构都得到了广泛的应用。图5-2分别描述了以上两种结构。 2.目录结构 文件目录的组织和管理是文件管理的一个重要方面，包括单层目录、两级目录和树形目录。 (1)单层目录 这种目录也被称为根目录。在整个系统中设置一张线性目录表，表中包括了所有文件的描述信息。早期的个人计算机中，这种系统很普遍。这种目录结构使得软件设计相对简单。 图5-3所示是一个单层目录系统的例子。该目录中有4个文件。 在多用户系统中，单层目录带来的一个显著问题是，不同用户可能会使用相同的文件名。例如，如果用户A创建了一个名为helo.c的文件，然后，用户B也创建了一个名为hello.c的文件，用户B的文件可能会覆盖用户A的文件。很显然，这种结构不适合在多用户系统中使用。 另外，由于使用单层目录时，要查找一个文件必须对单层目录表中的所有文件信息项进行搜索，因而搜索效率也较低。 (2)两级目录 为了避免上述文件名冲突，一种改进方法是为每个用户提供一个私有目录。这样，一个用户选择文件名时就不会影响到其他用户。图5-4展示了一个两级目录系统。 在两级目录结构中，目录被分为两级，第一级称为主目录，给出了用户名和用户子目录所在的物理位置。第二级称为用户目录，给出了该用户所有文件的文件控制块。这一设计隐含的机制是，当一个用户试图打开文件时，系统知道是哪个用户，从而知道应该查询哪个目录。 使用两级目录的优点是解决了文件的重名问题和文件共享问题，查找时间降低。缺点是增加了系统的存储开销。 (3)树形目录 把两级目录的层次关系加以推广，就形成了多级目录，又称树形目录，如图5-5所示。 在多级目录结构中，除了叶子结点对应的存储块中装有文件信息外，其他每一级目录中存放的都是下一级目录或文件的说明信息，由此形成层次关系。最高层为根目录，最底层为文件。在这种结构中，用户可以拥有多个所需的目录，自由地组织自己的文件。同时，用户可以创建任意数量子目录的功能，为用户组织其文件提供了一种强大的工具。 树形目录的优点是便于文件的分类，层次结构清晰，便于管理和保护、解决了重名问题，查找速度加快。缺点是查找一个文件按路径名逐层检查，由于每个文件都放在外存中，多次访问磁盘会影响速度，结构相对复杂。 二、路径名 用目录树组织文件系统时，需要有某种方法指明文件名。常用的方法有两种：绝对路径名和相对路径名。 1.绝对路径名 绝对路径名由从根目录到文件的路径组成。例如，路径/program//practice/est表示根目录中有子目录program，而program中又有子目录practice，文件test就在子目录practice下。绝对路径名总是从根目录开始，并且是唯一的。在Windows系统中，路径各部分之间用“\\”分隔；在UNIX系统中，分隔符为“/”。同样的路径名在两种系统中的书写形式如下。 Windows \\ program \\ practice \\ test UNIX /program/practice/test 不管采用哪种分隔符，只要路径名的第一个字符是分隔符，则这个路径就是绝对路径。 2.相对路径名 当访问一个文件系统的目录包含很多级时，如果访问每个文件都要从根目录开始，直到叶子的文件名为止，包含所有经过的各级分目录在内的全路径名是相当麻烦的。在设计文件系统时，可以允许用户指定一个目录作为当前的工作目录，所有的不从根目录开始的路径名都是相对于工作目录的。例如，如果当前的工作目录是/program/practice,则绝对路径名为/program/practice/test的文件可以直接用test来引用，用户使用相对路径名往往更方便，但是前提是用户访问的文件必须在当前工作目录下。一些程序需要存取某个特定的文件，而不管当前目录是什么，这时必须采用绝对路径名。不管当前的工作目录是什么，绝对路径名总能正常工作。 支持树形目录结构的大多数文件系统在每个目录中有两个特殊的目录项“.”和“..”。“.”指当前目录，“..”指当前目录的父目录。也就是说当在UNIX的终端或Windows的命令提示符中输入命令“cd..”时，将返回上一层目录，如果已经到根目录，再执行该命令，当前目录将一直留在根目录。 三、目录操作 本节以UNIX系统为例，说明对目录的操作。 1.CREATE 根据给定的目录文件名，创建目录。除了目录项“.”和“..”外，目录内容为空。“.”和 “..”是系统自动放在目录中的。 2.DELETE 删除目录，根据指定的目录名删除一个目录文件。 3.OPENDIR 目录内容可以被读取。例如，为列出目录中的所有文件和子目录，程序必须先打开该目录，然后读取其中所有文件的文件名，同打开和读取文件一样，在读目录之前必须打开目录。 4.CLOSEDIR 读目录结束后，应关闭目录以释放内部表空间。 5.READDIR 以标准格式返回打开目录的下一级目录项。 6.RENAME 更换目录名。 "},"pages/操作系统概论/第五章_文件系统/第三节_文件系统的实现.html":{"url":"pages/操作系统概论/第五章_文件系统/第三节_文件系统的实现.html","title":"第三节 文件系统的实现","keywords":"","body":"第三节 文件系统的实现 前面讨论的文件系统，主要是从用户的角度探讨问题。用户关心的是文件怎样命名、可以进行哪些操作、目录树是什么样的，以及类似界面问题。而实现者感兴趣的是文件和目录是怎样存储的、文件的存储空间是怎样管理的，以及怎样使系统有效而可靠地工作等。下面，从设计者和实现者的角度讨论文件系统是如何实现的。 一、实现文件 本节以文件存放在磁盘中为例，说明实现文件存储的几种常用方式。文件系统通常是以2ⁿ个连续的扇区为单位对文件进行磁盘空间的分配，下面把分配给文件的连续扇区构成的磁盘块称为簇。 1.连续分配 顾名思义，连续分配就是把每个文件作为一连串连续数据块存储在磁盘上。例如，系统规定的簇大小为1KB，则存储5KB大小的文件，需要5个连续的簇。若簇大小为2KB,则存储5KB大小的文件需要分配3个连续的簇，最后一个簇中，有1KB的磁盘空间将不能使用，下一个文件要从一个新的簇开始，如图5-6所示。 连续分配方式有两大优点。首先，实现简单，记录每个文件用到的簇仅需存储两个数字即可：第一块的磁盘地址和文件的块数。给定第一块的簇号，就可以找到任何其他块的簇号。其次，读操作性能好，在单个操作中就能从磁盘上读取整个文件。只需要寻找一次第一个块，之后不再需要寻道和旋转延迟，所以数据以磁盘全带宽的速率输入。 但是，连续分配方式也有十分明显的缺点：随着时间的推移，磁盘会变得零碎。当删除文件时，文件所占的簇被释放，这些空闲的连续簇形成“空洞”。随着磁盘的使用，磁盘上会有许多空洞。当磁盘被充满而又需要创建新文件时，需要挑选大小合适的空洞存入文件，这时就有必要知道该文件的最终大小。但很多情况下，无法预知一个文件的最终大小，比如录入一个文档，文档的内容有多少，需要占用多大空间都是未知的，也就是说文件的大小可变，系统管理文件的存储会比较麻烦。 2.使用磁盘链接表的分配 该方法为每个文件构造簇的链接表，每个簇开始的几个字节用于存放下一个簇的簇号，簇的其他部分存放数据，每个文件可以存放在不连续的簇中。在目录项中只需存放第一个数据块的磁盘地址，文件的其他块可以根据这个地址来查找。如图5-7所示，某文件的数据信息需要占用5个簇，第0块数据存储在4号簇中，第1块数据存储在7号簇中，第2块数据存储在2号簇中，第3块数据存储在10号簇中，第4块数据存储在12号簇中。4号簇的起始字节中存放了1号数据块所在簇的簇号7，7号簇的起始字节中存放了2号数据块所在簇的簇号2，2号簇的起始字节存放了3号数据块所在簇的簇号10，10号簇的起始字节存放了4号数据块所在簇的簇号12。12号簇的起始字节存放文件结束标志，表示12号簇是存放该文件的最后一个簇。对该文件的查找过程是先根据该文件的文件名搜索目录文件，找到该文件的目录项，从目录项中得到第0个数据块所在的簇号4，从4号簇的起始字节得到第1号数据块所在的簇号7，依此获得所有数据所在的簇号，访问文件的数据。 这一方法的优点是可以充分利用每个簇，不会因为磁盘碎片（除了最后一块中的内部碎片)而浪费存储空间，管理也比较简单。缺点是随机存取相当缓慢。要获得文件的第n块，每一次都要从头开始读取前面的n-1块。显然，进行如此多的读操作会占用很多时间。 3.使用内存的链接表分配 该方法是将文件所在的磁盘的簇号存放在内存的表（文件分配表）中，访问文件时，只需从内存文件分配表中顺着某种链接关系查找簇的簇号。不管文件有多大，在目录项中只需记录文件的第一块数据所在簇的簇号，根据它查找到文件的所有块。 MS-DOS就使用这种方法进行磁盘分配。图5-8表示了图5-7所示例子的文件分配表的内容。该文件依次使用了簇号为4、7、2、10和12的磁盘块。访问该文件时，先根据文件名检索目录文件，从目录文件中获得文件第0号数据块所在的簇号4，然后利用图5-8中的文件分配表，从第4号表项中读到簇号7，从7号表项中读到簇号2，从2号表项中读到簇号10，从10号表项中读到簇号12。在12号表项中存该文件的结束标志，至此找到图5-7所示例子中的文件占用的全部簇的簇号。 这种方法的一个缺点是必须把整个表都存放在内存中。对于目前比较常见的500GB的磁盘，如果簇大小为1KB，这张表需要有500M个表项，每一项对应一个簇。若每项占4个字节，根据系统对空间或时间的优化方案，这张表要占用约2GB的内存(500M×4B)。很显然，这种方法不适合大容量的磁盘。 4.i-结点 该方法为每个文件赋予一个被称为i结点的数据结构，其中列出了文件属性和文件块的磁盘地址。图5-9所示是一个简单的例子。给定一个文件的i结点，就有可能找到文件的所有块。系统打开文件时，将文件的ⅰ结点从磁盘读入内存。当访问文件时，系统先根据文件名搜索文件所在的目录文件，从该文件对应的目录项中找到文件的i结点号，根据ⅰ结点号从磁盘中将i结点信息读入内存，文件在磁盘中的地址信息都存放在i结点中。 如果每个i结点只能存储固定数量的磁盘地址，那么当一个文件比较大，所含簇的数目太多时，i结点将无法记录所有的簇号。一个解决方案是采用间接地址，使一个“磁盘地址”不存放数据块，而是存放簇号。对于一个大文件，ⅰ结点内的其中一个地址是一次间接块的簇号，这个块包含了存放文件数据的簇的簇号。如果还不够的话，在i结点中还有二次间接块的簇号，其中存放了若干个一次间接块的簇号。文件再大的话，可以使用三次间接块，如图5-9所示。 Linux的Ext2文件系统的一个i结点包括15个地址项，每个地址项存32位地址(4个字节)，用其中12个地址项存直接地址，一个地址项存一次间接地址，一个地址项存二次间接地址，一个地址项存三次间接地址。当簇大小为1KB时，系统能管理的单个文件的最大长度该如何计算呢？ 首先，12个地址项存放的是簇号，所以12个直接地址可表示的文件大小为12×1KB=12KB。其次，每个簇大小为1KB，每个地址项占4个字节，所以，每个簇中可以存放256个簇号。这样，一次间接块可以表示的文件大小为256×1KB=256KB。以此类推，二次间接块可以表示的文件大小为256×256×1KB=64MB。三次间接块可以表示的文件大小为256×256×256×1KB=16GB。最后，一个文件的最大长度为12KB+256KB+64MB+16GB。 二、实现目录 系统在读文件前，必须先打开文件。打开文件时，操作系统利用用户给出的路径名找到相应的目录项，目录项中提供了查找文件簇所需要的信息。下面，以CP/M、MS-DOS和UNIX为例，说明3种不同的目录实现方式。 1.CP/M中的目录 CP/M是一个微机操作系统，它只有一层目录，因此只有一个目录文件。要查找文件名，就是在这个唯一的目录文件中找到文件对应的目录项，从目录项中获得文件存放的磁盘地址。图5-10所示为CP/M的目录项结构。其中，各字段的含义如下。 1)用户码：记录了文件所有者。 2)文件名：存放文件名。 3)扩展名：标识文件类型。 4)范围：由该域可知某目录项是文件的第几个目录项，因为一个很大的文件（大于16KB)，其簇的数量多，在一个目录项中记录不下。 5)块数：文件实际使用的簇的数量。 6)最后16个域记录了簇号。 需要注意的是，文件占用的最后一个簇可能没有写满，这样系统无法确切地知道文件的 字节数，因为CP/M是以簇而不是以字节为单位来记录文件长度的。 2.MS-D0S中的目录 MS-DOS文件系统曾经是IBM PC系列所采用的文件系统，目前它已经不再是新的PC文件系统的标准了，但它和它的扩展(FAT32)一直被许多嵌人式系统所广泛使用。大部分的数码相机使用它，许多MP3播放器使用它，流行的苹果公司的iPod使用它作为默认的文件系统。现在使用MS-DOS文件系统的电子设备数量远远多于过去，并且远远多于使用更现代的NTFS文件系统的数量。下面介绍其中的一些细节。 图5-11所示是一个MS-D0S的目录项。它总共32个字节，其中包含了8个字节的文件名、1个字节的文件属性和2个字节的文件第一个簇的簇号。根据第一个簇号，顺着索引链表可以找到所有的文件块。MS-D0S中，目录可以包含其他子目录，从而形成树形目录。 MS-DOS用文件分配表即FAT作为索引表来存放文件数据所在簇的簇号。目录项包含了文件第一个数据块所在簇的簇号，将这个簇号用作FAT的索引，可依次找到文件的所有块。FAT的结构见前面的图5-8。 FAT文件系统有3个版本：FAT-12,FAT-16和FAT-32，取决于用多少个二进制位存放簇号。 3.UNIX中的目录 UNIX中采用的目录结构非常简单，如图5-12所示，每个目录项只包含一个文件名及其i结点号。有关文件类型、长度、时间、所有者和簇号等信息都放在ⅰ结点中。当打开某个文件时，文件系统必须要获得文件名并且定位文件所在的第一个簇。下面说明文件系统寻找一个文件的过程，以查找UNIX中的/usr/ast/mbox为例。 首先，文件系统找到根目录，在UNIX中根目录的ⅰ结点位于磁盘的固定位置。然后，系统读根目录文件并且在根目录文件中查找路径的第一部分usr，获得文件/usr的i结点号。由ⅰ结点号来定位ⅰ结点是很直接的，因为每个i结点在磁盘上都有固定的位置。根据这个ⅰ结点号，系统定位/usr目录并查找下一部分ast。一旦找到ast的目录项，便找到了/usr/ast的i结点。根据这个i结点，可以定位该目录并在其中查找mbox。然后，这个文件的i结点被读入内存，并且在文件关闭之前会一直保留在内存中。查找的过程如图5-13所示。 提示：相对路径的查找与绝对路径的查找方法相同，只不过是从当前工作目录开始查找而不是从根目录开始。 三、磁盘空间管理 磁盘空间管理是文件系统的重要功能，包括记录空闲磁盘信息、设计文件的存放方式，以及规定文件系统的簇大小等内容。 1.簇大小 文件系统为文件分配磁盘空间是以簇为单位的，一旦用固定大小的簇来存储文件，就会出现一个问题：簇的大小应该是多少？ 拥有大的簇尺寸意味着每个文件，甚至一个字节的文件，都要占用很大的空间，也就是说小的文件浪费了大量的磁盘空间。另一方面，小的尺寸意味着大多数文件会跨越多个簇，因此需要多次寻道与旋转延迟才能读出它们，从而降低了时间性能。因此，簇太大，容易造成空间的浪费；簇太小，则会使访问文件的时间延长。 一般簇大小是2的整数次幂个连续的扇区，如1个扇区，512个字节；连续两个扇区，大小为1KB；连续4个扇区，大小为2KB。根据系统管理的文件大小，可以选择合适的簇大小来格式化磁盘。 2.记录空闲块 确定了簇大小后，下一个问题是怎样跟踪记录空闲簇。有以下两种方法被广泛采用。 (1)空闲簇链接表 用一些空闲簇存放空闲簇的簇号。一个簇存放尽可能多的空闲簇的簇号，并专门留出最后几个字节存放指向下一个存放空闲簇的指针。对于1KB大小的簇，可以存放256个32位的簇号（有一个存放指向下一个块的指针）。图5-14a显示了该方法的记录方式。 (2)位图 用n位位图对应磁盘的n个簇，在位图中，空闲簇用1表示，已分配簇用0表示（或者反过来)，如图5-14b所示。很显然，位图方法所需空间少，因为每个簇只用一个二进制位标识，而在链接表方法中，每一个簇号都要用32位。 "},"pages/操作系统概论/第六章_I_O设备管理/第一节_I_O系统的组成.html":{"url":"pages/操作系统概论/第六章_I_O设备管理/第一节_I_O系统的组成.html","title":"第一节 I/O系统的组成","keywords":"","body":"第六章 I/O设备管理 计算机系统中的I/O设备即输入/输出设备是用于计算机系统与人通信或与其他机器通信的所有设备，以及所有外存设备。I/O设备管理是操作系统的重要功能之一，也简称为设备管理。 I/O设备是计算机系统的重要组成部分，计算机I/O设备的多样性使设备的管理非常复杂。本章先从硬件角度介绍I/O设备的一般构成和控制方式，然后从操作系统设备管理软件的角度说明I/O设备管理软件的构成和软件各组成部分的功能，并说明部分功能实现的原理。 第一节 I/O系统的组成 I/O系统不仅包括各种I/O设备，还包括与设备相连的设备控制器，有些系统还配备了专门用于输入/输出控制的专用计算机，即通道。此外，I/O系统要通过总线与CPU、内存相连。本节介绍I/O系统的结构、I/O设备的分类、设备控制器的功能和构成，以及通道的特点和功能。 一、I/O系统的结构 I/O系统的结构分为微机I/O系统和主机I/O系统两大类。 1.微机I/O系统 总线型I/O系统结构如图6-1所示。CPU与内存之间可以直接进行信息交换，但是不能与设备直接进行信息交换，必须经过设备控制器。 2.主机I/O系统 I/O系统可能采用四级结构，包括主机、通道、控制器和设备。一个通道可以控制多个设备控制器，一个设备控制器也可以控制多个设备。图6-2所示为具有通道的I/O系统结构。 二、I/O设备的分类 不同功能的I/O设备种类繁多，可以根据设备的某种特点或功能分别对设备进行分类。是否需要对设备进行分类，如何分类，可根据实际需要的情况而定。 1.按传输速率分类 1)低速设备。如键盘和鼠标，传输速率为几个~几百个字节/秒。 2)中速设备。如打印机，传输速率为数千个~数万个字节/秒。 3)高速设备。如磁带机、磁盘机、光盘机，传输速率为几十万~几兆字节/秒。 2.按信息交换的单位分类 1)块设备。数据的存取以数据块为单位，如磁盘。块设备在块中保存信息，块的大小通常是固定的，并且一次只传送一块，通常可以通过块号访问数据。 2)字符设备。传送字节流，没有使用块结构。终端、打印机、通信端口和鼠标等都是字符设备。 在操作系统实现驱动程序时，通常需要区分一个设备是块设备还是字符设备，操作系统对这两种设备的缓冲管理和驱动程序的实现方式是不同的。 3.按设备的共享属性分类 1)独占设备。是必须作为临界资源以互斥方式访问的设备。在一个进程没有使用完毕之前，其他任何进程不能访问该设备，直到设备被释放。例如，打印机是典型的独占设备。 2)共享设备。是允许多个进程共同访问的设备，如硬磁盘是典型的共享设备。这里共同访问的含义是：一个进程访问该设备的全部任务还没有完成，就可以把设备分配给另一个进程使用。如进程p1需要读5个磁盘块的数据，p2需要读3个磁盘块的数据。当进程p1读完一个磁盘块后，就可以把磁盘的访问权分配给p2，让进程p2读一个或若干个磁盘块，再让p1继续读其余的磁盘块。如果必须让pl1读完所有5个磁盘块后才能让p2开始访问磁盘，那磁盘就成了独占设备了。共享磁盘的含义并不是指在任意时刻不同的进程都可以同时读写磁盘。 3)虚拟设备。是通过某种虚拟技术把一台物理设备变成若干逻辑设备，从用户的角度看，多个用户拥有各自的设备，可以随时向设备发出访问请求并得到系统应答。 三、设备控制器 I/O设备分为机械和电子两部分，设备控制器对应电子部分，通常是可编程的。操作系统的程序员需要了解的是设备控制器的结构，编写设备驱动程序要针对具体的设备控制器，了解设备控制器中寄存器、端口的名称、地址和访问方式，以及使用方式。 1.什么是设备控制器 1)设备控制器是CPU与I/O设备之间的接口，接收I/O的命令并控制设备完成I/O工作。 2)设备控制器是一个可编址设备，连接多个设备时可有多个设备地址。控制器可以直接做在主板上，也可以做成插卡插在主板上。现在有些设备的控制器嵌入在设备中，如激光打印机的控制器。 2.设备控制器的功能 设备控制器的功能说明如下。 1)接收和识别命令。 接收CPU的命令和参数存放在控制器的控制寄存器中，并对命令和地址译码。 2)数据交换。通过数据寄存器进行数据交换。 ①将驱动器中的比特流汇集在控制器的缓冲区中以形成字节块。 ②实现CPU到控制器、控制器到CPU的双向数据传送。 ③将控制器对设备的控制命令传送给设备控制器。 3)设备状态的了解和报告。 设备控制器中有专门用来存放设备状态信息的寄存器或触发器，CPU可以通过读取这些信息了解设备的当前状态。 4)地址识别。 ①设备控制器必须能识别它所控制的每个设备的地址。 ②设备控制器中的寄存器本身应该有唯一的地址，以使CPU能向寄存器中读/写数据。 ③将CPU要访问的外设地址送入控制器，由控制器的地址译码器译码后选中目标设备。 5)数据缓冲。 在设备控制器中可以存储数据，作为CPU和I/O之间的缓冲。 6)差错控制。 设备控制器需要具有差错检测功能，当通过数据校验发现数据传输出错时，可以向CPU报告，废弃错误数据，重新启动一次数据传输。 3.设备控制器的组成 设备控制器的逻辑构成主要包括以下3部分。 1)设备控制器与处理机的接口：数据线、控制线和地址线。 2)设备控制器与设备的接口：设备与设备控制器接口中的3类信号为数据、状态和控 制信号。 3)I/O逻辑：I/O逻辑主要由指令译码器和地址译码器两部分功能部件构成，将CPU的命令和地址分别译码，控制指定设备进行I/O操作。 设备控制器的逻辑结构如图6-3所示。 四、I/O通道 通道用于大型主机系统控制I/O设备，与控制设备结合，与微机和小型机的设备控制器具有对等的功能。即用来代替微机、小型机中的设备控制器，实现大型主机系统的I/O设备控制功能，提供操作系统与I/O设备间的接口。 I/O通道是一种特殊的处理机，它具有执行I/O指令的能力，并通过执行通道程序来控制I/O操作。简单地说，通道是大型主机系统中专门用于I/O的专用计算机。引人通道能够使CPU从控制I/O的任务中解脱，使CPU与I/O并行工作，提高CPU的利用率和系统的吞吐量。 "},"pages/操作系统概论/第六章_I_O设备管理/第二节_I_O控制方式.html":{"url":"pages/操作系统概论/第六章_I_O设备管理/第二节_I_O控制方式.html","title":"第二节 I/O控制方式","keywords":"","body":"第二节 I/O控制方式 输入/输出方式有早期的程序轮询控制方式。在中断机制被引入计算机系统后，输入/输出控制采用中断控制方式。为了提高块设备的输入/输出性能，可以利用DMA(Direct Memory Access，直接内存访问)控制器对输入/输出进行DMA控制。通道控制方式则可以使输入/输出更大程度地独立于主机CPU。在输入/输出控制方式的发展中始终追求的目标是尽量减少主机对输入/输出控制的干预，提高主机与输入/输出的并行程度，以提高整个系统的性能。本节主要介绍轮询、中断和DMA控制方式。 一、轮询 早期的计算机系统，因为没有中断机制，处理器对输入/输出的控制不得不采取程序轮询的方式。采用这种控制方式，主机试图发送I/O控制命令之前，先通过反复检测设备控制器状态寄存器的忙/闲标志位，若设备“忙”，主机继续检测该标志位，直到该位为“空闲”，主机发送I/O指令。在主机发送完/0指令后，设备控制器把状态寄存器的忙/闲标志位再置成“忙”，主机再次进人轮询状态，以检测本次输人/输出是否结束。这种控制方式使CPU经常处于由于输入/输出而造成的循环测试状态，造成CPU的极大浪费，影响整个系统的吞吐量。 二、中断 现代计算机系统广泛采用中断控制方式完成对I/O的控制。采用中断控制方式的I/O工作模式是CPU执行进程中，发出输入/输出请求，若此时I/O设备忙，则进程阻塞等待。当处于“忙”状态的设备工作完毕，通过中断控制器发出中断请求信号，CPU响应中断，执行对应该设备的中断处理程序，然后唤醒因等待该设备而被阻塞的进程。CPU继续执行这个进程时，向设备控制器发送I/O指令，然后CPU被调度程序分配给某个进程，继续执行某个程序。自此，在设备控制器控制设备完成本次I/O的过程中，I/O与CPU并行工作。当本次I/O结束后，设备控制器通过向CPU发送中断请求信号告知CPU本次数据传输结束。 中断控制的工作方式能使CPU与I/O设备在某些时间段上并行工作，提高CPU的利用率和系统的吞吐量。图6-4所示为3种I/O控制方式流程。 三、DMA控制方式 对于磁盘驱动器这类的设备，每次数据传输量较大，采用中断控制方式，传输一个数据块就需要进行多次中断处理。典型情况下，一个数据块大小为512B，采用中断控制方式，若磁盘控制器中数据寄存器为一个字节大小，则每传输完一个字节，就需要CPU执行一次中断处理。为了进一步提高I/O的速度和CPU与I/O的并行程度，可以采用DMA控制方式。 DMA控制需要特殊结构的设备控制器，DMA控制器的逻辑组成包括3部分：主机与DMA的接口、DMA与设备的接口，以及I/O控制逻辑。 为了实现主机与设备控制器之间成块数据的传送，在DMA控制器中设计了4类寄存器：命令/状态寄存器CR、内存地址寄存器MAR、数据寄存器DR和数据计数器DC。DMA控制器的逻辑结构如图6-5所示。 1.命令/状态寄存器CR 用于接收从CPU发来的I/O命令或有关控制信息、设备状态。 2.内存地址寄存器MAR 存放内存地址，在输出数据时，存放输出数据在内存的起始地址，指示DMA应该从内存的什么地方读取输出数据。在输入数据时，存放输入数据将要被放入内存的起始地址，指示DMA应该把输入数据放到内存的什么地方。 3.数据计数器DC 指示DMA,本次向CPU发中断信号前要读或写数据的次数。 4.数据寄存器DR 用于暂存DMA传输中要输入或输出的数据。 下面以从磁盘读数据为例，说明DMA控制方式的工作原理，以及DMA控制和中断控制的区别。 1)没有使用DMA时磁盘读数据的过程。 ①首先控制器一个比特一个比特地从设备完整地读出一块数据放入内部缓冲区中。 ②确认该块数据的正确性。 ③控制器发中断信号，CPU执行中断处理程序，从控制器的设备寄存器中将数据读入内存。 2)通过DMA从磁盘读数据的过程。 当CPU要从磁盘读入一个数据块时，便向磁盘控制器发送一条读命令。该命令被送到其中的命令寄存器CR中。同时，CPU将本次读入数据将要放在内存中的起始地址送DMA的MAR寄存器，将本次要读的字节数送DC寄存器。然后，启动DMA控制器进行数据传送。在DMA控制输入的过程中，CPU可以执行其他的进程。当本次读入的数据全部传送完毕后，DMA向CPU发送中断请求。 在DMA控制磁盘读入数据的过程中，每读入一个字（节），便将该字（节）送到当前MAR指示的内存单元中，然后MAR的值递增，以指向下一个内存单元。DC减1，若DC递减后的值不为O，说明本次数据传送没有结束，继续在DMA控制下传送下一个字节；若DC减1后变为O，说明本次数据传输结束，由DMA向CPU发中断请求。图6-6所示为DMA工作方式流程。 "},"pages/操作系统概论/第六章_I_O设备管理/第三节_缓冲管理.html":{"url":"pages/操作系统概论/第六章_I_O设备管理/第三节_缓冲管理.html","title":"第三节 缓冲管理","keywords":"","body":"第三节 缓冲管理 缓冲区是用来保存两个设备之间或设备与应用程序之间传输数据的内存区域。由于CPU的速度远高于I/O设备，为了尽可能使CPU与设备并行工作，提高系统的性能，通常需要操作系统在设备管理软件中提供缓冲区管理功能。 一、缓冲的引入 在数据到达速率与数据离去速率不同的地方，都可以引入缓冲区。引入缓冲区的主要原因可以归纳为以下两点。 1)处理数据流的生产者与消费者之间的速度差异。例如，假如从调制解调器收到一个文件，并保存到硬盘上。调制解调器大约比硬盘慢数千倍，这种情况下，可以在内存中创建缓冲区以存放从调制解调器处收到的字节。当整个缓冲区填满时，就可以通过一次操作将缓冲区中的数据写入到磁盘中。由于写磁盘并不及时，而且调制解调器需要一个空间继续保存输入数据，因而需要两个缓冲区。当调制解调器填满第一个缓冲区后，就可以请求写磁盘。接着调制解调器开始填写第二个缓冲区，而这时第一个缓冲区中的数据正被写入磁盘。等到调制解调器写满第二个缓冲区时，第一个缓冲区的数据也已写入磁盘。因此，调制解调器可以切换到第一个缓冲区，而第二个缓冲区中的数据可以开始写入磁盘。这种双缓冲将生产者与消费者进行分离，因而缓和了两者之间的时序要求。 2)协调传输数据大小不一致的设备。这种不一致在计算机网络中特别常见，缓冲常常用来处理消息的分段和重组。在发送端，一个大消息分成若干小网络包。这些包通过网络传输，接收端将它们放在重组缓冲区内以生成完整的源数据镜像。 引入缓冲区除了可以缓和CPU与I/O设备之间速度不匹配的矛盾，还能降低对CPU中断频率的要求，放宽对中断响应时间的限制，提高CPU和I/O设备之间的并行性。 二、单缓冲 操作系统提供的最简单的缓冲类型是单缓冲区，如图6-7所示。当一个用户进程发出I/O请求时，操作系统为该操作分配一个位于主存的缓冲区。 对于面向块的设备，单缓冲区方案可以描述如下：输入数据被传送进人系统缓冲区。当传送完成时，进程把该块移到用户空间，并立即请求另一块系统缓冲区。用户进程可以在下一块数据正在读入系统缓冲区时，处理用户区中的那一块数据。相对于没有系统缓冲的情况，这种方法通常能提高速度。 类似的考虑也可以用于面向块的输出。当准备把数据发送到一台设备时，首先把它们从用户空间复制到系统缓冲区中，它们最终从系统缓冲区被传送给输出设备，此时CPU与输出设备可以并行工作。 对于面向流的I/O，在每次传送一行的方式下，或者每次传送一个字节的方式下，可以使用单缓冲方案。键盘、打印机和传感器等都属于面向流的设备。 对于每次传送一行的I/O，可以用缓冲区保存一行数据。在输入期间用户进程被阻塞，等待整行的到达。对于输出，用户进程可以把一行输出放置在缓冲区中，然后继续处理。只有当第一次输出操作的缓冲区内容清空之前，又需要发送第二行输出时，它才需要被阻塞。对于每次传送一个字节的I/O，操作系统和用户进程之间的交互按照第三章进程同步讲述的生产者一消费者模型进行。 三、双缓冲 可以通过给操作系统指定两个系统缓冲区，如图6-8所示，对单缓冲方案进行改进。当一个进程往这一个缓冲区中传送数据（或从这个缓冲区中读取数据）时，操作系统正在清空(或填充)另一个缓冲区，这种技术称为双缓冲(Doube Buffering)，或缓冲交换(Buffering Swapping)。 双缓冲的性能比单缓冲的性能有所提高，但是这种提高是以增加复杂性为代价的。 四、循环缓冲 双缓冲方案可以平滑I/O设备和进程之间的数据流，如果某个特定进程的性能是关注的焦点，常常会希望相关I/O操作能够跟得上这个进程。如果这个进程突然快速执行了大量的I/O，仅有双缓冲就不够了，在这种情况下，通常使用多于两个的缓冲区，如循环缓冲来解决这个问题。可以这样理解：在数据到达和数据离去的速度差别很大的情况下，需要增加缓冲区的数量。 1.循环缓冲的组成 循环缓冲的组成如图6-9所示。 (1)多个缓冲区 1)空缓冲区R:生产者进程下一个可用的空缓冲区。 2)已装满数据的缓冲区G:用于指示消费者进程下一个可用的装有产品的缓冲区。 3)现行工作缓冲区C:消费者进程正在使用的工作缓冲区。 (2)多个指针 1)Nextg:用于指示消费者进程下一个可用的装有数据的缓冲区。 2)Nexti:用于指示生产者进程下一个可用的空缓冲区。 3)Current:用于指示进程正在使用的工作缓冲区。 2.循环缓冲的使用 生产者进程和消费者进程可以利用Getbuf过程和Releasebuf过程来使用循环缓冲区。 (1)Getbuf过程 消费者进程要使用缓冲区中的数据时，可调用Getbuf过程。该过程将Nextg指向的缓冲区提供给进程使用，并把它改成由Current指针指向的现行工作缓冲区，同时使Nextg指向下一个可用的装有数据的缓冲区G。而当生产者进程要使用空缓冲区来装数据时，也通过调用Getbuf过程，把Nexti指示的空缓冲区提供给生产者进程使用，然后使Nexti指向下一个空缓冲区R。 (2)Releasebuf过程 当进程使用完缓冲区之后，调用Releasebuf过程释放缓冲区。当消费者进程把C缓冲区中的数据提取完毕时，便调用Releasebuf过程释放C缓冲区，把该缓冲区改为空缓冲区R。当生产者进程把缓冲区装满数据后，调用Releasebuf过程释放缓冲区，使装满数据的当前工作缓冲区成为G缓冲区。 3.进程同步 由于使用缓冲可使生产者进程和消费者进程并行执行，相应地，指针Nexti和指针Nextg将不断地沿顺时针方向移动，在下列两种情况下需要保持两种进程同步的机制。 1)Nexti指针追上Nextg指针，即生产者进程速度大于消费者进程速度，没有空缓冲区，全部缓冲区已满。此时，需要阻塞生产者进程，等待消费者进程为生产者进程释放空缓冲区R。 2)Nextg指针追上Nexti指针，消费者进程速度大于生产者进程速度，全部缓冲已空。此时，需要阻塞消费者进程，等待生产者进程为消费者进程释放装有数据的缓冲区G。 五、缓冲池 公共缓冲池是被广泛应用的一种缓冲管理技术，公共缓冲池中设置多个可供若干进程共享的缓冲区，这种方式能提高缓冲区的利用率。 1.缓冲池的组成 公共缓冲池既可用于输入，又可用于输出，其中至少包含3种类型的缓冲区、3种缓冲队列和4种工作缓冲区。 1)3种类型的缓冲区。空缓冲区、装满输入数据的缓冲区和装满输出数据的缓冲区。 2)3种队列。 ①空缓冲队列emq:是由空缓冲区链接而成的队列。 ②输入队列inq:是由装满输入数据的缓冲区链接成的队列。 ③输出队列outq:是由装满输出数据的缓冲区链接成的队列。 3)4种工作缓冲区。 ①收容输人数据的缓冲区。收容完输入数据后，缓冲区被插入输入队列中。 ②提取输人数据的缓冲区。存在于输入队列中，进程需要输入数据时，先从输入队列中获取这种缓冲区。 ③收容输出数据的缓冲区。收容完输出数据后，缓冲区被插入输出队列中。 ④提取输出数据的缓冲区。存在于输出队列中，进程需要输出数据时，先从输出队列中获取这种缓冲区。 2.Getbuf过程和Putbuf过程 对缓冲池的操作由Getbuf过程和Putbuf过程来完成。为了使进程能以互斥的方式访问缓冲池队列，可为每个队列设置一个相应的互斥信号量MS(type)。为了保证进程同步使用缓冲区，可以为每个队列设置一个资源信号量RS(type)。其中，type指示缓冲区队列类型，可以是emq、inq或outq。 (1)Getbuf过程 Procedure Getbuf(type) Begin Wait(RS(type));//申请缓冲区 Wait(MS(type));//申请缓冲队列的互斥访问权 B(number)=Takebuf(type);//提取缓冲区 Signal(MS(type));//释放缓冲队列的互斥访问权 End (2)Putbuf过程 Procedure Putbuf(type,number) Begin Wait(MS(type));//申请缓冲队列的互斥访问权 Addbuf(type,number);//向type指示的队列中插入number指示的缓冲区，number可以是hin、sin、sout或hout,见图6-l0 Signal (MS(type));//释放缓冲队列的互斥访问权 Signal (RS(type));//释放缓冲区斯等量 End 3.缓冲区的工作方式 缓冲区可以工作在收容输入、提取输入、收容输出和提取输出4种工作方式下，如图6-10所示。 (1)收容输入 在进程需要收容输入数据时，要先从空缓冲队列提取一个空缓冲区，将输入数据写入缓冲后，再把装入了输入数据的缓冲区入到输入队列中去。操作步骤如下。 1)Getbuf(emq)。 2)将输入数据写入缓冲区。 3)putbuf(inq,hin)。 (2)提取输入 当进程需要输入数据时（如计算进程需要提取输入数据，然后对数据进行计算处理），先从输入队列提取输入缓冲区，然后从中提取输入数据，最后把缓冲区作为空缓冲区插人空缓冲队列。操作步骤如下。 1)Getbuf(inq)。 2)从缓冲区提取数据. 3)putbuf(emq,sin)。 (3)收容输出 在进程需要收容输出数据时，要先从空缓冲队列提取一个空缓冲区，将输出数据写入缓冲后，再把装入了输出数据的缓冲区插入到输出队列中去。操作步骤如下。 1)Getbuf(emq)。 2)将输出数据写入缓冲区。 3)putbuf(outq,hout)。 (4)提取输出 当进程需要输出数据时（如打印进程需要提取输出数据送打印机），先从输出队列提取输出缓冲区，然后从中提取输出数据，最后把这个缓冲区插入空缓冲队列。操作步骤如下。 1)Getbuf(outq)。 2)输出数据。 3)putbuf(emq,sout)。 "},"pages/操作系统概论/第六章_I_O设备管理/第四节_设备分配.html":{"url":"pages/操作系统概论/第六章_I_O设备管理/第四节_设备分配.html","title":"第四节 设备分配","keywords":"","body":"第四节 设备分配 在多道程序环境下，系统中的设备不允许用户自行使用，而必须由系统分配。每当进程向系统提出I/O请求时，设备分配程序便按照一定的策略，把设备分配给用户。设备分配功能的完成，需要记录设备情况的数据结构和设备分配算法。 一、设备分配中的数据结构 支持设备分配的数据结构需要记录设备的状态（忙或空闲）、设备类型等基本信息。下面是一种支持设备分配的数据结构设计方案，该方案包括设备控制表、控制器控制表、通道控制表和系统设备表。这些数据结构记录了设备、控制器、通道的状态及控制器进行控制所需要的信息。 1.设备控制表DCT(Device Control Table) 系统为每个设备建立一张设备控制表，多台设备的设备控制表构成设备控制表集合。每张设备控制表包含以下信息。 1)设备类型。 2)设备标识符。 3)设备状态：忙/闲。 4)指向控制器表的指针。 5)重复执行的次数或时间。 6)设备队列的队首指针。设备队列也称设备请求队列，是因请求设备而被阻塞的进程的PCB构成的队列。设备队列的队首指针指向队首的PCB。 2.控制器控制表COCT(Controller Control Table) 系统为每个控制器设置一张用于记录该控制器信息的控制器控制表，控制器控制表中通常包含以下几个字段。 1)控制器标识符。 2)控制器状态。 3)与控制器相连接的通道表指针。 4)控制器队列的队首指针。 5)控制器队列的队尾指针。 3.通道控制表CHCT(Channel Control Table) 在一些主机系统中还有通道设备，系统会为每个通道设备设置一张通道控制表，通道控制表包括以下几个字段。 1)通道标识符。 2)通道状态。 3)与通道连接的控制器表首址。 4)通道队列的队首指针。 5)通道队列的队尾指针。 4.系统设备表SDT(System Device Table) 系统设备表是系统范围的数据结构，其中记录了系统中全部设备的情况。每个设备占一个表目，其中包括设备类型、设备标识符、设备控制表及设备驱动程序的入口地址。 系统设备表、控制器控制表、设备控制表之间的关系及利用数据结构进行设备分配的流 程如图6-11所示，其中的逻辑设备表LUT的用途及一般结构见后面“三、设备独立性”一节。 二、设备分配 为了使系统有条不紊地工作，系统在分配设备时应考虑以下3个因素。 1)设备的固有属性。 2)设备分配算法。 3)设备分配时的安全性。 1.设备的固有属性 在分配设备时，首先应考虑与设备分配有关的设备属性。设备的固有属性可分成3种：第一种是独占性，指这种设备在一段时间内只允许一个进程独占，即“临界资源”。第二种是共享性，指这种设备允许多个进程同时共享。第三种是可虚拟性，指设备本身虽是独占设备，但经过某种技术处理，可以把它改造成虚拟设备。对上述的独占、共享、可虚拟3种设备应采取不同的分配策略。 (1)独占设备 对于独占设备，应采用独享分配策略，即将一个设备分配给某进程后，使由该进程独占，直至进程完成或释放该设备。然后，系统才能再将该设备分配给其他进程使用。这种分配策略的缺点是，设备得不到充分利用，而且还可能引起死锁。 (2)共享设备 对于共享设备，可同时分配给多个进程使用，此时需要注意对这些进程访问该设备的先后顺序进行合理的调度。 (3)可虚拟设备 由于可虚拟设备是指一台物理设备在采用虚拟技术后可变成多台逻辑上的所谓虚拟设备，因而一台可虚拟设备是可共享的设备，可以将它同时分配给多个进程使用，并对进程访问该设备的先后顺序进行控制。 2.设备分配算法 对设备分配的算法，与进程调度的算法有相似之处，但相对简单，通常只采用以下两种分配算法。 (1)先来先服务 当有多个进程对同一设备提出I/O请求时，该算法是根据进程对某设备提出请求的先后顺序将这些进程排成一个设备请求队列，设备分配程序总是先把设备分配给队首进程。 (2)基于优先权的分配算法 在进程调度中的这种策略，是优先权高的进程优先获得处理机。如果对这种高优先权进程所提出的I/O请求也赋予高优先权，显然有助于这种进程尽快完成。在利用该算法形成设备队列时，将优先权高的进程排在设备队列前面，而对于优先级相同的I/O请求，则按先来先服务原则排队。 3.设备分配方式 从进程运行的安全性上考虑，设备分配有以下两种方式。 (1)安全分配方式 在这种分配方式中，每当进程发出I/O请求后，便进人阻塞状态，直到其I/O操作完成时才被唤醒。在采用这种分配策略时，一旦进程已经获得某种设备（资源）后便阻塞，使该进程不可能再请求任何其他资源，而在它运行时又不能保持任何资源。因此，这种分配方式已经摒弃了造成死锁的4个必要条件之一的“请求和保持”条件，从而使设备分配是安全的。其缺点是进程进展缓慢，即对于同一个进程，CPU与I/O设备是串行工作的。 (2)不安全分配方式 在这种分配方式中，进程在发出I/O请求后仍在继续运行，需要时又发出其他的I/O请求。仅当进程所请求的设备已被另一个进程占用时，请求进程才进人阻塞状态。这种分配方式的优点是，一个进程可同时操作多个设备，使进程推进迅速。其缺点是分配不安全，因为它可能具备“请求和保持”条件，从而可能造成死锁。 三、设备独立性 1.设备独立性的概念 为了提高操作系统的可适应性和可扩展性，在现代操作系统中都毫无例外地实现了设备独立性，也称为设备无关性。其基本含义是应用程序独立于具体使用的物理设备。为了实现设备独立性，引入了逻辑设备和物理设备这两个概念。在应用程序中，使用逻辑设备名称来请求使用某类设备，而系统在实际执行时，还必须使用物理设备名称。因此，系统必须具有将逻辑设备名称转换为物理设备名称的功能。 2.实现设备独立性带来的好处 实现设备独立性带来的好处如下。 1)应用程序与物理设备无关，系统增减或变更外围设备时不需要修改应用程序。 2)易于处理输入/输出设备的故障。例如，某台打印机发生故障时，可用另一台不同型号、不同品牌的打印机替换，而不用修改应用程序。 3)提高了系统的可靠性，增加了设备分配的灵活性。 3.设备独立软件 设备独立软件完成的主要功能如下。 (1)执行所有设备的公有操作 执行的操作包括：独占设备的分配与回收、将逻辑设备名映射为物理设备名、对设备进行保护、缓冲管理和差错控制。为了实现逻辑设备名到物理设备名的转化，可以利用称为逻辑设备表LUT(Logical Unit Table)的数据结构。在该表的每个表目中都包含逻辑设备名、物理设备名。LUT的设置可采取两种方式：为整个系统设置一张LUT或者为每个用户设置一张LUT。 (2)向用户层软件提供统一的接口 设备独立软件向用户层屏蔽访问硬件的细节，向应用软件和最终用户提供简单、统一的访问接口。例如，对各种设备的读操作在应用程序中都使用read函数调用，写操作都使用write函数调用。 例6-1：在编程级，如果要在程序中在打印机/dev/ptl上输出字符串“hello”，用下列语句即可。 fd open(\"/dev/ptl\"); write(\"hello\"); close(fd); 四、独占设备的分配程序 对于具有I/O通道的系统，在进程提出I/O请求后，系统的设备分配程序可按下列步骤进行设备分配。 (1)分配设备 根据用户请求的设备的物理名，查找系统设备表，从中找出该设备的设备控制表，检查设备控制表中的设备状态字。若设备忙，则将进程阻塞在该设备的阻塞队列中；若设备空闲，则根据设备分配算法将设备分配给进程。 (2)分配控制器 若系统为进程分配了其请求的设备，就到该设备的控制表中找出与该设备连接的控制器的COCT，即设备控制器控制表，检查其中的状态字段。若该控制器忙，则将请求I/O的进程阻塞在该设备控制器的阻塞队列中；若控制器空闲，则将它分配给进程。 (3)分配通道 在有通道的系统中，还需要从相应的设备控制器控制表中找到与该控制器连接的通道控制表，检查表中的通道状态字段。若通道忙，则将进程阻塞在该通道的阻塞队列上；若通道空闲，则将该通道分配给进程。 当进程获得了设备、设备控制器或者获得了设备、设备控制器和通道时（在有通道的系 统中)，系统的本次设备分配才算成功，系统可以启动进程的I/O。 五、SPOOLing技术 1.什么是SP00Ling 在多道程序环境下，利用一道程序来模拟脱机输入时的外围控制机的功能，把低速I/O设备上的数据传送到高速输出磁盘上，再利用另一道程序来模拟脱机输出时外围控制机的功能，把数据从磁盘传送到低速输出设备上。这种在联机情况下实现的同时外围操作称为SPOOLing (Simultaneous Perihernal Operations On-Line)。 (1)SPOOLing系统的组成 SPOOing系统的组成如图6-12所示。 1)输入井和输出井。这是位于磁盘上的两个分别存放输入数据和输出数据的存储区域，作为大量输入或输出数据的缓存。 2)输入缓冲区和输出缓冲区。输入缓冲区用来暂存由输入设备送来的输入数据，输出缓存区用来存放从输出井送来的输出数据，以后再传给输出设备。 3)输入进程SPi和输出进程SP0。输入进程把输入设备送来的数据送入输入缓存，再把缓存中的数据送入输入井。当消费者进程需要输入数据时，再从输人井把输入数据读入内存。 输出进程把要输出的数据从内存送入输出井，当需要输出数据时，再从输出井把数据读到输出缓存（如打印缓存），数据从输出缓存送往输出设备。 4)请求I/O队列。请求输入或输出的进程提交的输入/输出任务组成的队列。 2.利用SPOOLing技术实现共享打印机 当用户进程提出打印请求时，SPOOLing系统先为用户做下列两件事。 1)由输出进程在输出井中申请空闲盘块区，并将要打印的数据送入其中。 2)输出进程再为用户申请并填写一张用户请求打印表，将该表挂到请求打印队列上。当打印机空闲时，输出进程完成以下动作。 ①从请求打印队列队首取一张请求打印表。 ②将打印数据从输出井送到打印机缓冲区（输出缓冲区）。 ③打印。 ④打印完毕，若打印队列不为空，则转第①步。 SPOOLing系统的特点如下。 1)提高了I/O速度。由于使用了磁盘作为低速设备（如打印机、磁带等）的大容量缓存，提高了输入/输出的速度。 2)将独占设备改造为共享设备。通过SPOOLing系统使独占设备变为了逻辑上的共享设备，系统可以同时接受多个用户对设备的访问请求。 3)实现了虚拟设备功能。把一台物理上只能互斥使用的设备，变为了从用户眼里看到的共享设备。宏观上看，系统可以同时响应多个用户对设备的请求。微观上看，任意时刻设备只能为某一个用户进程服务，SPOOLing系统实现了将独占设备变换成多个逻辑设备的功能。 "},"pages/操作系统概论/第六章_I_O设备管理/第五节_I_O软件管理.html":{"url":"pages/操作系统概论/第六章_I_O设备管理/第五节_I_O软件管理.html","title":"第五节 I/O软件管理","keywords":"","body":"第五节 I/O软件管理 I/O软件的总体目标是将软件组织成一种层次结构，低层软件用来屏蔽硬件的具体细节，高层软件则主要是为用户提供一个简洁、规范的界面。用户程序及操作系统中设备管理软件的构成和关系如图6-13所示，将设备管理软件组织成4个层次，即 1)用户层软件。 2)与设备无关的软件层。 3)设备驱动程序。 4)中断处理程序（底层）。 设备管理软件与硬件关系最密切的是设备驱动程序，包括设备服务程序和中断处理程序。设备驱动程序上层是设备无关软件，通常完成设备命名、设备分配、设备独立性和缓冲管理等功能。最上层的用户进程向系统发送I/O请求，显示I/O操作的结果，提供用户与设备的接口。 一、设备管理软件的功能 本节综述设备管理软件的功能，后面的内容分别说明设备驱动程序、I/O设备中断处理程序的功能和设备无关软件。操作系统设备管理软件(I/O软件)应该实现以下功能。 1.实现I/O设备的独立性 当用户使用的设备发生变化时，比如用激光打印机替代了喷墨打印机，应用程序的代码不需要修改。操作系统向应用软件层提供的这一支持方便了应用程序的开发和维护。 2.错误处理 错误应该在尽可能接近硬件的地方处理，只有在低层软件处理不了的情况下才通知高层软件。例如，控制器进行一个读操作，它应该尽量处理并完成操作，如果因为某种故障控制器处理不了，则交给设备驱动程序，可能只需重读一次就可以解决问题。 3.异步传输 多数物理I/O是异步传输，即CPU在启动传输操作后便可以转向其他工作，直到中断到达。 4.缓冲管理 由于设备之间的速度差异，必须提供缓冲管理，为所有的块设备和字符设备提供缓冲管理功能，并向高层软件屏蔽由于设备差异带来的缓冲管理实现的具体细节。 5.设备的分配和释放 对共享设备和互斥设备应该采取不同的方式为用户请求分配设备。设备使用完毕，要完成对设备的释放。 6.实现I/O控制方式 针对不同的设备提供不同的I/O控制方式，如对打印机、键盘等字符设备实现中断控制。而对磁盘这样的块设备既可以采用中断控制方式，也可以采用DMA控制方式。但是，鉴于磁盘传输数据的特点，一般都采用DMA控制方式。 操作系统通过将I/O软件组织成如图6-13所示的4个层次，可以合理、高效地实现以上目标。 二、中断处理程序 I/O中断处理程序的作用是将发出I/O请求而被阻塞的进程唤醒。用户进程在发出I/O请求后，由于等待I/O的完成而被阻塞。CPU转去执行其他任务，当I/O任务完成，控制器向CPU发中断请求信号，CPU转去执行中断处理程序，由中断处理程序唤醒被阻塞的设备用户进程。 三、设备驱动程序 设备驱动程序是I/O进程与设备控制器之间的通信程序，其主要任务是接受上层软件发来的抽象的I/O请求，如read或write命令，把它们转换为具体要求后，发送给设备控制器，启动设备去执行。此外，它也将由设备控制器发来的信号传送给上层软件。设备驱动程序中包含所有与设备相关的代码。每个设备驱动程序只处理一种设备，或者一类紧密相关的设备。开发驱动程序需要知道某类设备控制器有多少个寄存器及它们的用途。驱动程序工作的简化流程如图6-14所示。 下面举例说明驱动程序的作用。例如，当有一个读第n块磁盘的请求时，磁盘驱动程序的工作如下。 1)计算出所请求块的物理地址。 2)检查驱动器电机是否正在运转。 3)检查磁头臂是否定位在正确的柱面。 4)确定需要哪些控制器命令及命令的执行顺序。 5)向设备控制器的设备寄存器中写入命令。 6)I/O完成后，向上层软件传送数据。 设备驱动程序属于操作系统的内核程序，但是一般由设备生产厂商开发，销售硬件设备时附送给用户，并不是由操作系统厂商提供。设备驱动程序要遵循操作系统提供的内核与设备驱动的接口标准。由于不同的操作系统提供的内核与驱动程序接口不一样，要使驱动程序在不同的操作系统环境中运行，对同一种设备需要开发针对不同操作系统的驱动程序。人们经常说某个驱动程序是for Windows或者for Linux的，就是出于这个原因。 四、与硬件无关的软件 设备无关软件和设备驱动程序之间的精确界限在各个系统中不尽相同。对于一些以设备无关方式完成的功能，在实际中由于考虑到执行效率等因素，也可以考虑由驱动程序完成。 设备无关I/O软件的功能如下。 1)设备命名：将设备名映射到相应的驱动程序。 2)设备保护：为设备设置合理的访问权限。 3)提供独立于设备的块大小。例如，不同磁盘的扇区大小可能不同，设备无关软件屏蔽了这一事实并向高层软件提供统一的数据块大小。例如，扇区大小为512B,若逻辑磁盘块大小为1KB，则软件发出读两个连续扇区的命令，将连续两个扇区作为一个大小为1KB的逻辑块来处理。 4)为块设备和字符设备提供必要的缓冲技术。 5)块设备的存储分配。当创建了一个文件并向其输入数据时，该文件必须被分配新的磁盘块。为了完成分配工作，操作系统需要为每个磁盘都配置一张记录空闲盘块的表或位图，但定位一个空闲块的算法是独立于设备的，因此可以在高于驱动程序的层次处理。 6)分配和释放独立设备。 7)错误处理。 "},"pages/操作系统概论/第六章_I_O设备管理/第六节_磁盘管理.html":{"url":"pages/操作系统概论/第六章_I_O设备管理/第六节_磁盘管理.html","title":"第六节 磁盘管理","keywords":"","body":"第六节 磁盘管理 磁盘存储器不仅容量大，存取速度快，而且可以实现随机存取，是存放大量程序和数据的理想设备。在现代计算机系统中都配置了磁盘存储器，并以它为主存放文件。计算机系统中对文件的操作，基本上都涉及对磁盘的访问。磁盘I/O速度的高低和磁盘系统的可靠性，都将直接影响系统性能。磁盘管理的重要目标是提高磁盘空间利用率和磁盘访问速度。 一、磁盘结构 磁盘设备是一种复杂的机电设备，本节仅对磁盘的数据组织和格式、磁盘的类型和访问时间等方面进行简要介绍。 1.数据的组织和格式 磁盘设备可包括一个或多个物理盘片，每个磁盘片分一个或两个存储面(Surface)，如图6-l5a所示。每个盘面被组织成若干个同心环，这种环称为磁道(Track)，各磁道之间留有必要的间隙。为使处理简单，在每条磁道上可存储相同数目的二进制位。这样，磁盘密度即是每英寸中所存储的位数，显然是内层磁道的密度较外层磁道的密度高。每条磁道又被划分成若干个扇区(Sector)。图6-l5b所示为一个磁道分成8个扇区，各扇区之间保留一定的间隙。 一个物理记录存储在一个扇区上，磁盘上存储的物理记录数目是由扇区数、磁道数及磁盘面数所决定的。 为了提高磁盘的存储容量，充分利用磁盘外磁道的存储能力，现代磁盘不再把内外磁道划分为相同数目的扇区，而是利用外层磁道容量较内层磁道大的特点，将盘面划分成若干条环带，使得同一环带内的所有磁道具有相同的扇区数。显然，外层环带的磁道拥有较内层环带的磁道更多的扇区。为了减少这种磁道和扇区在盘面分布的几何形式变化对驱动程序的影响，大多数现代磁盘都隐藏了这些细节。 为了在磁盘中存储数据，必须先将磁盘低级格式化。图6-16显示出了一种温盘（温切斯特盘)中一条磁道格式化的情况。其中每条磁道含有30个固定大小的扇区，每个扇区容量为600个字节。其中512个字节存放数据，其余的用于存放控制信息。每个扇区包括两个字段。 1)标识符字段，其中一个字节的SYNCH具有特定的位图像，作为该字段的定界符。利用磁道号、磁头号及扇区号三者来标识一个扇区。CRC字段用于段校验。 2)数据字段，其中可存放512个字节的数据。 磁盘格式化完成后，一般要对磁盘分区。在逻辑上，每个分区就是一个独立的逻辑磁盘。每个分区的起始扇区和大小都被记录在磁盘0扇区的主引导记录分区表所包含的分区表中。在这个分区表中必须有一个分区被标记成活动的，以保证能够从硬盘引导系统。 但是，在真正可以使用磁盘前，还需要对磁盘进行一次高级格式化，即设置一个引导块、根目录和一个空文件系统，同时在分区表中标记该分区所使用的文件系统。 2.磁盘的类型 对磁盘可以从不同的角度进行分类。最常见的有：将磁盘分成硬盘和软盘、单片盘和多片盘、固定头磁盘和活动头（移动头）磁盘等。下面仅对固定头磁盘和移动头磁盘进行介绍。 (1)固定头磁盘 这种磁盘在每条磁道上都有读/写磁头，所有的磁头都被装在一个刚性磁臂甲。通过这些磁头可访问各磁道且进行并行读/写，有效地提高了磁盘的I/O速度。这种结构的磁盘主要用于大容量磁盘上。 (2)移动头磁盘 这种磁盘每一个盘面仅配有一个磁头，也被装人磁臂中。为了能访问该盘面的所有磁道，该磁头必须能移动并进行寻道。可见，移动磁头仅能以串行方式读/写，致使磁盘读写速度较慢。但由于其结构简单，故仍广泛应用于中小型磁盘设备中。在微型机上配置的硬盘采用移动磁头结构，故本节主要针对这类磁盘的I/O进行讨论。 3.磁盘的访问时间 磁盘设备在工作时以恒定速率旋转。为了读或写，磁头必须能移动到所要求的磁道上，并等待所要求的扇区的开始位置旋转到磁头下，然后再开始读或写数据。故可把对磁盘的访问时间分成以下3部分。 (1)寻道时间 这是指把磁臂（磁头）移动到指定磁道上所经历的时间。该时间是启动磁臂的时间与磁头移动n条磁道所花费的时间之和。 (2)旋转延迟时间 这是指将指定扇区移动到磁头下面所经历的时间。不同的磁盘类型，旋转速度差别很大。 (3)传输时间 这是指把数据从磁盘读出或向磁盘写入数据时所经历的时间。其大小与每次所读/写的字节数和旋转速度有关。 在磁盘访问时间中，寻道时间和旋转延迟时间基本上都与所读/写数据的多少无关，而且寻道时间和旋转延迟时间通常占据了访问时间中的大头。适当地集中数据在磁盘上存放的位置，可以减少磁臂移动距离，这将有利于提高传输速率。 二、磁盘调度 磁盘是可供多个进程共享的设备，当有多个进程都要求访问磁盘时，应采用一种最佳调度算法，以使各个进程对磁盘的平均访问时间最短。由于在访问磁盘的时间中，主要是寻道时间，因此磁盘调度的一个重要目标是使磁盘的平均寻道时间最少。目前常用的磁盘调度算法有先来先服务、最短寻道时间优先及扫描等算法，下面逐一进行介绍。 1.先来先服务(First Come First Served,FCFS) 这是一种最简单的磁盘调度算法。它根据进程请求访问磁盘的先后顺序进行调度。此算法的优点是公平、简单，且每个进程的请求都能依次得到处理，不会出现某一进程的请求长期得不到满足的情况。但此算法由于未对寻道进行优化，致使平均寻道时间可能较长。表6-1给出了有9个进程先后提出磁盘I/O请求时，按FCS算法进行调度的情况。这里将进程号（请求者）按他们发出的请求的先后顺序排队。这样，平均寻道距离为55.3条磁道，与后面即将讲到的几种调度算法相比，其平均寻道距离较大，故FCFS算法仅适用于请求磁盘I/O的进程数目较少的场合。 2.最短寻道时间优先(Shortest Seek Time First,SSTF) 该算法选择这样的进程：其要求访问的磁道与当前磁头所在的磁道距离最近，以使每次的寻道时间最短。但这种算法不能保证平均寻道时间最短。表6-2给出了SSTF算法进行调度时，各进程被调度的顺序、每次磁头移动的距离，以及9次调度磁头平均移动的距离。比较表6-1和表6-2可以看出，SSTF算法的每次磁头移动平均距离明显低于FCFS的距离，因而SSTF较之FCFS有更好的寻道性能，故曾一度被广泛采用。 3.扫描(SCAN)算法 (1)进程“饥饿”现象 SSTF算法虽然能获得较好的寻道性能，但却可能导致某个进程发生“饥饿”(Starvation)现象。因为只要不断有新进程的请求到达，且其所要访问的磁道与磁头当前所在磁道的距离较近，这种新进程的I/O请求必然优先被满足，导致所要访问的磁道距离与磁头所在位置较远的磁盘任务总是不能得到调度。对SSTF算法略加修改后所形成的SCAN算法可防止进程出现“饥饿”现象。 (2)SCAN算法 该算法不仅考虑到要访问的磁道与当前磁道的距离，更优先考虑磁头当前的移动方向。例如，当磁头正在自里向外移动时，SCAN算法所考虑的下一个访问对象应是其要访问的磁道既在当前磁道之外，又是距离最近的。这样自里向外地访问，直至再无更外的磁道需要访问时，才将磁臂换为自外向里移动。这时，同样也是每次选择这样的进程来调度，即要访问的磁道在当前位置内距离最近者。这样，磁头逐步地自外向里移动，直至再无更里面的磁道 要访问，从而避免出现“饥饿”现象。由于在这种算法中磁头移动的规律颇似电梯的运行，因而又常称之为电梯调度算法。表6-3给出了按SCAN算法对9个进程进行调度及磁头移动的情况。 4.循环扫描(CSCAN)算法 SCAN算法既能获得较好的寻道性能，又防止了“饥饿”现象，故被广泛用于大、中、小型机器和网络中的磁盘调度。但SCAN也存在这样的问题：当磁头刚从里向外移动而越过了某一磁道时，恰好又有一进程请求访问此磁道。这时，该进程必须等待，待磁头继续自里向外，然后再自外向里扫描完所有要访问的磁道后，才处理该进程的请求，致使该进程的请求被大大推迟。为了减少这种延迟，CSCAN算法规定磁头是单向移动。例如，只是自里向外移动，当磁头移到最外的磁道后，磁头立即返回到最里的要访问的磁道，亦即将最小磁道号紧接着最大磁道号构成循环，进行循环扫描。表6-4给出了CSCAN算法对9个进程调度的顺序及每次磁头移动的距离。 5.NStepSCAN和FSCAN调度算法 (1)NStepSCAN算法 在SSTF、SCAN及CSCAN几种调度算法中，都可能会出现磁臂停留在某处不动的情况，例如，有一个或几个进程对某一磁道有较高的访问频率，即这个（些）进程反复请求对某磁道的I/O操作，从而垄断了整个磁盘设备。把这一现象称为“磁臂粘着”(Armstickiness)，在高密度磁盘上容易出现此情况。NStepSCAN算法是将磁盘请求队列分成若干个长度为N的子队列，磁盘调度将按FCFS算法依次处理这些子队列。每处理一个队列时又是按SCAN算法，对一个队列处理完后，再处理其他队列。当正在处理某子队列时，如果又出现新的磁盘I/O请求，便将新请求进程放入其他队列，这样就可避免出现磁臂粘着现象。当N值取得很大时，会使NStepSCAN算法的性能接近于SCAN算法的性能。当N=1时，NStepSCAN算法便蜕化为FCFS算法。 (2)FSCAN算法 FSCAN算法实质上是NStepSCAN算法的简化，即FSCAN只将磁盘请求队列分成两个子队列。一个是由当前所有请求磁盘I/O的进程形成的队列，由磁盘调度按SCAN算法进行处理。在扫描期间，将新出现的所有请求磁盘I/O的进程，放入另一个等待处理的请求队列。这样，所有的新请求都将被推迟到下一次扫描时处理。 三、提高磁盘I/O速度的方法 磁盘的I/O速度远低于内存的访问速度，通常要低上4~6个数量级。因此，磁盘的I/O已成为计算机系统的瓶颈。本节介绍儿种已经广泛采用的提高磁盘I/O速度的方法。 1.提前读 简单来说，提前读就是系统根据现在用户请求读的内容，把预计最近不久可能要读的内容与现在请求读的内容一起提前读人内存。用户读文件时，经常是顺序读文件内容，文件的存放是按内容顺序放人磁盘块的。操作系统知道文件内容是按照怎样的顺序存放在磁盘块中的，所以可以在按照用户请求读当前磁盘块时，提前把下一个磁盘块的内容也读入缓冲区。比如文件f1，内容依次存放在10、20、60号磁盘块中，如果当前用户请求读的内容在10号磁盘块中，系统在读入10号磁盘块的内容时，把20号磁盘块的内容也提前读入缓冲。这样，当下次要访问已经提前读入缓冲的磁盘块时，就不需要从磁盘中读，而是可以直接从内存的缓冲区中读。因此，大大减少了读数据的时间，提高了用户读磁盘数据的速度 2.延迟写 延迟写是在支持请求分页的虚拟存储管理中，对修改过的换出页，在把页标记为换出页时并不马上把页的内容写入磁盘，而是暂时保留在内存中，直到这些页所在的页框要被使用，导致页的内容将被覆盖前的“最后”时刻才启动磁盘操作，把修改过的一个或若干页写入磁盘，这种延迟写的策略减少了写磁盘的次数。 3.优化物理块的分布 寻道时间和磁盘旋转延迟时间通常占据了磁盘I/O所耗时间中的主要部分，所以适当地集中数据在磁盘上存放的位置，可以减少磁臂移动距离，有利于提高传输速率。为了达到这一目的，可以采取优化文件物理块分布的方法。现在的文件系统都允许文件离散存放。理论上，一个文件的物理块可以分散在磁盘的任意位置。但是，如果将一个文件存放在过于分散的多个磁盘块上，会增加磁头的移动距离。例如，如果分配给一个文件的第一个磁盘块在最里面的磁道上，而第二个磁盘块在最外面的磁道上，当读完该文件的第一个磁盘块时，需要把磁头从最里面的磁道移动到最外面一个磁道。而如果把这个文件存放在同一个磁道或者相邻的两个磁道上，读这个文件的两个磁盘块磁头的移动距离会短得多，读磁盘的速度会快得多。 因此，现在的文件系统基本都会考虑对文件的位置进行优化，原则就是尽可能地把一个文件存放在同一个磁道或者相邻的磁道上。实现这个原则的技术一个是以连续的几个扇区即一个簇作为磁盘块的分配单位，另一个技术是把磁盘分成块组，一个块组中的不同簇都在相邻的磁道上，一个文件尽可能地放在同一个块组中，如Liux的Ext2文件系统。 4.虚拟盘 虚拟盘是指利用内存空间去仿真磁盘，又称RAM盘。虚拟盘可以接受所有标准的磁盘操作，但这些操作的执行不是在磁盘上，而是在内存中。因此，对虚拟盘的访问比对磁盘的访问速度快。用户对虚拟盘的操作与对磁盘的操作完全相同，所有实现细节对用户都是透明的。虚拟盘通常用于存放临时性文件，如编译程序所产生的目标程序等。虚拟盘与磁盘高速缓存虽然都位于内存中，但是虚拟盘中的内容操作完全由用户控制，而高速缓存中的内容则 是由操作系统控制的。 5.磁盘高速缓存 磁盘高速缓存是指内存的一块存储空间，用来暂存从磁盘中读出的一系列盘块中的信息。因此，这里的高速缓存是一组逻辑上属于磁盘，而物理上是驻留在内存中的盘块。高速缓存在内存中可分成两种形式。第一种是在内存中开辟一个单独的存储空间来作为磁盘高速缓存，其大小是固定的，不会受应用程序多少的影响。第二种是把所有未利用的内存空间变为一个缓冲池，供请求分页系统和磁盘I/O时（作为磁盘高速缓存）共享。此时，高速缓存的大小不再是固定的。当磁盘频繁发生I/O时，该缓冲池可能包含更多的内存空间。而在应用程序运行得较多时，该缓冲池可能只剩下较少的内存空间。 "},"pages/lflivekit/LFLiveKit源码之LFLivePreview.html":{"url":"pages/lflivekit/LFLiveKit源码之LFLivePreview.html","title":"LFLiveKit源码之LFLivePreview","keywords":"","body":"LFLiveKit源码之LFLivePreview "},"pages/librtmp/librtmp源码之调试.html":{"url":"pages/librtmp/librtmp源码之调试.html","title":"librtmp源码之调试","keywords":"","body":"librtmp源码之调试 "},"pages/librtmp/librtmp源码之相关结构体.html":{"url":"pages/librtmp/librtmp源码之相关结构体.html","title":"librtmp源码之相关结构体","keywords":"","body":"librttmp源码之相关结构体 分析RTMP_Connect之前，有必要了解下一下定义。 Ip Protocol /* Standard well-defined IP protocols. */ enum { IPPROTO_IP = 0, /* Dummy protocol for TCP. */ IPPROTO_ICMP = 1, /* Internet Control Message Protocol. */ IPPROTO_IGMP = 2, /* Internet Group Management Protocol. */ IPPROTO_IPIP = 4, /* IPIP tunnels (older KA9Q tunnels use 94). */ IPPROTO_TCP = 6, /* Transmission Control Protocol. */ IPPROTO_EGP = 8, /* Exterior Gateway Protocol. */ IPPROTO_PUP = 12, /* PUP protocol. */ IPPROTO_UDP = 17, /* User Datagram Protocol. */ IPPROTO_IDP = 22, /* XNS IDP protocol. */ IPPROTO_TP = 29, /* SO Transport Protocol Class 4. */ IPPROTO_DCCP = 33, /* Datagram Congestion Control Protocol. */ IPPROTO_IPV6 = 41, /* IPv6 header. */ IPPROTO_RSVP = 46, /* Reservation Protocol. */ IPPROTO_GRE = 47, /* General Routing Encapsulation. */ IPPROTO_ESP = 50, /* encapsulating security payload. */ IPPROTO_AH = 51, /* authentication header. */ IPPROTO_MTP = 92, /* Multicast Transport Protocol. */ IPPROTO_BEETPH = 94, /* IP option pseudo header for BEET. */ IPPROTO_ENCAP = 98, /* Encapsulation Header. */ IPPROTO_PIM = 103, /* Protocol Independent Multicast. */ IPPROTO_COMP = 108, /* Compression Header Protocol. */ IPPROTO_SCTP = 132, /* Stream Control Transmission Protocol. */ IPPROTO_UDPLITE = 136, /* UDP-Lite protocol. */ IPPROTO_RAW = 255, /* Raw IP packets. */ IPPROTO_MAX }; struct sockaddr 内容来源于：https://www.cnblogs.com/cyx-b/p/12450811.html struct sockaddr { 　　unsigned short sa_family;// 2字节，地址族，AF_xxx 　　char sa_data[14]; // 14字节，包含套接字中的目标地址和端口信息 }; struct addrinfo 内容来源于：https://www.cnblogs.com/LubinLew/p/POSIX-DataStructure.html The header shall define the addrinfo structure, which shall include at least the following members: #include #include #include /* ======================Types of sockets====================== */ enum __socket_type { SOCK_STREAM = 1, /* Sequenced, reliable, connection-based byte streams. */ SOCK_DGRAM = 2, /* Connectionless, unreliable datagrams of fixed maximum length. */ SOCK_RAW = 3, /* Raw protocol interface. */ SOCK_RDM = 4, /* Reliably-delivered messages. */ SOCK_SEQPACKET = 5, /* Sequenced, reliable, connection-based,datagrams of fixed maximum length. */ SOCK_DCCP = 6, /* Datagram Congestion Control Protocol. */ SOCK_PACKET = 10, /* Linux specific way of getting packets at the dev level. For writing rarp and other similar things on the user level. */ /* Flags to be ORed into the type parameter of socket and socketpair and used for the flags parameter of paccept. */ SOCK_CLOEXEC = 02000000, /* Atomically set close-on-exec flag for the new descriptor(s). */ SOCK_NONBLOCK = 00004000 /* Atomically mark descriptor(s) as non-blocking. */ }; /* ============Protocol families(只列出常用几个)================= */ #define PF_UNSPEC 0 /* Unspecified. */ #define PF_LOCAL 1 /* Local to host (pipes and file-domain). */ #define PF_INET 2 /* IP protocol family. */ #define PF_IPX 4 /* Novell Internet Protocol. */ #define PF_APPLETALK 5 /* Appletalk DDP. */ #define PF_INET6 10 /* IP version 6. */ #define PF_TIPC 30 /* TIPC sockets. */ #define PF_BLUETOOTH 31 /* Bluetooth sockets. */ /* ==============Address families(只列出常用几个)================= */ #define AF_UNSPEC PF_UNSPEC #define AF_LOCAL PF_LOCAL #define AF_UNIX PF_UNIX #define AF_FILE PF_FILE #define AF_INET PF_INET #define AF_IPX PF_IPX #define AF_APPLETALK PF_APPLETALK #define AF_INET6 PF_INET6 #define AF_ROSE PF_ROSE #define AF_NETLINK PF_NETLINK #define AF_TIPC PF_TIPC #define AF_BLUETOOTH PF_BLUETOOTH /* ====Possible values for `ai_flags' field in `addrinfo' structure.===== */ #define AI_PASSIVE 0x0001 /* Socket address is intended for `bind'. */ #define AI_CANONNAME 0x0002 /* Request for canonical name. */ #define AI_NUMERICHOST 0x0004 /* Don't use name resolution. */ #define AI_V4MAPPED 0x0008 /* IPv4 mapped addresses are acceptable. */ #define AI_ALL 0x0010 /* Return IPv4 mapped and IPv6 addresses. */ #define AI_ADDRCONFIG 0x0020 /* Use configuration of this host to choose returned address type. */ #ifdef __USE_GNU #define AI_IDN 0x0040 /* IDN encode input (assuming it is encoded in the current locale's character set) before looking it up. */ #define AI_CANONIDN 0x0080 /* Translate canonical name from IDN format. */ #define AI_IDN_ALLOW_UNASSIGNED 0x0100 /* Don't reject unassigned Unicode code points. */ #define AI_IDN_USE_STD3_ASCII_RULES 0x0200 /* Validate strings according to STD3 rules. */ #endif #define AI_NUMERICSERV 0x0400 /* Don't use name resolution. */ /* =======================struct addrinfo======================= */ struct addrinfo { int ai_flags; /* 附加选项,多个选项可以使用或操作结合 */ int ai_family; /* 指定返回地址的协议簇,取值范围:AF_INET(IPv4)、AF_INET6(IPv6)、AF_UNSPEC(IPv4 and IPv6) */ int ai_socktype; /* enum __socket_type 类型，设置为0表示任意类型 */ int ai_protocol; /* 协议类型，设置为0表示任意类型,具体见上一节的 Ip Protocol */ socklen_t ai_addrlen; /* socket address 的长度 */ struct sockaddr *ai_addr; /* socket address 的地址 */ char *ai_canonname; /* Canonical name of service location. */ struct addrinfo *ai_next; /* 指向下一条信息,因为可能返回多个地址 */ }; RTMP 内容来源于：https://www.jianshu.com/p/05b1e5d70c06 RTMP定义在rtmp.h。 /* ** 远程调用方法 */ typedef struct RTMP_METHOD { AVal name; int num; } RTMP_METHOD; typedef struct RTMPSockBuf { int sb_socket; // 套接字 int sb_size; // 缓冲区可读大小 char *sb_start; // 缓冲区读取位置 char sb_buf[RTMP_BUFFER_CACHE_SIZE]; // 套接字读取缓冲区 int sb_timedout; // 超时标志 void *sb_ssl; // TLS上下文 } RTMPSockBuf; typedef struct RTMP { int m_inChunkSize; // 最大接收块大小 int m_outChunkSize; // 最大发送块大小 int m_nBWCheckCounter; // 带宽检测计数器 int m_nBytesIn; // 接收数据计数器 int m_nBytesInSent; // 当前数据已回应计数器 int m_nBufferMS; // 当前缓冲的时间长度，以MS为单位 int m_stream_id; // 当前连接的流ID int m_mediaChannel; // 当前连接媒体使用的块流ID uint32_t m_mediaStamp; // 当前连接媒体最新的时间戳 uint32_t m_pauseStamp; // 当前连接媒体暂停时的时间戳 int m_pausing; // 是否暂停状态 int m_nServerBW; // 服务器带宽 int m_nClientBW; // 客户端带宽 uint8_t m_nClientBW2; // 客户端带宽调节方式 uint8_t m_bPlaying; // 当前是否推流或连接中 uint8_t m_bSendEncoding; // 连接服务器时发送编码 uint8_t m_bSendCounter; // 设置是否向服务器发送接收字节应答 int m_numInvokes; // 0x14命令远程过程调用计数 int m_numCalls; // 0x14命令远程过程请求队列数量 RTMP_METHOD *m_methodCalls; // 远程过程调用请求队列 RTMPPacket *m_vecChannelsIn[RTMP_CHANNELS]; // 对应块流ID上一次接收的报文 RTMPPacket *m_vecChannelsOut[RTMP_CHANNELS]; // 对应块流ID上一次发送的报文 int m_channelTimestamp[RTMP_CHANNELS]; // 对应块流ID媒体的最新时间戳 double m_fAudioCodecs; // 音频编码器代码 double m_fVideoCodecs; // 视频编码器代码 double m_fEncoding; /* AMF0 or AMF3 */ double m_fDuration; // 当前媒体的时长 int m_msgCounter; // 使用HTTP协议发送请求的计数器 int m_polling; // 使用HTTP协议接收消息主体时的位置 int m_resplen; // 使用HTTP协议接收消息主体时的未读消息计数 int m_unackd; // 使用HTTP协议处理时无响应的计数 AVal m_clientID; // 使用HTTP协议处理时的身份ID RTMP_READ m_read; // RTMP_Read()操作的上下文 RTMPPacket m_write; // RTMP_Write()操作使用的可复用报文对象 RTMPSockBuf m_sb; // RTMP_ReadPacket()读包操作的上下文 RTMP_LNK Link; // RTMP连接上下文 } RTMP; RTMP_LINK 内容来源于：https://www.jianshu.com/p/05b1e5d70c06 typedef struct RTMP_LNK { AVal hostname; // 目标主机地址 AVal sockshost; // socks代理地址 // 连接和推拉流涉及的一些参数信息 AVal playpath0; /* parsed from URL */ AVal playpath; /* passed in explicitly */ AVal tcUrl; AVal swfUrl; AVal pageUrl; AVal app; AVal auth; AVal flashVer; AVal subscribepath; AVal token; AMFObject extras; int edepth; int seekTime; // 播放流的开始时间 int stopTime; // 播放流的停止时间 #define RTMP_LF_AUTH 0x0001 /* using auth param */ #define RTMP_LF_LIVE 0x0002 /* stream is live */ #define RTMP_LF_SWFV 0x0004 /* do SWF verification */ #define RTMP_LF_PLST 0x0008 /* send playlist before play */ #define RTMP_LF_BUFX 0x0010 /* toggle stream on BufferEmpty msg */ #define RTMP_LF_FTCU 0x0020 /* free tcUrl on close */ int lFlags; int swfAge; int protocol; // 连接使用的协议 int timeout; // 连接超时时间 unsigned short socksport; // socks代理端口 unsigned short port; // 目标主机端口 #ifdef CRYPTO #define RTMP_SWF_HASHLEN 32 void *dh; /* for encryption */ void *rc4keyIn; void *rc4keyOut; uint32_t SWFSize; uint8_t SWFHash[RTMP_SWF_HASHLEN]; char SWFVerificationResponse[RTMP_SWF_HASHLEN+10]; #endif } RTMP_LNK; RTMPPacket 内容来源于：https://blog.csdn.net/NB_vol_1/article/details/58660181 https://blog.csdn.net/bwangk/article/details/112802823 // 原始的rtmp消息块 typedef struct RTMPChunk { int c_headerSize; // 头部的长度 int c_chunkSize; // chunk的大小 char *c_chunk; // 数据 char c_header[RTMP_MAX_HEADER_SIZE]; // chunk头部 } RTMPChunk; // rtmp消息块 typedef struct RTMPPacket { // chunk basic header（大部分情况是一个字节） // #define RTMP_PACKET_SIZE_LARGE 0 onMetaData流开始的绝对时间戳控制消息（如connect） // #define RTMP_PACKET_SIZE_MEDIUM 1 大部分的rtmp header都是8字节的 // #define RTMP_PACKET_SIZE_SMALL 2 比较少见 // #define RTMP_PACKET_SIZE_MINIMUM 3 偶尔出现，低于8字节频率 // chunk type id (2bit)fmt 对应message head {0,3,7,11} + (6bit)chunk stream id // 块类型ID，消息头的第1个字节的前2位，决定消息头长度 uint8_t m_headerType; // Message type ID（1-7协议控制；8，9音视频；10以后为AMF编码消息） uint8_t m_packetType; // 是否含有Extend timeStamp字段 uint8_t m_hasAbsTimestamp; /* timestamp absolute or relative? */ // channel 即 stream id字段 // 第一个字节的低6位，命名为Chunk Stream ID，Chunk Stream ID用来表示消息的级别： // chunk stream id 级别 // 2 low level // 3 high level(像connect, create_stream一类消息) // 4 control stream // 5 video // 6 audio // 8 control stream int m_nChannel; // 时间戳 uint32_t m_nTimeStamp; /* timestamp */ // message stream id int32_t m_nInfoField2; /* last 4 bytes in a long header */ // chunk体的长度 uint32_t m_nBodySize; uint32_t m_nBytesRead; RTMPChunk *m_chunk; // 原始rtmp消息块 char *m_body; } RTMPPacket; typedef struct RTMPPacket { uint8_t m_headerType; //basic header 中的type头字节，值为(0,1,2,3)表示ChunkMsgHeader的类型（4种） uint8_t m_packetType; //Chunk Msg Header中msg type 1字节：消息类型id（8: audio；9:video；18:AMF0编码的元数据） uint8_t m_hasAbsTimestamp; //bool值，是否是绝对时间戳(类型1时为true) int m_nChannel; //块流ID ，通过设置ChannelID来设置Basic stream id的长度和值 uint32_t m_nTimeStamp; //时间戳，消息头前三字节 int32_t m_nInfoField2; //Chunk Msg Header中msg StreamID 4字节：消息流id uint32_t m_nBodySize; //Chunk Msg Header中msg length 4字节：消息长度 uint32_t m_nBytesRead; //已读取的数据 RTMPChunk *m_chunk; //raw chunk结构体指针，把RTMPPacket的真实头部和数据段拷贝进来 char *m_body; //数据段指针 } RTMPPacket; RTMP_READ 内容来源于：https://blog.csdn.net/NB_vol_1/article/details/58660181 /* ** AVal表示一个字符串 */ typedef struct AVal { char *av_val; int av_len; } AVal; /* state for read() wrapper */ // read函数的包装器，包括状态等等 typedef struct RTMP_READ { char *buf; char *bufpos; unsigned int buflen; uint32_t timestamp; uint8_t dataType; uint8_t flags; #define RTMP_READ_HEADER 0x01 #define RTMP_READ_RESUME 0x02 #define RTMP_READ_NO_IGNORE 0x04 #define RTMP_READ_GOTKF 0x08 #define RTMP_READ_GOTFLVK 0x10 #define RTMP_READ_SEEKING 0x20 int8_t status; #define RTMP_READ_COMPLETE -3 #define RTMP_READ_ERROR -2 #define RTMP_READ_EOF -1 #define RTMP_READ_IGNORE 0 /* if bResume == TRUE */ uint8_t initialFrameType; uint32_t nResumeTS; char *metaHeader; char *initialFrame; uint32_t nMetaHeaderSize; uint32_t nInitialFrameSize; uint32_t nIgnoredFrameCounter; uint32_t nIgnoredFlvFrameCounter; } RTMP_READ; "},"pages/librtmp/librtmp源码之RTMP_Connect.html":{"url":"pages/librtmp/librtmp源码之RTMP_Connect.html","title":"librtmp源码之RTMP_Connect","keywords":"","body":"librttmp源码之RTMP_Connect RTMP_Connect int RTMP_Connect(RTMP *r, RTMPPacket *cp) { struct addrinfo *service; if (!r->Link.hostname.av_len) return FALSE; // 设置直接连接的服务器地址 if (r->Link.socksport) { /* Connect via SOCKS */ if (!add_addr_info(&service, &r->Link.sockshost, r->Link.socksport)) return FALSE; } else { /* Connect directly */ if (!add_addr_info(&service, &r->Link.hostname, r->Link.port)) return FALSE; } RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_flags:%d\", service->ai_flags); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_family:%d\", service->ai_family); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_socktype:%d\", service->ai_socktype); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_protocol:%d\", service->ai_protocol); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addrlen:%d\", service->ai_addrlen); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addr->sa_family:%d\", service->ai_addr->sa_family); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addr->sa_data:%s\", service->ai_addr->sa_data); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_canonname:%s\", service->ai_canonname); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_next:%s\", service->ai_next); if (!RTMP_Connect0(r, service)) { freeaddrinfo(service); return FALSE; } freeaddrinfo(service); r->m_bSendCounter = TRUE; return RTMP_Connect1(r, cp); } 代码片段分析1 // 设置直接连接的服务器地址 if (r->Link.socksport) { /* Connect via SOCKS */ if (!add_addr_info(&service, &r->Link.sockshost, r->Link.socksport)) return FALSE; } else { /* Connect directly */ if (!add_addr_info(&service, &r->Link.hostname, r->Link.port)) return FALSE; } RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_flags:%d\", service->ai_flags); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_family:%d\", service->ai_family); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_socktype:%d\", service->ai_socktype); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_protocol:%d\", service->ai_protocol); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addrlen:%d\", service->ai_addrlen); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addr->sa_family:%d\", service->ai_addr->sa_family); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addr->sa_data:%s\", service->ai_addr->sa_data); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_canonname:%s\", service->ai_canonname); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_next:%s\", service->ai_next); // DEBUG: CCQ: addrinfo->ai_flags:0 /* 0代表没有设值，如果设值，取值范围见上`AI_PASSIVE`... */ // DEBUG: CCQ: addrinfo->ai_family:2 /* 2代表`AF_INET`，也就是`PF_INET`，取之范围见上`AF_UNSPEC`... */ // DEBUG: CCQ: addrinfo->ai_socktype:1 /* 1代表`SOCK_STREAM`，取值范围见上`__socket_type` */ // DEBUG: CCQ: addrinfo->ai_protocol:6 /* 6代表`IPPROTO_TCP`，取之范围见上`Ip Protocol` */ // DEBUG: CCQ: addrinfo->ai_addrlen:16 /* socket address 的长度 */ // DEBUG: CCQ: addrinfo->ai_addr->sa_family:2 /* 2代表`AF_INET`，也就是`PF_INET`，取之范围见上`AF_UNSPEC`... */ // DEBUG: CCQ: addrinfo->ai_addr->sa_data:\u0007\\217\\300\\250 // DEBUG: CCQ: addrinfo->ai_canonname:(null) /* Canonical name of service location. */ // DEBUG: CCQ: addrinfo->ai_next:(null) 代码片段分析2 if (!RTMP_Connect0(r, service)) { freeaddrinfo(service); return FALSE; } freeaddrinfo(service); r->m_bSendCounter = TRUE;// 设置是否向服务器发送接收字节应答 return RTMP_Connect1(r, cp); 内容来源于：https://www.jianshu.com/p/05b1e5d70c06 开始调用RTMP_Connect1，继续执行SSL或HTTP协商，以及RTMP握手。 add_addr_info 内容来源于：https://blog.csdn.net/weixin_37921201/article/details/90111641 填充struct addrinfo结构体用于之后的socket通信。 /** 填充struct addrinfo结构体用于之后的socket通信。 service: addrinfo指针的指针 host: 192.168.0.12:1935/zbcs/room port: 1935 */ static int add_addr_info(struct addrinfo **service, AVal *host, int port) { struct addrinfo hints; char *hostname, portNo[32]; int ret = TRUE; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: host->av_val：%s\", host->av_val); if (host->av_val[host->av_len]) { hostname = malloc(host->av_len+1); memcpy(hostname, host->av_val, host->av_len); hostname[host->av_len] = '\\0'; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: hostname：%s\", hostname); } else { hostname = host->av_val; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: hostname2：%s\", hostname); } sprintf(portNo, \"%d\", port); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: portNo：%s\", portNo); memset(&hints, 0, sizeof(struct addrinfo)); hints.ai_socktype = SOCK_STREAM; hints.ai_family = AF_UNSPEC; if(getaddrinfo(hostname, portNo, &hints, service) != 0) { RTMP_Log(RTMP_LOGERROR, \"Problem accessing the DNS. (addr: %s)\", hostname); ret = FALSE; } finish: if (hostname != host->av_val) free(hostname); return ret; } 代码片段分析1 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: host->av_val：%s\", host->av_val); if (host->av_val[host->av_len]) { hostname = malloc(host->av_len+1); memcpy(hostname, host->av_val, host->av_len); hostname[host->av_len] = '\\0'; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: hostname：%s\", hostname); } else { hostname = host->av_val; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: hostname2：%s\", hostname); } // 输出结果： // DEBUG: host->av_val：192.168.0.12:1935/zbcs/room // DEBUG: hostname：192.168.0.12 通过打印结果分析，这段代码就是给hostname赋值，得到IP地址段。 代码片段分析2 sprintf(portNo, \"%d\", port); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: portNo：%d\", portNo); // DEBUG: CCQ: portNo：1935 sprintf：https://baike.baidu.com/item/sprintf/9703430?fr=aladdin 这里就是给portNo赋值，将int转char *。 代码片段分析3 memset(&hints, 0, sizeof(struct addrinfo));// 给hints分配内存 hints.ai_socktype = SOCK_STREAM;// 设置sock类型，设置范围：__socket_type hints.ai_family = AF_UNSPEC;// 指定返回地址的协议簇 内容来源于： https://blog.csdn.net/u011003120/article/details/78277133 https://www.cnblogs.com/LubinLew/p/POSIX-getaddrinfo.html ai_family解释：指定返回地址的协议簇，取值范围:AF_INET(IPv4)、AF_INET6(IPv6)、AF_UNSPEC(IPv4 and IPv6)。 代码片段分析4 // hostname：IP地址，例如：192.168.0.12 // portNo：端口号，例如：1935 // hints：struct addrinfo地址 // service：传进来的struct addrinfo，用于获取信息结果 if(getaddrinfo(hostname, portNo, &hints, service) != 0) { RTMP_Log(RTMP_LOGERROR, \"Problem accessing the DNS. (addr: %s)\", hostname); ret = FALSE; } 内容来源于： https://www.cnblogs.com/LubinLew/p/POSIX-getaddrinfo.html 函数注释： int getaddrinfo(const char *restrict nodename, /* host 或者IP地址 */ const char *restrict servname, /* 十进制端口号 或者常用服务名称如\"ftp\"、\"http\"等 */ const struct addrinfo *restrict hints, /* 获取信息要求设置 */ struct addrinfo **restrict res); /* 获取信息结果 */ IPv4中使用gethostbyname()函数完成主机名到地址解析，这个函数仅仅支持IPv4，且不允许调用者指定所需地址类型的任何信息，返回的结构只包含了用于存储IPv4地址的空间。IPv6中引入了新的API getaddrinfo()，它是协议无关的，既可用于IPv4也可用于IPv6。getaddrinfo() 函数能够处理名字到地址以及服务到端口这两种转换，返回的是一个 struct addrinfo 的结构体(列表)指针而不是一个地址清单。这些 struct addrinfo 结构体随后可由套接口函数直接使用。如此以来，getaddrinfo()函数把协议相关性安全隐藏在这个库函数内部。应用程序只要处理由getaddrinfo()函数填写的套接口地址结构。 代码片段分析5 finish: if (hostname != host->av_val) free(hostname); 释放hostname内存，至此add_addr_info函数分析完毕。 RTMP_Connect0 int RTMP_Connect0(RTMP *r, struct addrinfo * service) { int on = 1; r->m_sb.sb_timedout = FALSE; r->m_pausing = 0; r->m_fDuration = 0.0; // 创建套接字 r->m_sb.sb_socket = socket(service->ai_family, service->ai_socktype, service->ai_protocol); if (r->m_sb.sb_socket != -1) {// 连接对端 if (connect(r->m_sb.sb_socket, service->ai_addr, service->ai_addrlen) Link.socksport) { RTMP_Log(RTMP_LOGDEBUG, \"%s ... SOCKS negotiation\", __FUNCTION__); if (!SocksNegotiate(r)) { RTMP_Log(RTMP_LOGERROR, \"%s, SOCKS negotiation failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } } } else { RTMP_Log(RTMP_LOGERROR, \"%s, failed to create socket. Error: %d\", __FUNCTION__, GetSockError()); return FALSE; } /* set timeout */ { SET_RCVTIMEO(tv, r->Link.timeout); if (setsockopt (r->m_sb.sb_socket, SOL_SOCKET, SO_RCVTIMEO, (char *)&tv, sizeof(tv))) { RTMP_Log(RTMP_LOGERROR, \"%s, Setting socket timeout to %ds failed!\", __FUNCTION__, r->Link.timeout); } } setsockopt(r->m_sb.sb_socket, SOL_SOCKET, SO_NOSIGPIPE, (char *) &on, sizeof(on)); setsockopt(r->m_sb.sb_socket, IPPROTO_TCP, TCP_NODELAY, (char *) &on, sizeof(on)); return TRUE; } 代码片段分析1 int on = 1;// setsockopt函数使用 r->m_sb.sb_timedout = FALSE;// 超时标志 r->m_pausing = 0;// 是否暂停状态 r->m_fDuration = 0.0;// 当前媒体的时长 代码片段分析2 // 创建套接字 // DEBUG: CCQ: addrinfo->ai_family:2 /* 2代表`AF_INET`，也就是`PF_INET`，取之范围见上`AF_UNSPEC`... */ // DEBUG: CCQ: addrinfo->ai_socktype:1 /* 1代表`SOCK_STREAM`，取值范围见上`__socket_type` */ // DEBUG: CCQ: addrinfo->ai_protocol:6 /* 6代表`IPPROTO_TCP`，取之范围见上`Ip Protocol` */ r->m_sb.sb_socket = socket(service->ai_family, service->ai_socktype, service->ai_protocol); if (r->m_sb.sb_socket != -1) { // 连接对端 // DEBUG: CCQ: addrinfo->ai_addrlen:16 /* socket address 的长度 */ // DEBUG: CCQ: addrinfo->ai_addr->sa_family:2 /* 2代表`AF_INET`，也就是`PF_INET`，取之范围见上`AF_UNSPEC`... */ // DEBUG: CCQ: addrinfo->ai_addr->sa_data:\u0007\\217\\300\\250 if (connect(r->m_sb.sb_socket, service->ai_addr, service->ai_addrlen) 内容来源于：http://c.biancheng.net/view/2131.html 在 Linux 下使用 头文件中 socket() 函数来创建套接字，原型为： int socket(int af, int type, int protocol); 1) af 为地址族（Address Family），也就是 IP 地址类型，常用的有 AF_INET 和 AF_INET6。AF 是“Address Family”的简写，INET是“Inetnet”的简写。AF_INET 表示 IPv4 地址，例如 127.0.0.1；AF_INET6 表示 IPv6 地址，例如 1030::C9B4:FF12:48AA:1A2B。 大家需要记住127.0.0.1，它是一个特殊IP地址，表示本机地址，后面的教程会经常用到。 你也可以使用 PF 前缀，PF 是“Protocol Family”的简写，它和 AF 是一样的。例如，PF_INET 等价于 AF_INET，PF_INET6 等价于 AF_INET6。 2) type 为数据传输方式/套接字类型，常用的有 SOCK_STREAM（流格式套接字/面向连接的套接字） 和 SOCK_DGRAM（数据报套接字/无连接的套接字），我们已经在《套接字有哪些类型》一节中进行了介绍。 3) protocol 表示传输协议，常用的有 IPPROTO_TCP 和 IPPTOTO_UDP，分别表示 TCP 传输协议和 UDP 传输协议。 connect(r->m_sb.sb_socket, service->ai_addr, service->ai_addrlen)抓包结果： 过滤条件：ip.addr eq 81.68.250.191 9833 2261.079011 192.168.1.3 81.68.250.191 TCP 78 51965 → 1935 [SYN] Seq=0 Win=65535 Len=0 MSS=1460 WS=64 TSval=45457028 TSecr=0 SACK_PERM=1 9834 2261.090254 81.68.250.191 192.168.1.3 TCP 74 1935 → 51965 [SYN, ACK] Seq=0 Ack=1 Win=28960 Len=0 MSS=1400 SACK_PERM=1 TSval=2657772015 TSecr=45457028 WS=128 9835 2261.090325 192.168.1.3 81.68.250.191 TCP 66 51965 → 1935 [ACK] Seq=1 Ack=1 Win=131840 Len=0 TSval=45457039 TSecr=2657772015 代码片段分析3 // 执行Socks协商 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: r->Link.socksport：%d\", r->Link.socksport); // DEBUG: CCQ: r->Link.socksport：0 if (r->Link.socksport) { RTMP_Log(RTMP_LOGDEBUG, \"%s ... SOCKS negotiation\", __FUNCTION__); if (!SocksNegotiate(r)) { RTMP_Log(RTMP_LOGERROR, \"%s, SOCKS negotiation failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } } r->Link.socksport：0，首次不会执行SocksNegotiate。 代码片段分析4 /* set timeout */ { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: r->Link.timeout：%d\", r->Link.timeout); // DEBUG: CCQ: r->Link.timeout：30 // #define SET_RCVTIMEO(tv,s) int tv = s*1000 SET_RCVTIMEO(tv, r->Link.timeout); // r->m_sb.sb_socket：标识一个套接口的描述字（RTMP->RTMPSockBuf->sb_socket） // SOL_SOCKET：选项定义的层次；目前仅支持SOL_SOCKET和IPPROTO_TCP层次。 // SO_RCVTIMEO：接收超时。 // tv：超时时间 if (setsockopt (r->m_sb.sb_socket, SOL_SOCKET, SO_RCVTIMEO, (char *)&tv, sizeof(tv))) { RTMP_Log(RTMP_LOGERROR, \"%s, Setting socket timeout to %ds failed!\", __FUNCTION__, r->Link.timeout); } } 内容来源于：https://www.cnblogs.com/cthon/p/9270778.html SET_RCVTIMEO是宏定义，给tv赋值，这里tv为30*1000。 代码片段分析5 setsockopt(r->m_sb.sb_socket, SOL_SOCKET, SO_NOSIGPIPE, (char *) &on, sizeof(on)); setsockopt(r->m_sb.sb_socket, IPPROTO_TCP, TCP_NODELAY, (char *) &on, sizeof(on)); 内容来源于：https://www.cnblogs.com/cthon/p/9270778.html TCP_NODELAY选项禁止Nagle算法。Nagle算法通过将未确认的数据存入缓冲区直到蓄足一个包一起发送的方法，来减少主机发送的零碎小数据包的数目。但对于某些应用来说，这种算法将降低系统性能。所以TCP_NODELAY可用来将此算法关闭。应用程序编写者只有在确切了解它的效果并确实需要的情况下，才设置TCP_NODELAY选项，因为设置后对网络性能有明显的负面影响。TCP_NODELAY是唯一使用IPPROTO_TCP层的选项，其他所有选项都使用SOL_SOCKET层。 内容来源于：http://www.sinohandset.com/mac-osx%E4%B8%8Bso_nosigpipe%E7%9A%84%E6%80%AA%E5%BC%82%E8%A1%A8%E7%8E%B0 在linux下为了避免网络出错引起程序退出，我们一般采用MSG_NOSIGNAL来避免系统发送singal。这种错误一般发送在网络断开，但是程序仍然发送数据时，在接收时，没有必要使用。但是在linux下，使用此参数，也不会引起不好的结果。 RTMP_Connect1 int RTMP_Connect1(RTMP *r, RTMPPacket *cp) { if (r->Link.protocol & RTMP_FEATURE_SSL) { #if defined(CRYPTO) && !defined(NO_SSL) TLS_client(RTMP_TLS_ctx, r->m_sb.sb_ssl); TLS_setfd(r->m_sb.sb_ssl, r->m_sb.sb_socket); if (TLS_connect(r->m_sb.sb_ssl) Link.protocol & RTMP_FEATURE_HTTP) { r->m_msgCounter = 1; r->m_clientID.av_val = NULL; r->m_clientID.av_len = 0; HTTP_Post(r, RTMPT_OPEN, \"\", 1); if (HTTP_read(r, 1) != 0) { r->m_msgCounter = 0; RTMP_Log(RTMP_LOGDEBUG, \"%s, Could not connect for handshake\", __FUNCTION__); RTMP_Close(r); return 0; } r->m_msgCounter = 0; } RTMP_Log(RTMP_LOGDEBUG, \"%s, ... connected, handshaking\", __FUNCTION__); if (!HandShake(r, TRUE)) { RTMP_Log(RTMP_LOGERROR, \"%s, handshake failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } RTMP_Log(RTMP_LOGDEBUG, \"%s, handshaked\", __FUNCTION__); if (!SendConnectPacket(r, cp)) { RTMP_Log(RTMP_LOGERROR, \"%s, RTMP connect failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } return TRUE; } 代码片段分析1 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, r->Link.protocol:%d\", __FUNCTION__, r->Link.protocol); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, RTMP_FEATURE_SSL:%d\", __FUNCTION__, RTMP_FEATURE_SSL); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, r->Link.protocol & RTMP_FEATURE_SSL:%d\", __FUNCTION__, r->Link.protocol & RTMP_FEATURE_SSL); // DEBUG: CCQ: RTMP_Connect1, r->Link.protocol:16 // DEBUG: CCQ: RTMP_Connect1, RTMP_FEATURE_SSL:4 // #define RTMP_FEATURE_SSL 0x04 // DEBUG: CCQ: RTMP_Connect1, r->Link.protocol & RTMP_FEATURE_SSL:0 if (r->Link.protocol & RTMP_FEATURE_SSL) { #if defined(CRYPTO) && !defined(NO_SSL) TLS_client(RTMP_TLS_ctx, r->m_sb.sb_ssl); TLS_setfd(r->m_sb.sb_ssl, r->m_sb.sb_socket); if (TLS_connect(r->m_sb.sb_ssl) 1.为什么r->Link.protocol=16？ #define RTMP_FEATURE_WRITE 0x10 /* publish, not play */ void RTMP_EnableWrite(RTMP *r) { r->Link.protocol |= RTMP_FEATURE_WRITE; } RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, r->Link.protocol1:%d\", __FUNCTION__, _rtmp->Link.protocol); // DEBUG: CCQ: -[RTMPPusher connectWithURL:], r->Link.protocol1:0 RTMP_EnableWrite(_rtmp); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, r->Link.protocol2:%d\", __FUNCTION__, _rtmp->Link.protocol); // DEBUG: CCQ: -[RTMPPusher connectWithURL:], r->Link.protocol2:16 2.r->Link.protocol & RTMP_FEATURE_SSL为0，不会执行if后的代码。 代码片段分析2 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, r->Link.protocol & RTMP_FEATURE_HTTP:%d\", __FUNCTION__, r->Link.protocol & RTMP_FEATURE_HTTP); // DEBUG: CCQ: RTMP_Connect1, r->Link.protocol & RTMP_FEATURE_HTTP:0 if (r->Link.protocol & RTMP_FEATURE_HTTP) { r->m_msgCounter = 1; r->m_clientID.av_val = NULL; r->m_clientID.av_len = 0; HTTP_Post(r, RTMPT_OPEN, \"\", 1); if (HTTP_read(r, 1) != 0) { r->m_msgCounter = 0; RTMP_Log(RTMP_LOGDEBUG, \"%s, Could not connect for handshake\", __FUNCTION__); RTMP_Close(r); return 0; } r->m_msgCounter = 0; } RTMP_Log(RTMP_LOGDEBUG, \"%s, ... connected, handshaking\", __FUNCTION__); r->Link.protocol & RTMP_FEATURE_HTTP为0，不会执行if后的代码。连接成功，开始执行handshaking。 代码片段3 // 进行HandShake if (!HandShake(r, TRUE)) { RTMP_Log(RTMP_LOGERROR, \"%s, handshake failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } RTMP_Log(RTMP_LOGDEBUG, \"%s, handshaked\", __FUNCTION__); connect(r->m_sb.sb_socket, service->ai_addr, service->ai_addrlen)： 14 6.159961 192.168.1.3 81.68.250.191 TCP 78 52049 → 1935 [SYN] Seq=0 Win=65535 Len=0 MSS=1460 WS=64 TSval=1695460720 TSecr=0 SACK_PERM=1 15 6.177631 81.68.250.191 192.168.1.3 TCP 74 1935 → 52049 [SYN, ACK] Seq=0 Ack=1 Win=28960 Len=0 MSS=1400 SACK_PERM=1 TSval=2658240513 TSecr=1695460720 WS=128 16 6.177717 192.168.1.3 81.68.250.191 TCP 66 52049 → 1935 [ACK] Seq=1 Ack=1 Win=131840 Len=0 TSval=1695460737 TSecr=2658240513 HandShake： 17 6.177792 192.168.1.3 81.68.250.191 TCP 1454 52049 → 1935 [ACK] Seq=1 Ack=1 Win=131840 Len=1388 TSval=1695460737 TSecr=2658240513 18 6.177793 192.168.1.3 81.68.250.191 RTMP 215 Handshake C0+C1 19 6.186903 81.68.250.191 192.168.1.3 TCP 66 1935 → 52049 [ACK] Seq=1 Ack=1538 Win=32128 Len=0 TSval=2658240524 TSecr=1695460737 20 6.187494 81.68.250.191 192.168.1.3 TCP 1454 1935 → 52049 [ACK] Seq=1 Ack=1538 Win=32128 Len=1388 TSval=2658240524 TSecr=1695460737 21 6.187498 81.68.250.191 192.168.1.3 TCP 1454 1935 → 52049 [ACK] Seq=1389 Ack=1538 Win=32128 Len=1388 TSval=2658240524 TSecr=1695460737 22 6.187499 81.68.250.191 192.168.1.3 RTMP 363 Handshake S0+S1+S2 23 6.187559 192.168.1.3 81.68.250.191 TCP 66 52049 → 1935 [ACK] Seq=1538 Ack=3074 Win=128768 Len=0 TSval=1695460746 TSecr=2658240524 24 6.187636 192.168.1.3 81.68.250.191 TCP 1454 52049 → 1935 [ACK] Seq=1538 Ack=3074 Win=131072 Len=1388 TSval=1695460746 TSecr=2658240524 25 6.187636 192.168.1.3 81.68.250.191 RTMP 214 Handshake C2 26 6.195197 81.68.250.191 192.168.1.3 TCP 66 1935 → 52049 [ACK] Seq=3074 Ack=3074 Win=35200 Len=0 TSval=2658240532 TSecr=1695460746 代码片段4 // /*握手成功之后，发送Connect Packet*/ if (!SendConnectPacket(r, cp)) { RTMP_Log(RTMP_LOGERROR, \"%s, RTMP connect failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } "},"pages/librtmp/librtmp源码之HandShake.html":{"url":"pages/librtmp/librtmp源码之HandShake.html","title":"librtmp源码之HandShake","keywords":"","body":"librttmp源码之HandShake HandShake static int HandShake(RTMP * r, int FP9HandShake) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s start\", __FUNCTION__); int i, offalg = 0; int dhposClient = 0; int digestPosClient = 0; int encrypted = r->Link.protocol & RTMP_FEATURE_ENC; RC4_handle keyIn = 0; RC4_handle keyOut = 0; int32_t *ip; uint32_t uptime; uint8_t clientbuf[RTMP_SIG_SIZE + 4], *clientsig=clientbuf+4; uint8_t serversig[RTMP_SIG_SIZE], client2[RTMP_SIG_SIZE], *reply; uint8_t type; getoff *getdh = NULL, *getdig = NULL; if (encrypted || r->Link.SWFSize) FP9HandShake = TRUE; else FP9HandShake = FALSE; r->Link.rc4keyIn = r->Link.rc4keyOut = 0; if (encrypted) { clientsig[-1] = 0x06; /* 0x08 is RTMPE as well */ offalg = 1; } else clientsig[-1] = 0x03; uptime = htonl(RTMP_GetTime()); memcpy(clientsig, &uptime, 4); if (FP9HandShake) { /* set version to at least 9.0.115.0 */ if (encrypted) { clientsig[4] = 128; clientsig[6] = 3; } else { clientsig[4] = 10; clientsig[6] = 45; } clientsig[5] = 0; clientsig[7] = 2; RTMP_Log(RTMP_LOGDEBUG, \"%s: Client type: %02X\", __FUNCTION__, clientsig[-1]); getdig = digoff[offalg]; getdh = dhoff[offalg]; } else { memset(&clientsig[4], 0, 4); } /* generate random data */ #ifdef _DEBUG memset(clientsig+8, 0, RTMP_SIG_SIZE-8); #else ip = (int32_t *)(clientsig+8); for (i = 2; i Link.dh = DHInit(1024); if (!r->Link.dh) { RTMP_Log(RTMP_LOGERROR, \"%s: Couldn't initialize Diffie-Hellmann!\", __FUNCTION__); return FALSE; } dhposClient = getdh(clientsig, RTMP_SIG_SIZE); RTMP_Log(RTMP_LOGDEBUG, \"%s: DH pubkey position: %d\", __FUNCTION__, dhposClient); if (!DHGenerateKey(r->Link.dh)) { RTMP_Log(RTMP_LOGERROR, \"%s: Couldn't generate Diffie-Hellmann public key!\", __FUNCTION__); return FALSE; } if (!DHGetPublicKey(r->Link.dh, &clientsig[dhposClient], 128)) { RTMP_Log(RTMP_LOGERROR, \"%s: Couldn't write public key!\", __FUNCTION__); return FALSE; } } digestPosClient = getdig(clientsig, RTMP_SIG_SIZE); /* reuse this value in verification */ RTMP_Log(RTMP_LOGDEBUG, \"%s: Client digest offset: %d\", __FUNCTION__, digestPosClient); CalculateDigest(digestPosClient, clientsig, GenuineFPKey, 30, &clientsig[digestPosClient]); RTMP_Log(RTMP_LOGDEBUG, \"%s: Initial client digest: \", __FUNCTION__); RTMP_LogHex(RTMP_LOGDEBUG, clientsig + digestPosClient, SHA256_DIGEST_LENGTH); } #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"Clientsig: \"); RTMP_LogHex(RTMP_LOGDEBUG, clientsig, RTMP_SIG_SIZE); #endif if (!WriteN(r, (char *)clientsig-1, RTMP_SIG_SIZE + 1)) return FALSE; if (ReadN(r, (char *)&type, 1) != 1) /* 0x03 or 0x06 */ return FALSE; RTMP_Log(RTMP_LOGDEBUG, \"%s: Type Answer : %02X\", __FUNCTION__, type); if (type != clientsig[-1]) RTMP_Log(RTMP_LOGWARNING, \"%s: Type mismatch: client sent %d, server answered %d\", __FUNCTION__, clientsig[-1], type); if (ReadN(r, (char *)serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE) return FALSE; /* decode server response */ memcpy(&uptime, serversig, 4); uptime = ntohl(uptime); RTMP_Log(RTMP_LOGDEBUG, \"%s: Server Uptime : %d\", __FUNCTION__, uptime); RTMP_Log(RTMP_LOGDEBUG, \"%s: FMS Version : %d.%d.%d.%d\", __FUNCTION__, serversig[4], serversig[5], serversig[6], serversig[7]); if (FP9HandShake && type == 3 && !serversig[4]) FP9HandShake = FALSE; #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"Server signature:\"); RTMP_LogHex(RTMP_LOGDEBUG, serversig, RTMP_SIG_SIZE); #endif if (FP9HandShake) { uint8_t digestResp[SHA256_DIGEST_LENGTH]; uint8_t *signatureResp = NULL; /* we have to use this signature now to find the correct algorithms for getting the digest and DH positions */ int digestPosServer = getdig(serversig, RTMP_SIG_SIZE); if (!VerifyDigest(digestPosServer, serversig, GenuineFMSKey, 36)) { RTMP_Log(RTMP_LOGWARNING, \"Trying different position for server digest!\"); offalg ^= 1; getdig = digoff[offalg]; getdh = dhoff[offalg]; digestPosServer = getdig(serversig, RTMP_SIG_SIZE); if (!VerifyDigest(digestPosServer, serversig, GenuineFMSKey, 36)) { RTMP_Log(RTMP_LOGERROR, \"Couldn't verify the server digest\"); /* continuing anyway will probably fail */ return FALSE; } } /* generate SWFVerification token (SHA256 HMAC hash of decompressed SWF, key are the last 32 bytes of the server handshake) */ if (r->Link.SWFSize) { const char swfVerify[] = { 0x01, 0x01 }; char *vend = r->Link.SWFVerificationResponse+sizeof(r->Link.SWFVerificationResponse); memcpy(r->Link.SWFVerificationResponse, swfVerify, 2); AMF_EncodeInt32(&r->Link.SWFVerificationResponse[2], vend, r->Link.SWFSize); AMF_EncodeInt32(&r->Link.SWFVerificationResponse[6], vend, r->Link.SWFSize); HMACsha256(r->Link.SWFHash, SHA256_DIGEST_LENGTH, &serversig[RTMP_SIG_SIZE - SHA256_DIGEST_LENGTH], SHA256_DIGEST_LENGTH, (uint8_t *)&r->Link.SWFVerificationResponse[10]); } /* do Diffie-Hellmann Key exchange for encrypted RTMP */ if (encrypted) { /* compute secret key */ uint8_t secretKey[128] = { 0 }; int len, dhposServer; dhposServer = getdh(serversig, RTMP_SIG_SIZE); RTMP_Log(RTMP_LOGDEBUG, \"%s: Server DH public key offset: %d\", __FUNCTION__, dhposServer); len = DHComputeSharedSecretKey(r->Link.dh, &serversig[dhposServer], 128, secretKey); if (len Link.rc4keyIn = keyIn; r->Link.rc4keyOut = keyOut; /* update the keystreams */ if (r->Link.rc4keyIn) { RC4_encrypt(r->Link.rc4keyIn, RTMP_SIG_SIZE, (uint8_t *) buff); } if (r->Link.rc4keyOut) { RC4_encrypt(r->Link.rc4keyOut, RTMP_SIG_SIZE, (uint8_t *) buff); } } } else { if (memcmp(serversig, clientsig, RTMP_SIG_SIZE) != 0) { RTMP_Log(RTMP_LOGWARNING, \"%s: client signature does not match!\", __FUNCTION__); } } RTMP_Log(RTMP_LOGDEBUG, \"%s: Handshaking finished....\", __FUNCTION__); return TRUE; } 代码片段分析1 int i, offalg = 0;// offalg加密才使用 int dhposClient = 0;// 加密才使用 int digestPosClient = 0;// 加密才使用 int encrypted = r->Link.protocol & RTMP_FEATURE_ENC; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s encrypted:%d\", __FUNCTION__, encrypted); // DEBUG: CCQ: HandShake encrypted:0 // 不加密 RC4_handle keyIn = 0;// 加密才使用 RC4_handle keyOut = 0;// 加密才使用 int32_t *ip;/* generate random data */ uint32_t uptime;// 当前时间，填充C1前4字节 代码片段分析2 RC4_handle keyIn = 0;// 加密才使用 RC4_handle keyOut = 0;// 加密才使用 int32_t *ip;/* generate random data */ uint32_t uptime;// 当前时间，填充C1前4字节 // #define RTMP_SIG_SIZE 1536:C1和S1消息有1536字节长 uint8_t clientbuf[RTMP_SIG_SIZE + 4], *clientsig=clientbuf+4; uint8_t serversig[RTMP_SIG_SIZE], client2[RTMP_SIG_SIZE], *reply;// reply,client2加密才使用 uint8_t type;// ReadN(r, (char *)&type, 1)之后获得 getoff *getdh = NULL, *getdig = NULL;// 加密才使用 代码片段分析3 if (encrypted || r->Link.SWFSize) FP9HandShake = TRUE; else //普通的 FP9HandShake = FALSE; r->Link.rc4keyIn = r->Link.rc4keyOut = 0;// 加密才使用 /*C0 字段已经写入clientsig*/ if (encrypted) { clientsig[-1] = 0x06; /* 0x08 is RTMPE as well */ offalg = 1; } else //0x03代表RTMP协议的版本（客户端要求的） //数组竟然能有“-1”下标,因为clientsig指向的是clientbuf+4,所以不存在非法地址 //C0中的字段(1B) clientsig[-1] = 0x03; uptime = htonl(RTMP_GetTime()); //void *memcpy(void *dest, const void *src, int n); //由src指向地址为起始地址的连续n个字节的数据复制到以dest指向地址为起始地址的空间内 //把uptime的前4字节（其实一共就4字节）数据拷贝到clientsig指向的地址中 //C1中的字段(4B) // ———————————————— // 版权声明：本文为CSDN博主「雷霄骅」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 // 原文链接：https://blog.csdn.net/leixiaohua1020/article/details/12954329 memcpy(clientsig, &uptime, 4); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: uptime: \"); RTMP_LogHexString(RTMP_LOGDEBUG, clientsig, 4); // DEBUG: Clientsig: // DEBUG: 0000: 0c b7 58 56 ..XV clientsig[-1] = 0x03;将RTMP协议的版本号写入，1字节。 memcpy(clientsig, &uptime, 4);将当前时间写入clientsig，4字节。 代码片段分析4 if (FP9HandShake)// 加密才处理 { /* set version to at least 9.0.115.0 */ if (encrypted) { clientsig[4] = 128; clientsig[6] = 3; } else { clientsig[4] = 10; clientsig[6] = 45; } clientsig[5] = 0; clientsig[7] = 2; RTMP_Log(RTMP_LOGDEBUG, \"%s: Client type: %02X\", __FUNCTION__, clientsig[-1]); getdig = digoff[offalg]; getdh = dhoff[offalg]; } else { memset(&clientsig[4], 0, 4); } memset(&clientsig[4], 0, 4);当前时间的后补0，占4字节。 代码片段分析5 /* generate random data */ #ifdef _DEBUG //将clientsig+8开始的1528个字节替换为0（这是一种简单的方法） //这是C1中的random字段 memset(clientsig+8, 0, RTMP_SIG_SIZE-8); #else //实际中使用rand()循环生成1528字节的伪随机数 ip = (int32_t *)(clientsig+8); for (i = 2; i 根据上面的分析，得知clientsig的-1位置存放这0x03（RTMP）的版本号，占1字节；0-4位置存放这当前时间uptime，占4字节；之后5-8位置补0，占4字节；而clientsig的总长度是1536个字节，已经占用了8字节，所以上面代码是在补全剩下的1528个字节，补全的方式是使用随机数。 代码片段分析5 #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"Clientsig: \"); RTMP_LogHex(RTMP_LOGDEBUG, clientsig, RTMP_SIG_SIZE); #endif RTMP_Log(RTMP_LOGDEBUG, \"CCQ: Clientsig: \"); RTMP_LogHexString(RTMP_LOGDEBUG, clientsig, RTMP_SIG_SIZE); //发送数据报C0+C1 //从clientsig-1开始发，长度1536+1，两个包合并 //握手---------------- RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。发送握手数据C0+C1\", __FUNCTION__); if (!WriteN(r, (char *)clientsig-1, RTMP_SIG_SIZE + 1)) return FALSE; //读取数据报，长度1，存入type //是服务器的S0，表示服务器使用的RTMP版本 if (ReadN(r, (char *)&type, 1) != 1) /* 0x03 or 0x06 */ return FALSE; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。接收握手数据S0\", __FUNCTION__); RTMP_Log(RTMP_LOGDEBUG, \"%s: Type Answer : %02X\", __FUNCTION__, type); //客户端要求的版本和服务器提供的版本不同 if (type != clientsig[-1]) RTMP_Log(RTMP_LOGWARNING, \"%s: Type mismatch: client sent %d, server answered %d\", __FUNCTION__, clientsig[-1], type); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。接收握手数据S1\", __FUNCTION__); //客户端和服务端随机序列长度是否相同 if (ReadN(r, (char *)serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE) return FALSE; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。接收握手数据S1\", __FUNCTION__); /* decode server response */ //把serversig的前四个字节赋值给uptime memcpy(&uptime, serversig, 4); uptime = ntohl(uptime);//大端转小端 RTMP_Log(RTMP_LOGDEBUG, \"%s: Server Uptime : %d\", __FUNCTION__, uptime); RTMP_Log(RTMP_LOGDEBUG, \"%s: FMS Version : %d.%d.%d.%d\", __FUNCTION__, serversig[4], serversig[5], serversig[6], serversig[7]); 内容来源于：https://blog.csdn.net/bwangk/article/details/112802823 C0 和 S0消息格式：C0和S0是单独的一个字节，表示版本信息。 在C0中这个字段表示客户端要求的RTMP版本 。在S0中这个字段表示服务器选择的RTMP版本。本规范所定义的版本是3；0-2是早期产品所用的，已被丢弃；4-31保留在未来使用 ；32-255不允许使用 （为了区分其他以某一字符开始的文本协议）。如果服务无法识别客户端请求的版本，应该返回3 。客户端可以选择减到版本3或选择取消握手 C1 和 S1消息格式：C1和S1消息有1536字节长。 时间：4字节：本字段包含时间戳。该时间戳应该是发送这个数据块的端点的后续块的时间起始点。可以是0，或其他的任何值。为了同步多个流，端点可能发送其块流的当前值。 零：4字节：本字段必须是全零。 随机数据：1528字节。本字段可以包含任何值。因为每个端点必须用自己初始化的握手和对端初始化的握手来区分身份，所以这个数据应有充分的随机性。但是并不需要加密安全的随机值，或者动态值。 C2 和 S2 消息格式：C2和S2消息有1536字节长，只是S1和C1的回复。 时间：4字节：本字段必须包含对等段发送的时间（对C2来说是S1，对S2来说是C1）。 时间2：4字节：本字段必须包含先前发送的并被对端读取的包的时间戳。 随机回复：1528字节：本字段必须包含对端发送的随机数据字段（对C2来说是S1，对S2来说是C1）。每个对等端可以用时间和时间2字段中的时间戳来快速地估计带宽和延迟。但这样做可能并不实用。 上面代码的功能简单说：发送C0+C1，解析S0、S1。 代码片段分析6 #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"Server signature:\"); RTMP_LogHex(RTMP_LOGDEBUG, serversig, RTMP_SIG_SIZE); #endif RTMP_Log(RTMP_LOGDEBUG, \"CCQ: Server signature:\"); RTMP_LogHexString(RTMP_LOGDEBUG, serversig, RTMP_SIG_SIZE); if (FP9HandShake) {} else { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s reply = serversig\", __FUNCTION__); //直接赋值 reply = serversig; #if 0 uptime = htonl(RTMP_GetTime()); memcpy(reply+4, &uptime, 4); #endif } 代码片段分析7 #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"%s: Sending handshake response: \", __FUNCTION__); RTMP_LogHex(RTMP_LOGDEBUG, reply, RTMP_SIG_SIZE); #endif //把reply中的1536字节数据发送出去 //对应C2 //握手---------------- RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。发送握手数据C2\", __FUNCTION__); if (!WriteN(r, (char *)reply, RTMP_SIG_SIZE)) return FALSE; /* 2nd part of handshake */ //读取1536字节数据到serversig //握手---------------- RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。读取握手数据S2\", __FUNCTION__); if (ReadN(r, (char *)serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE) return FALSE; #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"%s: 2nd handshake: \", __FUNCTION__); RTMP_LogHex(RTMP_LOGDEBUG, serversig, RTMP_SIG_SIZE); #endif RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s: 2nd handshake: \", __FUNCTION__); RTMP_LogHexString(RTMP_LOGDEBUG, serversig, RTMP_SIG_SIZE); if (FP9HandShake)// 加密才执行 {} else { //int memcmp(const void *buf1, const void *buf2, unsigned int count); 当buf1=buf2时，返回值=0 //比较serversig和clientsig是否相等 //握手---------------- RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。比较握手数据签名\", __FUNCTION__); if (memcmp(serversig, clientsig, RTMP_SIG_SIZE) != 0) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。握手数据签名不匹配！\", __FUNCTION__); RTMP_Log(RTMP_LOGWARNING, \"%s: client signature does not match!\", __FUNCTION__); } } 上面代码简单说：发送C2，解析S2，客户端成功解析S2，服务端成功接收C2，第一次连接握手成功。 至此，HandShake的代码分析完毕。 "},"pages/librtmp/librtmp源码之SendConnectPacket.html":{"url":"pages/librtmp/librtmp源码之SendConnectPacket.html","title":"librtmp源码之SendConnectPacket","keywords":"","body":"librtmp源码之SendConnectPacket 代码片段分析1 RTMPPacket packet; // pend：AMF_EncodeNamedString函数参数 // pbuf: packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; char pbuf[4096], *pend = pbuf + sizeof(pbuf); char *enc; if (cp) // 不会走 return RTMP_SendPacket(r, cp, TRUE); 代码片段分析2 // chunk stream id(chunk basic header)字段 // 块流ID，消息头的第1个字节的后6位 packet.m_nChannel = 0x03; /* control channel (invoke) */ // #define RTMP_PACKET_SIZE_LARGE 0 onMetaData流开始的绝对时间戳控制消息（如connect） // #define RTMP_PACKET_SIZE_MEDIUM 1 大部分的rtmp header都是8字节的 // #define RTMP_PACKET_SIZE_SMALL 2 比较少见 // #define RTMP_PACKET_SIZE_MINIMUM 3 偶尔出现，低于8字节频率 // chunk type id (2bit)fmt 对应message head {0,3,7,11} + (6bit)chunk stream id // 块类型ID，消息头的第1个字节的前2位，决定消息头长度 packet.m_headerType = RTMP_PACKET_SIZE_LARGE; // 消息类型为20的用AMF0编码，这些消息用于在远端实现连接，创建流，发布，播放和暂停等操作 // #define RTMP_PACKET_TYPE_INVOKE 0x14 // Message type ID（1-7协议控制；8，9音视频；10以后为AMF编码消息） // 消息类型ID，消息头的第8个字节，0x14表示以AMF0编码。另外还有如0x04表示用户控制消息，0x05表示Window Acknowledgement Size，0x06表示 Set Peer Bandwith等等。 packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; // 时间搓，消息头的第2-4个字节 packet.m_nTimeStamp = 0; // Stream ID通常用以完成某些特定的工作，如使用ID为0的Stream来完成客户端和服务器的连接和控制，使用ID为1的Stream来完成视频流的控制和播放等工作。 // 流ID，消息头的第9-12个字节（末尾最后4个字节） packet.m_nInfoField2 = 0; // 是否含有Extend timeStamp字段 packet.m_hasAbsTimestamp = 0; // #define RTMP_MAX_HEADER_SIZE 18 // 设置chunk body(data)的起始指针，前18个字节用来存储消息头，之后就用来存消息体 packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; 消息头的第5-7个字节是放的是body的长度，后面会设置。 代码片段分析3 // 指针赋值，通过enc来设置消息体的内容 enc = packet.m_body; // 压入connect命令和操作流水号 // connect使用##进行字符串化连接，此处编码connect字符串 enc = AMF_EncodeString(enc, pend, &av_connect); // m_numInvokes：0x14命令远程过程调用计数 enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); // 压入对象 *enc++ = AMF_OBJECT; // 压入对象的“app”字符串，客户端连接到的服务器端应用的名字 enc = AMF_EncodeNamedString(enc, pend, &av_app, &r->Link.app); if (!enc) return FALSE; // 压入“type”，这里是“nonprivate” if (r->Link.protocol & RTMP_FEATURE_WRITE) { enc = AMF_EncodeNamedString(enc, pend, &av_type, &av_nonprivate); if (!enc) return FALSE; } // 压入“flashver”，Flash Player 版本号。和ApplicationScript getversion() 方法返回的是同一个字符串。FMSc/1.0 if (r->Link.flashVer.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_flashVer, &r->Link.flashVer); if (!enc) return FALSE; } // 压入“swfUrl”，进行当前连接的 SWF 文件源地址。file://C:/FlvPlayer.swf if (r->Link.swfUrl.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_swfUrl, &r->Link.swfUrl); if (!enc) return FALSE; } // 压入“tcUrl”，服务器 URL。具有以下格式：protocol://servername:port/appName/appInstance，rtmp://localhost:1935/testapp/instance1 if (r->Link.tcUrl.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_tcUrl, &r->Link.tcUrl); if (!enc) return FALSE; } if (!(r->Link.protocol & RTMP_FEATURE_WRITE)) { // 压入“fpad”，如果使用了代理就是 true。true 或者 false。 enc = AMF_EncodeNamedBoolean(enc, pend, &av_fpad, FALSE); if (!enc) return FALSE; enc = AMF_EncodeNamedNumber(enc, pend, &av_capabilities, 15.0); if (!enc) return FALSE; // 压入“audioCodecs“，表明客户端所支持的音频编码。SUPPORT_SND_MP3 enc = AMF_EncodeNamedNumber(enc, pend, &av_audioCodecs, r->m_fAudioCodecs); if (!enc) return FALSE; // 压入“videoCodecs”，表明支持的视频编码。SUPPORT_VID_SORENSON enc = AMF_EncodeNamedNumber(enc, pend, &av_videoCodecs, r->m_fVideoCodecs); if (!enc) return FALSE; // 压入“videoFunction”，表明所支持的特殊视频方法。SUPPORT_VID_CLIENT_SEEK enc = AMF_EncodeNamedNumber(enc, pend, &av_videoFunction, 1.0); if (!enc) return FALSE; if (r->Link.pageUrl.av_len) { // 压入“pageUrl“，SWF 文件所加载的网页 URL。http://somehost/sample.html enc = AMF_EncodeNamedString(enc, pend, &av_pageUrl, &r->Link.pageUrl); if (!enc) return FALSE; } } if (r->m_fEncoding != 0.0 || r->m_bSendEncoding) { /* AMF0, AMF3 not fully supported yet */ enc = AMF_EncodeNamedNumber(enc, pend, &av_objectEncoding, r->m_fEncoding); if (!enc) return FALSE; } // 判断是否溢出 if (enc + 3 >= pend) return FALSE; // 压入属性结束标记 *enc++ = 0; *enc++ = 0; /* end of object - 0x00 0x00 0x09 */ *enc++ = AMF_OBJECT_END; /* add auth string */ if (r->Link.auth.av_len) { enc = AMF_EncodeBoolean(enc, pend, r->Link.lFlags & RTMP_LF_AUTH); if (!enc) return FALSE; enc = AMF_EncodeString(enc, pend, &r->Link.auth); if (!enc) return FALSE; } if (r->Link.extras.o_num) { int i; for (i = 0; i Link.extras.o_num; i++) { enc = AMFProp_Encode(&r->Link.extras.o_props[i], enc, pend); if (!enc) return FALSE; } } 代码片段分析4 if (!enc) return FALSE; if (r->Link.protocol & RTMP_FEATURE_WRITE) { enc = AMF_EncodeNamedString(enc, pend, &av_type, &av_nonprivate); if (!enc) return FALSE; } if (r->Link.flashVer.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_flashVer, &r->Link.flashVer); if (!enc) return FALSE; } if (r->Link.swfUrl.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_swfUrl, &r->Link.swfUrl); if (!enc) return FALSE; } if (r->Link.tcUrl.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_tcUrl, &r->Link.tcUrl); if (!enc) return FALSE; } if (!(r->Link.protocol & RTMP_FEATURE_WRITE)) { enc = AMF_EncodeNamedBoolean(enc, pend, &av_fpad, FALSE); if (!enc) return FALSE; enc = AMF_EncodeNamedNumber(enc, pend, &av_capabilities, 15.0); if (!enc) return FALSE; enc = AMF_EncodeNamedNumber(enc, pend, &av_audioCodecs, r->m_fAudioCodecs); if (!enc) return FALSE; enc = AMF_EncodeNamedNumber(enc, pend, &av_videoCodecs, r->m_fVideoCodecs); if (!enc) return FALSE; enc = AMF_EncodeNamedNumber(enc, pend, &av_videoFunction, 1.0); if (!enc) return FALSE; if (r->Link.pageUrl.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_pageUrl, &r->Link.pageUrl); if (!enc) return FALSE; } } if (r->m_fEncoding != 0.0 || r->m_bSendEncoding) { /* AMF0, AMF3 not fully supported yet */ enc = AMF_EncodeNamedNumber(enc, pend, &av_objectEncoding, r->m_fEncoding); if (!enc) return FALSE; } if (enc + 3 >= pend) return FALSE; // 压入属性结束标记 *enc++ = 0; *enc++ = 0; /* end of object - 0x00 0x00 0x09 */ *enc++ = AMF_OBJECT_END; 代码片段分析5 /* add auth string */ if (r->Link.auth.av_len) { enc = AMF_EncodeBoolean(enc, pend, r->Link.lFlags & RTMP_LF_AUTH); if (!enc) return FALSE; enc = AMF_EncodeString(enc, pend, &r->Link.auth); if (!enc) return FALSE; } if (r->Link.extras.o_num) { int i; for (i = 0; i Link.extras.o_num; i++) { enc = AMFProp_Encode(&r->Link.extras.o_props[i], enc, pend); if (!enc) return FALSE; } } // 经过AMF编码组包后，message的大小 (如果是音视频数据 即FLV格式一个Tag中Tag Data 大小) // 消息体长度，消息头的第5-7个字节 packet.m_nBodySize = enc - packet.m_body; // 发送报文，并记入应答队列 return RTMP_SendPacket(r, &packet, TRUE); 报文数据 DEBUG2: 0000: 03 00 00 00 00 00 55 14 00 00 00 00 ......U..... DEBUG2: 0000: 02 00 07 63 6f 6e 6e 65 63 74 00 3f f0 00 00 00 ...connect.?.... DEBUG2: 0010: 00 00 00 03 00 03 61 70 70 02 00 04 6c 69 76 65 ......app...live DEBUG2: 0020: 00 04 74 79 70 65 02 00 0a 6e 6f 6e 70 72 69 76 ..type...nonpriv DEBUG2: 0030: 61 74 65 00 05 74 63 55 72 6c 02 00 15 72 74 6d ate..tcUrl...rtm DEBUG2: 0040: 70 3a 2f 2f 31 32 32 31 2e 73 69 74 65 2f 6c 69 p://1221.site/li DEBUG2: 0050: 76 65 00 00 09 ve... 参考： RTMP 协议规范(中文版) 流媒体-RTMP协议-librtmp库学习（二） librtmp源码分析之核心实现解读 rtmp源码分析之RTMP_Write 和 RTMP_SendPacket ASCII对照表 RTMP协议详解及实例分析 手撕rtmp协议细节（2）——rtmp Header "},"pages/librtmp/librtmp源码之RTMP_SendPacket.html":{"url":"pages/librtmp/librtmp源码之RTMP_SendPacket.html","title":"librtmp源码之RTMP_SendPacket","keywords":"","body":"librtmp源码之RTMP_SendPacket 代码片段分析1 // 取出对应块流ID上一次发送的报文 const RTMPPacket *prevPacket; // 上一次相对时间戳 uint32_t last = 0; // 表示块头初始大小 int nSize; // hSize表示块头大小 // 块基本头是1-3字节，因此用变量cSize来表示剩下的0-2字节 int hSize, cSize; // header头指针指向头部，hend块尾指针指向body头部 // hbuf表示头部最大18（3字节最大块基本头+11字节最大快消息头+4字节扩展时间戳）缓冲数组 // hptr指向header // c = packet->m_headerType m_nTimeStamp - last; // 时间戳增量 uint32_t t; // buffer 表示 data 的指针，tbuf 块初始指针，toff 块指针 char *buffer, *tbuf = NULL, *toff = NULL; // Chunk大小，默认是128字节 int nChunkSize; // tlen ? int tlen; // ? if (packet->m_nChannel >= r->m_channelsAllocatedOut) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if (packet->m_nChannel >= r->m_channelsAllocatedOut)\", __FUNCTION__); int n = packet->m_nChannel + 10; RTMPPacket **packets = realloc(r->m_vecChannelsOut, sizeof(RTMPPacket*) * n); if (!packets) { free(r->m_vecChannelsOut); r->m_vecChannelsOut = NULL; r->m_channelsAllocatedOut = 0; return FALSE; } r->m_vecChannelsOut = packets; memset(r->m_vecChannelsOut + r->m_channelsAllocatedOut, 0, sizeof(RTMPPacket*) * (n - r->m_channelsAllocatedOut)); r->m_channelsAllocatedOut = n; } 代码片段分析2 // 获取该通道，上一次的数据 prevPacket = r->m_vecChannelsOut[packet->m_nChannel]; // 尝试对非LARGE报文进行字段压缩 // 前一个packet存在且不是完整的ChunkMsgHeader，因此有可能需要调整块消息头的类型 // fmt字节 /* case 0:chunk msg header 长度为11 * case 1:chunk msg header 长度为7 * case 2:chunk msg header 长度为3 * case 3:chunk msg header 长度为0 */ if (prevPacket && packet->m_headerType != RTMP_PACKET_SIZE_LARGE) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if (prevPacket && packet->m_headerType != RTMP_PACKET_SIZE_LARGE)\", __FUNCTION__); /* compress a bit by using the prev packet's attributes */ // 获取ChunkMsgHeader类型，前一个Chunk与当前Chunk比较 // 如果前后两个块的大小、包类型及块头类型都相同，则将块头类型fmt设为2， // 即可省略消息长度、消息类型id、消息流id // 可以参考官方协议：流的分块 --- 6.1.2.3节 if (prevPacket->m_nBodySize == packet->m_nBodySize && prevPacket->m_packetType == packet->m_packetType && packet->m_headerType == RTMP_PACKET_SIZE_MEDIUM) packet->m_headerType = RTMP_PACKET_SIZE_SMALL; // 前后两个块的时间戳相同，且块头类型fmt为2，则相应的时间戳也可省略，因此将块头类型置为3 // 可以参考官方协议：流的分块 --- 6.1.2.4节 if (prevPacket->m_nTimeStamp == packet->m_nTimeStamp && packet->m_headerType == RTMP_PACKET_SIZE_SMALL) packet->m_headerType = RTMP_PACKET_SIZE_MINIMUM; // 上一次相对时间戳 last = prevPacket->m_nTimeStamp; } // 块头类型fmt取值0、1、2、3，超过3就表示出错(fmt占二个字节) if (packet->m_headerType > 3) /* sanity */ { RTMP_Log(RTMP_LOGERROR, \"sanity failed!! trying to send header of type: 0x%02x.\", (unsigned char)packet->m_headerType); return FALSE; } // 块头初始大小 = 基本头(1字节) + 块消息头大小(11/7/3/0) = [12, 8, 4, 1] // 块基本头是1-3字节，因此用变量cSize来表示剩下的0-2字节 // nSize 表示块头初始大小， hSize表示块头大小 nSize = packetSize[packet->m_headerType]; hSize = nSize; cSize = 0; // 时间戳增量 t = packet->m_nTimeStamp - last; 代码片段分析3 if (packet->m_body) { // 块头的首指针 向前平移了 基本头(1字节) + 块消息头大小(11/7/3/0) 字节 header = packet->m_body - nSize; // 块头的尾指针 hend = packet->m_body; } else { header = hbuf + 6; hend = hbuf + sizeof(hbuf); } // 计算基本头的扩充大小 // 块基本头是1-3字节，因此用变量cSize来表示剩下的0-2字节 if (packet->m_nChannel > 319) // 块流id(cs id)大于319，则块基本头占3个字节 cSize = 2; else if (packet->m_nChannel > 63) // 块流id(cs id)在64与319之间，则块基本头占2个字节 cSize = 1; if (cSize) { // 向前平移了 块基本头是1-3字节，因此用变量cSize来表示剩下的0-2字节 个字节 header -= cSize; hSize += cSize; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s cSize:%d\", __FUNCTION__, cSize); } // 根据时间戳计算是否需要扩充头大小 // 如果 t>0xffffff， 则需要使用 extended timestamp if (t >= 0xffffff) { // 向前平移了 extended timestamp header -= 4; hSize += 4; RTMP_Log(RTMP_LOGWARNING, \"Larger timestamp than 24-bit: 0x%x\", t); } // 确定好 Header 的位置后，就可以开始赋值了 // 向缓冲区压入基本头 hptr = header; // 把ChunkBasicHeader的Fmt类型左移6位 // cSize 表示块基本头剩下的0-2字节 // 设置basic header的第一个字节值，前两位为fmt. 可以参考官方协议：流的分块 --- 6.1.1节 c = packet->m_headerType m_nChannel;// chunk stream ID RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 把ChunkBasicHeader的低6位设置成ChunkStreamID\", __FUNCTION__); break; case 1:// 同理，但低6位设置成000000 break; case 2:// 同理，但低6位设置成000001 c |= 1; break; } // 可以拆分成两句*hptr=c; hptr++，此时hptr指向第2个字节 *hptr++ = c; // 设置basic header的第二(三)个字节值 if (cSize) { // 将要放到第2字节的内容tmp int tmp = packet->m_nChannel - 64; // 获取低位存储与第2字节 *hptr++ = tmp & 0xff; // ChunkBasicHeader是最大的3字节时,获取高位存储于最后1个字节（注意：排序使用大端序列，和主机相反） if (cSize == 2) *hptr++ = tmp >> 8; } // nSize 块头初始大小 = 基本头(1字节) + 块消息头大小(11/7/3/0) = [12, 8, 4, 1] // 向缓冲区压入时间戳 if (nSize > 1) { // 写入 timestamp 或 timestamp delta hptr = AMF_EncodeInt24(hptr, hend, t > 0xffffff ? 0xffffff : t); } // 向缓冲区压入负载大小和报文类型 if (nSize > 4) { // 写入 message length hptr = AMF_EncodeInt24(hptr, hend, packet->m_nBodySize); // 写入 message type id *hptr++ = packet->m_packetType; } // 向缓冲区压入流ID if (nSize > 8) // 写入 message stream id // 还原Chunk为Message的时候都是根据这个ID来辨识是否是同一个消息的Chunk的 hptr += EncodeInt32LE(hptr, packet->m_nInfoField2); // 向缓冲区压入扩展时间戳 if (t >= 0xffffff) // 写入 extended timestamp hptr = AMF_EncodeInt32(hptr, hend, t); // 到此为止，已经将块头填写好了 // 此时nSize表示负载数据的长度, buffer是指向负载数据区的指针 nSize = packet->m_nBodySize; buffer = packet->m_body; // Chunk大小，默认是128字节 nChunkSize = r->m_outChunkSize; RTMP_Log(RTMP_LOGDEBUG2, \"%s: fd=%d, size=%d\", __FUNCTION__, r->m_sb.sb_socket, nSize); 代码片段分析4 /* send all chunks in one HTTP request */ if (r->Link.protocol & RTMP_FEATURE_HTTP) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if (r->Link.protocol & RTMP_FEATURE_HTTP)\", __FUNCTION__); // nSize:Message负载长度；nChunkSize：Chunk长度； // 例nSize：307，nChunkSize:128； // 可分为（307 + 128 - 1）/128 = 3个 // 为什么加 nChunkSize - 1？因为除法会只取整数部分！ int chunks = (nSize+nChunkSize-1) / nChunkSize; if (chunks > 1) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s chunks > 1，chunks: %d\", __FUNCTION__, chunks); // 注意：ChunkBasicHeader的长度 = cSize + 1 // 消息分n块后总的开销： // n个ChunkBasicHeader，1个ChunkMsgHeader，1个Message负载 // 实际上只有第一个Chunk是完整的，剩下的只有ChunkBasicHeader tlen = chunks * (cSize + 1) + nSize + hSize; tbuf = malloc(tlen); if (!tbuf) return FALSE; toff = tbuf; } } // 消息的负载 + 头 while (nSize + hSize) { int wrote; // 消息负载大小 Link.protocol采用Http协议，则将RTMP包数据封装成多个Chunk，然后一次性发送。 // 否则每封装成一个块，就立即发送出去 if (tbuf) { // 将从Chunk头开始的nChunkSize + hSize个字节拷贝至toff中， // 这些拷贝的数据包括块头数据(hSize字节)和nChunkSize个负载数据 memcpy(toff, header, nChunkSize + hSize); toff += nChunkSize + hSize; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if (tbuf)\", __FUNCTION__); } // 负载数据长度不超过设定的块大小，不需要分块，因此tbuf为NULL；或者r->Link.protocol不采用Http else { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if !(tbuf)\", __FUNCTION__); // 直接将负载数据和块头数据发送出去 wrote = WriteN(r, header, nChunkSize + hSize); if (!wrote) return FALSE; } // 消息负载长度 - Chunk负载长度 nSize -= nChunkSize; // buffer指针前移1个Chunk负载长度 buffer += nChunkSize; // 重置块头大小为0，后续的块只需要有基本头(或加上扩展时间戳)即可 hSize = 0; // 如果消息负载数据还没有发完，准备填充下一个块的块头数据 // 若只有部分负载发送成功，则需继续构造块再次发送 if (nSize > 0) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 继续构造块再次发送\", __FUNCTION__); // 只需要构造3号类型的块头 header = buffer - 1; // basic header 字节 hSize = 1; if (cSize) { header -= cSize; hSize += cSize; } if (t >= 0xffffff) { header -= 4; hSize += 4; } // c 为 basic header 第一个字节 *header = (0xc0 | c); if (cSize) { int tmp = packet->m_nChannel - 64; header[1] = tmp & 0xff; if (cSize == 2) header[2] = tmp >> 8; } if (t >= 0xffffff) { char* extendedTimestamp = header + 1 + cSize; AMF_EncodeInt32(extendedTimestamp, extendedTimestamp + 4, t); } } } 代码片段分析5 if (tbuf) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if (tbuf)， write tbuf\", __FUNCTION__); int wrote = WriteN(r, tbuf, toff-tbuf); free(tbuf); tbuf = NULL; if (!wrote) return FALSE; } /* we invoked a remote method */ // 如果是0x14远程调用，则需要解出调用名称，加入等待响应的队列中 if (packet->m_packetType == RTMP_PACKET_TYPE_INVOKE) { AVal method; char *ptr; ptr = packet->m_body + 1; AMF_DecodeString(ptr, &method); RTMP_Log(RTMP_LOGDEBUG, \"Invoking %s\", method.av_val); /* keep it in call queue till result arrives */ if (queue) { int txn; ptr += 3 + method.av_len; txn = (int)AMF_DecodeNumber(ptr); AV_queue(&r->m_methodCalls, &r->m_numCalls, &method, txn); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 如果是0x14远程调用，则需要解出调用名称，加入等待响应的队列中\", __FUNCTION__); } } // 记录这个块流ID刚刚发送的报文，但是应忽略负载 if (!r->m_vecChannelsOut[packet->m_nChannel]) r->m_vecChannelsOut[packet->m_nChannel] = malloc(sizeof(RTMPPacket)); memcpy(r->m_vecChannelsOut[packet->m_nChannel], packet, sizeof(RTMPPacket)); return TRUE; 报文数据 DEBUG2: 0000: 03 00 00 00 00 00 55 14 00 00 00 00 ......U..... DEBUG2: 0000: 02 00 07 63 6f 6e 6e 65 63 74 00 3f f0 00 00 00 ...connect.?.... DEBUG2: 0010: 00 00 00 03 00 03 61 70 70 02 00 04 6c 69 76 65 ......app...live DEBUG2: 0020: 00 04 74 79 70 65 02 00 0a 6e 6f 6e 70 72 69 76 ..type...nonpriv DEBUG2: 0030: 61 74 65 00 05 74 63 55 72 6c 02 00 15 72 74 6d ate..tcUrl...rtm DEBUG2: 0040: 70 3a 2f 2f 31 32 32 31 2e 73 69 74 65 2f 6c 69 p://1221.site/li DEBUG2: 0050: 76 65 00 00 09 ve... 参考： librtmp源码分析之核心实现解读 rtmp源码分析之RTMP_Write 和 RTMP_SendPacket librtmp协议分析---RTMP_SendPacket函数 librtmp 源码分析笔记 RTMP_SendPacket "},"pages/librtmp/librtmp源码之WriteN.html":{"url":"pages/librtmp/librtmp源码之WriteN.html","title":"librtmp源码之WriteN","keywords":"","body":"librtmp源码之WriteN // 发送数据报的时候调用（连接，buffer，长度） static int WriteN(RTMP *r, const char *buffer, int n) { const char *ptr = buffer; #ifdef CRYPTO char *encrypted = 0; char buf[RTMP_BUFFER_CACHE_SIZE]; if (r->Link.rc4keyOut) { if (n > sizeof(buf)) encrypted = (char *)malloc(n); else encrypted = (char *)buf; ptr = encrypted; RC4_encrypt2(r->Link.rc4keyOut, n, buffer, ptr); } #endif while (n > 0) { int nBytes; // 因方式的不同而调用不同函数 // 如果使用的是HTTP协议进行连接 if (r->Link.protocol & RTMP_FEATURE_HTTP) nBytes = HTTP_Post(r, RTMPT_SEND, ptr, n); else nBytes = RTMPSockBuf_Send(&r->m_sb, ptr, n); /*RTMP_Log(RTMP_LOGDEBUG, \"%s: %d\\n\", __FUNCTION__, nBytes); */ // 成功发送字节数sb_ssl) { rc = TLS_write(sb->sb_ssl, buf, len); } else #endif { // 向一个已连接的套接口发送数据。 // int send( SOCKET s, const char * buf, int len, int flags); // s：一个用于标识已连接套接口的描述字。 // buf：包含待发送数据的缓冲区。 // len：缓冲区中数据的长度。 // flags：调用执行方式。 // rc:所发数据量。 rc = send(sb->sb_socket, buf, len, 0); } return rc; } 参考： RTMPdump（libRTMP） 源代码分析 8： 发送消息（Message） "},"pages/librtmp/librtmp源码之RTMP_ConnectStream.html":{"url":"pages/librtmp/librtmp源码之RTMP_ConnectStream.html","title":"librtmp源码之RTMP_ConnectStream","keywords":"","body":"librtmp源码之RTMP_ConnectStream // 建立流（NetStream） int RTMP_ConnectStream(RTMP *r, int seekTime) { RTMPPacket packet = { 0 }; /* seekTime was already set by SetupStream / SetupURL. * This is only needed by ReconnectStream. */ if (seekTime > 0) r->Link.seekTime = seekTime; // 当前连接媒体使用的块流ID r->m_mediaChannel = 0; // 接收到的实际上是块(Chunk)，而不是消息(Message)，因为消息在网上传输的时候要分割成块. while (!r->m_bPlaying && RTMP_IsConnected(r) && RTMP_ReadPacket(r, &packet)) { // 一个消息可能被封装成多个块(Chunk)，只有当所有块读取完才处理这个消息包 if (RTMPPacket_IsReady(&packet)) { if (!packet.m_nBodySize) continue; // 读取到flv数据包，则继续读取下一个包 if ((packet.m_packetType == RTMP_PACKET_TYPE_AUDIO) || (packet.m_packetType == RTMP_PACKET_TYPE_VIDEO) || (packet.m_packetType == RTMP_PACKET_TYPE_INFO)) { RTMP_Log(RTMP_LOGWARNING, \"Received FLV packet before play()! Ignoring.\"); RTMPPacket_Free(&packet); continue; } // 处理收到的数据包 RTMP_ClientPacket(r, &packet); // 处理完毕，清除数据 RTMPPacket_Free(&packet); } } // 当前是否推流或连接中 return r->m_bPlaying; } 简单的一个逻辑判断，重点在while循环里。首先，必须要满足三个条件。其次，进入循环以后只有出错或者建立流（NetStream）完成后，才能退出循环。 在RTMP_ConnectStream()处理交互准备的过程中，有两个重要函数：RTMP_ReadPacket()负责接收报文，RTMP_ClientPacket()负责逻辑的分派处理。 参考： RTMPdump（libRTMP） 源代码分析 6： 建立一个流媒体连接 （NetStream部分 1） RTMP推流及协议学习 librtmp源码分析之核心实现解读 【原】librtmp源码详解 "},"pages/librtmp/librtmp源码之RTMP_ReadPacket.html":{"url":"pages/librtmp/librtmp源码之RTMP_ReadPacket.html","title":"librtmp源码之RTMP_ReadPacket","keywords":"","body":"librtmp源码之RTMP_ReadPacket /** * @brief 读取接收到的消息块(Chunk)，存放在packet中. 对接收到的消息不做任何处理。 块的格式为： * * | basic header(1-3字节）| chunk msg header(0/3/7/11字节) | Extended Timestamp(0/4字节) | chunk data | * * 其中 basic header还可以分解为：| fmt(2位) | cs id (3 m_sb.sb_socket); // 收下来的数据存入hbuf if (ReadN(r, (char *)hbuf, 1) == 0) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read RTMP packet header\", __FUNCTION__); return FALSE; } // 块类型fmt packet->m_headerType = (hbuf[0] & 0xc0) >> 6; // 块流ID（2-63） packet->m_nChannel = (hbuf[0] & 0x3f); header++; // 块流ID第一个字节为0，表示块流ID占2个字节，表示ID的范围是64-319（第二个字节 + 64） if (packet->m_nChannel == 0) { // 读取接下来的1个字节存放在hbuf[1]中 if (ReadN(r, (char *)&hbuf[1], 1) != 1) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read RTMP packet header 2nd byte\", __FUNCTION__); return FALSE; } // 块流ID = 第二个字节 + 64 = hbuf[1] + 64 packet->m_nChannel = hbuf[1]; packet->m_nChannel += 64; header++; } // 块流ID第一个字节为1，表示块流ID占3个字节，表示ID范围是64 -- 65599（第三个字节*256 + 第二个字节 + 64） else if (packet->m_nChannel == 1) { int tmp; // 读取2个字节存放在hbuf[1]和hbuf[2]中 if (ReadN(r, (char *)&hbuf[1], 2) != 2) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read RTMP packet header 3nd byte\", __FUNCTION__); return FALSE; } // 块流ID = 第三个字节*256 + 第二个字节 + 64 tmp = (hbuf[2] m_nChannel = tmp + 64; RTMP_Log(RTMP_LOGDEBUG, \"%s, m_nChannel: %0x\", __FUNCTION__, packet->m_nChannel); header += 2; } // 块消息头(ChunkMsgHeader)有四种类型，大小分别为11、7、3、0,每个值加1 就得到该数组的值 // 块头 = BasicHeader(1-3字节) + ChunkMsgHeader + ExtendTimestamp(0或4字节) nSize = packetSize[packet->m_headerType]; if (packet->m_nChannel >= r->m_channelsAllocatedIn) { int n = packet->m_nChannel + 10; int *timestamp = realloc(r->m_channelTimestamp, sizeof(int) * n); RTMPPacket **packets = realloc(r->m_vecChannelsIn, sizeof(RTMPPacket*) * n); if (!timestamp) free(r->m_channelTimestamp); if (!packets) free(r->m_vecChannelsIn); r->m_channelTimestamp = timestamp; r->m_vecChannelsIn = packets; if (!timestamp || !packets) { r->m_channelsAllocatedIn = 0; return FALSE; } memset(r->m_channelTimestamp + r->m_channelsAllocatedIn, 0, sizeof(int) * (n - r->m_channelsAllocatedIn)); memset(r->m_vecChannelsIn + r->m_channelsAllocatedIn, 0, sizeof(RTMPPacket*) * (n - r->m_channelsAllocatedIn)); r->m_channelsAllocatedIn = n; } // 块类型fmt为0的块，在一个块流的开始和时间戳返回的时候必须有这种块 // 块类型fmt为1、2、3的块使用与先前块相同的数据 // 关于块类型的定义，可参考官方协议：流的分块 --- 6.1.2节 // 如果是标准大头，设置时间戳为绝对的 if (nSize == RTMP_LARGE_HEADER_SIZE) /* if we get a full header the timestamp is absolute */ // 11个字节的完整ChunkMsgHeader的TimeStamp是绝对时间戳 packet->m_hasAbsTimestamp = TRUE; // 如果非标准大头，首次尝试拷贝上一次的报头 else if (nSize m_vecChannelsIn[packet->m_nChannel]) memcpy(packet, r->m_vecChannelsIn[packet->m_nChannel], sizeof(RTMPPacket)); } // 真实的ChunkMsgHeader的大小，此处减1是因为前面获取包类型的时候多加了1 nSize--; // 读取nSize个字节存入header if (nSize > 0 && ReadN(r, header, nSize) != nSize) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read RTMP packet header. type: %x\", __FUNCTION__, (unsigned int)hbuf[0]); return FALSE; } // 目前已经读取的字节数 = chunk msg header + basic header // 计算基本块头+消息块头的大小 hSize = nSize + (header - (char *)hbuf); // chunk msg header为11、7、3字节，fmt类型值为0、1、2 if (nSize >= 3) { // TimeStamp(注意 BigEndian to SmallEndian)(11，7，3字节首部都有) // 首部前3个字节为timestamp packet->m_nTimeStamp = AMF_DecodeInt24(header); /*RTMP_Log(RTMP_LOGDEBUG, \"%s, reading RTMP packet chunk on channel %x, headersz %i, timestamp %i, abs timestamp %i\", __FUNCTION__, packet.m_nChannel, nSize, packet.m_nTimeStamp, packet.m_hasAbsTimestamp); */ // chunk msg header为11或7字节，fmt类型值为0或1 // 消息长度(11，7字节首部都有) if (nSize >= 6) { // 解析负载长度 packet->m_nBodySize = AMF_DecodeInt24(header + 3); packet->m_nBytesRead = 0; //(11，7字节首部都有) if (nSize > 6) { // Msg type ID // 解析包类型 packet->m_packetType = header[6]; // Msg Stream ID // 解析流ID if (nSize == 11) // msg stream id，小端字节序 packet->m_nInfoField2 = DecodeInt32LE(header + 7); } } } // 读取扩展时间戳并解析 // Extend Tiemstamp,占4个字节 extendedTimestamp = packet->m_nTimeStamp == 0xffffff; if (extendedTimestamp) { if (ReadN(r, header + nSize, 4) != 4) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read extended timestamp\", __FUNCTION__); return FALSE; } packet->m_nTimeStamp = AMF_DecodeInt32(header + nSize); hSize += 4; } RTMP_LogHexString(RTMP_LOGDEBUG2, (uint8_t *)hbuf, hSize); // 负载非0，需要分配内存，或第一个分块的初使化工作 if (packet->m_nBodySize > 0 && packet->m_body == NULL) { if (!RTMPPacket_Alloc(packet, packet->m_nBodySize)) { RTMP_Log(RTMP_LOGDEBUG, \"%s, failed to allocate packet\", __FUNCTION__); return FALSE; } didAlloc = TRUE; packet->m_headerType = (hbuf[0] & 0xc0) >> 6; } // 剩下的消息数据长度如果比块尺寸大，则需要分块,否则块尺寸就等于剩下的消息数据长度 // 准备读取的数据和块大小 nToRead = packet->m_nBodySize - packet->m_nBytesRead; nChunk = r->m_inChunkSize; if (nToRead m_chunk) { // 块头大小 packet->m_chunk->c_headerSize = hSize; // 填充块头数据 memcpy(packet->m_chunk->c_header, hbuf, hSize); // 块消息数据缓冲区指针 packet->m_chunk->c_chunk = packet->m_body + packet->m_nBytesRead; // 块大小 packet->m_chunk->c_chunkSize = nChunk; } // 读取负载到缓冲区中 // 读取一个块大小的数据存入块消息数据缓冲区 if (ReadN(r, packet->m_body + packet->m_nBytesRead, nChunk) != nChunk) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read RTMP packet body. len: %u\", __FUNCTION__, packet->m_nBodySize); return FALSE; } RTMP_LogHexString(RTMP_LOGDEBUG2, (uint8_t *)packet->m_body + packet->m_nBytesRead, nChunk); // 更新已读数据字节个数 packet->m_nBytesRead += nChunk; /* keep the packet as ref for other packets on this channel */ // 将这个包作为通道中其他包的参考 // 保存当前块流ID最新的报文，与RTMP_SendPacket()不同的是，负载部分也被保存了，以应对不完整的分块报文 if (!r->m_vecChannelsIn[packet->m_nChannel]) r->m_vecChannelsIn[packet->m_nChannel] = malloc(sizeof(RTMPPacket)); memcpy(r->m_vecChannelsIn[packet->m_nChannel], packet, sizeof(RTMPPacket)); // 设置扩展时间戳 if (extendedTimestamp) { r->m_vecChannelsIn[packet->m_nChannel]->m_nTimeStamp = 0xffffff; } // 若报文负载接收完整 if (RTMPPacket_IsReady(packet)) { /* make packet's timestamp absolute */ // 处理增量时间戳 // 绝对时间戳 = 上一次绝对时间戳 + 时间戳增量 if (!packet->m_hasAbsTimestamp) packet->m_nTimeStamp += r->m_channelTimestamp[packet->m_nChannel]; /* timestamps seem to be always relative!! */ // 保存当前块流ID的时间戳 // 当前绝对时间戳保存起来，供下一个包转换时间戳使用 r->m_channelTimestamp[packet->m_nChannel] = packet->m_nTimeStamp; /* reset the data from the stored packet. we keep the header since we may use it later if a new packet for this channel */ /* arrives and requests to re-use some info (small packet header) */ // 清理上下文中当前块流ID最新的报文的负载信息 // 重置保存的包。保留块头数据，因为通道中新到来的包(更短的块头)可能需要使用前面块头的信息. r->m_vecChannelsIn[packet->m_nChannel]->m_body = NULL; r->m_vecChannelsIn[packet->m_nChannel]->m_nBytesRead = 0; r->m_vecChannelsIn[packet->m_nChannel]->m_hasAbsTimestamp = FALSE; /* can only be false if we reuse header */ } else { packet->m_body = NULL; /* so it won't be erased on free */ } return TRUE; } 参考： RTMPdump（libRTMP） 源代码分析 6： 建立一个流媒体连接 （NetStream部分 1） RTMP推流及协议学习 librtmp源码分析之核心实现解读 【原】librtmp源码详解 "},"pages/librtmp/librtmp源码之RTMP_ClientPacket.html":{"url":"pages/librtmp/librtmp源码之RTMP_ClientPacket.html","title":"librtmp源码之RTMP_ClientPacket","keywords":"","body":"librtmp源码之RTMP_ClientPacket // 处理消息（Message），并做出响应 int RTMP_ClientPacket(RTMP *r, RTMPPacket *packet) { // 返回值为1表示推拉流正在正作中，为2表示已经停止 int bHasMediaPacket = 0; switch (packet->m_packetType) { // RTMP消息类型ID=1,设置块大小 case RTMP_PACKET_TYPE_CHUNK_SIZE: /* chunk size */ RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 处理消息 Set Chunk Size (typeID=1)\", __FUNCTION__); // 更新接收处理时的块限制 HandleChangeChunkSize(r, packet); break; // 对端反馈的已读大小 // RTMP消息类型ID=3 case RTMP_PACKET_TYPE_BYTES_READ_REPORT: /* bytes read report */ RTMP_Log(RTMP_LOGDEBUG, \"%s, received: bytes read report\", __FUNCTION__); break; // RTMP消息类型ID=4，用户控制 // 处理对端发送的控制报文 case RTMP_PACKET_TYPE_CONTROL: /* ctrl */ RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 处理消息 User Control (typeID=4)\", __FUNCTION__); HandleCtrl(r, packet); break; // RTMP消息类型ID=5 case RTMP_PACKET_TYPE_SERVER_BW: /* server bw */ // 处理对端发送的应答窗口大小，这里由服务器发送，即告之客户端收到对应大小的数据后应发送反馈 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 处理消息 Window Acknowledgement Size (typeID=5)\", __FUNCTION__); HandleServerBW(r, packet); break; // RTMP消息类型ID=6 case RTMP_PACKET_TYPE_CLIENT_BW: /* client bw */ // 处理对端发送的设置发送带宽大小，这里由服务器发送，即设置客户端的发送带宽 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 处理消息 Set Peer Bandwidth (typeID=6)\", __FUNCTION__); HandleClientBW(r, packet); break; // RTMP消息类型ID=8，音频数据 case RTMP_PACKET_TYPE_AUDIO: /* audio data */ /*RTMP_Log(RTMP_LOGDEBUG, \"%s, received: audio %lu bytes\", __FUNCTION__, packet.m_nBodySize); */ HandleAudio(r, packet); bHasMediaPacket = 1; if (!r->m_mediaChannel) r->m_mediaChannel = packet->m_nChannel; if (!r->m_pausing) r->m_mediaStamp = packet->m_nTimeStamp; break; // RTMP消息类型ID=9，视频数据 case RTMP_PACKET_TYPE_VIDEO: /* video data */ /*RTMP_Log(RTMP_LOGDEBUG, \"%s, received: video %lu bytes\", __FUNCTION__, packet.m_nBodySize); */ HandleVideo(r, packet); bHasMediaPacket = 1; if (!r->m_mediaChannel) r->m_mediaChannel = packet->m_nChannel; if (!r->m_pausing) r->m_mediaStamp = packet->m_nTimeStamp; break; // RTMP消息类型ID=15，AMF3编码，忽略 case RTMP_PACKET_TYPE_FLEX_STREAM_SEND: /* flex stream send */ RTMP_Log(RTMP_LOGDEBUG, \"%s, flex stream send, size %u bytes, not supported, ignoring\", __FUNCTION__, packet->m_nBodySize); break; // RTMP消息类型ID=16，AMF3编码，忽略 case RTMP_PACKET_TYPE_FLEX_SHARED_OBJECT: /* flex shared object */ RTMP_Log(RTMP_LOGDEBUG, \"%s, flex shared object, size %u bytes, not supported, ignoring\", __FUNCTION__, packet->m_nBodySize); break; // RTMP消息类型ID=17，AMF3编码，忽略 case RTMP_PACKET_TYPE_FLEX_MESSAGE: /* flex message */ { RTMP_Log(RTMP_LOGDEBUG, \"%s, flex message, size %u bytes, not fully supported\", __FUNCTION__, packet->m_nBodySize); /*RTMP_LogHex(packet.m_body, packet.m_nBodySize); */ /* some DEBUG code */ #if 0 RTMP_LIB_AMFObject obj; int nRes = obj.Decode(packet.m_body+1, packet.m_nBodySize-1); if(nRes m_body + 1, packet->m_nBodySize - 1) == 1) bHasMediaPacket = 2; break; } // RTMP消息类型ID=18，AMF0编码，数据消息 case RTMP_PACKET_TYPE_INFO: /* metadata (notify) */ RTMP_Log(RTMP_LOGDEBUG, \"%s, received: notify %u bytes\", __FUNCTION__, packet->m_nBodySize); if (HandleMetadata(r, packet->m_body, packet->m_nBodySize)) bHasMediaPacket = 1; break; // RTMP消息类型ID=19，AMF0编码，忽略 case RTMP_PACKET_TYPE_SHARED_OBJECT: RTMP_Log(RTMP_LOGDEBUG, \"%s, shared object, not supported, ignoring\", __FUNCTION__); break; // RTMP消息类型ID=20，AMF0编码，命令消息 // 处理命令消息！ case RTMP_PACKET_TYPE_INVOKE: /* invoke */ RTMP_Log(RTMP_LOGDEBUG, \"%s, received: invoke %u bytes\", __FUNCTION__, packet->m_nBodySize); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 处理消息 (typeID=20，AMF0编码)\", __FUNCTION__); /*RTMP_LogHex(packet.m_body, packet.m_nBodySize); */ if (HandleInvoke(r, packet->m_body, packet->m_nBodySize) == 1) bHasMediaPacket = 2; break; // RTMP消息类型ID=22 case RTMP_PACKET_TYPE_FLASH_VIDEO: { /* go through FLV packets and handle metadata packets */ unsigned int pos = 0; uint32_t nTimeStamp = packet->m_nTimeStamp; while (pos + 11 m_nBodySize) { uint32_t dataSize = AMF_DecodeInt24(packet->m_body + pos + 1); /* size without header (11) and prevTagSize (4) */ if (pos + 11 + dataSize + 4 > packet->m_nBodySize) { RTMP_Log(RTMP_LOGWARNING, \"Stream corrupt?!\"); break; } if (packet->m_body[pos] == 0x12) { HandleMetadata(r, packet->m_body + pos + 11, dataSize); } else if (packet->m_body[pos] == 8 || packet->m_body[pos] == 9) { nTimeStamp = AMF_DecodeInt24(packet->m_body + pos + 4); nTimeStamp |= (packet->m_body[pos + 7] m_pausing) r->m_mediaStamp = nTimeStamp; /* FLV tag(s) */ /*RTMP_Log(RTMP_LOGDEBUG, \"%s, received: FLV tag(s) %lu bytes\", __FUNCTION__, packet.m_nBodySize); */ bHasMediaPacket = 1; break; } default: RTMP_Log(RTMP_LOGDEBUG, \"%s, unknown packet type received: 0x%02x\", __FUNCTION__, packet->m_packetType); #ifdef _DEBUG RTMP_LogHex(RTMP_LOGDEBUG, packet->m_body, packet->m_nBodySize); #endif } // 返回值为1表示推拉流正在正作中，为2表示已经停止 return bHasMediaPacket; } 参考： RTMPdump（libRTMP） 源代码分析 7： 建立一个流媒体连接 （NetStream部分 2） librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp源码之ReadN.html":{"url":"pages/librtmp/librtmp源码之ReadN.html","title":"librtmp源码之ReadN","keywords":"","body":"librtmp源码之ReadN // 从HTTP或SOCKET中读取n个数据存放在buffer中. static int ReadN(RTMP *r, char *buffer, int n) { int nOriginalSize = n; int avail; char *ptr; r->m_sb.sb_timedout = FALSE; #ifdef _DEBUG memset(buffer, 0, n); #endif ptr = buffer; while (n > 0) { int nBytes = 0, nRead; if (r->Link.protocol & RTMP_FEATURE_HTTP) { int refill = 0; while (!r->m_resplen) { int ret; if (r->m_sb.sb_size m_unackd) HTTP_Post(r, RTMPT_IDLE, \"\", 1); if (RTMPSockBuf_Fill(&r->m_sb) m_sb.sb_timedout) RTMP_Close(r); return 0; } } if ((ret = HTTP_read(r, 0)) == -1) { RTMP_Log(RTMP_LOGDEBUG, \"%s, No valid HTTP response found\", __FUNCTION__); RTMP_Close(r); return 0; } else if (ret == -2) { refill = 1; } else { refill = 0; } } if (r->m_resplen && !r->m_sb.sb_size) RTMPSockBuf_Fill(&r->m_sb); avail = r->m_sb.sb_size; if (avail > r->m_resplen) avail = r->m_resplen; } else { avail = r->m_sb.sb_size; if (avail == 0) { if (RTMPSockBuf_Fill(&r->m_sb) m_sb.sb_timedout) RTMP_Close(r); return 0; } avail = r->m_sb.sb_size; } } nRead = ((n 0) { memcpy(ptr, r->m_sb.sb_start, nRead); r->m_sb.sb_start += nRead; r->m_sb.sb_size -= nRead; nBytes = nRead; r->m_nBytesIn += nRead; if (r->m_bSendCounter && r->m_nBytesIn > ( r->m_nBytesInSent + r->m_nClientBW / 10)) if (!SendBytesReceived(r)) return FALSE; } /*RTMP_Log(RTMP_LOGDEBUG, \"%s: %d bytes\\n\", __FUNCTION__, nBytes); */ #ifdef _DEBUG fwrite(ptr, 1, nBytes, netstackdump_read); #endif if (nBytes == 0) { RTMP_Log(RTMP_LOGDEBUG, \"%s, RTMP socket closed by peer\", __FUNCTION__); /*goto again; */ RTMP_Close(r); break; } if (r->Link.protocol & RTMP_FEATURE_HTTP) r->m_resplen -= nBytes; #ifdef CRYPTO if (r->Link.rc4keyIn) { RC4_encrypt(r->Link.rc4keyIn, nBytes, ptr); } #endif n -= nBytes; ptr += nBytes; } return nOriginalSize - n; } // 调用Socket编程中的recv()函数，接收数据 int RTMPSockBuf_Fill(RTMPSockBuf *sb) { int nBytes; if (!sb->sb_size) sb->sb_start = sb->sb_buf; while (1) { // 缓冲区长度：总长-未处理字节-已处理字节 // |-----已处理--------|-----未处理--------|---------缓冲区----------| // sb_buf sb_start sb_size nBytes = sizeof(sb->sb_buf) - 1 - sb->sb_size - (sb->sb_start - sb->sb_buf); #if defined(CRYPTO) && !defined(NO_SSL) if (sb->sb_ssl) { nBytes = TLS_read(sb->sb_ssl, sb->sb_start + sb->sb_size, nBytes); } else #endif { // int recv( SOCKET s, char * buf, int len, int flags); // s ：一个标识已连接套接口的描述字。 // buf ：用于接收数据的缓冲区。 // len ：缓冲区长度。 // flags：指定调用方式。 // 从sb_start（待处理的下一字节） + sb_size（）还未处理的字节开始buffer为空，可以存储 nBytes = recv(sb->sb_socket, sb->sb_start + sb->sb_size, nBytes, 0); } if (nBytes != -1) { // 未处理的字节又多了 sb->sb_size += nBytes; } else { int sockerr = GetSockError(); RTMP_Log(RTMP_LOGDEBUG, \"%s, recv returned %d. GetSockError(): %d (%s)\", __FUNCTION__, nBytes, sockerr, strerror(sockerr)); if (sockerr == EINTR && !RTMP_ctrlC) continue; if (sockerr == EWOULDBLOCK || sockerr == EAGAIN) { sb->sb_timedout = TRUE; nBytes = 0; } } break; } return nBytes; } 参考： RTMP推流及协议学习 RTMPdump（libRTMP） 源代码分析 9： 接收消息（Message）（接收视音频数据） "},"pages/librtmp/librtmp源码之HandleInvoke.html":{"url":"pages/librtmp/librtmp源码之HandleInvoke.html","title":"librtmp源码之HandleInvoke","keywords":"","body":"librtmp源码之HandleInvoke // 处理服务器发来的AMF0编码的命令 /* Returns 0 for OK/Failed/error, 1 for 'Stop or Complete' */ static int HandleInvoke(RTMP *r, const char *body, unsigned int nBodySize){ AMFObject obj; AVal method; double txn; int ret = 0, nRes; // 确保响应报文是0x14的命令字 // 0x02:string if (body[0] != 0x02) /* make sure it is a string method name we start with */ { RTMP_Log(RTMP_LOGWARNING, \"%s, Sanity failed. no string method in invoke packet\", __FUNCTION__); return 0; } // 将各参数以无名称的对象属性方式进行解析 nRes = AMF_Decode(&obj, body, nBodySize, FALSE); if (nRes \", __FUNCTION__, method.av_val); // 接收到服务端返回的一个_result包，所以我们需要找到这个包对应的那条命令，从而处理这条命令的对应事件。 // 比如我们之前发送了个connect给服务端，服务端必然会返回_result，然后我们异步收到result后，会调用 // RTMP_SendServerBW,RTMP_SendCtrl,以及RTMP_SendCreateStream来创建一个stream // 过程名称为_result if (AVMATCH(&method, &av__result)) { AVal methodInvoked = {0}; int i; // 删除请求队列中的流水项 for (i=0; im_numCalls; i++) { // 找到这条指令对应的触发的方法 if (r->m_methodCalls[i].num == (int)txn) { methodInvoked = r->m_methodCalls[i].name; AV_erase(r->m_methodCalls, &r->m_numCalls, i, FALSE); break; } } if (!methodInvoked.av_val) { RTMP_Log(RTMP_LOGDEBUG, \"%s, received result id %f without matching request\", __FUNCTION__, txn); goto leave; } RTMP_Log(RTMP_LOGDEBUG, \"%s, received result for method call \", __FUNCTION__, methodInvoked.av_val); // 找到了连接请求，确认是连接响应 if (AVMATCH(&methodInvoked, &av_connect)) { if (r->Link.token.av_len) { AMFObjectProperty p; if (RTMP_FindFirstMatchingProperty(&obj, &av_secureToken, &p)) { DecodeTEA(&r->Link.token, &p.p_vu.p_aval); SendSecureTokenResponse(r, &p.p_vu.p_aval); } } // 客户端推流 if (r->Link.protocol & RTMP_FEATURE_WRITE) { // 通知服务器释放流通道和清理推流资源 SendReleaseStream(r); SendFCPublish(r); } // 客户端拉流 else { // 设置服务器的应答窗口大小 // 告诉服务端，我们的期望是什么，窗口大小，等 RTMP_SendServerBW(r); RTMP_SendCtrl(r, 3, 0, 300); } // 发送创建流通道请求 // 因为服务端同意了我们的connect，所以这里发送createStream创建一个流 // 创建完成后，会再次进如这个函数从而走到下面的av_createStream分支，从而发送play过去 RTMP_SendCreateStream(r); if (!(r->Link.protocol & RTMP_FEATURE_WRITE)) { /* Authenticate on Justin.tv legacy servers before sending FCSubscribe */ if (r->Link.usherToken.av_len) SendUsherToken(r, &r->Link.usherToken); /* Send the FCSubscribe if live stream or if subscribepath is set */ if (r->Link.subscribepath.av_len) SendFCSubscribe(r, &r->Link.subscribepath); else if (r->Link.lFlags & RTMP_LF_LIVE) SendFCSubscribe(r, &r->Link.playpath); } } // 找到了创建流请求，确认是创建流的响应 else if (AVMATCH(&methodInvoked, &av_createStream)) { // 从响应中取流ID r->m_stream_id = (int)AMFProp_GetNumber(AMF_GetProp(&obj, NULL, 3)); // 客户端推流 if (r->Link.protocol & RTMP_FEATURE_WRITE) { // 发送推流点 // 如果是要发送，那么高尚服务端，我们要发数据 SendPublish(r); } // 客户端拉流 else { // 否则告诉他我们要接受数据 if (r->Link.lFlags & RTMP_LF_PLST) SendPlaylist(r); // 发送play过去 // 发送拉流点 SendPlay(r); // 以及我们的buf大小 // 发送拉流缓冲时长 RTMP_SendCtrl(r, 3, r->m_stream_id, r->m_nBufferMS); } } // 找到了推流和拉流请求，确认是它们的响应 else if (AVMATCH(&methodInvoked, &av_play) || AVMATCH(&methodInvoked, &av_publish)) { // 接收到了play的回复，那么标记为play // 标识已经进入流状态 r->m_bPlaying = TRUE; } free(methodInvoked.av_val); } else if (AVMATCH(&method, &av_onBWDone)) { if (!r->m_nBWCheckCounter) SendCheckBW(r); } else if (AVMATCH(&method, &av_onFCSubscribe)) { /* SendOnFCSubscribe(); */ } else if (AVMATCH(&method, &av_onFCUnsubscribe)) { RTMP_Close(r); ret = 1; } // 过程名称为ping else if (AVMATCH(&method, &av_ping)) { // 发送pong响应 SendPong(r, txn); } else if (AVMATCH(&method, &av__onbwcheck)) { SendCheckBWResult(r, txn); } else if (AVMATCH(&method, &av__onbwdone)) { int i; for (i = 0; i m_numCalls; i++) if (AVMATCH(&r->m_methodCalls[i].name, &av__checkbw)) { AV_erase(r->m_methodCalls, &r->m_numCalls, i, TRUE); break; } } // 过程名称为_error else if (AVMATCH(&method, &av__error)) { #ifdef CRYPTO AVal methodInvoked = {0}; int i; if (r->Link.protocol & RTMP_FEATURE_WRITE) { for (i=0; im_numCalls; i++) { if (r->m_methodCalls[i].num == txn) { methodInvoked = r->m_methodCalls[i].name; AV_erase(r->m_methodCalls, &r->m_numCalls, i, FALSE); break; } } if (!methodInvoked.av_val) { RTMP_Log(RTMP_LOGDEBUG, \"%s, received result id %f without matching request\", __FUNCTION__, txn); goto leave; } RTMP_Log(RTMP_LOGDEBUG, \"%s, received error for method call \", __FUNCTION__, methodInvoked.av_val); if (AVMATCH(&methodInvoked, &av_connect)) { AMFObject obj2; AVal code, level, description; AMFProp_GetObject(AMF_GetProp(&obj, NULL, 3), &obj2); AMFProp_GetString(AMF_GetProp(&obj2, &av_code, -1), &code); AMFProp_GetString(AMF_GetProp(&obj2, &av_level, -1), &level); AMFProp_GetString(AMF_GetProp(&obj2, &av_description, -1), &description); RTMP_Log(RTMP_LOGDEBUG, \"%s, error description: %s\", __FUNCTION__, description.av_val); /* if PublisherAuth returns 1, then reconnect */ if (PublisherAuth(r, &description) == 1) { CloseInternal(r, 1); if (!RTMP_Connect(r, NULL) || !RTMP_ConnectStream(r, 0)) goto leave; } } } else { RTMP_Log(RTMP_LOGERROR, \"rtmp server sent error\"); } free(methodInvoked.av_val); #else RTMP_Log(RTMP_LOGERROR, \"rtmp server sent error\"); #endif } // 过程名称为close else if (AVMATCH(&method, &av_close)) { RTMP_Log(RTMP_LOGERROR, \"rtmp server requested close\"); RTMP_Close(r); } // 过程名称为onStatus else if (AVMATCH(&method, &av_onStatus)) { // 获取返回对象及其主要属性 AMFObject obj2; AVal code, level; AMFProp_GetObject(AMF_GetProp(&obj, NULL, 3), &obj2); AMFProp_GetString(AMF_GetProp(&obj2, &av_code, -1), &code); AMFProp_GetString(AMF_GetProp(&obj2, &av_level, -1), &level); RTMP_Log(RTMP_LOGDEBUG, \"%s, onStatus: %s\", __FUNCTION__, code.av_val); // 出错返回 if (AVMATCH(&code, &av_NetStream_Failed) || AVMATCH(&code, &av_NetStream_Play_Failed) || AVMATCH(&code, &av_NetStream_Play_StreamNotFound) || AVMATCH(&code, &av_NetConnection_Connect_InvalidApp)) { r->m_stream_id = -1; RTMP_Close(r); RTMP_Log(RTMP_LOGERROR, \"Closing connection: %s\", code.av_val); } // 启动拉流成功 else if (AVMATCH(&code, &av_NetStream_Play_Start) || AVMATCH(&code, &av_NetStream_Play_PublishNotify)) { int i; r->m_bPlaying = TRUE; for (i = 0; i m_numCalls; i++) { if (AVMATCH(&r->m_methodCalls[i].name, &av_play)) { AV_erase(r->m_methodCalls, &r->m_numCalls, i, TRUE); break; } } } // 启动推流成功 else if (AVMATCH(&code, &av_NetStream_Publish_Start)) { int i; r->m_bPlaying = TRUE; for (i = 0; i m_numCalls; i++) { if (AVMATCH(&r->m_methodCalls[i].name, &av_publish)) { AV_erase(r->m_methodCalls, &r->m_numCalls, i, TRUE); break; } } } // 通知流完成或结束 /* Return 1 if this is a Play.Complete or Play.Stop */ else if (AVMATCH(&code, &av_NetStream_Play_Complete) || AVMATCH(&code, &av_NetStream_Play_Stop) || AVMATCH(&code, &av_NetStream_Play_UnpublishNotify)) { RTMP_Close(r); ret = 1; } else if (AVMATCH(&code, &av_NetStream_Seek_Notify)) { r->m_read.flags &= ~RTMP_READ_SEEKING; } // 通知流暂停 else if (AVMATCH(&code, &av_NetStream_Pause_Notify)) { if (r->m_pausing == 1 || r->m_pausing == 2) { RTMP_SendPause(r, FALSE, r->m_pauseStamp); r->m_pausing = 3; } } } else if (AVMATCH(&method, &av_playlist_ready)) { int i; for (i = 0; i m_numCalls; i++) { if (AVMATCH(&r->m_methodCalls[i].name, &av_set_playlist)) { AV_erase(r->m_methodCalls, &r->m_numCalls, i, TRUE); break; } } } else { } leave: AMF_Reset(&obj); return ret; } 参考： librtmp实时消息传输协议(RTMP)库代码浅析 librtmp源码分析之核心实现解读 RTMPdump（libRTMP） 源代码分析 7： 建立一个流媒体连接 （NetStream部分 2） 手撕Rtmp协议细节（3）——Rtmp Body "},"pages/librtmp/librtmp源码之SendReleaseStream.html":{"url":"pages/librtmp/librtmp源码之SendReleaseStream.html","title":"librtmp源码之SendReleaseStream","keywords":"","body":"librtmp源码之SendReleaseStream // 发送RealeaseStream命令 // Real Time Messaging Protocol (AMF0 Command releaseStream()) // RTMP Header // 01.. .... = Format: 1 // ..00 0011 = Chunk Stream ID: 3 // Timestamp delta: 0 // Timestamp: 0 (calculated) // Body size: 39 // Type ID: AMF0 Command (0x14) // RTMP Body // String 'releaseStream' // Number 2 // Null // String 'livestream' static int SendReleaseStream(RTMP *r) { RTMPPacket packet; char pbuf[1024], *pend = pbuf + sizeof(pbuf); char *enc; // 块流ID，`..00 0011 = Chunk Stream ID: 3`中的11，第1个字节的低两位 packet.m_nChannel = 0x03; /* control channel (invoke) */ // 块类型ID，`01.. .... = Format: 1`中的01，第1个字节的高两位 packet.m_headerType = RTMP_PACKET_SIZE_MEDIUM; // 命令类型ID，`Type ID: AMF0 Command (0x14)`，第8个字节 packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; // 时间戳第2、3、4字节 packet.m_nTimeStamp = 0; // 消息流ID，这里块类型ID为1，只有8个字节，这里是没有消息流ID的 packet.m_nInfoField2 = 0; // 是否是绝对时间戳(类型1时为true) packet.m_hasAbsTimestamp = 0; // 块body指针，前面18（RTMP_MAX_HEADER_SIZE）字节用来放块头，后面放body packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; // 设置body开始位置 enc = packet.m_body; // 对“releaseStream”字符串进行AMF编码 enc = AMF_EncodeString(enc, pend, &av_releaseStream); // 对传输ID（0）进行AMF编码，0x14命令远程过程调用计数 enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); // 编码Null *enc++ = AMF_NULL; // 对播放路径字符串进行AMF编码，livestream enc = AMF_EncodeString(enc, pend, &r->Link.playpath); if (!enc) return FALSE; // 块body大小，第5、6、7个字节 packet.m_nBodySize = enc - packet.m_body; return RTMP_SendPacket(r, &packet, FALSE); } 参考： librtmp源码分析之核心实现解读 RTMPdump（libRTMP） 源代码分析 8： 发送消息（Message）） 手撕rtmp协议细节（2）——rtmp Header 手撕Rtmp协议细节（3）——Rtmp Body "},"pages/librtmp/librtmp源码之SendFCPublish.html":{"url":"pages/librtmp/librtmp源码之SendFCPublish.html","title":"librtmp源码之SendFCPublish","keywords":"","body":"librtmp源码之SendFCPublish // 发送准备推流点请求 // Real Time Messaging Protocol (AMF0 Command FCPublish()) // RTMP Header // 01.. .... = Format: 1 // ..00 0011 = Chunk Stream ID: 3 // Timestamp delta: 0 // Timestamp: 0 (calculated) // Body size: 35 // Type ID: AMF0 Command (0x14) // RTMP Body // String 'FCPublish' // AMF0 type: String (0x02) // String length: 9 // String: FCPublish // Number 3 // AMF0 type: Number (0x00) // Number: 3 // Null // AMF0 type: Null (0x05) // String 'livestream' // AMF0 type: String (0x02) // String length: 10 // String: livestream static int SendFCPublish(RTMP *r) { RTMPPacket packet; char pbuf[1024], *pend = pbuf + sizeof(pbuf); char *enc; packet.m_nChannel = 0x03; /* control channel (invoke) */ packet.m_headerType = RTMP_PACKET_SIZE_MEDIUM; packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; packet.m_nTimeStamp = 0; packet.m_nInfoField2 = 0; packet.m_hasAbsTimestamp = 0; packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; enc = packet.m_body; // 对“FCPublish”字符串进行AMF编码 enc = AMF_EncodeString(enc, pend, &av_FCPublish); enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); *enc++ = AMF_NULL; enc = AMF_EncodeString(enc, pend, &r->Link.playpath); if (!enc) return FALSE; packet.m_nBodySize = enc - packet.m_body; return RTMP_SendPacket(r, &packet, FALSE); } 和SendReleaseStream函数类似，区别在与body的第一个字段是FCPublish。 参考： librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp源码之RTMP_SendServerBW.html":{"url":"pages/librtmp/librtmp源码之RTMP_SendServerBW.html","title":"librtmp源码之RTMP_SendServerBW","keywords":"","body":"librtmp源码之RTMP_SendServerBW // 设置缓冲大小 int RTMP_SendServerBW(RTMP *r) { RTMPPacket packet; char pbuf[256], *pend = pbuf + sizeof(pbuf); // Chunk Stream ID用来表示消息的级别 // 2:low level // 块流ID，`..00 0010 = Chunk Stream ID: 2`中的11，第1个字节的低两位 packet.m_nChannel = 0x02; /* control channel (invoke) */ // Format为0的时候，RTMP Header的长度为12 // 块类型ID，`01.. .... = Format: 1`中的01，第1个字节的高两位 packet.m_headerType = RTMP_PACKET_SIZE_LARGE; // #define RTMP_PACKET_TYPE_SERVER_BW 0x05 // 命令类型ID，`Type ID: AMF0 Command (0x14)`，第8个字节 packet.m_packetType = RTMP_PACKET_TYPE_SERVER_BW; // 时间戳第2、3、4字节 packet.m_nTimeStamp = 0; // 消息流ID，这里块类型ID为1，有12个字节，这里是没有消息流ID的 packet.m_nInfoField2 = 0; // 是否是绝对时间戳(类型1时为true) packet.m_hasAbsTimestamp = 0; // 块body指针，前面18（RTMP_MAX_HEADER_SIZE）字节用来放块头，后面放body packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; // 块body大小，第5、6、7个字节 packet.m_nBodySize = 4; // 压入4字节带宽 AMF_EncodeInt32(packet.m_body, pend, r->m_nServerBW); return RTMP_SendPacket(r, &packet, FALSE); } 参考： librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp源码之RTMP_SendCtrl.html":{"url":"pages/librtmp/librtmp源码之RTMP_SendCtrl.html","title":"librtmp源码之RTMP_SendCtrl","keywords":"","body":"librtmp源码之RTMP_SendCtrl // 发送用戶控制消息 //Ping 是 RTMP 中最神秘的消息，直到现在我们还没有完全解释它。总之，Ping 消息用作客户端和服务器之间交换的特殊命令。此页面旨在记录所有已知的 Ping 消息。预计名单会增长。 // //Ping 数据包的类型为 0x4，包含两个强制参数和两个可选参数。第一个参数是 Ping 的类型，简称整数。第二个参数是 ping 的目标。由于 Ping 始终在 Channel 2（控制通道）中发送，并且 RTMP 标头中的目标对象始终为 0，这意味着 Connection 对象，因此有必要放置一个额外的参数来指示 Ping 发送到的确切目标对象。第二个参数承担这个责任。该值与 RTMP 头中的目标对象字段含义相同。 （第二个值也可以做其他用途，比如RTT Ping/Pong。它用作时间戳。）第三个和第四个参数是可选的，可以看作是Ping包的参数。以下是 Ping 消息的无穷无尽的列表。 // // * 类型 0：清除流。没有第三个和第四个参数。第二个参数可以是 0。建立连接后，服务器会向客户端发送 Ping 0,0。该消息还将在 Play 开始时发送给客户端，并响应 Seek 或 Pause/Resume 请求。此 Ping 告诉客户端使用下一个数据包服务器发送的时间戳重新校准时钟。 // * 类型 1：告诉流清除播放缓冲区。 // * 类型 3：客户端的缓冲时间。第三个参数是以毫秒为单位的缓冲时间。 // * 类型 4：重置流。在 VOD 的情况下与类型 0 一起使用。通常在类型 0 之前发送。 // * 类型 6：从服务器 Ping 客户端。第二个参数是当前时间。 // * 类型 7：来自客户的 Pong 回复。第二个参数是服务器发送他的 ping 请求的时间。 // * 类型 26：SWFVerification 请求 // * 类型 27：SWFVerification 响应 int RTMP_SendCtrl(RTMP *r, short nType, unsigned int nObject, unsigned int nTime) { RTMPPacket packet; char pbuf[256], *pend = pbuf + sizeof(pbuf); int nSize; char *buf; RTMP_Log(RTMP_LOGDEBUG, \"sending ctrl. type: 0x%04x\", (unsigned short)nType); packet.m_nChannel = 0x02; /* control channel (ping) */ packet.m_headerType = RTMP_PACKET_SIZE_MEDIUM; // #define RTMP_PACKET_TYPE_CONTROL 0x04 packet.m_packetType = RTMP_PACKET_TYPE_CONTROL; packet.m_nTimeStamp = 0; /* RTMP_GetTime(); */ packet.m_nInfoField2 = 0; packet.m_hasAbsTimestamp = 0; packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; switch(nType) { case 0x03: nSize = 10; break; /* buffer time */ case 0x1A: nSize = 3; break; /* SWF verify request */ case 0x1B: nSize = 44; break; /* SWF verify response */ default: nSize = 6; break; } packet.m_nBodySize = nSize; buf = packet.m_body; buf = AMF_EncodeInt16(buf, pend, nType); if (nType == 0x1B) { #ifdef CRYPTO memcpy(buf, r->Link.SWFVerificationResponse, 42); RTMP_Log(RTMP_LOGDEBUG, \"Sending SWFVerification response: \"); RTMP_LogHex(RTMP_LOGDEBUG, (uint8_t *)packet.m_body, packet.m_nBodySize); #endif } else if (nType == 0x1A) { *buf = nObject & 0xff; } else { if (nSize > 2) buf = AMF_EncodeInt32(buf, pend, nObject); if (nSize > 6) buf = AMF_EncodeInt32(buf, pend, nTime); } return RTMP_SendPacket(r, &packet, FALSE); } 参考： librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp源码之RTMP_SendCreateStream.html":{"url":"pages/librtmp/librtmp源码之RTMP_SendCreateStream.html","title":"librtmp源码之RTMP_SendCreateStream","keywords":"","body":"librtmp源码之RTMP_SendCreateStream // 发送创建流请求 int RTMP_SendCreateStream(RTMP *r) { RTMPPacket packet; char pbuf[256], *pend = pbuf + sizeof(pbuf); char *enc; packet.m_nChannel = 0x03; /* control channel (invoke) */ packet.m_headerType = RTMP_PACKET_SIZE_MEDIUM; packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; packet.m_nTimeStamp = 0; packet.m_nInfoField2 = 0; packet.m_hasAbsTimestamp = 0; packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; enc = packet.m_body; enc = AMF_EncodeString(enc, pend, &av_createStream); enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); *enc++ = AMF_NULL; /* NULL */ packet.m_nBodySize = enc - packet.m_body; return RTMP_SendPacket(r, &packet, TRUE); } 内容和SendReleaseStream函数差不多，只不过命令改用了av_createStream。 参考： librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp源码之SendPublish.html":{"url":"pages/librtmp/librtmp源码之SendPublish.html","title":"librtmp源码之SendPublish","keywords":"","body":"librtmp源码之SendPublish // 发送Publish命令 static int SendPublish(RTMP *r) { RTMPPacket packet; char pbuf[1024], *pend = pbuf + sizeof(pbuf); char *enc; // 块流ID，04: control stream packet.m_nChannel = 0x04; /* source channel (invoke) */ packet.m_headerType = RTMP_PACKET_SIZE_LARGE; packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; packet.m_nTimeStamp = 0; // 消息流ID，这个时候就有了流ID，因为这个方法是在收到服务器返回创建流的时候调用 packet.m_nInfoField2 = r->m_stream_id; packet.m_hasAbsTimestamp = 0; packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; enc = packet.m_body; // 压入publish enc = AMF_EncodeString(enc, pend, &av_publish); enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); *enc++ = AMF_NULL; enc = AMF_EncodeString(enc, pend, &r->Link.playpath); if (!enc) return FALSE; /* FIXME: should we choose live based on Link.lFlags & RTMP_LF_LIVE? */ enc = AMF_EncodeString(enc, pend, &av_live); if (!enc) return FALSE; packet.m_nBodySize = enc - packet.m_body; return RTMP_SendPacket(r, &packet, TRUE); } 参考： RTMPdump（libRTMP） 源代码分析 8： 发送消息（Message） librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp源码之SendPlay.html":{"url":"pages/librtmp/librtmp源码之SendPlay.html","title":"librtmp源码之SendPlay","keywords":"","body":"librtmp源码之SendPlay // 发送命令“播放” static int SendPlay(RTMP *r) { RTMPPacket packet; char pbuf[1024], *pend = pbuf + sizeof(pbuf); char *enc; // 8:control stream packet.m_nChannel = 0x08; /* we make 8 our stream channel */ packet.m_headerType = RTMP_PACKET_SIZE_LARGE; packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; packet.m_nTimeStamp = 0; // 指定流ID packet.m_nInfoField2 = r->m_stream_id; /*0x01000000; */ packet.m_hasAbsTimestamp = 0; packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; enc = packet.m_body; enc = AMF_EncodeString(enc, pend, &av_play); enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); *enc++ = AMF_NULL; RTMP_Log(RTMP_LOGDEBUG, \"%s, seekTime=%d, stopTime=%d, sending play: %s\", __FUNCTION__, r->Link.seekTime, r->Link.stopTime, r->Link.playpath.av_val); enc = AMF_EncodeString(enc, pend, &r->Link.playpath); if (!enc) return FALSE; // 指定开始时间 /* Optional parameters start and len. * * start: -2, -1, 0, positive number * -2: looks for a live stream, then a recorded stream, * if not found any open a live stream * -1: plays a live stream * >=0: plays a recorded streams from 'start' milliseconds */ // #define RTMP_LF_LIVE 0x0002 /* stream is live */ if (r->Link.lFlags & RTMP_LF_LIVE) enc = AMF_EncodeNumber(enc, pend, -1000.0); else { if (r->Link.seekTime > 0.0) enc = AMF_EncodeNumber(enc, pend, r->Link.seekTime); /* resume from here */ else enc = AMF_EncodeNumber(enc, pend, 0.0); /*-2000.0);*/ /* recorded as default, -2000.0 is not reliable since that freezes the player if the stream is not found */ } if (!enc) return FALSE; // 指点播放时长 /* len: -1, 0, positive number * -1: plays live or recorded stream to the end (default) * 0: plays a frame 'start' ms away from the beginning * >0: plays a live or recoded stream for 'len' milliseconds */ /*enc += EncodeNumber(enc, -1.0); */ /* len */ if (r->Link.stopTime) { enc = AMF_EncodeNumber(enc, pend, r->Link.stopTime - r->Link.seekTime); if (!enc) return FALSE; } packet.m_nBodySize = enc - packet.m_body; return RTMP_SendPacket(r, &packet, TRUE); } 参考： RTMP学习（十一）rtmpdump源码阅读（6）请求播放 librtmp源码分析之核心实现解读 "},"pages/FFmpeg/相关链接.html":{"url":"pages/FFmpeg/相关链接.html","title":"相关链接","keywords":"","body":"相关链接 FFmpeg官网 程序员秘密:”ffmpeg编译ios“ 的搜索结果 1.编译FFmpeg库 FFmpeg使用遇到问题记录 实战FFmpeg－－编译iOS平台使用的FFmpeg库（支持arm64的FFmpeg2.6.2） gas-preprocessor.pl build-ffmpeg.sh FFMpeg下载编译iOS库 iOS audio and video - compiled FFmpeg 八.FFmepg--编译静态库(iOS) FFMpeg cross compiler library Android-SO ffmpeg从入门到实战 Android权限 java.io.IOException：对createNewFile（）的权限被拒绝 请求应用权限 视频解码 一文读懂 Android FFmpeg 视频解码过程与实战分析 视频编码 FFmpeg视频编码 YUV420P编码H264 Audio and video development (c) - encoded video Audio and video development-audio and video learning materials ffmpeg入门教程之YUV编码成h264 博客 音视频学习 雷霄骅 雷神的《基于 FFmpeg + SDL 的视频播放器的制作》课程的视频 断点实验室 如何写一个不到 1000 行的视频播放器 落影loyinglin 不朽的传奇 HBStream 12年专注直播点播、网络视频技术 wcf 课程 【免费】FFmpeg/WebRTC/RTMP/NDK/Android音视频流媒体高级开发 ffmpeg入門教程 李超 SDL·lazyfoo tongtong 视频下载 ultravideo yuv视频处理 media adb mac下安装adb环境的三种方式 adb命令读取Android手机内存卡文件 adb基本命令操作(四) 推拉流 Nginx学习之配置RTMP模块搭建推流服务 FFmpeg问题 《FFmpeg精讲与实战》常见问题与解答 音频编码 ffmpeg导入x264、libfdk_aac外部库 播放器 vlc 一个小玩具：NDK编译SDL的例子 交叉编译sdl2成android的.so库 利用ffmpeg和SDL实现一个跨android版本的音视频播放器 SDL播放YUV——循环 SDL播放yuv 使用SDL显示YUV ffmpeg实战教程（二）用SDL播放YUV，并结合ffmpeg实现简易播放器 最简iOS播放器（二） kxmovie iOS 基于ffmpeg的音视频编、解码以及播放器的制作 iOS-FFmpeg实现简单播放器(编译fak-aac+x264+sdl) ffmpeg-video-player ijkplayer ijkplayer ijkplayer#topics ffplayer ffplayer ffplayer 原理、架构及代码分析——播放器功能原理 ffplay for mfc 代码备忘 ffplay基本使用 ffplayer 调试 使用XCode debug ffmpeg/ffplay ffplay调试环境搭建 Windows平台上编译ffmpeg源码，调试ffplay Xcode调试ffmpeg源码(十五) ffplayer 源码解析 ffplay.c源码阅读之音频、字幕、视频渲染原理(一) ffplay源码分析1-概述 FFPLAY的原理（一） Ffplay源码read_thread解读（一） 零基础读懂视频播放器控制原理： ffplay 播放器源代码分析 ffplaymfc-ffplaycore.cpp ffplay 播放器源代码分析 Fplay源代码分析：整体流程图 调试分析FFmpeg 常用结构体 FFMPEG 安装 教程（支持mp3） Ffplay源码read_thread解读（一） ffplay frame queue分析 ffmpeg源码分析之ffplay主流程 开源 ffmpeg_develop_doc 权限处理 Android permission denied原因归纳和解决办法 Android11缺少权限导致无法修改原文件，获取所有文件访问权限的方法 sdl SDL中文教程 SDL论坛 Android平台上使用SDL官方demo播放视频（使用ffmpeg解码） ant NDK NDK下载 Unsupported-Downloads android-ndk-r10e-darwin-x86_64 视频编辑 YXYVideoEditor SRS 手把手带你实现srs流媒体推流和拉流操作 Chmod 777 to a folder and all contents [duplicate] 腾讯云轻量服务器开放端口方法教程 v4_CN_Home SRS 4.0开发环境搭建:包括推流、服务器配置、拉流测试 ffmpeg -re -i ./doc/source.flv -c copy -f flv rtmp://www.1221.site/live/livestream ffmpeg -re -i /Users/you5yi/Downloads/蓝莓之夜/蓝莓之夜.1080p.国英双语.BD中英双字[66影视www.66Ys.Co].mkv -c copy -f flv rtmp://www.1221.site/live/livestream LFLiveKit LFLiveKit源码分析 LFLiveKit-Review.md ios基于LFLiveKit的直播项目 点播 HLS点播实现（H.264和AAC码流） RTMP 利用librtmp库在iOS上进行推流 【原】librtmp源码详解 调试libRTMP代码来分析RTMP协议 srs-librtmp-for-ios 从动手到放弃之我不要自己编译ffmpeg了 【踩坑+草稿篇】 简述RTMPDump与编译移植 RTMPdump 源代码分析 1： main()函数 rtmp源码分析之RTMP_Write 和 RTMP_SendPacket librtmp源码分析之核心实现解读 流媒体-RTMP协议-librtmp库学习（二） RTMP推流及协议学习 RTMP协议详解及实例分析 RTMP 协议规范(中文版) 手撕Rtmp协议细节（3）——Rtmp Body Socket socket()函数用法详解：创建套接字 "},"pages/FFmpeg/FFmpeg编译.html":{"url":"pages/FFmpeg/FFmpeg编译.html","title":"FFmpeg编译","keywords":"","body":"编译FFmpeg库 一、下载音视频框架 1. 上网下载 下载网址：http://www.ffmpeg.org/download.html 2. Shell脚本下载 1) 下载命令curl 它可以通过http\\ftp等等这样的网络方式下载和上传文件（它是一个强大网络工具）。 2）解压命令tar 表示解压和压缩（打包），基本语法：tar options，例如：tar xj，options选项分为很多中类型： -x 表示：解压文件选项， -j 表示：是否需要解压bz2压缩包（压缩包格式类型有很多：zip、bz2等等…）。 3）Shell脚本：download-ffmpeg.sh。 二、编译配置选项 1. 查看选项 进入FFmpeg框架包中，执行configure命令： cd ffmpeg-3.4 ./configure --help 2. 解释选项 1）Help options：帮助选项 --list-decoders：显示所有的解码器 --list-encoders：显示所有的编码器 --list-hwaccels：显示所有可用硬件加速 2）Standard options：标准选项 --logfile=FILE：日志输入文件 --disable-logging：不要打印debug日志 --prefix=PREFIX：安装目录 3）Licensing options：许可证选项 --enable-gpl：允许使用gpl代码，由此生成你的库或者二进制文件 GPL（许可证）：开源、免费、公用、修改、扩展。 4）Configuration options：配置备选选项 --disable-static：不能编译静态库 --enable-shared：构建动态库 5）Component options：组件选项 --disable-avdevice disable libavdevice build --disable-avcodec disable libavcodec build --disable-avformat disable libavformat build --disable-swresample disable libswresample build --disable-swscale disable libswscale build --disable-postproc disable libpostproc build --disable-avfilter disable libavfilter build --enable-avresample enable libavresample build [no] 6）External library support：外部库支持 Using any of the following switches will allow FFmpeg to link to the corresponding external library. All the components depending on that library will become enabled, if all their other dependencies are met and they are not explicitly disabled. E.g. --enable-libwavpack will enable linking to libwavpack and allow the libwavpack encoder to be built, unless it is specifically disabled with --disable-encoder=libwavpack. Note that only the system libraries are auto-detected. All the other external libraries must be explicitly enabled. Also note that the following help text describes the purpose of the libraries themselves, not all their features will necessarily be usable by FFmpeg. --enable-libfdk-aac：启用acc编码 7）Toolchain options：工具链选项 --arch=ARCH：指定我么需要编译平台CPU架构类型，例如：arm64、x86等等… --target-os=OS：指定操作系统 8）Advanced options：高级选项（暂时用不到） 9）Optimization options (experts only)：优化选项 10）Developer options：开发者模式 --disable-debug：禁用调试 --enable-debug=LEVEL：调试级别 四、Android平台编译 1. 下载源码 download-ffmpeg.sh是下载脚本，执行sh download-ffmpeg.sh下载源码，这里以ffmpeg-3.4为例。 2. 下载ndk https://developer.android.google.cn/ndk/downloads/older_releases.html 我这里使用的是ndkr10e。 在ffmpeg源码的同目录下新建ndk文件交，将下载好的ndk放入ndk文件夹。 3. 修改configure # SLIBNAME_WITH_MAJOR='$(SLIBNAME).$(LIBMAJOR)' # LIB_INSTALL_EXTRA_CMD='$$(RANLIB) \"$(LIBDIR)/$(LIBNAME)\"' # SLIB_INSTALL_NAME='$(SLIBNAME_WITH_VERSION)' # SLIB_INSTALL_LINKS='$(SLIBNAME_WITH_MAJOR) $(SLIBNAME)' The above code is modified as the following SLIBNAME_WITH_MAJOR='$(SLIBPREF)$(FULLNAME)-$(LIBMAJOR)$(SLIBSUF)' LIB_INSTALL_EXTRA_CMD='$$(RANLIB) \"$(LIBDIR)/$(LIBNAME)\"' SLIB_INSTALL_NAME='$(SLIBNAME_WITH_MAJOR)' SLIB_INSTALL_LINKS='$(SLIBNAME)' 4. 执行编译 android-build-ffmpeg.sh是编译脚本，将编译脚本放在和源码的同一目录，执行： sh android-build-ffmpeg.sh "},"pages/FFmpeg/FFmpeg集成.html":{"url":"pages/FFmpeg/FFmpeg集成.html","title":"FFmpeg集成","keywords":"","body":"FFmpeg集成 一、iOS集成FFmpeg 代码工程 1. 集成FFmpeg 测试我们自己编译库FFmpeg。 (1) 第一步：新建工程 删除Scenedelegate，参考：Xcode 11新建项目多了Scenedelegate。 (2) 第二步：导入库文件。 在工程目录新建FFmpeg-3.4，拷贝已经编译好的arm64静态库文件夹，删除不需要的share、lib/pkgconfig文件夹，最后将FFmpeg-3.4Add进入工程。 (3）第三步：添加依赖库 CoreMedia.framework CoreGraphics.framework VideoToolbox.framework AudioToolbox.framework libiconv.tbd libz.tbd libbz2.tbd (4) 配置头文件 1) 复制头文件路径 选中Target>Build Setting>搜索Library Search>双击Library Search Paths复制FFmpeg lib路径>修改lib为include就是FFmpeg头文件路径： $(PROJECT_DIR)/FFmpegCompiled/FFmpeg-3.4/arm64/include。 2) 配置头文件路径 选中Target>Build Setting>搜索Header Search>选中Header Search Paths>增加上面复制好头文件路径。 (5) 编译成功 2. 测试案例 (1) 打印配置信息 新建FFmpegTest测试类，增加类方法： // 引入头文件 // 核心库->音视频编解码库 #import /// 测试FFmpeg配置 + (void)ffmpegTestConfig { const char *configuration = avcodec_configuration(); NSLog(@\"配置信息: %s\", configuration); } 在ViewController中测试： #import \"FFmpegTest.h\" - (void)viewDidLoad { [super viewDidLoad]; // Do any additional setup after loading the view. //测试一 [FFmpegTest ffmpegTestConfig]; } console输出： 配置信息: --target-os=darwin --arch=arm64 --cc='xcrun -sdk iphoneos clang' --as='gas-preprocessor.pl -arch aarch64 -- xcrun -sdk iphoneos clang' --enable-cross-compile --disable-debug --disable-programs --disable-doc --enable-pic --extra-cflags='-arch arm64 -mios-version-min=7.0 -fembed-bitcode' --extra-ldflags='-arch arm64 -mios-version-min=7.0 -fembed-bitcode' --prefix=/Users/chenchangqing/Documents/FFmpeg/ffmpeg-3.4-target-iOS/arm64 (2) 打开视频文件 FFmpegTest增加类方法： // 导入封装格式库 #import /// 打开视频文件 + (void)ffmpegVideoOpenfile:(NSString*)filePath { // 第一步：注册组件 av_register_all(); // 第二步：打开封装格式文件 // 参数一：封装格式上下文 AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：打开视频地址->path const char *url = [filePath UTF8String]; // 参数三：指定输入封装格式->默认格式 // 参数四：指定默认配置信息->默认配置 int avformat_open_input_reuslt = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_reuslt != 0){ // 失败了 // 获取错误信息 // char* error_info = NULL; // av_strerror(avformat_open_input_reuslt, error_info, 1024); NSLog(@\"打开文件失败\"); return; } NSLog(@\"打开文件成功\"); } 项目新增视频Test.mov，在ViewController中测试： // 测试二 NSString *path = [[NSBundle mainBundle] pathForResource:@\"Test\" ofType:@\".mov\"]; [FFmpegTest ffmpegVideoOpenfile:path]; console输出： 打开文件成功 二、Android集成FFmpeg 代码工程 1. 集成FFmpeg (1) 第一步：新建工程 File->NewProject->Native C++->输入工程信息->Next->Finish。 三星手机开启开发者选项 (2) 第二步：导入库文件。 1) 项目选中Project模式->app->src->main->右键new->Directory->输入jniLibs->enter。 2) 将编译好的include和lib文件夹copy->选中刚才新建的jniLibs->paste。 (3) 第三步：修改CMakeLists.txt 1) app->src->main->cpp->双击CMakeLists.txt。 2) 修改CMakeLists.txt。 # FFMpeg配置 # FFmpeg配置目录 set(JNILIBS_DIR ${CMAKE_SOURCE_DIR}/../jniLibs) # 编解码(最重要的库) add_library( avcodec SHARED IMPORTED) set_target_properties( avcodec PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libavcodec.so) # 滤镜特效处理库 add_library( avfilter SHARED IMPORTED) set_target_properties( avfilter PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libavfilter.so) # 封装格式处理库 add_library( avformat SHARED IMPORTED) set_target_properties( avformat PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libavformat.so) # 工具库(大部分库都需要这个库的支持) add_library( avutil SHARED IMPORTED) set_target_properties( avutil PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libavutil.so) # 音频采样数据格式转换库 add_library( swresample SHARED IMPORTED) set_target_properties( swresample PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libswresample.so) # 视频像素数据格式转换 add_library( swscale SHARED IMPORTED) set_target_properties( swscale PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libswscale.so) add_library( avdevice SHARED IMPORTED) set_target_properties( avdevice PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libavdevice.so) add_library( postproc SHARED IMPORTED) set_target_properties( postproc PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libpostproc.so) #set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=gnu++11\") #判断编译器类型,如果是gcc编译器,则在编译选项中加入c++11支持 if(CMAKE_COMPILER_IS_GNUCXX) set(CMAKE_CXX_FLAGS \"-std=c++11 ${CMAKE_CXX_FLAGS}\") message(STATUS \"optional:-std=c++11\") endif(CMAKE_COMPILER_IS_GNUCXX) #配置编译的头文件 include_directories(${JNILIBS_DIR}/include) . . . target_link_libraries( # Specifies the target library. myapplication avcodec swresample avfilter avformat avutil swscale avdevice postproc # Links the target library to the log library # included in the NDK. ${log-lib}) (4) 配置CPU架构类型 修改app->build.gradle： externalNativeBuild { cmake { cppFlags '' abiFilters 'armeabi' } } 发现编译失败，解决方法，修改为如下( 参考链接)： defaultConfig { ndk { abiFilters 'armeabi-v7a' } } (5) 编译成功 2. 测试案例 (1) 打印配置信息 1) 定义Java方法 新建Java类FFmpegTest，定义ffmpegTestConfig方法： // 测试FFmpeg配置 // native：标记这个方法是一个特殊方法，不是普通的java方法，而是用于与NDK进行交互的方法（C/C++语言交互） // 用native修饰方法，方法没有实现，具体的实现在C/C++里面。 public static native void ffmpegTestConfig(); 2) 定义NDK方法 在native-lib.cpp中，导入FFmpeg头文件，由于 FFmpeg 是使用 C 语言编写的，所在 C++ 文件中引用 #include 的时候，也需要包裹在 extern \"C\" { }，才能正确的编译。 #import extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include } 在native-lib.cpp中新增Java方法ffmpegTestConfig的C++实现。 extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegcompiled_FFmpegTest_ffmpegTestConfig(JNIEnv *env, jclass clazz) { const char *configuration = avcodec_configuration(); __android_log_print(ANDROID_LOG_INFO, \"ffmpeg configuration\", \"%s\", configuration); } 之所以可以这么写是因为在CMakeLists.txt中有如下配置，将Java和C/C++进行关联。 add_library( # Sets the name of the library. androidffmpegcompiled # Sets the library as a shared library. SHARED # Provides a relative path to your source file(s). native-lib.cpp) 3) MainActivity增加测试代码。 protected void onCreate(Bundle savedInstanceState) { ... FFmpegTest.ffmpegTestConfig(); } 4) 运行工程，正确打印。 I/ffmpeg configuration: --prefix=/Users/chenchangqing/Documents/code/ffmpeg/01_ffmpeg_compiled/ffmpeg-3.4-target-android/armeabi-v7a --enable-shared --enable-gpl --disable-static --disable-doc --disable-ffmpeg --disable-ffplay --disable-ffprobe --disable-ffserver --disable-doc --disable-symver --enable-small --cross-prefix=/Users/chenchangqing/Documents/code/ffmpeg/01_ffmpeg_compiled/ndk/android-ndk-r10e/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi- --target-os=android --arch=armeabi-v7a --enable-cross-compile --sysroot=/Users/chenchangqing/Documents/code/ffmpeg/01_ffmpeg_compiled/ndk/android-ndk-r10e/platforms/android-18/arch-arm --extra-cflags='-Os -fpic -marm' --enable-pic (2) 打开视频文件 1) 定义Java方法 FFmpegTest定义ffmpegVideoOpenFile方法： // 测试FFmpeg打开视频 // filePath:路径 public static native void ffmpegVideoOpenFile(String filePath); 2) 定义NDK方法 在native-lib.cpp中，导入FFmpeg头文件。 #import extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include // 导入封装格式库 #import } 在native-lib.cpp中新增Java方法ffmpegVideoOpenFile的C++实现。 extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegcompiled_FFmpegTest_ffmpegVideoOpenFile(JNIEnv *env, jclass clazz, jstring file_path) { // 第一步：注册组件 av_register_all(); // 第二步：打开封装格式文件 // 参数一：封装格式上下文 AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：打开视频地址->path const char *url = env->GetStringUTFChars(file_path, NULL); // 参数三：指定输入封装格式->默认格式 // 参数四：指定默认配置信息->默认配置 int avformat_open_input_reuslt = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_reuslt != 0){ // 失败了 // 获取错误信息 // char* error_info = NULL; // av_strerror(avformat_open_input_reuslt, error_info, 1024); __android_log_print(ANDROID_LOG_INFO, \"ffmpeg\", \"打开文件失败\"); return; } __android_log_print(ANDROID_LOG_INFO, \"ffmpeg\", \"打开文件成功\"); } 3) 增加权限 在AndroidManifest.xml增加SD卡的读写权限。 4) MainActivity增加测试代码。 注意：如果打开失败，可能读写存储设备的权限被禁用。 摩托罗拉·刀锋：设置->应用和通知->高级->权限管理器->隐私相关·读写存储设备->找到应用->如果禁用，则修改为允许。 String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String inFilePath = rootPath.concat(\"/DCIM/Camera/VID_20220220_181306412.mp4\"); FFmpegTest.ffmpegVideoOpenFile(inFilePath); "},"pages/FFmpeg/iOS集成FFmpeg.html":{"url":"pages/FFmpeg/iOS集成FFmpeg.html","title":"iOS集成FFmpeg","keywords":"","body":"iOS集成FFmpeg 代码工程 下载FFmpeg源码 ffmpeg-3.4下载脚本：download-ffmpeg.sh。 sh download-ffmpeg.sh 安装gas-preprocessor ffmpeg-3.4对应的gas-preprocessor.pl。 cd gas-preprocessor directory sudo cp -f gas-preprocessor.pl /usr/local/bin/ chmod 777 /usr/local/bin/gas-preprocessor.pl 注意：使用Github最新的gas-preprocessor.pl编译FFmpeg，报错\"GNU assembler not found, install/update gas-preprocessor\"，所以使用指定版本的gas-preprocessor。 编译FFmpeg ffmpeg-3.4编译脚本：ios-build-ffmpeg.sh，将编译脚本放在和源码的同一目录。 默认分别编译arm64、armv7、i386、x86_64，代码如下： sh ios-build-ffmpeg.sh 指定架构编译，可以指定arm64、armv7、i386、x86_64，代码如下： sh ios-build-ffmpeg.sh arm64 指定armv7编译时出现问题，待解决： AS libavcodec/arm/aacpsdsp_neon.o src/libavutil/arm/asm.S:50:9: error: unknown directive .arch armv7-a ^ make: *** [libavcodec/arm/aacpsdsp_neon.o] Error 1 make: *** Waiting for unfinished jobs.... 新建工程 删除Scenedelegate，参考：Xcode 11新建项目多了Scenedelegate。 导入库文件 在工程目录新建FFmpeg-3.4，拷贝已经编译好的arm64静态库文件夹至FFmpeg-3.4，删除不需要的share、lib/pkgconfig文件夹，最后将FFmpeg-3.4通过Add Files加入工程。 配置头文件 1) 复制头文件路径 选中Target>Build Setting>搜索Library Search>双击Library Search Paths复制FFmpeg lib路径>修改lib为include就是FFmpeg头文件路径： $(PROJECT_DIR)/iOSIntegrationWithFFmpeg（工程名）/FFmpeg-3.4/arm64/include 2) 配置头文件路径 选中Target>Build Setting>搜索Header Search>选中Header Search Paths>增加上面复制好头文件路径。 添加依赖库 CoreMedia.framework CoreGraphics.framework VideoToolbox.framework AudioToolbox.framework libiconv.tbd libz.tbd libbz2.tbd 添加完毕，编译成功。 简单测试 下载test.mov，加入工程，新建测试类FFmpegTest，FFmpegTest.h增加方法定义。 /// 测试FFmpeg配置 + (void)ffmpegTestConfig; /// 打开视频文件 + (void)ffmpegVideoOpenfile:(NSString*)filePath; FFmpegTest.m引入FFmpeg头文件。 // 核心库->音视频编解码库 #import // 导入封装格式库 #import FFmpegTest.m增加方法实现。 /// 测试FFmpeg配置 + (void)ffmpegTestConfig { const char *configuration = avcodec_configuration(); NSLog(@\"配置信息: %s\", configuration); } /// 打开视频文件 + (void)ffmpegVideoOpenfile:(NSString*)filePath { // 第一步：注册组件 av_register_all(); // 第二步：打开封装格式文件 // 参数一：封装格式上下文 AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：打开视频地址->path const char *url = [filePath UTF8String]; // 参数三：指定输入封装格式->默认格式 // 参数四：指定默认配置信息->默认配置 int avformat_open_input_reuslt = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_reuslt != 0){ // 失败了 // 获取错误信息 // char* error_info = NULL; // av_strerror(avformat_open_input_reuslt, error_info, 1024); NSLog(@\"打开文件失败\"); return; } NSLog(@\"打开文件成功\"); } 在ViewController引入FFmpegTest.h头文件。 #import \"FFmpegTest.h\" 在ViewController的viewDidLoad加入方法调用。 // 测试一 [FFmpegTest ffmpegTestConfig]; // 测试二 NSString *path = [[NSBundle mainBundle] pathForResource:@\"Test\" ofType:@\".mov\"]; [FFmpegTest ffmpegVideoOpenfile:path]; run工程，console正确输出。 2022-05-08 01:32:12.320527+0800 iOSIntegrationWithFFmpeg[34898:1017443] 配置信息: --target-os=darwin --arch=arm64 --cc='xcrun -sdk iphoneos clang' --as='gas-preprocessor.pl -arch aarch64 -- xcrun -sdk iphoneos clang' --enable-cross-compile --disable-debug --disable-programs --disable-doc --enable-pic --extra-cflags='-arch arm64 -mios-version-min=7.0 -fembed-bitcode' --extra-ldflags='-arch arm64 -mios-version-min=7.0 -fembed-bitcode' --prefix=/Users/chenchangqing/Documents/code/ffmpeg/01_ffmpeg_compiled/ios_build/arm64 2022-05-08 01:32:12.338556+0800 iOSIntegrationWithFFmpeg[34898:1017443] 打开文件成功 "},"pages/FFmpeg/FFmpeg基础知识.html":{"url":"pages/FFmpeg/FFmpeg基础知识.html","title":"FFmpeg基础知识","keywords":"","body":"FFmpeg基础知识 一、视频播放 1. 视频播放流程 通常看到视频格式：mp4、mov、flv、wmv等等…，称之为：封装格式。 2. 视频播放器 两种模式播放器： (1) 可视化界面播放器 直接用户直观操作、简单易懂。例如腾讯视频、爱奇艺视频、QQ影音、暴风影音、快播、优酷等等。 (2) 非可视化界面播放器 命令操作播放器，用户看不懂，使用起来非常麻烦。例如FFmpeg的fplay（命令）播放器（内置播放器）、vlc播放器、mplayer播放器。 3. 视频信息查看工具 MediaInfo：帮助我们查看视频完整信息，直接去AppStore下载即可，不过要3块钱。 ULtraEdit：直接查看视频二进制数据。 视频单项信息： Elecard Format Analyzer：封装格式信息工具。 Elecard Stream Eye：视频编码信息工具。 GLYUVPlayer：视频像素信息工具，最新的mac系统不支持了，这里用YuvEye替换。 Adobe Audition：音频采样数据工具。 二、音视频封装格式 1. 封装格式 mp4、mov、flv、wmv、avi、ts、mkv等等。 2. 封装格式作用 视频流+音频流按照格式进行存储在一个文件中。 3.MPEG2-TS格式 视频压缩数据格式：MPEG2-TS。特定：数据排版，不包含头文件，数据大小固定（188byte）的TS-Packet。 4. FLV格式 (1) 优势 由于它形成的文件极小、加载速度极快，使得网络观看视频文件成为可能，它的出现有效地解决了视频文件导入Flash后，使导出的SWF文件体积庞大，不能在网络上很好的使用等问题。 (2) 文件结构 FLV是一个二进制文件，由文件头（FLV header）和很多tag组成。tag又可以分成三类：audio,video,script，分别代表音频流，视频流，脚本流（关键字或者文件信息之类）。 (3) FLV文件 FLV文件=FLV头文件+ tag1+tag内容1 + tag2+tag内容2 + ...+... + tagN+tag内容N。 (4) FLV头文件 1-3： 前3个字节是文件格式标识(FLV 0x46 0x4C 0x56)。 4-4： 第4个字节是版本（0x01）。 5-5： 第5个字节的前5个bit是保留的必须是0。 6-9: 第6-9的四个字节还是保留的.其数据为 00000009。 整个文件头的长度，一般是9（3+1+1+4）。 三、视频编码数据 1. 视频编码作用 将视频像素数据（YUV、RGB）进行压缩成为视频码流，从而降低视频数据量。（减小内存暂用） 2. 视频编码格式 3. H.264视频压缩数据格式 非常复杂算法->压缩->占用内存那么少？（例如：帧间预测、帧内预测…）->提高压缩性能。 四、音频编码数据 1. 音频编码作用 将音频采样数据（PCM格式）进行压缩成为音频码流，从而降低音频数据量。（减小内存暂用） 2. 音频编码格式 3. AAC格式 AAC，全称Advanced Audio Coding，是一种专为声音数据设计的文件压缩格式。与MP3不同，它采用了全新的算法进行编码，更加高效，具有更高的“性价比”。利用AAC格式，可使人感觉声音质量没有明显降低的前提下，更加小巧。苹果ipod、诺基亚手机支持AAC格式的音频文件。 (1) 优点 相对于mp3，AAC格式的音质更佳，文件更小。 (2) 不足 AAC属于有损压缩的格式，与时下流行的APE、FLAC等无损格式相比音质存在“本质上”的差距。加之，传输速度更快的USB3.0和16G以上大容量MP3正在加速普及，也使得AAC头上“小巧”的光环不复存在。 (3) 特点 ①提升的压缩率：可以以更小的文件大小获得更高的音质； ②支持多声道：可提供最多48个全音域声道； ③更高的解析度：最高支持96KHz的采样频率； ④提升的解码效率：解码播放所占的资源更少； 五、视频像素数据 1. 作用 保存了屏幕上面每一个像素点的值。 2. 数据格式 常见格式：RGB24、RGB32、YUV420P、YUV422P、YUV444P等等…一般最常见：YUV420P。 RGB格式： 3. 数据文件大小 例如：RGB24高清视频体积？（1个小时时长）。 体积：3600 x 25 x 1920 x 1080 * 3 = 559GB（非常大）。 假设：帧率25HZ，采样精度8bit，3个字节。 4. YUV播放器 人类：对色度不敏感，对亮度敏感。 Y表示：亮度，UV表示：色度。 六、音频采样数据 1. 作用 保存了音频中的每一个采样点值。 2. 数据文件大小 例如：1分钟PCM格式歌曲。 体积：60 x 44100 x 2 x 2 = 11MB。 分析：60表示时间，44100表示采样率（一般情况下，都是这个采样率，人的耳朵能够分辨的声音），2表示声道数量，2表示采样精度16位 = 2字节。 3. 工具 Audition 4. PCM格式 七、FFmpeg安装 1. 安装弯路 参考这片文章（mac下ffmpeg安装），我没搞成功。 执行命令： brew install ffmpeg 出现错误： Error: Xcode alone is not sufficient on Big Sur. Install the Command Line Tools: xcode-select --install 开始执行xcode-select --install，安装xcode命令行，这里又个问题，为啥我有xcode还得装这个。 出现错误： Error: No such file or directory @ rb_sysopen - /Users/chenchangqing/Library/Caches/Homebrew/downloads/c1e04fd8a5516a3a63dd106e38df40a2d44af18cc3f3d366e5ba0563d2f95570--openexr-3.1.4.big_sur.bottle.tar.gz 查资料后，先后执行了以下命令： brew install openexr brew install libvmaf brew install freetype 一直处于： 又卡住了： ==> Applying configure-big_sur.diff patching file configure Hunk #1 succeeded at 9513 (offset 780 lines). ==> ./configure --prefix=/usr/local/Homebrew/Cellar/gettext/0.21 --with-included-glib --w ==> make 出现错误，依次执行： brew install sqlite brew install meson brew install harfbuzz 放弃... 2. 正确方法 最后看了这篇文章，Mac OS上使用ffmpeg的“血泪”总结，按文章执行如下步骤： brew tap homebrew-ffmpeg/ffmpeg brew install homebrew-ffmpeg/ffmpeg/ffmpeg 安装成功。 八、FFmpeg应用 提供了一套比较完整代码，开源免费。核心架构设计思想：（核心 + 插件）设计。 1. ffmpeg (1) 作用 用于对视频进行转码，将mp4->mov，mov->mp4，wmv->mp4等等。 (2) 命令格式 ffmpeg -i {指定输入文件路径} -b:v {输出视频码率} {输出文件路径}。 (3) 测试运行 下载test.mov，修改码率，mov转mp4。 ffmpeg -i test.mov -b:v 234k -b:a 64k test.mp4 (4) 常用脚本 topmp4.sh，to1080pmp4.sh (4) 案例：视频转为高质量GIF动图 ffmpeg -ss 00:00:03 -t 3 -i Test.mov -s 640x360 -r “15” dongtu.gif 1) -ss 00:00:03 表示从第 00 分钟 03 秒开始制作 GIF，如果你想从第 9 秒开始，则输入 -ss 00:00:09，或者 -ss 9，支持小数点，所以也可以输入 -ss 00:00:11.3，或者 -ss 34.6 之类的，如果不加该命令，则从 0 秒开始制作； 2) -t 3 表示把持续 3 秒的视频转换为 GIF，你可以把它改为其他数字，例如 1.5，7 等等，时间越长，GIF 体积越大，如果不加该命令，则把整个视频转为 GIF； 3) -i 表示 invert 的意思吧，转换； 4) Test.mov 就是你要转换的视频，名称最好不要有中文，不要留空格，支持多种视频格式； 5) -s 640x360 是 GIF 的分辨率，视频分辨率可能是 1080p，但你制作的 GIF 可以转为 720p 等，允许自定义，分辨率越高体积越大，如果不加该命令，则保持分辨率不变； 6) -r “15” 表示帧率，网上下载的视频帧率通常为 24，设为 15 效果挺好了，帧率越高体积越大，如果不加该命令，则保持帧率不变； 7) dongtu.gif：就是你要输出的文件，你也可以把它命名为 hello.gif 等等。 2. ffplay 格式：ffplay {文件路径}，如下： ffplay test.mov "},"pages/FFmpeg/FFmpeg视频解码.html":{"url":"pages/FFmpeg/FFmpeg视频解码.html","title":"FFmpeg视频解码","keywords":"","body":"FFmpeg视频解码 代码工程 一、视频解码流程 第一步：注册组件 av_register_all：例如：编码器、解码器等等。 // 第一步：注册组件 av_register_all(); 第二步：打开封装格式 avformat_open_input：例如：打开.mp4、.mov、.wmv文件等等。 // 第二步：打开封装格式 // 参数一：封装格式上下文 // 作用：保存整个视频信息(解码器、编码器等等...) // 信息：码率、帧率等... AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：视频路径 // 在我们iOS里面 // NSString* path = @\"test.mov\"; // const char *url = [path UTF8String] const char *url = env->GetStringUTFChars(in_file_path, NULL); // 参数三：指定输入的格式 // 参数四：设置默认参数 int avformat_open_input_result = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_result != 0){ // 安卓平台下log __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开文件失败\"); // iOS平台下log // NSLog(\"打开文件失败\"); // 不同的平台替换不同平台log日志 return; } 第三步：查找视频基本信息 avformat_find_stream_info：如果是视频解码，那么查找视频流，如果是音频解码，那么就查找音频流。 // 第三步：查找视频流，拿到视频信息 // 参数一：封装格式上下文 // 参数二：指定默认配置 int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result 第四步：查找视频解码器 avcodec_find_decoder：查找解码器。 1. 查找视频流索引位置 // 第四步：查找视频解码器 // 4.1 查找视频流索引位置 int av_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 判断流类型：视频流、音频流、字母流等等... if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_stream_index = i; break; } } 2. 获取解码器上下文 根据视频流索引，获取解码器上下文。 // 4.2 根据视频流索引，获取解码器上下文 AVCodecContext *avcodec_context = avformat_context->streams[av_stream_index]->codec; 3. 获得解码器ID 根据解码器上下文，获得解码器ID，然后查找解码器。 // 4.3 根据解码器上下文，获得解码器ID，然后查找解码器 AVCodec *avcodec = avcodec_find_decoder(avcodec_context->codec_id); 第五步：打开解码器 avcodec_open2：打开解码器。 // 第五步：打开解码器 int avcodec_open2_result = avcodec_open2(avcodec_context, avcodec, NULL); if (avcodec_open2_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开解码器失败\"); return; } // 测试一下 // 打印信息 __android_log_print(ANDROID_LOG_INFO, \"main\", \"解码器名称：%s\", avcodec->name); 第六步：定义类型转换参数 用于sws_scale()，进行音频采样数据转换操作。 1. 创建视频采样数据上下文 // 第六步：定义类型转换参数 // 6.1 创建视频采样数据帧上下文 // 参数一：源文件->原始视频像素数据格式宽 // 参数二：源文件->原始视频像素数据格式高 // 参数三：源文件->原始视频像素数据格式类型 // 参数四：目标文件->目标视频像素数据格式宽 // 参数五：目标文件->目标视频像素数据格式高 // 参数六：目标文件->目标视频像素数据格式类型 SwsContext *swscontext = sws_getContext(avcodec_context->width, avcodec_context->height, avcodec_context->pix_fmt, avcodec_context->width, avcodec_context->height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); 2. 创建视频压缩数据帧 // 6.2 创建视频压缩数据帧 // 视频压缩数据：H264 AVFrame* avframe_in = av_frame_alloc(); // 定义解码结果 int decode_result = 0; 3. 创建视频采样数据帧 // 6.3 创建视频采样数据帧 // 视频采样数据：YUV格式 AVFrame* avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型->yuv420类型 // 得到YUV420P缓冲区大小 // 参数一：视频像素数据格式类型->YUV420P格式 // 参数二：一帧视频像素数据宽 = 视频宽 // 参数三：一帧视频像素数据高 = 视频高 // 参数四：字节对齐方式->默认是1 int buffer_size = av_image_get_buffer_size(AV_PIX_FMT_YUV420P, avcodec_context->width, avcodec_context->height, 1); // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 // 参数一：目标->填充数据(avframe_yuv420p) // 参数二：目标->每一行大小 // 参数三：原始数据 // 参数四：目标->格式类型 // 参数五：宽 // 参数六：高 // 参数七：字节对齐方式 av_image_fill_arrays(avframe_yuv420p->data, avframe_yuv420p->linesize, out_buffer, AV_PIX_FMT_YUV420P, avcodec_context->width, avcodec_context->height, 1); 第七步：打开.yuv文件 // 第七步：打开.yuv文件 const char *outfile = env->GetStringUTFChars(out_file_path, NULL); FILE* file_yuv420p = fopen(outfile, \"wb+\"); if (file_yuv420p == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"输出文件打开失败\"); return; } 第八步：读取视频压缩数据帧 av_read_frame：读取视频压缩数据帧。 // 第八步：读取视频压缩数据帧 int current_index = 0; // 写入时yuv数据位置 int y_size, u_size, v_size; // 分析av_read_frame参数。 // 参数一：封装格式上下文 // 参数二：一帧压缩数据 // 如果是解码视频流，是视频压缩帧数据，例如H264 AVPacket* packet = (AVPacket*)av_malloc(sizeof(AVPacket)); while (av_read_frame(avformat_context, packet) >= 0) { // >=:读取到了 // stream_index == av_stream_index) { // 第九步：开始视频解码 // ... current_index++; __android_log_print(ANDROID_LOG_INFO, \"main\", \"当前解码第%d帧\", current_index); } } 第九步：开始视频解码 注意：代码位置在第八步。 avcodec_send_packet：发送一帧视频压缩数据。 avcodec_receive_frame：解码一帧视频数据。 // 第九步：开始视频解码 // 发送一帧视频压缩数据 avcodec_send_packet(avcodec_context, packet); // 解码一帧视频数据 decode_result = avcodec_receive_frame(avcodec_context, avframe_in); if (decode_result == 0) { // 视频解码成功 // 第十步：开始类型转换 // ... // 第十一步：写入.yuv文件 // ... } 第十步：开始类型转换 注意：代码位置在第九步。 // 第十步：开始类型转换 // 将解码出来的视频像素点数据格式统一转类型为yuv420P // 参数一：视频像素数据格式上下文 // 参数二：原来的视频像素数据格式->输入数据 // 参数三：原来的视频像素数据格式->输入画面每一行大小 // 参数四：原来的视频像素数据格式->输入画面每一行开始位置(填写：0->表示从原点开始读取) // 参数五：原来的视频像素数据格式->输入数据行数 // 参数六：转换类型后视频像素数据格式->输出数据 // 参数七：转换类型后视频像素数据格式->输出画面每一行大小 sws_scale(swscontext, (const uint8_t *const *)avframe_in->data, avframe_in->linesize, 0, avcodec_context->height, avframe_yuv420p->data, avframe_yuv420p->linesize); 第十一步：写入.yuv文件 注意：代码位置在第九步。 // 第十一步：写入.yuv文件 // 计算YUV大小 // Y表示：亮度 // UV表示：色度 // YUV420P格式规范一：Y结构表示一个像素(一个像素对应一个Y) // YUV420P格式规范二：4个像素点对应一个(U和V: 4Y = U = V) y_size = avcodec_context->width * avcodec_context->height; u_size = y_size / 4; v_size = y_size / 4; // 首先->Y数据 fwrite(avframe_yuv420p->data[0], 1, y_size, file_yuv420p); // 其次->U数据 fwrite(avframe_yuv420p->data[1], 1, u_size, file_yuv420p); // 再其次->V数据 fwrite(avframe_yuv420p->data[2], 1, v_size, file_yuv420p); 第十二步：释放内存资源，关闭解码器 // 第十二步：释放内存资源，关闭解码器 av_packet_free(&packet); fclose(file_yuv420p); av_frame_free(&avframe_in); av_frame_free(&avframe_yuv420p); free(out_buffer); avcodec_close(avcodec_context); avformat_free_context(avformat_context); 二、新建Android视频解码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程AndroidFFmpegDecodingVideo。 2. 定义java方法 寻找MainActivity：app->src->main->java->MainActivity，增加代码如下： public native void ffmepgDecodeVideo(String inFilePath, String outFilePath); 3. 定义NDK方法 增加android打印。 #include 在native-lib.cpp中，导入FFmpeg头文件。 extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include // 封装格式处理库 #include \"libavformat/avformat.h\" // 工具库 #include \"libavutil/imgutils.h\" // 视频像素数据格式库 #include \"libswscale/swscale.h\" } 在native-lib.cpp中新增java方法ffmepgDecodeVideo的C++实现，输入MainActivity.就会有代码提示，选择正确ffmepgDecodeVideo方法补全代码。 extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegdecodingvideo_MainActivity_ffmepgDecodeVideo(JNIEnv *env, jobject thiz, jstring in_file_path, jstring out_file_path) { // 这里拷贝上面的视频解码流程的代码即可。 } 三、测试Android视频解码工程 准备视频文件：test.mov 在AndroidManifest.xml增加SD卡的读写权限。 MainActivity增加测试代码。 注意：如果打开失败，可能读写存储设备的权限被禁用。 摩托罗拉·刀锋：设置->应用和通知->高级->权限管理器->隐私相关·读写存储设备->找到应用->如果禁用，则修改为允许。 import android.os.Environment; import java.io.File; import java.io.IOException; import android.util.Log; String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.mov\"); String outFilePath = downloadPath.concat(\"/test.yuv\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmepgDecodeVideo(inFilePath, outFilePath); 出现问题，待解决： I/main: 解码器名称：h264 A/libc: Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 5713 (egdecodingvideo), pid 5713 (egdecodingvideo) 增加打印pix_fmt代码： __android_log_print(ANDROID_LOG_INFO, \"main\", \"avcodec_context->pix_fmt：%d\", avcodec_context->pix_fmt); 发现avcodec_context->pix_fmt = -1，导致sws_getContext方法出错，修改sws_getContext的srcFormat参数。 SwsContext *swscontext = sws_getContext(avcodec_context->width, avcodec_context->height, AV_PIX_FMT_YUV420P, //avcodec_context->pix_fmt, avcodec_context->width, avcodec_context->height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); run工程代码，正确打印，同时正确生成yuv文件。 I/main: 解码器名称：h264 I/main: avcodec_context->width：640 I/main: avcodec_context->height：352 I/main: avcodec_context->pix_fmt：-1 I/main: 当前解码第1帧 . . . I/main: 当前解码第600帧 yuv文件太大（202.1M），不方便上传。yuv播放： ffplay -f rawvideo -video_size 640x352 /Users/chenchangqing/Documents/code/ffmpeg/resources/test.yuv "},"pages/FFmpeg/FFmpeg音频解码.html":{"url":"pages/FFmpeg/FFmpeg音频解码.html","title":"FFmpeg音频解码","keywords":"","body":"FFmpeg视频解码 代码工程 一、视频解码流程 第一步：注册组件 av_register_all：例如：编码器、解码器等等。 // 第一步：注册组件 av_register_all(); 第二步：打开封装格式 avformat_open_input：例如：打开.mp4、.mov、.wmv文件等等。 // 第二步：打开封装格式 // 参数一：封装格式上下文 // 作用：保存整个视频信息(解码器、编码器等等...) // 信息：码率、帧率等... AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：视频路径 // 在我们iOS里面 // NSString* path = @\"test.mov\"; // const char *url = [path UTF8String] const char *url = env->GetStringUTFChars(in_file_path, NULL); // 参数三：指定输入的格式 // 参数四：设置默认参数 int avformat_open_input_result = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_result != 0){ // 安卓平台下log __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开文件失败\"); // iOS平台下log // NSLog(\"打开文件失败\"); // 不同的平台替换不同平台log日志 return; } 第三步：查找视频基本信息 avformat_find_stream_info：如果是视频解码，那么查找视频流，如果是音频解码，那么就查找音频流。 // 第三步：查找视频流，拿到视频信息 // 参数一：封装格式上下文 // 参数二：指定默认配置 int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result 第四步：查找音频解码器 avcodec_find_decoder：查找解码器。 1. 查找音频流索引位置 // 第四步：查找音频解码器 // 4.1 查找音频流索引位置 int av_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 判断流类型：视频流、音频流、字母流等等... if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_stream_index = i; break; } } 2. 获取解码器上下文 根据音频流索引，获取解码器上下文。 // 4.2 根据音频流索引，获取解码器上下文 AVCodecContext *avcodec_context = avformat_context->streams[av_stream_index]->codec; 3. 获得解码器ID 根据解码器上下文，获得解码器ID，然后查找解码器。 // 4.3 根据解码器上下文，获得解码器ID，然后查找解码器 AVCodec *avcodec = avcodec_find_decoder(avcodec_context->codec_id); 第五步：打开解码器 avcodec_open2：打开解码器。 // 第五步：打开解码器 int avcodec_open2_result = avcodec_open2(avcodec_context, avcodec, NULL); if (avcodec_open2_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开解码器失败\"); return; } // 测试一下 // 打印信息 __android_log_print(ANDROID_LOG_INFO, \"main\", \"解码器名称：%s\", avcodec->name); 第六步：定义类型转换参数 用于swr_convert()，进行音频采样数据转换操作。 1. 创建音频采样数据上下文 // 第六步：定义类型转换参数 // 6.1 创建音频采样数据上下文 // 参数一：音频采样数据上下文 // 上下文：保存音频信息 SwrContext* swr_context = swr_alloc(); // 参数二：输出声道布局类型(立体声、环绕声、机器人等等...) // 立体声 int64_t out_ch_layout = AV_CH_LAYOUT_STEREO; // int out_ch_layout = av_get_default_channel_layout(avcodec_context->channels); // 参数三：输出采样精度（编码） // 例如：采样精度8位 = 1字节，采样精度16位 = 2字节 // 直接指定 // int out_sample_fmt = AV_SAMPLE_FMT_S16; // 动态获取，保持一致 AVSampleFormat out_sample_fmt = avcodec_context->sample_fmt; // 参数四：输出采样率(44100HZ) int out_sample_rate = avcodec_context->sample_rate; // 参数五：输入声道布局类型 int64_t in_ch_layout = av_get_default_channel_layout(avcodec_context->channels); // 参数六：输入采样精度 AVSampleFormat in_sample_fmt = avcodec_context->sample_fmt; // 参数七：输入采样率 int in_sample_rate = avcodec_context->sample_rate; // 参数八：log_offset->log日志，从那里开始统计 int log_offset = 0; // 参数九：log上下文 swr_alloc_set_opts(swr_context, out_ch_layout, out_sample_fmt, out_sample_rate, in_ch_layout, in_sample_fmt, in_sample_rate, log_offset, NULL); // 初始化音频采样数据上下文 swr_init(swr_context); 2. 创建音频压缩数据帧 // 6.2 创建音频压缩数据帧 // 音频压缩数据：acc格式、mp3格式 AVFrame* avframe_in = av_frame_alloc(); // 定义解码结果 int decode_result = 0; 3. 创建音频采样数据帧 // 6.3 创建音频采样数据帧 // 音频采样数据：PCM格式 // 缓冲区大小 = 采样率(44100HZ) * 采样精度(16位 = 2字节) int MAX_AUDIO_SIZE = 44100 * 2; uint8_t *out_buffer = (uint8_t *)av_malloc(MAX_AUDIO_SIZE); 第七步：打开.pcm文件 // 第七步：打开.yuv文件 const char *outfile = env->GetStringUTFChars(out_file_path, NULL); FILE* file_pcm = fopen(outfile, \"wb+\"); if (file_pcm == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"输出文件打开失败\"); return; } 第八步：读取视频压缩数据帧 av_read_frame：读取视频压缩数据帧。 // 第八步：读取视频压缩数据帧 int current_index = 0; // 分析av_read_frame参数。 // 参数一：封装格式上下文 // 参数二：一帧压缩数据 // 如果是解码音频流，是音频压缩帧数据，例如acc、mp3 AVPacket* packet = (AVPacket*)av_malloc(sizeof(AVPacket)); while (av_read_frame(avformat_context, packet) >= 0) { // >=:读取到了 // stream_index == av_stream_index) { // 第九步：开始音频解码 // ... current_index++; __android_log_print(ANDROID_LOG_INFO, \"main\", \"当前解码第%d帧\", current_index); } } 第九步：开始视频解码 注意：代码位置在第八步。 avcodec_send_packet：发送一帧视频压缩数据。 avcodec_receive_frame：解码一帧视频数据。 // 第九步：开始音频解码 // 发送一帧音频压缩数据 avcodec_send_packet(avcodec_context, packet); // 解码一帧视频数据 decode_result = avcodec_receive_frame(avcodec_context, avframe_in); if (decode_result == 0) { // 音频解码成功 // 第十步：开始类型转换 // ... // 第十一步：写入.pcm文件 // ... } 第十步：开始类型转换 注意：代码位置在第九步。 // 第十步：开始类型转换 // 将解码出来的音频数据格式统一转类型为PCM // 参数一：音频采样数据上下文 // 参数二：输出音频采样数据 // 参数三：输出音频采样数据->大小 // 参数四：输入音频采样数据 // 参数五：输入音频采样数据->大小 swr_convert(swr_context, &out_buffer, MAX_AUDIO_SIZE, (const uint8_t **)avframe_in->data, avframe_in->nb_samples); 第十一步：写入.pcm文件 注意：代码位置在第九步。 // 第十一步：写入.pcm文件 // 获取缓冲区实际存储大小 // 参数一：行大小 // 参数二：输出声道数量 int out_nb_channels = av_get_channel_layout_nb_channels(out_ch_layout); // 参数三：输入大小 // 参数四：输出音频采样数据格式 // 参数五：字节对齐方式 int out_buffer_size = av_samples_get_buffer_size(NULL, out_nb_channels, avframe_in->nb_samples, out_sample_fmt, 1); // 写入文件 fwrite(out_buffer, 1, out_buffer_size, file_pcm); 第十二步：释放内存资源，关闭解码器 // 第十二步：释放内存资源，关闭解码器 fclose(file_pcm); av_packet_free(&packet); swr_free(&swr_context); av_free(out_buffer); av_frame_free(&avframe_in); avcodec_close(avcodec_context); avformat_close_input(&avformat_context); 二、新建Android音频解码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程AndroidFFmpegDecodingAudio。 2. 定义java方法 寻找MainActivity：app->src->main->java->MainActivity，增加代码如下： public native void ffmepgDecodeAudio(String inFilePath, String outFilePath); 3. 定义NDK方法 增加android打印。 #include 在native-lib.cpp中，导入FFmpeg头文件。 extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include // 封装格式处理库 #include \"libavformat/avformat.h\" // 工具库 #include \"libavutil/imgutils.h\" // 视频像素数据格式库 #include \"libswscale/swscale.h\" // 音频采样数据格式库 #include \"libswresample/swresample.h\" } 在native-lib.cpp中新增java方法ffmepgDecodeAudio的C++实现，输入MainActivity.就会有代码提示，选择正确ffmepgDecodeAudio方法补全代码。 extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegdecodingaudio_MainActivity_ffmepgDecodeAudio(JNIEnv *env, jobject thiz, jstring in_file_path, jstring out_file_path) { // 这里拷贝上面的音频解码流程的代码即可。 } 三、测试Android音频解码工程 准备视频文件：test.mov 在AndroidManifest.xml增加SD卡的读写权限。 MainActivity增加测试代码。 注意：如果打开失败，可能读写存储设备的权限被禁用。 摩托罗拉·刀锋：设置->应用和通知->高级->权限管理器->隐私相关·读写存储设备->找到应用->如果禁用，则修改为允许。 import android.os.Environment; import java.io.File; import java.io.IOException; import android.util.Log; String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.mov\"); String outFilePath = downloadPath.concat(\"/test.pcm\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmepgDecodeAudio(inFilePath, outFilePath); run工程代码，正确打印，同时正确生成pcm文件。 I/main: 解码器名称：acc I/main: 当前解码第1帧 . . . I/main: 当前解码第502帧 pcm文件音频播放： ffplay -f s16le -ac 2 -ar 44100 /Users/chenchangqing/Downloads/test.pcm "},"pages/FFmpeg/FFmpeg视频编码.html":{"url":"pages/FFmpeg/FFmpeg视频编码.html","title":"FFmpeg视频编码","keywords":"","body":"FFmpeg视频编码 Android代码工程 iOS代码工程 一、视频编码流程 第一步：注册组件 av_register_all：例如：编码器、解码器等等。 // 第一步：注册组件 av_register_all(); 第二步：初始化封装格式上下文 // 第二步：初始化封装格式上下文 AVFormatContext *avformat_context = avformat_alloc_context(); const char *coutFilePath = env->GetStringUTFChars(out_file_path, NULL); // iOS使用 // const char *coutFilePath = [outFilePath UTF8String]; AVOutputFormat *avoutput_format = av_guess_format(NULL, coutFilePath, NULL); // 设置视频压缩数据格式类型(h264、h265、mpeg2等等...) avformat_context->oformat = avoutput_format; 第三步：打开输出文件 // 第三步：打开输出文件 // 参数一：输出流 // 参数二：输出文件 // 参数三：权限->输出到文件中 if (avio_open(&avformat_context->pb, coutFilePath, AVIO_FLAG_WRITE) 第四步：创建输出码流 // 第四步：创建输出码流 // 注意：创建了一块内存空间，并不知道他是什么类型流，希望他是视频流 AVStream *av_video_stream = avformat_new_stream(avformat_context, NULL); 第五步：初始化编码器上下文 1. 获取编码器上下文 // 5.1 获取编码器上下文 AVCodecContext *avcodec_context = av_video_stream->codec; 2. 设置视频编码器ID // 5.2 设置视频编码器ID avcodec_context->codec_id = avoutput_format->video_codec; 3. 设置为视频编码器 // 5.3 设置为视频编码器 avcodec_context->codec_type = AVMEDIA_TYPE_VIDEO; 4. 设置像素数据格式 // 5.4 设置像素数据格式 // 编码的是像素数据格式，视频像素数据格式为YUV420P(YUV422P、YUV444P等等...) // 注意：这个类型是根据你解码的时候指定的解码的视频像素数据格式类型 avcodec_context->pix_fmt = AV_PIX_FMT_YUV420P; 5. 设置视频尺寸 // 5.5 设置视频尺寸 avcodec_context->width = 640; avcodec_context->height = 352; 6. 设置视频帧率 // 5.6 设置视频帧率 // 视频帧率：25fps（每秒25帧） // 单位：fps，\"f\"表示帧数，\"ps\"表示每秒 avcodec_context->time_base.num = 1; avcodec_context->time_base.den = 25; 7. 设置视频码率 // 5.7 设置视频码率 //（1）什么是码率？ // 含义：每秒传送的比特(bit)数单位为 bps(Bit Per Second)，比特率越高，传送数据速度越快。 // 单位：bps，\"b\"表示数据量，\"ps\"表示每秒。 //（2）什么是视频码率? // 含义：视频码率就是数据传输时单位时间传送的数据位数，一般我们用的单位是kbps即千位每秒。 //（3）视频码率计算如下？ // 基本的算法是：码率（kbps）= 视频大小 - 音频大小（bit位）/ 时间（秒）。 // 例如：Test.mov时间 = 24秒，文件大小（视频+音频） = 1.73MB。 // 视频大小 = 1.34MB（文件占比：77%）= 1.34MB * 1024 * 1024 * 8 / 24 = 字节大小 = 468365字节 = 468Kbps。 // 音频大小 = 376KB（文件占比：21%）。 // 计算出来的码率 : 468Kbps，K表示1000，b表示位（bit）。 // 总结：码率越大，视频越大。 avcodec_context->bit_rate = 468000; 8. 设置GOP // 5.8 设置GOP // 影响到视频质量问题，是一组连续画面 //（1）MPEG格式画面类型 // 3种类型：I帧、P帧、B帧。 //（2）I帧： // 内部编码帧，是原始帧（原始视频数据） // 是完整画面，是关键帧（必需的有，如果没有I，那么你无法进行编码，解码）。 // 视频第1帧：视频序列中的第一个帧始终都是I帧，因为它是关键帧。 //（3）P帧 // 向前预测帧 // 预测前面的一帧类型，处理前面的一阵数据(->I帧、B帧)。 // P帧数据根据前面的一帧数据进行处理得到了P帧。 //（4）B帧 // 前后预测帧（双向预测帧），前面一帧和后面一帧的差别。 // B帧压缩率高，但是对解码性能要求较高。 //（5）总结 // I只需要考虑自己 = 1帧，P帧考虑自己+前面一帧 = 2帧，B帧考虑自己+前后帧 = 3帧 // 说白了，P帧和B帧是对I帧压缩。 // 每250帧，插入1个I帧，I帧越少，视频越小 avcodec_context->gop_size = 250; 9. 设置量化参数 // 5.9 设置量化参数 // 数学算法（高级算法），量化系数越小，视频越是清晰 // 一般情况下都是默认值，最小量化系数默认值是10，最大量化系数默认值是51 avcodec_context->qmin = 10; avcodec_context->qmax = 51; 10. 设置b帧最大值 // 5.10 设置b帧最大值 // 设置不需要B帧 avcodec_context->max_b_frames = 0; 第六步：查找视频编码器 // 第六步：查找视频编码器 AVCodec *avcodec = avcodec_find_encoder(avcodec_context->codec_id); if (avcodec == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"找不到编码器\"); // iOS使用 // NSLog(@\"找不到编码器\"); return; } __android_log_print(ANDROID_LOG_INFO, \"main\", \"编码器名称为：%s\", avcodec->name); // iOS使用 // NSLog(@\"编码器名称为：%s\", avcodec->name); 1. 出现问题 新建测试工程（稍后会介绍建工程测试），代码运行到这一步会出现“找不到编码器”，因为编译库没有依赖x264库（默认情况下FFmpeg没有编译进行h264库）。 2. 解决问题 (1) 下载源码 x264库，翻墙更快。 git clone https://code.videolan.org/videolan/x264.git (2) 下载ndk https://developer.android.google.cn/ndk/downloads/older_releases.html 我这里使用的是ndkr10e。 在x264源码的同目录下新建ndk文件交，将下载好的ndk放入ndk文件夹。 (3) 编译x264脚本 编译x264的.a静态库，指定编译平台类型：iOS平台、安卓平台、Mac平台、Windows平台等等。 android_build_x264.sh是编译脚本，将编译脚本放在和源码的同一目录，执行： sh android-build-x264.sh 执行过程会提示开机密码，看到Android h264 builds finished说明编译成功。 (4) 编译FFmpeg 修改Android的FFmpeg动态库编译脚本，将x264库其编译进去。android-build-ffmpeg.sh是原来的编译脚本，在原来的编译脚本./configure增加如下选项。 # 以下是编译x264库增加的 # 禁用所有编码器 --disable-encoders \\ # 通过libx264库启用H.264编码 --enable-libx264 \\ # 启用编码器名称 --enable-encoder=libx264 \\ # 启用几个图片编码，由于生成视频预览 --enable-encoder=mjpeg \\ --enable-encoder=png \\ #和FFmpeg动态库一起编译，指定你之前编译好的x264静态库和头文件 --extra-cflags=\"-I/Users/chenchangqing/Documents/code/ffmpeg/06_ffmpeg_video_encoding/android_build_x264/include\" \\ --extra-ldflags=\"-L/Users/chenchangqing/Documents/code/ffmpeg/06_ffmpeg_video_encoding/android_build_x264/lib\" \\ android-build-ffmpeg-x264.sh是修改后的脚本，再次编译FFmpeg库，重新生成.so动态库。 重新编译，发现错误： libavcodec/libx264.c: In function 'X264_frame': libavcodec/libx264.c:282:9: error: 'x264_bit_depth' undeclared (first use in this function) if (x264_bit_depth > 8) ^ libavcodec/libx264.c:282:9: note: each undeclared identifier is reported only once for each function it appears in libavcodec/libx264.c: In function 'X264_init_static': libavcodec/libx264.c:892:9: error: 'x264_bit_depth' undeclared (first use in this function) if (x264_bit_depth == 8) ^ make: *** [libavcodec/libx264.o] Error 1 查询资料（“x264_bit_depth”未声明），是因为ffmpeg和x264不兼容，这里不使用最新版本的x264，尝试另一个版本的x264，重新编译，再重新生成.so动态库。 再次运行测试工程，成功输出： I/main: 编码器名称为：libx264 问题解决。 3. 解决问题（iOS） 这个问题在iOS上也是存在的，这里也列出解决步骤。 (1) 下载源码 x264库，翻墙更快。 git clone https://code.videolan.org/videolan/x264.git (2) 编译x264脚本 ios-build-x264.sh是编译脚本，将编译脚本放在和源码的同一目录，执行： sh ios-build-x264.sh 注意：如果使用旧版本的x264，比如这个x264，会出现下面的问题，所以我这里使用的当时最新的x264。 Out of tree builds are impossible with config.h/x264_config.h in source dir. (3) 编译FFmpeg 修改iOS的FFmpeg库编译脚本，将x264库其编译进去。ios-build-ffmpeg.sh是原来的编译脚本，在原来的编译脚本./configure增加如下选项。 # 以下是编译x264库增加的 --enable-gpl \\ --disable-encoders \\ --enable-libx264 \\ --enable-encoder=libx264 \\ --enable-encoder=mjpeg \\ --enable-encoder=png \\ --extra-cflags=\"-I/Users/yangshaohong/Desktop/ffmpeg-test/test/thin-x264/arm64/include\" \\ --extra-ldflags=\"-L/Users/yangshaohong/Desktop/ffmpeg-test/test/thin-x264/arm64/lib\" \\ ios-build-ffmpeg-x264.sh是修改后的脚本，再次编译FFmpeg库，重新生成.a静态库。 用了最新的x264，还是出现了问题： src/libavcodec/libx264.c:282:9: error: use of undeclared identifier 'x264_bit_depth' if (x264_bit_depth > 8) ^ src/libavcodec/libx264.c:892:9: error: use of undeclared identifier 'x264_bit_depth' if (x264_bit_depth == 8) ^ src/libavcodec/libx264.c:894:14: error: use of undeclared identifier 'x264_bit_depth' else if (x264_bit_depth == 9) ^ src/libavcodec/libx264.c:896:14: error: use of undeclared identifier 'x264_bit_depth' else if (x264_bit_depth == 10) ^ 4 errors generated. make: *** [libavcodec/libx264.o] Error 1 make: *** Waiting for unfinished jobs.... 查询资料（“x264_bit_depth”未声明），是因为ffmpeg和x264不兼容，这里不使用最新版本的x264，尝试另一个版本的x264-snapshot-20180730-2245-stable.tar.bz2，重新编译，成功生成了支持h264编码的FFmpeg静态库。 注意：这里x264和ffmpeg都指定了arm64的架构。 第七步：打开视频编码器 注意：代码中的“优化步骤”是必须的，要不然编码过程会有坑。 // 第七步：打开视频编码器 // 以下是编码优化步骤，必须有，要不然编码会出问题 // 编码延时问题，编码选项->编码设置 AVDictionary *param = 0; if (avcodec_context->codec_id == AV_CODEC_ID_H264) { // 需要查看x264源码->x264.c文件 // 第一个值：预备参数 // key: preset // value: slow->慢 // value: superfast->超快 av_dict_set(&param, \"preset\", \"slow\", 0); // 第二个值：调优 // key: tune->调优 // value: zerolatency->零延迟 av_dict_set(&param, \"tune\", \"zerolatency\", 0); } // 打开编码器 if (avcodec_open2(avcodec_context, avcodec, &param) 第八步：写入文件头信息 // 第八步：写入文件头信息 avformat_write_header(avformat_context, NULL); 第九步：打开yuv文件 // 第九步：打开yuv文件 // 遇到问题：fopen Permission denied const char *cinFilePath = env->GetStringUTFChars(in_file_path, NULL); // iOS使用 // const char *cinFilePath = [inFilePath UTF8String]; int errNum = 0; FILE *in_file = fopen(cinFilePath, \"rb\"); if (in_file == NULL) { errNum = errno; __android_log_print(ANDROID_LOG_INFO, \"main\", \"文件不存在:%s,in_file:%s,errNum:%d,reason:%s\", cinFilePath, in_file, errNum, strerror(errNum)); // iOS使用 // NSLog(@\"文件不存在\"); return; } 这一步有坑，打开yuv文件（fopen）一直出现“Permission denied”错误，困扰了有一天，最后还是没有找到很好的办法，但是有个临时解决办法，就是先执行视频解码为.yuv文件，这个时候去打开（fopen）刚生成的.yuv文件，是可以成功的。 第十步：视频编码准备 // 第十步：视频编码准备 // 10.1 创建视频原始数据帧 // 作用：存储视频原始数据帧 AVFrame *av_frame = av_frame_alloc(); // 10.2 创建一个缓冲区 // 作用：用于缓存读取视频数据 // 先获取缓冲区大小 int buffer_size = av_image_get_buffer_size(avcodec_context->pix_fmt, avcodec_context->width, avcodec_context->height, 1); // 创建一个缓冲区，作用是缓存一帧视频像素数据 uint8_t *out_buffer = (uint8_t *) av_malloc(buffer_size); // 10.3 填充视频原始数据帧 av_image_fill_arrays(av_frame->data, av_frame->linesize, out_buffer, avcodec_context->pix_fmt, avcodec_context->width, avcodec_context->height, 1); // 10.4 创建压缩数据帧数据 // 作用：接收压缩数据帧数据 AVPacket *av_packet = (AVPacket *) av_malloc(buffer_size); 第十一步：循环读取视频像素数据 视频编码读取视频像素数据问题分析？ 答案如下： 比例规范：y : u : v = 4 : 1 : 1 然后规范：y = width（视频宽）* height（高） 假设：width = 100，height = 10 所以：y = width * height = 1000 所以：u = y / 4 = 1000 / 4 = 250，v = y / 4 = 1000 / 4 = 250 也就是说：一帧yuv大小 = 1500 编码的时候读取一帧数据：fread(out_buffer, 1, y_size * 3 / 2, in_file) y_size * 3 / 2 = 1000 * 3 / 2 = 1500 代码：av_frame->data[0] = out_buffer 解释：指针是从out_buffer = 0开始，所以data[0]读取范围：0-1000 代码：av_frame->data[1] = out_buffer + y_size 解释：指针是从out_buffer + y_size = 0 + 1000 = 1000开始，所以data[1]读取范围：1000-1250 代码：av_frame->data[2] = out_buffer + y_size * 5 / 4 解释：指针是从out_buffer + y_size * 5 / 4 = 0 + 1000 * 5 / 4 = 1250开始，所以data[2]读取范围：1250-1500 一帧数据->大小 = Y大小 + U大小 + V大小 假设：width = 100，height = 10 Y大小：y = width * height = 100 * 10 = 1000 U大小：u = y / 4 = 1000 / 4 = 250 V大小：v = y / 4 = 1000 / 4 = 250 一帧数据大小 = Y + U + V = 1500 视频解码计算->指针位移处理 保存Y大小： fwrite(avframe_yuv420p->data[0], 1, y_size, file_yuv420p); avframe_yuv420p->data[0]->表示Y值 读取：0->1000 保存U大小 fwrite(avframe_yuv420p->data[1], 1, u_size, file_yuv420p); avframe_yuv420p->data[1]->表示U值 读取：1000->1250 保存V大小 fwrite(avframe_yuv420p->data[2], 1, v_size, file_yuv420p); avframe_yuv420p->data[2]->表示V值 读取：1250->1500 视频编码计算->指针位移计算 分析读取数据大小？ y = 1000 数据大小 = 一帧YUV数据 = Y + U + V = 1500 数据大小 = y * 3 / 2 = 1000 * 3 / 2 = 1500 现在我们视频编码根据Y大小，求出YUV大小计算公式 out_buffer = 1500（总的数据量） 保存Y大小 av_frame->data[0] = out_buffer; 读取Y数据->1000 读取：0->1000 保存U大小 av_frame->data[1] = out_buffer + y_size; 读取U数据->250 读取：0 + 1000 -> 1250 保存V大小 av_frame->data[2] = out_buffer + y_size * 5 / 4; 读取V数据->250 读取：0 + 1000 * 5 / 4 = 1250->1500 说白了：通过Y值得到V读取起点位置 // 第十一步：循环读取视频像素数据 // 编码是否成功 int result = 0; int current_frame_index = 1; int i = 0; // 计算y的大小 int y_size = avcodec_context->width * avcodec_context->height; while (true) { // 从yuv文件里面读取缓冲区 // 读取大小：y_size * 3 / 2 if (fread(out_buffer, 1, y_size * 3 / 2, in_file) data[0] = out_buffer; // U值 av_frame->data[1] = out_buffer + y_size; // V值 av_frame->data[2] = out_buffer + y_size * 5 / 4; // 帧数 av_frame->pts = i; // 注意时间戳 i++; // 第十二步：视频编码处理 // ... current_frame_index++; } 第十二步：视频编码处理 代码位置在第十一步。 // 第十二步：视频编码处理 // 发送一帧视频像素数据 avcodec_send_frame(avcodec_context, av_frame); // 接收一帧视频像素数据，编码为视频压缩数据格式 result = avcodec_receive_packet(avcodec_context, av_packet); // 判定是否编码成功 if (result == 0) { // 编码成功 // 第十三步：将视频压缩数据写入到输出文件中 // ... } else { __android_log_print(ANDROID_LOG_INFO, \"main\", \"编码第%d帧失败2\", current_frame_index); // iOS使用 // NSLog(@\"编码第%d帧失败2\", current_frame_index); return; } 第十三步：将视频压缩数据写入到输出文件中 代码位置在第十二步。 // 第十三步：将视频压缩数据写入到输出文件中 av_packet->stream_index = av_video_stream->index; result = av_write_frame(avformat_context, av_packet); current_frame_index++; // 是否输出成功 if (result 第十四步：写入剩余帧数据 // 第十四步：写入剩余帧数据 // 作用：输出编码器中剩余AVPacket，可能没有 flush_encoder(avformat_context, 0); 第十五步：写入文件尾部信息 // 第十五步：写入文件尾部信息 av_write_trailer(avformat_context); 第十六步：释放内存，关闭编码器 // 第十六步：释放内存，关闭编码器 avcodec_close(avcodec_context); av_free(av_frame); av_free(out_buffer); av_packet_free(&av_packet); avio_close(avformat_context->pb); avformat_free_context(avformat_context); fclose(in_file); 二、新建Android视频编码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程AndroidFFmpegEncodingVideo。 2. 定义java方法 寻找MainActivity：app->src->main->java->MainActivity，增加代码如下： public native void ffmpegVideoEncode(String inFilePath, String outFilePath); public native void ffmepgDecodeVideo(String inFilePath, String outFilePath); 3. 定义NDK方法 增加android打印。 #include 在native-lib.cpp中，导入FFmpeg头文件。 extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include // 封装格式处理库 #include \"libavformat/avformat.h\" // 工具库 #include \"libavutil/imgutils.h\" // 视频像素数据格式库 #include \"libswscale/swscale.h\"} 在native-lib.cpp中新增java方法flush_encoder、ffmepgVideoEncode、ffmepgDecodeVideo的C++实现，输入MainActivity.就会有代码提示，选择正确ffmepgVideoEncode方法补全代码。 ffmepgDecodeVideo方法实现参考FFmpeg视频解码。 int flush_encoder(AVFormatContext *fmt_ctx, unsigned int stream_index) { int ret; int got_frame; AVPacket enc_pkt; if (!(fmt_ctx->streams[stream_index]->codec->codec->capabilities & CODEC_CAP_DELAY)) return 0; while (1) { enc_pkt.data = NULL; enc_pkt.size = 0; av_init_packet(&enc_pkt); ret = avcodec_encode_video2(fmt_ctx->streams[stream_index]->codec, &enc_pkt, NULL, &got_frame); av_frame_free(NULL); if (ret 三、测试Android视频编码工程 准备视频文件：test.mov 在AndroidManifest.xml增加SD卡的读写权限。 MainActivity增加测试代码，这里先进行视频解码，生成的.yuv文件后，直接对.yuv文件再进行编码。 注意：如果打开失败，可能读写存储设备的权限被禁用。 摩托罗拉·刀锋：设置->应用和通知->高级->权限管理器->隐私相关·读写存储设备->找到应用->如果禁用，则修改为允许。 import android.os.Environment; import java.io.File; import java.io.IOException; import android.util.Log; private void ffmpegVideoEncode() { String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.yuv\"); String outFilePath = downloadPath.concat(\"/test.h264\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmpegVideoEncode(inFilePath, outFilePath); } private void ffmepgDecodeVideo() { String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.mov\"); String outFilePath = downloadPath.concat(\"/test.yuv\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()){ Log.i(\"日志：\",\"存在\"); }else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmepgDecodeVideo(inFilePath, outFilePath); } ffmepgDecodeVideo(); ffmpegVideoEncode(); run工程代码，正确打印，同时正确生成.h264文件。 I/日志：: 存在 I/main: 解码器名称：h264 I/main: 当前解码第1帧 . . . I/main: 当前解码第600帧 I/日志：: 存在 I/main: 编码器名称为：libx264 I/main: 编码第1帧成功 . . . I/main: 编码第598帧成功 读取完毕... h264文件播放： ffplay test.h264 四、新建iOS视频编码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程iOSFFmpegEncodingVideo。 注意：工程使用的是支持h264编码的FFmpeg库文件。 2. 导入资源文件 资源文件就是视频解码后的.yuv文件。先将.yuv文件拷贝至工程目录下，再通过add files的方式加入工程。 3. 导入x264静态库 在工程目录新建x264，拷贝编译好的thin-x264文件夹至x264目录，只保留arm64的文件夹，删除lib文件夹中的pkgconfig，再通过add files的方式加入工程。 配置x264头文件，参考FFmpeg集成。 4. 增加视频编码方法 (1) 导入FFmpeg头文件 修改FFmpegTest.h，新增如下： //核心库 #include \"libavcodec/avcodec.h\" //封装格式处理库 #include \"libavformat/avformat.h\" //工具库 #include \"libavutil/imgutils.h\" (2) 新增视频编码方法 修改FFmpegTest.h，新增如下： /// FFmpeg视频编码 + (void)ffmpegVideoEncode:(NSString*)filePath outFilePath:(NSString*)outFilePath; 修改FFmpegTest.m，新增如下： int flush_encoder(AVFormatContext *fmt_ctx, unsigned int stream_index) { int ret; int got_frame; AVPacket enc_pkt; if (!(fmt_ctx->streams[stream_index]->codec->codec->capabilities & CODEC_CAP_DELAY)) return 0; while (1) { enc_pkt.data = NULL; enc_pkt.size = 0; av_init_packet(&enc_pkt); ret = avcodec_encode_video2(fmt_ctx->streams[stream_index]->codec, &enc_pkt, NULL, &got_frame); av_frame_free(NULL); if (ret (3) 增加方法测试 修改ViewController.m，新增测试代码如下： NSString* inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"yuv\"]; NSArray* paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES); NSString* path = [paths objectAtIndex:0]; NSString* tmpPath = [path stringByAppendingPathComponent:@\"temp\"]; [[NSFileManager defaultManager] createDirectoryAtPath:tmpPath withIntermediateDirectories:YES attributes:nil error:NULL]; NSString* outFilePath = [tmpPath stringByAppendingPathComponent:[NSString stringWithFormat:@\"test.h264\"]]; [FFmpegTest ffmpegVideoEncode:inPath outFilePath:outFilePath]; run工程代码，正确打印，同时正确生成.h264文件。 iOSFFmpegEncodingVideo[828:210395] 编码器名称为：libx264 [libx264 @ 0x107021e00] using cpu capabilities: ARMv8 NEON [libx264 @ 0x107021e00] profile High, level 3.0 [h264 @ 0x10701c200] Using AVStream.codec.time_base as a timebase hint to the muxer is deprecated. Set AVStream.time_base instead. [h264 @ 0x10701c200] Using AVStream.codec to pass codec parameters to muxers is deprecated, use AVStream.codecpar instead. [libx264 @ 0x107021e00] AVFrame.format is not set [libx264 @ 0x107021e00] AVFrame.width or height is not set 2022-04-23 00:16:34.170713+0800 iOSFFmpegEncodingVideo[828:210395] 编码第1帧成功 [libx264 @ 0x107021e00] AVFrame.format is not set [libx264 @ 0x107021e00] AVFrame.width or height is not set 2022-04-23 00:16:34.175975+0800 iOSFFmpegEncodingVideo[828:210395] 编码第2帧成功 [libx264 @ 0x107021e00] AVFrame.format is not set [libx264 @ 0x107021e00] AVFrame.width or height is not set . . . 2022-04-23 00:16:39.831069+0800 iOSFFmpegEncodingVideo[828:210395] 编码第598帧成功 2022-04-23 00:16:39.831365+0800 iOSFFmpegEncodingVideo[828:210395] 读取完毕... [libx264 @ 0x107021e00] frame I:3 Avg QP:24.64 size: 20162 [libx264 @ 0x107021e00] frame P:595 Avg QP:25.38 size: 2245 [libx264 @ 0x107021e00] mb I I16..4: 13.5% 50.9% 35.6% [libx264 @ 0x107021e00] mb P I16..4: 0.3% 0.5% 0.4% P16..4: 40.8% 12.2% 4.0% 0.0% 0.0% skip:41.7% [libx264 @ 0x107021e00] final ratefactor: 23.64 [libx264 @ 0x107021e00] 8x8 transform intra:43.8% inter:54.5% [libx264 @ 0x107021e00] coded y,uvDC,uvAC intra: 64.8% 75.5% 30.8% inter: 9.9% 13.2% 0.3% [libx264 @ 0x107021e00] i16 v,h,dc,p: 21% 21% 7% 52% [libx264 @ 0x107021e00] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 15% 13% 8% 8% 10% 11% 11% 10% 14% [libx264 @ 0x107021e00] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 15% 8% 9% 10% 11% 11% 8% 11% [libx264 @ 0x107021e00] i8c dc,h,v,p: 46% 21% 24% 10% [libx264 @ 0x107021e00] Weighted P-Frames: Y:0.3% UV:0.2% [libx264 @ 0x107021e00] kb/s:466.92 "},"pages/FFmpeg/FFmpeg音频编码.html":{"url":"pages/FFmpeg/FFmpeg音频编码.html","title":"FFmpeg音频编码","keywords":"","body":"FFmpeg音频编码 Android代码工程 iOS代码工程 一、音频编码流程 第一步：注册组件 av_register_all：例如：编码器、解码器等等。 // 第一步：注册组件 av_register_all(); 第二步：初始化封装格式上下文 // 第二步：初始化封装格式上下文 AVFormatContext *avformat_context = avformat_alloc_context(); const char *coutFilePath = env->GetStringUTFChars(out_file_path, NULL); // iOS使用 // const char *coutFilePath = [outFilePath UTF8String]; AVOutputFormat *avoutput_format = av_guess_format(NULL, coutFilePath, NULL); // 设置音频压缩数据格式类型(aac、mp3等等...) avformat_context->oformat = avoutput_format; 第三步：打开输出文件 // 第三步：打开输出文件 // 参数一：输出流 // 参数二：输出文件 // 参数三：权限->输出到文件中 if (avio_open(&avformat_context->pb, coutFilePath, AVIO_FLAG_WRITE) 第四步：创建输出码流 // 第四步：创建输出码流 // 注意：创建了一块内存空间，并不知道他是什么类型流，希望他是音频流 AVStream *av_audio_stream = avformat_new_stream(avformat_context, NULL); 第五步：初始化编码器上下文 1. 获取编码器上下文 // 第五步：初始化编码器上下文 // 5.1 获取编码器上下文 AVCodecContext *avcodec_context = av_audio_stream->codec; 2. 设置音频编码器ID // 5.2 设置音频编码器ID avcodec_context->codec_id = avoutput_format->audio_codec; run工程时，这一步出现了问题： A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x10 in tid 13086 (egencodingaudio), pid 13086 (egencodingaudio) 困挠了好几个小时，终于找到原因了，我把编码音频的输出文件后缀写成了acc，将后缀改成aac就解决了。 // 错误 String outFilePath = downloadPath.concat(\"/test.acc\"); // 正确 String outFilePath = downloadPath.concat(\"/test.aac\"); 3. 设置为音频编码器 // 5.3 设置为视频编码器 avcodec_context->codec_type = AVMEDIA_TYPE_AUDIO; 4. 设置音频数据格式等 // 5.4 设置像素数据格式 // 编码的是音频采样数据格式，视频像素数据格式为PCM // 注意：这个类型是根据你解码的时候指定的解码的音频采样数据格式类型 avcodec_context->sample_fmt = AV_SAMPLE_FMT_S16; // 设置采样率 avcodec_context->sample_rate = 44100; // 立体声 avcodec_context->channel_layout = AV_CH_LAYOUT_STEREO; // 声道数量 int channels = av_get_channel_layout_nb_channels(avcodec_context->channel_layout); avcodec_context->channels = channels; // 设置码率 // 基本的算法是：【码率】(kbps)=【视频大小 - 音频大小】(bit位) /【时间】(秒) avcodec_context->bit_rate = 128000; 第六步：查找音频编码器 // 第六步：查找音频编码器 AVCodec *avcodec = avcodec_find_encoder(avcodec_context->codec_id); if (avcodec == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"找不到编码器\"); // iOS使用 // NSLog(@\"找不到编码器\"); return; } __android_log_print(ANDROID_LOG_INFO, \"main\", \"编码器名称为：%s\", avcodec->name); // iOS使用 // NSLog(@\"编码器名称为：%s\", avcodec->name); 第七步：打开音频编码器 // 第七步：打开音频编码器 // 打开编码器 if (avcodec_open2(avcodec_context, avcodec, NULL) 1. 出现问题 新建测试工程（稍后会介绍建工程测试），代码运行到这一步会出现“打开编码器失败”，因为虽然找到了aac编码器，但是无法打开。 I/main: 编码器名称为：aac I/main: 打开编码器失败 2. 查找原因 下面我们通过打印错误日志定位错误原因。 修改MainActivity.java，新增方法： private void createAVLogFile() { // String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String logFilePath = downloadPath.concat(\"/av_log.txt\"); // 文件不存在我创建一个文件 File file = new File(logFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } } 修改native-lib.cpp，新增方法： // 这个函数作用：统计程序报错信息，FFmpeg报错信息打印到av_log文件中 // 我们将av_log保存到了sdcard（外部存储） // 或者你打印到控制台也可以，这里我们将错误信息打印到文件中 void custom_log(void *ptr, int level, const char* fmt, va_list vl) { __android_log_print(ANDROID_LOG_INFO, \"main\", fmt, vl); // 由于权限问题，还是无法将日志打印至sdcard，临时解决方案就是每次都先创建一个新的av_log.txt。 FILE *fp=fopen(\"/storage/emulated/0/Download/av_log.txt\",\"a+\"); if(fp){ vfprintf(fp,fmt,vl); fflush(fp); fclose(fp); } } 修改第七步，这样就可以打印日志至av_log.txt。 // 第七步：打开音频编码器 // 设置log错误监听函数 av_log_set_callback(custom_log); // 打开编码器 if (avcodec_open2(avcodec_context, avcodec, NULL) run测试工程，查看av_log.txt，发现编码器有问题，不支持s16（AV_SAMPLE_FMT_S16）格式。 Specified sample format s16 is invalid or not supported 3. 解决问题 通过分析，我们发现编码器有问题，那么我们需要换一个编码器。老得FFmpeg框架里面支持faac格式，新的FFmpeg框架里面fdk_aac格式。 faac和fdk_aac区别：fdk_aac编码出来音频质量高，占用内存少。 这里我们需要更换编码器为libfdk_aac，fdk_aac同时也支持s16（AV_SAMPLE_FMT_S16）格式。 (1) 下载源码 fdk-aac。 我使用的是fdk-aac-0.1.4.zip。 注意：编译0.1.5是有问题。 (2) 下载ndk https://developer.android.google.cn/ndk/downloads/older_releases.html 我这里使用的是ndkr10e。 在fdkaac源码的同目录下新建ndk文件交，将下载好的ndk放入ndk文件夹。 (3) 编译fdk-aac 编译fdk-aac的.a静态库。 android-build-fdkaac.sh是编译脚本，将编译脚本放在和源码的同一目录，执行： sh android-build-fdkaac.sh 执行过程会提示开机密码，看到Android aac builds finished说明编译成功。 (4) 编译FFmpeg 修改Android的FFmpeg动态库编译脚本，将fdkaac库其编译进去。android-build-ffmpeg.sh是原来的编译脚本，在原来的编译脚本./configure增加如下选项。 # 以下是编译fdkaac库增加的 # 禁用所有编码器 --disable-encoders \\ --enable-libfdk-aac \\ --enable-encoder=libfdk_aac \\ --enable-decoder=libfdk_aac \\ # 和FFmpeg动态库一起编译，指定你之前编译好的fdkaac静态库和头文件 --extra-cflags=\"-I/Users/chenchangqing/Documents/code/ffmpeg/07_ffmpeg_audio_encoding/android_build_fdkaac/include\" \\ --extra-ldflags=\"-L/Users/chenchangqing/Documents/code/ffmpeg/07_ffmpeg_audio_encoding/android_build_fdkaac/lib\" \\ android-build-ffmpeg-fdkaac.sh是修改后的脚本，再次编译FFmpeg库，重新生成.so动态库。 重新编译，发现错误，删除--enable-gpl \\。 libfdk_aac is incompatible with the gpl and --enable-nonfree is not specified. 查看ffmpeg-3.4/ffbuild/config.log重新编译，发现错误： /var/folders/vx/w486nkxn1dx05w199n5dl76m0000gn/T//ffconf.C7cXMk2x/test.o:test.c:function check_aacEncOpen: error: undefined reference to 'aacEncOpen' collect2: error: ld returned 1 exit status ERROR: libfdk_aac not found 哈哈，最后解决方案还是让我找到了，又耗费了几个小时，资料在这mac下编译android下aac,不愿孤独-Mac 上用NDK编译lib库的问题 no archive symbol table (run ran lib)...。 解决方案是，手动调用$NDK_HOME/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi-runlib，对生成的.a进行接口的导出。 ./arm-linux-androideabi-runlib libfdk-aac.a 经过这么一步，就可以顺利的执行支持fdk-aac的FFmpeg脚本（android-build-ffmpeg-fdkaac.sh）了。 注意：这里有个细节，在fdk-aac编译后的安装目录执行ranlib命令是无效的，所以我新建了android_build_fdkaac2文件夹，将lib和include文件夹复制进来，在执行ranlib命令就可以了，编译ffmpeg时指定fdk-aac的目录为android_build_fdkaac2即可。 4. 解决问题（iOS） 这个问题在iOS上也是存在的，这里也列出解决步骤。 (1) 下载源码 fdk-aac。 我使用的是fdk-aac-0.1.4.zip。 注意：编译0.1.5是有问题。 (2) 编译fdk-aac ios-build-fdkaac.sh是编译脚本，将编译脚本放在和源码的同一目录，执行： sh ios-build-fdkaac.sh 出现错误： configure: error: source directory already configured; run \"make distclean\" there first make: *** No rule to make target `install'. Stop. 根据提示，执行make distclean可以解决。 (3) 编译FFmpeg 修改iOS的FFmpeg库编译脚本，将fdkaac库其编译进去。ios-build-ffmpeg.sh是原来的编译脚本，在原来的编译脚本./configure增加如下选项。 # 以下是编译fdkaac库增加的 # 禁用所有编码器 --disable-encoders \\ --enable-libfdk-aac \\ --enable-encoder=libfdk_aac \\ --enable-decoder=libfdk_aac \\ # 和FFmpeg动态库一起编译，指定你之前编译好的fdkaac静态库和头文件 --extra-cflags=\"-I/Users/chenchangqing/Documents/code/ffmpeg/07_ffmpeg_audio_encoding/ios_build_fdkaac/include\" \\ --extra-ldflags=\"-L/Users/chenchangqing/Documents/code/ffmpeg/07_ffmpeg_audio_encoding/ios_build_fdkaac/lib\" \\ ios-build-ffmpeg-fdkaac.sh是修改后的脚本，再次编译FFmpeg库，重新生成.a静态库。 sh ios-build-ffmpeg-fdkaac.sh arm64 出现下面的错误，重新下载FFmpeg就可以解决了。 Out of tree builds are impossible with config.h in source dir. 注意：这里fdkaac和ffmpeg都指定了arm64的架构。 5. 使用fdk-aac编码器 // 错误 // AVCodec *avcodec = avcodec_find_encoder(avcodec_context->codec_id); // 正确 AVCodec *avcodec = avcodec_find_encoder_by_name(\"libfdk_aac\"); run工程，正常打开编码器。 I/main: 编码器名称为：libfdk_aac 第八步：写入文件头信息 // 第八步：写入文件头信息 avformat_write_header(avformat_context, NULL); 第九步：打开pcm文件 // 第九步：打开pcm文件 // 遇到问题：fopen Permission denied const char *cinFilePath = env->GetStringUTFChars(in_file_path, NULL); // iOS使用 // const char *cinFilePath = [inFilePath UTF8String]; int errNum = 0; FILE *in_file = fopen(cinFilePath, \"rb\"); if (in_file == NULL) { errNum = errno; __android_log_print(ANDROID_LOG_INFO, \"main\", \"文件不存在:%s,in_file:%s,errNum:%d,reason:%s\", cinFilePath, in_file, errNum, strerror(errNum)); // iOS使用 // NSLog(@\"文件不存在\"); return; } 这一步有坑，打开pcm文件（fopen）一直出现“Permission denied”错误，困扰了有一天，最后还是没有找到很好的办法，但是有个临时解决办法，就是先执行音频解码为.pcm文件，这个时候去打开（fopen）刚生成的.pcm文件，是可以成功的。 第十步：音频编码准备 // 第十步：音频编码准备 // 10.1 创建音频原始数据帧 // 作用：存储音频原始数据帧 AVFrame *av_frame = av_frame_alloc(); av_frame->nb_samples = avcodec_context->frame_size; av_frame->format = avcodec_context->sample_fmt; // 10.2 创建一个缓冲区 // 作用：用于缓存读取音频数据 // 先获取缓冲区大小 int buffer_size = av_samples_get_buffer_size(NULL, avcodec_context->channels, avcodec_context->frame_size, avcodec_context->sample_fmt, 1); // 创建一个缓冲区，作用是缓存一帧音频像素数据 uint8_t *out_buffer = (uint8_t *) av_malloc(buffer_size); // 10.3 填充音频原始数据帧 avcodec_fill_audio_frame(av_frame, avcodec_context->channels, avcodec_context->sample_fmt, (const uint8_t *)out_buffer, buffer_size, 1); // 10.4 创建压缩数据帧数据 // 作用：接收压缩数据帧数据 AVPacket *av_packet = (AVPacket *) av_malloc(buffer_size); 第十一步：循环读取视频像素数据 // 第十一步：循环读取音频数据 // 编码是否成功 int result = 0; int current_frame_index = 1; int i = 0; while (true) { // 从pcm文件里面读取缓冲区 if (fread(out_buffer, 1, buffer_size, in_file) data[0] = out_buffer; av_frame->pts = i; // 注意时间戳 i++; // 第十二步：音频编码处理 // ... current_frame_index++; } 第十二步：音频编码处理 代码位置在第十一步。 // 第十二步：音频编码处理 // 发送一帧音频数据 avcodec_send_frame(avcodec_context, av_frame); if (result != 0) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"Failed to send frame!\"); // iOS使用 // NSLog(@\"Failed to send frame!\", current_frame_index); return; } // 接收一帧音频数据，编码为音频压缩数据格式 result = avcodec_receive_packet(avcodec_context, av_packet); // 判定是否编码成功 if (result == 0) { // 编码成功 // 第十三步：将音频压缩数据写入到输出文件中 // ... } else { __android_log_print(ANDROID_LOG_INFO, \"main\", \"编码第%d帧失败2\", current_frame_index); // iOS使用 // NSLog(@\"编码第%d帧失败2\", current_frame_index); return; } 第十三步：将音频压缩数据写入到输出文件中 代码位置在第十二步。 // 第十三步：将音频压缩数据写入到输出文件中 av_packet->stream_index = av_audio_stream->index; result = av_write_frame(avformat_context, av_packet); // 是否输出成功 if (result 第十四步：写入剩余帧数据 增加flush_encoder方法： int flush_encoder(AVFormatContext *fmt_ctx, unsigned int stream_index) { int ret; int got_frame; AVPacket enc_pkt; if (!(fmt_ctx->streams[stream_index]->codec->codec->capabilities & CODEC_CAP_DELAY)) return 0; while (1) { enc_pkt.data = NULL; enc_pkt.size = 0; av_init_packet(&enc_pkt); ret = avcodec_encode_audio2(fmt_ctx->streams[stream_index]->codec, &enc_pkt, NULL, &got_frame); av_frame_free(NULL); if (ret 调用flush_encoder方法： // 第十四步：写入剩余帧数据 // 作用：输出编码器中剩余AVPacket，可能没有 result = flush_encoder(avformat_context, 0); if (result 第十五步：写入文件尾部信息 // 第十五步：写入文件尾部信息 av_write_trailer(avformat_context); 第十六步：释放内存，关闭编码器 // 第十六步：释放内存，关闭编码器 avcodec_close(avcodec_context); av_free(av_frame); av_free(out_buffer); av_packet_free(&av_packet); avio_close(avformat_context->pb); avformat_free_context(avformat_context); fclose(in_file); 二、新建Android音频编码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程AndroidFFmpegEncodingAudio。 2. 定义java方法 寻找MainActivity：app->src->main->java->MainActivity，增加代码如下： public native void ffmpegDecodeAudio(String inFilePath, String outFilePath); public native void ffmpegEncodeAudio(String inFilePath, String outFilePath); 3. 定义NDK方法 增加android打印。 #include 在native-lib.cpp中，导入FFmpeg头文件。 extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include // 封装格式处理库 #include \"libavformat/avformat.h\" // 工具库 #include \"libavutil/imgutils.h\" // 音频采样数据格式库 #include \"libswresample/swresample.h\" } 在native-lib.cpp中新增java方法flush_encoder、ffmpegDecodeAudio、ffmpegEncodeVideo的C++实现，输入MainActivity.就会有代码提示，选择正确ffmepgEncodeAudio方法补全代码。 ffmpegDecodeAudio方法实现参考FFmpeg视频解码。 extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegencodingaudio_MainActivity_ffmpegEncodeAudio(JNIEnv *env, jobject thiz, jstring in_file_path, jstring out_file_path) { // 这里拷贝上面的音频编码流程的代码即可。 } extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegencodingaudio_MainActivity_ffmpegDecodeAudio(JNIEnv *env, jobject thiz, jstring in_file_path, jstring out_file_path) { } 三、测试Android音频编码工程 准备视频文件：test.mov 在AndroidManifest.xml增加SD卡的读写权限。 MainActivity增加测试代码，这里先进行视频解码，生成的.pcm文件后，直接对.pcm文件再进行编码。 注意：如果打开失败，可能读写存储设备的权限被禁用。 摩托罗拉·刀锋：设置->应用和通知->高级->权限管理器->隐私相关·读写存储设备->找到应用->如果禁用，则修改为允许。 import android.os.Environment; import java.io.File; import java.io.IOException; import android.util.Log; private void ffmpegEncodeAudio() { // String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.pcm\"); // 错误，会导致程序崩溃 // String outFilePath = downloadPath.concat(\"/test.aac\"); // 正确 String outFilePath = downloadPath.concat(\"/test.aac\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmpegEncodeAudio(inFilePath, outFilePath); } private void createAVLogFile() { // String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String logFilePath = downloadPath.concat(\"/av_log.txt\"); // 文件不存在我创建一个文件 File file = new File(logFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } } private void ffmepgDecodeAudio() { // String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.mov\"); String outFilePath = downloadPath.concat(\"/test.pcm\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmpegDecodeAudio(inFilePath, outFilePath); } ffmepgDecodeAudio(); createAVLogFile(); ffmpegEncodeAudio(); run工程代码，正确打印，同时正确生成.h264文件。 I/日志：: 存在 I/main: 解码器名称：aac I/main: 当前解码第1帧 . . . I/main: 当前解码第1036帧 I/日志：: 存在 I/main: 编码器名称为：libfdk_aac I/main: Using AVStream.codec.time_base as a timebase hint to the muxer is deprecated. Set AVStream.time_base instead. I/main: Using AVStream.codec to pass codec parameters to muxers is deprecated, use AVStream.codecpar instead. I/main: 编码第1帧成功 . . . I/main: 编码第1035帧成功 读取完毕... I/main: Flush Encoder: Succeed to encode 1 frame! size: 363 I/main: Flush Encoder: Succeed to encode 1 frame! size: 95 I/main: Statistics: -2770196 seeks, -498852848 writeouts aac文件播放： ffplay test.aac 四、新建iOS视频编码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程iOSFFmpegEncodingAudio。 注意：工程使用的是支持fdkacc编码的FFmpeg库文件。 2. 导入资源文件 资源文件就是音频解码后的.pcm文件。先将.pcm文件拷贝至工程目录下，再通过add files的方式加入工程。 3. 导入fdkacc静态库 在工程目录新建fdk-aac，拷贝编译好的ios_build_fdkaac/thin文件夹至fdkaac-0.1.4目录，只保留arm64的文件夹，删除lib文件夹中的pkgconfig和libfdk-aac.la，再通过add files的方式加入工程。 配置fdkaac头文件，参考FFmpeg集成。 4. 增加音编码方法 (1) 导入FFmpeg头文件 修改FFmpegTest.h，新增如下： //核心库 #include \"libavcodec/avcodec.h\" //封装格式处理库 #include \"libavformat/avformat.h\" //工具库 #include \"libavutil/imgutils.h\" #include \"libswresample/swresample.h\" (2) 新增音频编码方法 修改FFmpegTest.h，新增如下： /// FFmpeg音频编码 + (void)ffmpegAudioEncode: (NSString *)inFilePath outFilePath: (NSString *)outFilePath; 修改FFmpegTest.m，新增如下： + (void)ffmpegAudioEncode: (NSString *)inFilePath outFilePath: (NSString *)outFilePath { // 代码复制音频编码流程中的代码 // 将备注`iOS使用`的代码打开 } (3) 增加方法测试 修改ViewController.m，新增测试代码如下： NSString* inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"pcm\"]; NSArray* paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES); NSString* path = [paths objectAtIndex:0]; NSString* tmpPath = [path stringByAppendingPathComponent:@\"temp\"]; [[NSFileManager defaultManager] createDirectoryAtPath:tmpPath withIntermediateDirectories:YES attributes:nil error:NULL]; NSString* outFilePath = [tmpPath stringByAppendingPathComponent:[NSString stringWithFormat:@\"test.aac\"]]; [FFmpegTest ffmpegAudioEncode:inPath outFilePath:outFilePath]; run工程代码，正确打印，同时正确生成.aac文件。 "},"pages/FFmpeg/SDL播放YUV.html":{"url":"pages/FFmpeg/SDL播放YUV.html","title":"SDL播放YUV","keywords":"","body":"SDL播放YUV Android代码工程 Mac代码工程 iOS代码工程 一、SDL播放流程 第一步：初始化SDL多媒体框架 // 第一步：初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { LOG_I_ARGS(\"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } 第二步：初始化SDL窗口 // 第二步：初始化SDL窗口 // 参数一：窗口名称 // 参数二：窗口在屏幕上的x坐标 // 参数三：窗口在屏幕上的y坐标 // 参数四：窗口在屏幕上宽 // 参数五：窗口在屏幕上高 // 参数六：窗口状态(打开) int width = 640; int height = 352; SDL_Window* sdl_window = SDL_CreateWindow(\"SDL播放YUV视频\", SDL_WINDOWPOS_CENTERED, SDL_WINDOWPOS_CENTERED, width, height, SDL_WINDOW_OPENGL); if (sdl_window == NULL) { LOG_I_ARGS(\"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第三步：创建渲染器->渲染窗口 // 第三步：创建渲染器->渲染窗口 // 参数一：渲染目标创建->目标 // 参数二：从那里开始渲染(-1:表示从第一个位置开始) // 参数三：渲染类型(软件渲染) SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window, -1, 0); if (sdl_renderer == NULL) { LOG_I_ARGS(\"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第四步：创建纹理 // 第四步：创建纹理 // 参数一：纹理->目标渲染器 // 参数二：渲染格式->YUV格式->像素数据格式(视频)或者是音频采样数据格式(音频) // 参数三：绘制方式->频繁绘制->SDL_TEXTUREACCESS_STREAMING // 参数四：纹理宽 // 参数五：纹理高 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer, SDL_PIXELFORMAT_IYUV, SDL_TEXTUREACCESS_STREAMING, width, height); if (sdl_texture == NULL) { LOG_I_ARGS(\"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第五步：打开yuv文件 // 第五步：打开yuv文件 int errNum = 0; FILE* yuv_file = fopen(\"/storage/emulated/0/Download/test.yuv\", \"rb\"); // MAC使用 // FILE* yuv_file = fopen(\"/Users/chenchangqing/Documents/code/ffmpeg/resources/test.yuv\", \"rb\"); // iOS // NSString* inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"yuv\"]; // FILE* yuv_file = fopen([inPath UTF8String], \"rb\"); if (yuv_file == NULL) { errNum = errno; LOG_I_ARGS(\"打开文件失败：errNum:%d,reason:%s\", errNum, strerror(errNum)); // Mac使用 // printf(\"打开文件失败：errNum:%d,reason:%s\", errNum, strerror(errNum)); // 退出程序 SDL_Quit(); return 0; } 第六步：循环读取yuv视频像素数据帧 // 第六步：循环读取yuv视频像素数据帧 int y_size = width * height; // 定义缓冲区(内存空间开辟多大?) // 缓存一帧视频像素数据 = Y + U + V // Y:U:V = 4 : 1 : 1 // 假设：Y = 1.0 U = 0.25 V = 0.25 // 宽度：Y + U + V = 1.5 // 换算：Y + U + V = width * height * 1.5 char buffer_pix[y_size * 3 / 2]; // 定义渲染器区域 SDL_Rect sdl_rect; int currentIndex = 1; while (true) { // 一帧一帧读取 fread(buffer_pix, 1, y_size * 3 / 2, yuv_file); // 判定是否读取完毕 if (feof(yuv_file)){ break; } // 第七步：设置纹理数据 // ... // 第八步：将纹理数据拷贝给渲染器 // ... // 第九步：呈现画面帧 // ... // 第十步：渲染每一帧直接间隔时间 // ... printf(\"当前到了第%d帧\\n\", currentIndex); currentIndex++; } 第七步：设置纹理数据 // 第七步：设置纹理数据 // 参数一：纹理 // 参数二：渲染区域 // 参数三：需要渲染数据->视频像素数据帧 // 参数四：帧宽 SDL_UpdateTexture(sdl_texture, NULL, buffer_pix, width); 第八步：将纹理数据拷贝给渲染器 // 第八步：将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) sdl_rect.x = 100; sdl_rect.y = 100; sdl_rect.w = width; sdl_rect.h = height; SDL_RenderClear(sdl_renderer); SDL_RenderCopy(sdl_renderer, sdl_texture, NULL, &sdl_rect); 第九步：呈现画面帧 // 第九步：呈现画面帧 SDL_RenderPresent(sdl_renderer); 第十步：渲染每一帧直接间隔时间 // 第十步：渲染每一帧直接间隔时间 SDL_Delay(30); 第十一步：释放资源 // 第十一步：释放资源 fclose(yuv_file); SDL_DestroyTexture(sdl_texture); SDL_DestroyRenderer(sdl_renderer); 第十二步：退出程序 // 第十二步：退出程序 SDL_Quit(); 二、Android编译SDL 1. 下载工具包 (1) SDL http://www.libsdl.org/release/SDL2-2.0.5.tar.gz /Users/chenchangqing/Documents/code/ffmpeg/08_ffmpeg_sdl/SDL2-2.0.5 注意：由于最新的SDL编译后使用遇到无法显示视频的问题，这里使用SDL2-2.0.5。 问题：eglSwapBuffersWithDamageKHRImpl:1402 error 300d (EGL_BAD_SURFACE) (2) NDK https://dl.google.com/android/repository/android-ndk-r10e-darwin-x86_64.zip /Users/chenchangqing/Documents/code/ffmpeg/resources/ndk/android-ndk-r10e (3) SDK https://dl.google.com/android/adt/adt-bundle-linux-x86_64-20140702.zip /Users/chenchangqing/Documents/code/ffmpeg/resources/sdk/adt-bundle-linux-x86_64-20140702 (3) ANT https://dlcdn.apache.org//ant/binaries/apache-ant-1.10.12-bin.tar.gz /Users/chenchangqing/Documents/code/ffmpeg/resources/ant/apache-ant-1.10.12 2. 修改androidbuild.sh 查看SDL2-2.0.5/docs/README-android.md得知分别需要配置NDK、SDK、ANT，并且如果编译APK文件需要java环境，这里我们暂时不需要编译APK，忽略java环境即可。androidbuild.sh文件在SDL-2.0.5/build-scripts。 注意：最新版（目前2.0.20）是不需要修改androidbuild.sh的，但是需要配置SDK、NDK的环境变量。 (1) 配置NDK # NDKBUILD=`which ndk-build` NDKBUILD=\"/Users/chenchangqing/Documents/code/ffmpeg/resources/ndk/android-ndk-r10e/ndk-build\" (2) 配置SDK # ANDROID=`which android` ANDROID=\"/Users/chenchangqing/Documents/code/ffmpeg/resources/sdk/adt-bundle-linux-x86_64-20140702/sdk\" (3) 配置ANT # ANT=`which ant` ANT=\"/Users/chenchangqing/Documents/code/ffmpeg/resources/ant/apache-ant-1.10.12/bin/ant\" 3. 运行脚本 ./androidbuild.sh org.libsdl.testgles ../test/testgles.c 脚本执行完毕，分别生成了armeabi、armeabi-v7a、x86的.so动态库，SDL2-2.0.5/build/org.libsdl.testgles/libs是动态库的路径。 注意：最新版（目前2.0.20）脚本执行完毕生成的是一个Android工程，编译后生成动态库文件，暂时没研究好动态库文件的路径在哪里。 三、Android集成SDL 第一步：新建工程 File->NewProject->Native C++->输入工程信息->Next->Finish。 工程名称：AndroidSDLPlayYUV。 第二步：导入库文件。 1. 新建jniLibs文件夹 项目选中Project模式->app->src->main->右键new->Directory->输入jniLibs->enter。 同样的方式在jniLibs下心间lib文件夹，用来存放.so库文件。 2. 拷贝文件至jniLibs 拷贝SDL2-2.0.5/build/org.libsdl.testgles/libs/armeabi-v7a/libSDL2.so至jniLibs/lib。 拷贝SDL2-2.0.5/src至jniLibs。 拷贝SDL2-2.0.5/include至jniLibs。 第三步：配置SDL库 修改CMakeLists.txt。 1. 设置jniLibs # 1. 设置jniLibs set(JNILIBS_DIR ${CMAKE_SOURCE_DIR}/../jniLibs) 2. SDL核心库 # 2. SDL核心库 add_library( SDL2 SHARED IMPORTED) set_target_properties( SDL2 PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libSDL2.so) 3. 配置SDL_android_main.c 修改androidsdlplayyuv为SDL2main，增加${JNILIBS_DIR}/src/main/android/SDL_android_main.c。 androidsdlplayyuv为工程名称。 # 3. 配置SDL_android_main.c add_library( # Sets the name of the library. SDL2main # Sets the library as a shared library. SHARED # Provides a relative path to your source file(s). ${JNILIBS_DIR}/src/main/android/SDL_android_main.c native-lib.cpp) 4. 链接SDL2mian和SDL2库 修改androidsdlplayyuv为SDL2main，增加SDL2。 androidsdlplayyuv为工程名称。 # 4. 链接SDL2mian和SDL2库 target_link_libraries( # Specifies the target library. SDL2main SDL2 # Links the target library to the log library # included in the NDK. ${log-lib}) 注意1：3，4步完成后，然后马上编译，会出现error: undefined reference to 'SDL_main'错误，是因为native-lib.cpp还没写main函数，这里先忽略。注意2: androidsdlplayyuv库已经改成了SDL2main，MainActivity.java的loadLibrary也应该改下名称。 5. SDL头文件和源码 # 5. SDL头文件和源码 include_directories(${JNILIBS_DIR}/src) include_directories(${JNILIBS_DIR}/include) 第四步：配置CPU架构类型 修改app->build.gradle，defaultConfig增加ndk配置。 ndk { abiFilters 'armeabi-v7a' } 第五步：修改native-lib.cpp 引入头文件，增加SDL入口，新增main函数，实现SDL播放YUV。 #include #include #include \"SDL.h\" #define LOG_I_ARGS(FORMAT,...) __android_log_print(ANDROID_LOG_INFO,\"main\",FORMAT,__VA_ARGS__); #define LOG_I(FORMAT) LOG_I_ARGS(FORMAT,0); // SDL入口 extern \"C\" int main(int argc, char *argv[]) { // SDL播放YUV实现 // 拷贝SDL播放流程的代码 return 0; } 第六步：增加播放界面 拷贝SDL2-2.0.5/build/org.libsdl.testgles/src/org至java。 修改SDLActivity.java，原来 protected String[] getLibraries() { return new String[] { \"SDL2\", // \"SDL2_image\", // \"SDL2_mixer\", // \"SDL2_net\", // \"SDL2_ttf\", \"main\" }; } 修改为 protected String[] getLibraries() { return new String[] { \"SDL2\", // \"SDL2_image\", // \"SDL2_mixer\", // \"SDL2_net\", // \"SDL2_ttf\", \"SDL2main\"// 这里的名字是上一步通过add_library配置好的。 }; } 第七步：修改AndroidManifest.xml 在AndroidManifest.xml中声明MANAGE_EXTERNAL_STORAGE权限。 第八步：增加播放按钮 打开main->res->layout->activity_main.xml，点击右上角的Code，将原来的Text改为现在的Button。 第九步：修改MainActivity.java 这是最后一步，完成这一步，运行工程点击播放按钮就可以直接播放了。 注意：在/storage/emulated/0/Download文件夹下放test.yuv。 // 修改1：增加import import android.view.View; import android.content.Intent; import org.libsdl.app.SDLActivity; import android.widget.Toast; import android.os.Build; import android.provider.Settings; import android.os.Environment; public class MainActivity extends AppCompatActivity { // Used to load the 'androidsdlplayyuv' library on application startup. static { // 修改2：androidsdlplayyuv改为SDL2main System.loadLibrary(\"SDL2main\"); } private ActivityMainBinding binding; @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); binding = ActivityMainBinding.inflate(getLayoutInflater()); setContentView(binding.getRoot()); // 修改3：注释TextView，增加checkPermission // Example of a call to a native method // TextView tv = binding.sampleText; // tv.setText(stringFromJNI()); checkPermission(); } /** * 修改4：新增checkPermission方法 * 检查所有文件的权限 */ public void checkPermission() { if (Build.VERSION.SDK_INT 四、Mac集成SDL 第一步：配置SDL开发环境 1. 下载SDL2.dmg https://www.libsdl.org/release/SDL2-2.0.5.dmg 下载好了，点击安装，会得到SDL2.framework。 为了避免不必要的麻烦，这里我们依然使用2.0.5的版本。 2. 安装SDL2 将SDL2.Framework拷贝到/Library/Frameworks目录下。 第二步：新建Mac工程 新建命令行项目：New->Project->macOS->Command Line Tool，项目名称MacSDLPlayYUV。 第三步：导入SDL库 在工程目录新建SDLFramework，将SDL2.Framework拷贝到SDLFramework，通过Add的方式加入工程。 第四步：修改main.m 引入SDL头文件，在main函数拷贝“SDL播放流程”的代码即可。 注意：打印日志的方式需要修改为Mac的方式。 #import #include // 引入SDL头文件 #include // SDL入口 int main(int argc, const char * argv[]) { // SDL播放YUV实现 // ... return 0; } 五、iOS集成SDL 第一步：编译.a静态库 1. 下载SDL源码 http://www.libsdl.org/release/SDL2-2.0.5.tar.gz /Users/chenchangqing/Documents/code/ffmpeg/08_ffmpeg_sdl/SDL2-2.0.5 2. 编译SDL静态库 打开SDL2-2.0.5/Xcode-iOS/SDL工程，选择libSDL目标，再选择Any iOS Device真机编译，编译完成后可以在工程的Products看到libSDL2.a由红色变为了白色，说明静态库已经编译好了，右键show in Finder获取生成好的静态库。 注意：如果编译失败，可能是iOS编译版本不支持，修改SDL的iOS Deployment Target为9.0即可，默认是5.1.1。 第二步：新建iOS工程 删除Scenedelegate，参考：Xcode 11新建项目多了Scenedelegate。 工程名称为iOSSDLPlayYUV。 第三步：导入库文件。 在工程目录新建SDLFramework，拷贝libSDL2.a、SDL2-2.0.5/include至SDLFramework，最后将SDLFramework进入工程。 第四步：添加依赖库 CoreGraphics.framework AudioToolbox.framework AVFoundation.framework CoreAudio.framework OpenGLES.framework CoreMotion.framework GameController.framework 第五步：配置头文件 1. 复制头文件路径 选中Target>Build Setting>搜索Library Search>双击Library Search Paths复制SDLFramework路径>追加/include就是SDL头文件路径： $(PROJECT_DIR)/iOSSDLPlayYUV/SDLFramework/include 2. 配置头文件路径 选中Target>Build Setting>搜索Header Search>选中Header Search Paths>增加上面复制好头文件路径。 第六步：修改main.m 引入SDL头文件，在main函数拷贝“SDL播放流程”的代码即可。 注意：打印日志的方式需要修改为iOS的方式，需要检查下yuv的路径。目前播放还是黑屏，待解决。 #import #include // 引入SDL头文件 #include \"SDL.h\" // SDL入口 int main(int argc, char * argv[]) { // SDL播放YUV实现 // 拷贝SDL播放流程的代码 return 0; } "},"pages/FFmpeg/iOS集成SDL.html":{"url":"pages/FFmpeg/iOS集成SDL.html","title":"iOS集成SDL","keywords":"","body":"iOS集成SDL 代码工程 下载SDL源码 SDL2-2.0.5下载脚本：download-sdl.sh。 sh download-sdl.sh 编译SDL 打开SDL2-2.0.5/Xcode-iOS/SDL工程，选择libSDL目标，再选择Any iOS Device真机编译，编译完成后可以在工程的Products看到libSDL2.a由红色变为了白色，说明静态库已经编译好了，右键show in Finder获取生成好的静态库。 iOS文档位置：源码/docs/README-ios.md。 新建工程 删除Scenedelegate，参考：Xcode 11新建项目多了Scenedelegate。 导入库文件 在工程目录新建SDL2-2.0.5/lib，拷贝已经编译好的libSDL2.a至SDL2-2.0.5/lib，继续拷贝SDL2-2.0.5/include至SDL2-2.0.5，最后将SDL2-2.0.5通过Add Files加入工程。 配置头文件 1) 复制头文件路径 选中Target>Build Setting>搜索Library Search>双击Library Search Paths复制SDL lib路径>修改lib为include就是SDL头文件路径： $(PROJECT_DIR)/iOSIntegrationWithSDL（工程名）/SDL2-2.0.5/include 2) 配置头文件路径 选中Target>Build Setting>搜索Header Search>选中Header Search Paths>增加上面复制好头文件路径。 添加依赖库 AudioToolbox.framework AVFoundation.framework CoreAudio.framework CoreGraphics.framework CoreMotion.framework Foundation.framework GameController.framework OpenGLES.framework QuartzCore.framework UIKit.framework 添加完毕，编译成功。 简单测试 在main.m中引入SDL头文件，编译，编译成功就可以使用SDL开发了。 import \"SDL.h\" "},"pages/FFmpeg/iOS集成SDL_源码.html":{"url":"pages/FFmpeg/iOS集成SDL_源码.html","title":"iOS集成SDL（源码）","keywords":"","body":"iOS集成SDL（源码） 代码工程 下载SDL源码 SDL2-2.0.5下载脚本：download-sdl.sh。 sh download-sdl.sh iOS文档位置：源码/docs/README-ios.md。 新建工程 删除Scenedelegate，参考：Xcode 11新建项目多了Scenedelegate。 导入SDL工程 1）将SDL2-2.0.5/Xcode-iOS/SDL/SDL.xcodeproj工程通过Add Files加入工程。 2）选中Target->Build Phases->Link Binary With Libraries->点击+增加libSDL2.a。 3）选中Target>Build Setting>搜索Header Search>选中User Header Search Paths>源码include相对位置（例：../SDL2-2.0.5/include）。 添加依赖库 AudioToolbox.framework AVFoundation.framework CoreAudio.framework CoreGraphics.framework CoreMotion.framework Foundation.framework GameController.framework OpenGLES.framework QuartzCore.framework UIKit.framework 添加完毕，编译成功。 简单测试 在main.m中引入SDL头文件，编译，编译成功就可以使用SDL开发了。 import \"SDL.h\" 遇到问题 1）SDL工程编译，GCDevice报错 解决：选中SDL的PROJECT->iOS Deployment Target->修改为9.0（源码里的好像是5.1）。 GCDevice编译错误 2）引入SDL.h后无法编译 解决：User Header Search Paths配置的SDL头文件位置错误，修改正确即可。 "},"pages/FFmpeg/FFmpeg_SDL播放视频.html":{"url":"pages/FFmpeg/FFmpeg_SDL播放视频.html","title":"FFmpeg+SDL播放视频","keywords":"","body":"FFmpeg+SDL播放视频 Android工程代码 一、代码实现 第一步：注册组件 // 第一步：注册组件 av_register_all(); 第二步：打开封装格式 // 第二步：打开封装格式 // 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 // 参数一：封装格式上下文 // 作用：保存整个视频信息(解码器、编码器等等...) // 信息：码率、帧率等... AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：视频路径 const char *url = \"/storage/emulated/0/Download/test.mov\"; // 参数三：指定输入的格式 // 参数四：设置默认参数 int avformat_open_input_result = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_result != 0){ // 安卓平台下log __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开文件失败\"); // iOS平台下log // NSLog(\"打开文件失败\"); // 不同的平台替换不同平台log日志 return -1; } 第三步：查找视频流，拿到视频信息 // 第三步：查找视频流，拿到视频信息 // 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 // 参数一：封装格式上下文 // 参数二：指定默认配置 int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result 第四步：查找视频解码器 // 第四步：查找视频解码器 // 4.1 查找视频流索引位置 int av_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 判断流类型：视频流、音频流、字母流等等... if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_stream_index = i; break; } } // 4.2 根据视频流索引，获取解码器上下文 AVCodecContext *avcodec_context = avformat_context->streams[av_stream_index]->codec; // 4.3 根据解码器上下文，获得解码器ID，然后查找解码器 AVCodec *avcodec = avcodec_find_decoder(avcodec_context->codec_id); 第五步：打开解码器 // 第五步：打开解码器 int avcodec_open2_result = avcodec_open2(avcodec_context, avcodec, NULL); if (avcodec_open2_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开解码器失败\"); return -1; } // 测试一下 // 打印信息 __android_log_print(ANDROID_LOG_INFO, \"main\", \"解码器名称：%s\", avcodec->name); 第六步：定义类型转换参数 // 第六步：定义类型转换参数 // 6.1 设置图像转换像素格式为AV_PIX_FMT_YUV420P // 参数一：源文件->原始视频像素数据格式宽 // 参数二：源文件->原始视频像素数据格式高 // 参数三：源文件->原始视频像素数据格式类型 // 参数四：目标文件->目标视频像素数据格式宽 // 参数五：目标文件->目标视频像素数据格式高 // 参数六：目标文件->目标视频像素数据格式类型 SwsContext *swscontext = sws_getContext(avcodec_context->width, avcodec_context->height, avcodec_context->pix_fmt, avcodec_context->width, avcodec_context->height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); // 6.2 解码后的视频信息结构体 // 视频压缩数据：H264 AVFrame* avframe_in = av_frame_alloc(); // 定义解码结果 int decode_result = 0; // 6.3 保存转换为AV_PIX_FMT_YUV420P格式的视频帧 // 视频采样数据：YUV格式 AVFrame* avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型->yuv420类型 // 得到YUV420P缓冲区大小 // 参数一：视频像素数据格式类型->YUV420P格式 // 参数二：一帧视频像素数据宽 = 视频宽 // 参数三：一帧视频像素数据高 = 视频高 // 参数四：字节对齐方式->默认是1 int buffer_size = av_image_get_buffer_size(AV_PIX_FMT_YUV420P, avcodec_context->width, avcodec_context->height, 1); // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 // 参数一：目标->填充数据(avframe_yuv420p) // 参数二：目标->每一行大小 // 参数三：原始数据 // 参数四：目标->格式类型 // 参数五：宽 // 参数六：高 // 参数七：字节对齐方式 av_image_fill_arrays(avframe_yuv420p->data, avframe_yuv420p->linesize, out_buffer, AV_PIX_FMT_YUV420P, avcodec_context->width, avcodec_context->height, 1); 第七步：初始化SDL多媒体框架 // 第七步：初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } 第八步：初始化SDL窗口 // 第八步：初始化SDL窗口 // 参数一：窗口名称 // 参数二：窗口在屏幕上的x坐标 // 参数三：窗口在屏幕上的y坐标 // 参数四：窗口在屏幕上宽 // 参数五：窗口在屏幕上高 // 参数六：窗口状态(打开) SDL_Window* sdl_window = SDL_CreateWindow(\"SDL播放YUV视频\", SDL_WINDOWPOS_CENTERED, SDL_WINDOWPOS_CENTERED, avcodec_context->width, avcodec_context->height, SDL_WINDOW_OPENGL); if (sdl_window == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建成功width：%d，height：%d\", avcodec_context->width,avcodec_context->height); 第九步：创建渲染器->渲染窗口 // 第九步：创建渲染器->渲染窗口 // 参数一：渲染目标创建->目标 // 参数二：从那里开始渲染(-1:表示从第一个位置开始) // 参数三：渲染类型(软件渲染) SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window, -1, 0); if (sdl_renderer == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 定义渲染器区域 SDL_Rect sdl_rect; 第十步：创建纹理 // 第十步：创建纹理 // 参数一：纹理->目标渲染器 // 参数二：渲染格式->YUV格式->像素数据格式(视频)或者是音频采样数据格式(音频) // 参数三：绘制方式->频繁绘制->SDL_TEXTUREACCESS_STREAMING // 参数四：纹理宽 // 参数五：纹理高 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer, SDL_PIXELFORMAT_IYUV, SDL_TEXTUREACCESS_STREAMING, avcodec_context->width, avcodec_context->height); if (sdl_texture == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第十一步：读取视频压缩数据帧 // 第十一步：读取视频压缩数据帧 int current_index = 0; // 写入时yuv数据位置 int y_size, u_size, v_size; // 分析av_read_frame参数。 // 参数一：封装格式上下文 // 参数二：一帧压缩数据 // 如果是解码视频流，是视频压缩帧数据，例如H264 AVPacket* packet = (AVPacket*)av_malloc(sizeof(AVPacket)); while (av_read_frame(avformat_context, packet) >= 0) { // >=:读取到了 // stream_index == av_stream_index) { // 第十二步：开始视频解码 // ... current_index++; __android_log_print(ANDROID_LOG_INFO, \"main\", \"当前解码第%d帧\", current_index); } } 第十二步：开始视频解码 // 第十二步：开始视频解码 // 发送一帧视频压缩数据 avcodec_send_packet(avcodec_context, packet); // 解码一帧视频数据 decode_result = avcodec_receive_frame(avcodec_context, avframe_in); if (decode_result == 0) { // 视频解码成功 // 第十三步：开始类型转换 // ... // 第十四步：设置纹理数据 // ... // 第十五步：将纹理数据拷贝给渲染器 // ... // 第十六步：呈现画面帧 // ... // 第十七步：渲染每一帧直接间隔时间 // ... } 第十三步：开始类型转换 // 第十三步：开始类型转换 // 将解码出来的视频像素点数据格式统一转类型为yuv420P // 参数一：视频像素数据格式上下文 // 参数二：原来的视频像素数据格式->输入数据 // 参数三：原来的视频像素数据格式->输入画面每一行大小 // 参数四：原来的视频像素数据格式->输入画面每一行开始位置(填写：0->表示从原点开始读取) // 参数五：原来的视频像素数据格式->输入数据行数 // 参数六：转换类型后视频像素数据格式->输出数据 // 参数七：转换类型后视频像素数据格式->输出画面每一行大小 sws_scale(swscontext, (const uint8_t *const *)avframe_in->data, avframe_in->linesize, 0, avcodec_context->height, avframe_yuv420p->data, avframe_yuv420p->linesize); 第十四步：设置纹理数据 // 第十四步：设置纹理数据 // 参数一：纹理 // 参数二：渲染区域 // 参数三：需要渲染数据->视频像素数据帧 // 参数四：帧宽 SDL_UpdateTexture(sdl_texture, NULL, avframe_yuv420p->data[0], avframe_yuv420p->linesize[0]); 第十五步：将纹理数据拷贝给渲染器 // 第十五步：将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) sdl_rect.x = 100; sdl_rect.y = 100; sdl_rect.w = avcodec_context->width; sdl_rect.h = avcodec_context->height; SDL_RenderClear(sdl_renderer); SDL_RenderCopy(sdl_renderer, sdl_texture, NULL, &sdl_rect); 第十六步：呈现画面帧 // 第十六步：呈现画面帧 SDL_RenderPresent(sdl_renderer); 第十七步：渲染每一帧直接间隔时间 // 第十七步：渲染每一帧直接间隔时间 SDL_Delay(30); 第十八步：释放资源 // 第十八步：释放资源 SDL_DestroyTexture(sdl_texture); SDL_DestroyRenderer(sdl_renderer); 第十九步：退出程序 // 第十九步：退出程序 SDL_Quit(); 第二十步：释放内存资源，关闭解码器 // 第二十步：释放内存资源，关闭解码器 av_packet_free(&packet); av_frame_free(&avframe_in); av_frame_free(&avframe_yuv420p); free(out_buffer); avcodec_close(avcodec_context); avformat_free_context(avformat_context); 一、Android实现 第一步：Android集成SDL 参考：http://www.1221.site/FFmpeg/08_SDL%E6%92%AD%E6%94%BEYUV.html 新建工程名称为AndroidDisplayVideoWhileDecoding，按上面文章配置，能正常播放YUV文件就算成功完成了第一步。 第二步：Android集成FFmpeg 参考：http://www.1221.site/FFmpeg/02_FFmpeg%E9%9B%86%E6%88%90.html 第三步：修改native-lib.cpp #include #include #include #include #include \"SDL.h\" extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include #include \"libavformat/avformat.h\" #include #include } // SDL入口 extern \"C\" int main(int argc, char *argv[]) { // 边解码边显示视频实现 // 复制代码实现 } "},"pages/FFmpeg/FFmpeg_SDL播放音频.html":{"url":"pages/FFmpeg/FFmpeg_SDL播放音频.html","title":"FFmpeg+SDL播放音频","keywords":"","body":"FFmpeg+SDL播放音频 Android工程代码 一、代码实现 增加头文件 #include 定义一 // 定义一 // SDL读音频缓存的大小 #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 int quit = 0;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 定义二：数据包队列(链表)结构体 // 定义二：数据包队列(链表)结构体 /*-------链表节点结构体------- typedef struct AVPacketList { AVPacket pkt;//链表数据 struct AVPacketList *next;//链表后继节点 } AVPacketList; ---------------------------*/ typedef struct PacketQueue { AVPacketList *first_pkt, *last_pkt;// 队列首尾节点指针 int nb_packets;// 队列长度 int size;// 保存编码数据的缓存长度，size=packet->size SDL_mutex *qlock;// 队列互斥量，保护队列数据 SDL_cond *qready;// 队列就绪条件变量 } PacketQueue; PacketQueue audioq;// 定义全局队列对象 定义三：队列初始化函数 // 定义三：队列初始化函数 void packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue));//全零初始化队列结构体对象 q->qlock = SDL_CreateMutex();//创建互斥量对象 q->qready = SDL_CreateCond();//创建条件变量对象 } 定义四：向队列中插入数据包 // 定义四：向队列中插入数据包 int packet_queue_put(PacketQueue *q, AVPacket *pkt) { /*-------准备队列(链表)节点对象------*/ AVPacketList *pktlist;// 创建链表节点对象指针 pktlist = static_cast(av_malloc(sizeof(AVPacketList)));// 在堆上创建链表节点对象 if (!pktlist) {// 检查链表节点对象是否创建成功 return -1; } pktlist->pkt = *pkt;// 将输入数据包赋值给新建链表节点对象中的数据包对象 pktlist->next = NULL;// 链表后继指针为空 // if (av_packet_ref(pkt, pkt)qlock);// 队列互斥量加锁，保护队列数据 if (!q->last_pkt) {// 检查队列尾节点是否存在(检查队列是否为空) q->first_pkt = pktlist;// 若不存在(队列尾空)，则将当前节点作队列为首节点 } else { q->last_pkt->next = pktlist;// 若已存在尾节点，则将当前节点挂到尾节点的后继指针上，并作为新的尾节点 } q->last_pkt = pktlist;// 将当前节点作为新的尾节点 q->nb_packets++;// 队列长度+1 q->size += pktlist->pkt.size;// 更新队列编码数据的缓存长度 SDL_CondSignal(q->qready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(q->qlock);// 释放互斥量 return 0; } 定义五：从队列中提取数据包，并将提取的数据包出队列 // 定义五：从队列中提取数据包，并将提取的数据包出队列 static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block) { AVPacketList *pktlist;// 临时链表节点对象指针 int ret;// 操作结果 SDL_LockMutex(q->qlock);// 队列互斥量加锁，保护队列数据 for (;;) { if (quit) {// 检查退出进程标识 ret = -1;// 操作失败 break; } pktlist = q->first_pkt;// 传递将队列首个数据包指针 if (pktlist) {// 检查数据包是否为空(队列是否有数据) q->first_pkt = pktlist->next;// 队列首节点指针后移 if (!q->first_pkt) {// 检查首节点的后继节点是否存在 q->last_pkt = NULL;// 若不存在，则将尾节点指针置空 } q->nb_packets--;// 队列长度-1 q->size -= pktlist->pkt.size;// 更新队列编码数据的缓存长度 *pkt = pktlist->pkt;// 将队列首节点数据返回 av_free(pktlist);// 清空临时节点数据(清空首节点数据，首节点出队列) ret = 1;// 操作成功 break; } else if (!block) { ret = 0; break; } else {// 队列处于未就绪状态，此时通过SDL_CondWait函数等待qready就绪信号，并暂时对互斥量解锁 /*--------------------- * 等待队列就绪信号qready，并对互斥量暂时解锁 * 此时线程处于阻塞状态，并置于等待条件就绪的线程列表上 * 使得该线程只在临界区资源就绪后才被唤醒，而不至于线程被频繁切换 * 该函数返回时，互斥量再次被锁住，并执行后续操作 --------------------*/ SDL_CondWait(q->qready, q->qlock);// 暂时解锁互斥量并将自己阻塞，等待临界区资源就绪(等待SDL_CondSignal发出临界区资源就绪的信号) } }// end for for-loop SDL_UnlockMutex(q->qlock);// 释放互斥量 return ret; } 定义六：音频解码 /*--------------------------- * 定义六：音频解码 * 从缓存队列中提取数据包、解码，并返回解码后的数据长度(对一个完整的packet解码，将解码数据写入audio_buf缓存，并返回多帧解码数据的总长度) * aCodecCtx:音频解码器上下文 * audio_buf：保存解码一个完整的packe后的原始音频数据(缓存中可能包含多帧解码后的音频数据) * buf_size：解码后的音频数据长度，未使用 --------------------------*/ int audio_decode_frame(AVCodecContext *aCodecCtx, uint8_t *audio_buf, int buf_size) { static AVPacket pkt;// 保存从队列中提取的数据包 static AVFrame frame;// 保存从数据包中解码的音频数据 static uint8_t *audio_pkt_data = NULL;// 保存数据包编码数据缓存指针 static int audio_pkt_size = 0;// 数据包中剩余的编码数据长度 int coded_consumed_size, data_size = 0;// 每次消耗的编码数据长度[input](len1)，输出原始音频数据的缓存长度[output] for (;;) { while(audio_pkt_size>0) {// 检查缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) int got_frame = 0;// 解码操作成功标识，成功返回非零值 coded_consumed_size=avcodec_decode_audio4(aCodecCtx,&frame,&got_frame,&pkt);//解码一帧音频数据，并返回消耗的编码数据长度 if (coded_consumed_size channels,frame.nb_samples,aCodecCtx->sample_fmt,1); memcpy(audio_buf, frame.data[0], data_size);// 将解码数据复制到输出缓存 } if (data_size 定义七：音频输出回调函数 /*------Audio Callback------- * 定义七：音频输出回调函数 * sdl通过该回调函数将解码后的pcm数据送入声卡播放, * sdl通常一次会准备一组缓存pcm数据，通过该回调送入声卡，声卡根据音频pts依次播放pcm数据 * 待送入缓存的pcm数据完成播放后，再载入一组新的pcm缓存数据(每次音频输出缓存为空时，sdl就调用此函数填充音频输出缓存，并送入声卡播放) * When we begin playing audio, SDL will continually call this callback function * and ask it to fill the audio buffer with a certain number of bytes * The audio function callback takes the following parameters: * stream: A pointer to the audio buffer to be filled，输出音频数据到声卡缓存 * len: The length (in bytes) of the audio buffer,缓存长度wanted_spec.samples=SDL_AUDIO_BUFFER_SIZE(1024) --------------------------*/ void audio_callback(void *userdata, Uint8 *stream, int len) { AVCodecContext *aCodecCtx = (AVCodecContext *)userdata;// 传递用户数据 int wt_stream_len, audio_size;// 每次写入stream的数据长度，解码后的数据长度 static uint8_t audio_buf[(MAX_AUDIO_FRAME_SIZE*3)/2];// 保存解码一个packet后的多帧原始音频数据 static unsigned int audio_buf_size = 0;// 解码后的多帧音频数据长度 static unsigned int audio_buf_index = 0;// 累计写入stream的长度 while (len>0) {// 检查音频缓存的剩余长度 if (audio_buf_index >= audio_buf_size) {// 检查是否需要执行解码操作 // We have already sent all our data; get more，从缓存队列中提取数据包、解码，并返回解码后的数据长度，audio_buf缓存中可能包含多帧解码后的音频数据 audio_size = audio_decode_frame(aCodecCtx, audio_buf, audio_buf_size); if (audio_size len) {// 检查每次写入缓存的数据长度是否超过指定长度(1024) wt_stream_len = len;// 指定长度从解码的缓存中取数据 } // 每次从解码的缓存数据中以指定长度抽取数据并写入stream传递给声卡 memcpy(stream,(uint8_t*)audio_buf+audio_buf_index,wt_stream_len); len -= wt_stream_len;// 更新解码音频缓存的剩余长度 stream += wt_stream_len;// 更新缓存写入位置 audio_buf_index += wt_stream_len;// 更新累计写入缓存数据长度 }// end for while } 第一步：注册组件 /*------------------------- * 第一步：注册组件 * 注册所有ffmpeg支持的多媒体格式及编解码器 -------------------------*/ av_register_all(); 第二步：打开封装格式 /*------------------------- * 第二步：打开封装格式 * 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 * 参数一：封装格式上下文 * 参数二：视频路径 * 参数三：指定输入的格式 * 参数四：设置默认参数 --------------------------*/ AVFormatContext* avformat_context = avformat_alloc_context();// 参数一：封装格式上下文 const char *url = \"/storage/emulated/0/Download/test.mov\";// 参数二：视频路径 int avformat_open_input_result = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"查找音视频流\\n\"); return -1; } 第三步：查找码流 /*------------------------- * 第三步：查找码流 * 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 * 参数一：封装格式上下文 * 参数二：指定默认配置 -------------------------*/ int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result 第四步：查找解码器 // 第四步：查找解码器 // 视频流类型标号初始化为-1 int av_video_stream_index = -1; // 音频流类型标号初始化为-1 int av_audio_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 若文件中包含有视频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_video_stream_index = i; } // 若文件中包含有音频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_audio_stream_index = i; } } // 检查文件中是否存在视频流 if (av_video_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到视频流\\n\"); return -1; } // 检查文件中是否存在音频流 if (av_audio_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到音频流\\n\"); return -1; } // 根据流类型标号从avformat_context->streams中取得流对应的解码器上下文 AVCodecContext *video_avcodec_context = avformat_context->streams[av_video_stream_index]->codec; AVCodecContext *audio_avcodec_context = avformat_context->streams[av_audio_stream_index]->codec; // 根据流对应的解码器上下文查找对应的解码器，返回对应的解码器(信息结构体) AVCodec *video_avcodec = avcodec_find_decoder(video_avcodec_context->codec_id); AVCodec *audio_avcodec = avcodec_find_decoder(audio_avcodec_context->codec_id); // 检查视频解码器 if (!video_avcodec) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没找到视频解码器\\n\"); return -1; } // 检查音频解码器 if (!audio_avcodec) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没找到音频解码器\\n\"); return -1; } 第五步：打开解码器 // 第五步：打开解码器 // 打开视频解码器 int avcodec_open2_result = avcodec_open2(video_avcodec_context, video_avcodec, NULL); if (avcodec_open2_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开视频解码器失败\\n\"); return -1; } // 打开音频解码器 avcodec_open2_result = avcodec_open2(audio_avcodec_context, audio_avcodec, NULL); if (avcodec_open2_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开音频解码器失败\\n\"); return -1; } // 打印解码器信息 __android_log_print(ANDROID_LOG_INFO, \"main\", \"视频解码器：%s\\n\", video_avcodec->name); __android_log_print(ANDROID_LOG_INFO, \"main\", \"音频解码器：%s\\n\", audio_avcodec->name); 第六步：定义类型转换参数 /*------------------------- * 第六步：定义类型转换参数 * 参数一：原始视频像素数据格式宽 * 参数二：原始视频像素数据格式高 * 参数三：原始视频像素数据格式类型 * 参数四：目标视频像素数据格式宽 * 参数五：目标视频像素数据格式高 * 参数六：目标视频像素数据格式类型 -------------------------*/ // 设置图像转换像素格式为AV_PIX_FMT_YUV420P SwsContext *swscontext = sws_getContext(video_avcodec_context->width, video_avcodec_context->height, video_avcodec_context->pix_fmt, video_avcodec_context->width, video_avcodec_context->height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); // 保存音视频解码后的数据，如状态信息、编解码器信息、宏块类型表，QP表，运动矢量表等数据 AVFrame* avframe_in = av_frame_alloc(); // 定义解码结果 int decode_result = 0; // AV_PIX_FMT_YUV420P格式的视频帧 AVFrame* avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型 int buffer_size =av_image_get_buffer_size(AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 video_avcodec_context->width,// 一帧视频像素数据宽 = 视频宽 video_avcodec_context->height,// 一帧视频像素数据高 = 视频高 1);// 字节对齐方式，默认是1 // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 av_image_fill_arrays(avframe_yuv420p->data,// 目标视频帧数据 avframe_yuv420p->linesize,// 目标视频帧行大小 out_buffer,// 原始数据 AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 video_avcodec_context->width,// 视频宽 video_avcodec_context->height,//视频高 1);// 字节对齐方式 第七步：初始化SDL多媒体框架 // 第七步：初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } 第八步：缓存队列初始化 // 第八步：缓存队列初始化 packet_queue_init(&audioq); 第九步：设置音频播放参数 // 第九步：设置音频播放参数 // SDL_AudioSpec a structure that contains the audio output format，创建 SDL_AudioSpec 结构体，设置音频播放数据 SDL_AudioSpec wanted_spec, spec; // 创建SDL_AudioSpec结构体，设置音频播放参数 // 采样频率 DSP frequency -- samples per second wanted_spec.freq = audio_avcodec_context->sample_rate; // 采样格式 Audio data format wanted_spec.format = AUDIO_S16SYS; // 声道数 Number of channels: 1 mono, 2 stereo wanted_spec.channels = audio_avcodec_context->channels; wanted_spec.silence = 0;// 无输出时是否静音 // 默认每次读音频缓存的大小，推荐值为 512~8192，ffplay使用的是1024 wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE; // 设置取音频数据的回调接口函数 the function to call when the audio device needs more data wanted_spec.callback = audio_callback; // 传递用户数据 wanted_spec.userdata = audio_avcodec_context; 第十步：打开音频设备 /*-------------------------- * 第十步：打开音频设备 * 以指定参数打开音频设备，并返回与指定参数最为接近的参数，该参数为设备实际支持的音频参数 * Opens the audio device with the desired parameters(wanted_spec) * return another specs we actually be using * and not guaranteed to get what we asked for --------------------------*/ if (SDL_OpenAudio(&wanted_spec, &spec) 第十一步：初始化SDL窗口 // 第十一步：初始化SDL窗口 SDL_Window* sdl_window = SDL_CreateWindow(\"FFmpeg+SDL播放视频\",// 参数一：窗口名称 SDL_WINDOWPOS_CENTERED,// 参数二：窗口在屏幕上的x坐标 SDL_WINDOWPOS_CENTERED,// 参数三：窗口在屏幕上的y坐标 video_avcodec_context->width,// 参数四：窗口在屏幕上宽 video_avcodec_context->height,// 参数五：窗口在屏幕上高 SDL_WINDOW_OPENGL);// 参数六：窗口状态(打开) if (sdl_window == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建成功，width：%d，height：%d\", video_avcodec_context->width, video_avcodec_context->height); 第十二步：创建渲染器 // 第十二步：创建渲染器 // 定义渲染器区域 SDL_Rect sdl_rect; SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window,// 渲染目标创建 -1, // 从那里开始渲染(-1:表示从第一个位置开始) 0);// 渲染类型(软件渲染) if (sdl_renderer == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第十三步：创建纹理 // 第十三步：创建纹理 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer,// 渲染器 SDL_PIXELFORMAT_IYUV,// 像素数据格式 SDL_TEXTUREACCESS_STREAMING,// 绘制方式：频繁绘制- video_avcodec_context->width,// 纹理宽 video_avcodec_context->height);// 纹理高 if (sdl_texture == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第十四步：读取视频压缩数据帧 // 第十四步：读取视频压缩数据帧 int video_current_index = 0; // 负责保存压缩编码数据相关信息的结构体,每帧图像由一到多个packet包组成 AVPacket* packet = (AVPacket*)av_malloc(sizeof(AVPacket)); // 从文件中依次读取每个图像编码数据包，并存储在AVPacket数据结构中，>=:读取到了，= 0) { // 检查数据包类型是否是视频流 if (packet->stream_index == av_video_stream_index) { /*----------------------- * 第十五步：视频解码 * 解码完整的一帧数据，decode_result返回true * 可能无法通过只解码一个packet就获得一个完整的视频帧frame，可能需要读取多个packet才行 * avcodec_receive_frame()会在解码到完整的一帧时，decode_result为true -----------------------*/ // ... video_current_index++; __android_log_print(ANDROID_LOG_INFO, \"main\", \"当前解码视频第%d帧\", video_current_index); } // 检查数据包类型是否是音频流 else if (packet->stream_index == av_audio_stream_index) { // 第二十一步：向缓存队列中填充编码数据包 // ... } // 字幕流类型标识 else { // 释放AVPacket数据结构中编码数据指针 av_packet_free(&packet); } /*------------------------ * 第二十二步：获取SDL事件 * 在每次循环中从SDL后台队列取事件并填充到SDL_Event对象中 * SDL的事件系统使得你可以接收用户的输入，从而完成一些控制操作 ------------------------*/ // ... } 第十五步：视频解码 /*----------------------- * 第十五步：视频解码 * 解码完整的一帧数据，decode_result返回true * 可能无法通过只解码一个packet就获得一个完整的视频帧frame，可能需要读取多个packet才行 * avcodec_receive_frame()会在解码到完整的一帧时，decode_result为true -----------------------*/ // 发送一帧视频压缩数据 avcodec_send_packet(video_avcodec_context, packet); // 解码一帧视频数据 decode_result = avcodec_receive_frame(video_avcodec_context, avframe_in); if (decode_result == 0) { // 视频解码成功 // 第十六步：开始类型转换 // ... // 第十七步：设置纹理数据 // ... // 第十八步：将纹理数据拷贝给渲染器 // ... // 第十九步：呈现画面帧 // ... // 第二十步：渲染每一帧直接间隔时间 // ... } 第十六步：开始类型转换 // 第十六步：开始类型转换 // 将解码出来的视频像素点数据格式统一转类型为yuv420P sws_scale(swscontext,// 视频像素数据格式上下文 (const uint8_t *const *)avframe_in->data,// 输入数据 avframe_in->linesize,// 输入画面每一行大小 0,// 输入画面每一行开始位置(0表示从原点开始读取) video_avcodec_context->height,// 输入数据行数 avframe_yuv420p->data,// 输出数据 avframe_yuv420p->linesize);// 输出画面每一行大小 第十七步：设置纹理数据 // 第十七步：设置纹理数据 SDL_UpdateTexture(sdl_texture, // 纹理 NULL,// 渲染区域 avframe_yuv420p->data[0],// 需要渲染数据：视频像素数据帧 avframe_yuv420p->linesize[0]);// 帧宽 第十八步：将纹理数据拷贝给渲染器 // 第十八步：将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) sdl_rect.x = 100; sdl_rect.y = 100; sdl_rect.w = video_avcodec_context->width; sdl_rect.h = video_avcodec_context->height; SDL_RenderClear(sdl_renderer); SDL_RenderCopy(sdl_renderer, sdl_texture, NULL, &sdl_rect); 第十九步：呈现画面帧 // 第十九步：呈现画面帧 SDL_RenderPresent(sdl_renderer); 第二十步：渲染每一帧直接间隔时间 // 第二十步：渲染每一帧直接间隔时间 SDL_Delay(30); 第二十一步：向缓存队列中填充编码数据包 // 第二十一步：向缓存队列中填充编码数据包 packet_queue_put(&audioq, packet); 第二十二步：获取SDL事件 /*------------------------ * 第二十二步：获取SDL事件 * 在每次循环中从SDL后台队列取事件并填充到SDL_Event对象中 * SDL的事件系统使得你可以接收用户的输入，从而完成一些控制操作 ------------------------*/ SDL_Event event;//SDL事件对象 SDL_PollEvent(&event); switch (event.type) {//检查SDL事件对象 case SDL_QUIT://退出事件 quit = 1;//退出进程标识置1 SDL_Quit();//退出操作 exit(0);//结束进程 break; default: break; }// end for switch 第二十三步：释放资源，退出程序 // 第二十三步：释放资源，退出程序 av_packet_free(&packet); av_frame_free(&avframe_in); av_frame_free(&avframe_yuv420p); free(out_buffer); avcodec_close(video_avcodec_context); avformat_free_context(avformat_context); SDL_DestroyTexture(sdl_texture); SDL_DestroyRenderer(sdl_renderer); SDL_Quit(); 一、Android实现 第一步：Android集成SDL 参考：http://www.1221.site/FFmpeg/08_SDL%E6%92%AD%E6%94%BEYUV.html 新建工程名称为AndroidDisplayVideoWhileDecoding，按上面文章配置，能正常播放YUV文件就算成功完成了第一步。 第二步：Android集成FFmpeg 参考：http://www.1221.site/FFmpeg/02_FFmpeg%E9%9B%86%E6%88%90.html 第三步：修改native-lib.cpp #include #include #include #include #include \"SDL.h\" #include extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include #include \"libavformat/avformat.h\" #include #include } // 定义一 // SDL读音频缓存的大小 #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 int quit = 0;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 // SDL入口 extern \"C\" int main(int argc, char *argv[]) { // 边解码边播放音视频实现 // 复制代码实现 } "},"pages/FFmpeg/FFmpeg_SDL创建线程.html":{"url":"pages/FFmpeg/FFmpeg_SDL创建线程.html","title":"FFmpeg+SDL创建线程","keywords":"","body":"FFmpeg+SDL创建线程 Android工程代码 ffmpeg播放器实现详解 - 创建线程：https://www.cnblogs.com/breakpointlab/p/13508271.html 源代码一览 #include #include #include #include #include \"SDL.h\" #include #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 #define MAX_AUDIOQ_SIZE (5 * 16 * 1024) #define MAX_VIDEOQ_SIZE (5 * 256 * 1024) #define FF_ALLOC_EVENT (SDL_USEREVENT) #define FF_REFRESH_EVENT (SDL_USEREVENT + 1) #define FF_QUIT_EVENT (SDL_USEREVENT + 2) #define VIDEO_PICTURE_QUEUE_SIZE 1 extern \"C\" { #include #include \"libavformat/avformat.h\" #include #include #include } /*-------链表节点结构体-------- typedef struct AVPacketList { AVPacket pkt;//链表数据 struct AVPacketList *next;//链表后继节点 } AVPacketList; ---------------------------*/ // 数据包队列(链表)结构体 typedef struct PacketQueue { AVPacketList *first_pkt, *last_pkt;// 队列首尾节点指针 int nb_packets;// 队列长度 int size;// 保存编码数据的缓存长度，size=packet->size SDL_mutex *qlock;// 队列互斥量，保护队列数据 SDL_cond *qready;// 队列就绪条件变量 } PacketQueue; // 图像帧结构体 typedef struct VideoPicture { AVFrame *avframe_yuv420p; int width, height;//Source height & width. int allocated;//是否分配内存空间，视频帧转换为SDL overlay标识 } VideoPicture; typedef struct VideoState { AVFormatContext *pFormatCtx;// 保存文件容器封装信息及码流参数的结构体 AVStream *video_st;// 视频流信息结构体 AVStream *audio_st;//音频流信息结构体 struct SwsContext *sws_ctx;// 描述转换器参数的结构体 PacketQueue videoq;// 视频编码数据包队列(编码数据队列，以链表方式实现) VideoPicture pictq[VIDEO_PICTURE_QUEUE_SIZE]; int pictq_size, pictq_rindex, pictq_windex;// 队列长度，读/写位置索引 SDL_mutex *pictq_lock;// 队列读写锁对象，保护图像帧队列数据 SDL_cond *pictq_ready;// 队列就绪条件变量 SDL_Rect sdl_rect; SDL_Renderer* sdl_renderer; SDL_Texture* sdl_texture; PacketQueue audioq;// 音频编码数据包队列(编码数据队列，以链表方式实现) uint8_t audio_buf[(MAX_AUDIO_FRAME_SIZE*3)/2];//保存解码一个packet后的多帧原始音频数据(解码数据队列，以数组方式实现) unsigned int audio_buf_size;//解码后的多帧音频数据长度 unsigned int audio_buf_index;//累计写入stream的长度 uint8_t *audio_pkt_data;//编码数据缓存指针位置 int audio_pkt_size;//缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) AVPacket audio_pkt;//保存从队列中提取的数据包 AVFrame audio_frame;//保存从数据包中解码的音频数据 int video_width; int video_height; char filename[1024];// 输入文件完整路径名 int videoStream, audioStream;// 音视频流类型标号 SDL_Thread *parse_tid;// 编码数据包解析线程id SDL_Thread *decode_tid;// 解码线程id int quit;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 } VideoState;// Since we only have one decoding thread, the Big Struct can be global in case we need it. VideoState *global_video_state; // 定时器触发的回调函数 static Uint32 sdl_refresh_timer_cb(Uint32 interval, void *opaque) { SDL_Event event;//SDL事件对象 event.type = FF_REFRESH_EVENT;//视频显示刷新事件 event.user.data1 = opaque;//传递用户数据 SDL_PushEvent(&event);//发送事件 return 0; // 0 means stop timer. } /*--------------------------- * Schedule a video refresh in 'delay' ms. * 告诉sdl在指定的延时后来推送一个 FF_REFRESH_EVENT 事件 * 这个事件将在事件队列里触发sdl_refresh_timer_cb函数的调用 --------------------------*/ static void schedule_refresh(VideoState *is, int delay) { SDL_AddTimer(delay, sdl_refresh_timer_cb, is);//在指定的时间(ms)后回调用户指定的函数 } // 视频(图像)帧渲染 void video_display(VideoState *is) { SDL_Rect rect;// SDL矩形对象 VideoPicture *vp;// 图像帧结构体指针 vp = &is->pictq[is->pictq_rindex];//从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // 设置纹理数据 SDL_UpdateTexture(is->sdl_texture, // 纹理 NULL,// 渲染区域 vp->avframe_yuv420p->data[0],// 需要渲染数据：视频像素数据帧 vp->avframe_yuv420p->linesize[0]);// 帧宽 // 将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) is->sdl_rect.x = 100; is->sdl_rect.y = 100; is->sdl_rect.w = vp->width; is->sdl_rect.h = vp->height; SDL_RenderClear(is->sdl_renderer); SDL_RenderCopy(is->sdl_renderer, is->sdl_texture, NULL, &is->sdl_rect); // 呈现画面帧 SDL_RenderPresent(is->sdl_renderer); }// end for if }// end for video_display // 显示刷新函数(FF_REFRESH_EVENT响应函数) void video_refresh_timer(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 // vp is used in later tutorials for synchronization. if (is->video_st) { if (is->pictq_size == 0) {// 检查图像帧队列是否有待显示图像 schedule_refresh(is, 1); } else {// 刷新图像 /*------------------------- * Now, normally here goes a ton of code about timing, etc. * we're just going to guess at a delay for now. * You can increase and decrease this value and hard code the timing * but I don't suggest that ;) We'll learn how to do it for real later.. ------------------------*/ schedule_refresh(is, 40);// 设置显示下一帧图像的刷新时间，通过定时器timer方式触发 // Show the picture! video_display(is);// 图像帧渲染 // Update queue for next picture! if (++is->pictq_rindex == VIDEO_PICTURE_QUEUE_SIZE) {// 更新并检查图像帧队列读位置索引 is->pictq_rindex = 0;// 重置读位置索引 } SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 is->pictq_size--;// 更新图像帧队列长度 SDL_CondSignal(is->pictq_ready);// 发送队列就绪信号 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } } else { schedule_refresh(is, 100); } } // 数据包队列初始化函数 void packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue));// 全零初始化队列结构体对象 q->qlock = SDL_CreateMutex();// 创建互斥量对象 q->qready = SDL_CreateCond();// 创建条件变量对象 } // 向队列中插入数据包 int packet_queue_put(PacketQueue *q, AVPacket *pkt) { /*-------准备队列(链表)节点对象------*/ AVPacketList *pktlist=(AVPacketList *)av_malloc(sizeof(AVPacketList));// 在堆上创建链表节点对象 if (!pktlist) {// 检查链表节点对象是否创建成功 return -1; } pktlist->pkt = *pkt;// 将输入数据包赋值给新建链表节点对象中的数据包对象 pktlist->next = NULL;// 链表后继指针为空 // if (av_packet_ref(pkt, pkt) qlock);// 队列互斥量加锁，保护队列数据 if (!q->last_pkt) {// 检查队列尾节点是否存在(检查队列是否为空) q->first_pkt = pktlist;// 若不存在(队列尾空)，则将当前节点作队列为首节点 } else { q->last_pkt->next = pktlist;// 若已存在尾节点，则将当前节点挂到尾节点的后继指针上，并作为新的尾节点 } q->last_pkt = pktlist;// 将当前节点作为新的尾节点 q->nb_packets++;// 队列长度+1 q->size += pktlist->pkt.size;// 更新队列编码数据的缓存长度 SDL_CondSignal(q->qready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(q->qlock);// 释放互斥量 return 0; } // 从队列中提取数据包，并将提取的数据包出队列 static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block) { AVPacketList *pktlist;// 临时链表节点对象指针 int ret;// 操作结果 SDL_LockMutex(q->qlock);// 队列互斥量加锁，保护队列数据 for (;;) { if (global_video_state->quit) {// 检查退出进程标识 ret = -1;// 操作失败 break; }//end for if pktlist = q->first_pkt;// 传递将队列首个数据包指针 if (pktlist) {// 检查数据包是否为空(队列是否有数据) q->first_pkt = pktlist->next;// 队列首节点指针后移 if (!q->first_pkt) {// 检查首节点的后继节点是否存在 q->last_pkt = NULL;// 若不存在，则将尾节点指针置空 } q->nb_packets--;// 队列长度-1 q->size -= pktlist->pkt.size;// 更新队列编码数据的缓存长度 *pkt = pktlist->pkt;// 将队列首节点数据返回 av_free(pktlist);// 清空临时节点数据(清空首节点数据，首节点出队列) ret = 1;// 操作成功 break; } else if (!block) { ret = 0; break; } else {// 队列处于未就绪状态，此时通过SDL_CondWait函数等待qready就绪信号，并暂时对互斥量解锁 /*--------------------- * 等待队列就绪信号qready，并对互斥量暂时解锁 * 此时线程处于阻塞状态，并置于等待条件就绪的线程列表上 * 使得该线程只在临界区资源就绪后才被唤醒，而不至于线程被频繁切换 * 该函数返回时，互斥量再次被锁住，并执行后续操作 --------------------*/ SDL_CondWait(q->qready, q->qlock);// 暂时解锁互斥量并将自己阻塞，等待临界区资源就绪(等待SDL_CondSignal发出临界区资源就绪的信号) } }//end for for-loop SDL_UnlockMutex(q->qlock);// 释放互斥量 return ret; } // 创建/重置图像帧，为图像帧分配内存空间 void alloc_picture(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {// 检查图像帧是否已存在 // We already have one make another, bigger/smaller. av_frame_free(&vp->avframe_yuv420p); } vp->width = is->video_st->codec->width;// 设置图像帧宽度 vp->height = is->video_st->codec->height;// 设置图像帧高度 SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 vp->allocated = 1;// 图像帧像素缓冲区已分配内存 // AV_PIX_FMT_YUV420P格式的视频帧 vp->avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型 int buffer_size =av_image_get_buffer_size(AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 一帧视频像素数据宽 = 视频宽 is->video_st->codec->height,// 一帧视频像素数据高 = 视频高 1);// 字节对齐方式，默认是1 // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 av_image_fill_arrays(vp->avframe_yuv420p->data,// 目标视频帧数据 vp->avframe_yuv420p->linesize,// 目标视频帧行大小 out_buffer,// 原始数据 AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 视频宽 is->video_st->codec->height,//视频高 1);// 字节对齐方式 SDL_CondSignal(is->pictq_ready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } /*--------------------------- * queue_picture：图像帧插入队列等待渲染 * @is：全局状态参数集 * @pFrame：保存图像解码数据的结构体 * 1、首先检查图像帧队列(数组)是否存在空间插入新的图像，若没有足够的空间插入图像则使当前线程休眠等待 * 2、在初始化的条件下，队列(数组)中VideoPicture的bmp对象(YUV overlay)尚未分配空间，通过FF_ALLOC_EVENT事件的方法调用alloc_picture分配空间 * 3、当队列(数组)中所有VideoPicture的bmp对象(YUV overlay)均已分配空间的情况下，直接跳过步骤2向bmp对象拷贝像素数据，像素数据在进行格式转换后执行拷贝操作 ---------------------------*/ int queue_picture(VideoState *is, AVFrame *pFrame) { /*--------1、检查队列是否有插入空间-------*/ // Wait until we have space for a new pic. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (is->pictq_size >= VIDEO_PICTURE_QUEUE_SIZE && !is->quit) {// 检查队列当前长度 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待pictq_ready信号 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } /*-------2、初始化/重置YUV overlay-------*/ // windex is set to 0 initially. VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列中抽取图像帧对象 // Allocate or resize the buffer，检查YUV overlay是否已存在，否则初始化YUV overlay，分配像素缓存空间 if (!vp->avframe_yuv420p || vp->width!=is->video_st->codec->width || vp->height!=is->video_st->codec->height) { vp->allocated = 0;// 图像帧未分配空间 // We have to do it in the main thread. SDL_Event event;// SDL事件对象 event.type = FF_ALLOC_EVENT;//指定分配图像帧内存事件 event.user.data1 = is;//传递用户数据 SDL_PushEvent(&event);//发送SDL事件 // Wait until we have a picture allocated. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (!vp->allocated && !is->quit) {// 检查当前图像帧是否已初始化 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待alloc_picture发送pictq_ready信号唤醒当前线程 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } }// end for if /*--------3、拷贝视频帧到YUV overlay-------*/ // We have a place to put our picture on the queue. if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // Convert the image into YUV format that SDL uses，将解码后的图像帧转换为AV_PIX_FMT_YUV420P格式，并拷贝到图像帧队列 sws_scale(is->sws_ctx, (uint8_t const * const *)pFrame->data, pFrame->linesize, 0, is->video_st->codec->height, vp->avframe_yuv420p->data, vp->avframe_yuv420p->linesize); // Now we inform our display thread that we have a pic ready. if (++is->pictq_windex == VIDEO_PICTURE_QUEUE_SIZE) {//更新并检查当前图像帧队列写入位置 is->pictq_windex = 0;//重置图像帧队列写入位置 } SDL_LockMutex(is->pictq_lock);//锁定队列读写锁，保护队列数据 is->pictq_size++;//更新图像帧队列长度 SDL_UnlockMutex(is->pictq_lock);//释放队列读写锁 }// end for if return 0; } int video_current_index = 0; // 视频解码线程函数 int decode_thread(void *arg) { VideoState *is = (VideoState *) arg;// 传递用户数据 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 int frameFinished;// 解码操作是否成功标识 // Allocate video frame，为解码后的视频信息结构体分配空间并完成初始化操作(结构体中的图像缓存按照下面两步手动安装) AVFrame *pFrame = av_frame_alloc(); for (;;) { if (packet_queue_get(&is->videoq,packet,1)video_st->codec, pFrame, &frameFinished, packet); // Did we get a video frame，检查是否解码出完整一帧图像 if (frameFinished) { if (queue_picture(is, pFrame)audio_pkt;// 保存从队列中提取的数据包 for (;;) { while (is->audio_pkt_size>0) {// 检查缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) int got_frame = 0;// 解码操作成功标识，成功返回非零值 // 解码一帧音频数据，并返回消耗的编码数据长度 coded_consumed_size = avcodec_decode_audio4(is->audio_st->codec, &is->audio_frame, &got_frame, pkt); if (coded_consumed_size audio_pkt_size = 0;// 更新编码数据缓存长度 break; } if (got_frame) {// 检查解码操作是否成功 // 计算解码后音频数据长度[output] data_size = av_samples_get_buffer_size(NULL, is->audio_st->codec->channels, is->audio_frame.nb_samples, is->audio_st->codec->sample_fmt, 1); memcpy(is->audio_buf, is->audio_frame.data[0], data_size);// 将解码数据复制到输出缓存 } is->audio_pkt_data += coded_consumed_size;// 更新编码数据缓存指针位置 is->audio_pkt_size -= coded_consumed_size;// 更新缓存中剩余的编码数据长度 if (data_size data) {// 检查数据包是否已从队列中提取 av_packet_unref(pkt);// 释放pkt中保存的编码数据 } if (is->quit) {// 检查退出进程标识 return -1; } // Next packet，从队列中提取数据包到pkt if (packet_queue_get(&is->audioq, pkt, 1) audio_pkt_data = pkt->data;// 传递编码数据缓存指针 is->audio_pkt_size = pkt->size;// 传递编码数据缓存长度 } } /*------Audio Callback------- * 音频输出回调函数，sdl通过该回调函数将解码后的pcm数据送入声卡播放, * sdl通常一次会准备一组缓存pcm数据，通过该回调送入声卡，声卡根据音频pts依次播放pcm数据 * 待送入缓存的pcm数据完成播放后，再载入一组新的pcm缓存数据(每次音频输出缓存为空时，sdl就调用此函数填充音频输出缓存，并送入声卡播放) * When we begin playing audio, SDL will continually call this callback function * and ask it to fill the audio buffer with a certain number of bytes * The audio function callback takes the following parameters: * stream: A pointer to the audio buffer to be filled，输出音频数据到声卡缓存 * len: The length (in bytes) of the audio buffer,缓存长度wanted_spec.samples=SDL_AUDIO_BUFFER_SIZE(1024) --------------------------*/ void audio_callback(void *userdata, Uint8 *stream, int len) { VideoState *is = (VideoState *) userdata;// 传递用户数据 int wt_stream_len, audio_size;// 每次写入stream的数据长度，解码后的数据长度 while (len > 0) {//检查音频缓存的剩余长度 if (is->audio_buf_index >= is->audio_buf_size) {// 检查是否需要执行解码操作 // We have already sent all our data; get more，从缓存队列中提取数据包、解码，并返回解码后的数据长度，audio_buf缓存中可能包含多帧解码后的音频数据 audio_size = audio_decode_frame(is); if (audio_size audio_buf_size = 1024; memset(is->audio_buf, 0, is->audio_buf_size);// 全零重置缓冲区 } else { is->audio_buf_size = audio_size;// 返回packet中包含的原始音频数据长度(多帧) } is->audio_buf_index = 0;// 初始化累计写入缓存长度 }//end for if wt_stream_len=is->audio_buf_size-is->audio_buf_index;// 计算解码缓存剩余长度 if (wt_stream_len > len) {// 检查每次写入缓存的数据长度是否超过指定长度(1024) wt_stream_len = len;// 指定长度从解码的缓存中取数据 } // 每次从解码的缓存数据中以指定长度抽取数据并写入stream传递给声卡 memcpy(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, wt_stream_len); len -= wt_stream_len;// 更新解码音频缓存的剩余长度 stream += wt_stream_len;// 更新缓存写入位置 is->audio_buf_index += wt_stream_len;// 更新累计写入缓存数据长度 }//end for while } // 根据指定类型打开流，找到对应的解码器、创建对应的音频配置、保存关键信息到 VideoState、启动音频和视频线程 int stream_component_open(VideoState *is, int stream_index) { AVFormatContext *pFormatCtx = is->pFormatCtx;// 传递文件容器的封装信息及码流参数 AVCodecContext *codecCtx = NULL;// 解码器上下文对象，解码器依赖的相关环境、状态、资源以及参数集的接口指针 AVCodec *codec = NULL;// 保存编解码器信息的结构体，提供编码与解码的公共接口，可以看作是编码器与解码器的一个全局变量 //检查输入的流类型是否在合理范围内 if (stream_index = pFormatCtx->nb_streams) { return -1; } // Get a pointer to the codec context for the video stream. codecCtx = pFormatCtx->streams[stream_index]->codec;// 取得解码器上下文 if (codecCtx->codec_type == AVMEDIA_TYPE_AUDIO) {//检查解码器类型是否为音频解码器 SDL_AudioSpec wanted_spec, spec;//SDL_AudioSpec a structure that contains the audio output format，创建 SDL_AudioSpec 结构体，设置音频播放数据 // Set audio settings from codec info,SDL_AudioSpec a structure that contains the audio output format // 创建SDL_AudioSpec结构体，设置音频播放参数 wanted_spec.freq = codecCtx->sample_rate;//采样频率 DSP frequency -- samples per second wanted_spec.format = AUDIO_S16SYS;//采样格式 Audio data format wanted_spec.channels = codecCtx->channels;//声道数 Number of channels: 1 mono, 2 stereo wanted_spec.silence = 0;//无输出时是否静音 //默认每次读音频缓存的大小，推荐值为 512~8192，ffplay使用的是1024 specifies a unit of audio data refers to the size of the audio buffer in sample frames wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE; wanted_spec.callback = audio_callback;//设置读取音频数据的回调接口函数 the function to call when the audio device needs more data wanted_spec.userdata = is;//传递用户数据 /*--------------------------- * 以指定参数打开音频设备，并返回与指定参数最为接近的参数，该参数为设备实际支持的音频参数 * Opens the audio device with the desired parameters(wanted_spec) * return another specs we actually be using * and not guaranteed to get what we asked for --------------------------*/ if (SDL_OpenAudio(&wanted_spec, &spec) codec_id); AVDictionary *optionsDict = NULL; if (!codec || (avcodec_open2(codecCtx, codec, &optionsDict) name); // 检查解码器类型 switch(codecCtx->codec_type) { case AVMEDIA_TYPE_AUDIO:// 音频解码器 is->audioStream = stream_index;// 音频流类型标号初始化 is->audio_st = pFormatCtx->streams[stream_index]; is->audio_buf_size = 0;// 解码后的多帧音频数据长度 is->audio_buf_index = 0;//累 计写入stream的长度 memset(&is->audio_pkt, 0, sizeof(is->audio_pkt)); packet_queue_init(&is->audioq);// 音频数据包队列初始化 SDL_PauseAudio(0);// audio callback starts running again，开启音频设备，如果这时候没有获得数据那么它就静音 break; case AVMEDIA_TYPE_VIDEO:// 视频解码器 is->videoStream = stream_index;// 视频流类型标号初始化 is->video_st = pFormatCtx->streams[stream_index]; packet_queue_init(&is->videoq);// 视频数据包队列初始化 is->decode_tid = SDL_CreateThread(decode_thread,\"视频解码线程\" ,is);// 创建视频解码线程 // Initialize SWS context for software scaling，设置图像转换像素格式为AV_PIX_FMT_YUV420P is->sws_ctx = sws_getContext(is->video_st->codec->width, is->video_st->codec->height, is->video_st->codec->pix_fmt, is->video_st->codec->width, is->video_st->codec->height, AV_PIX_FMT_YUV420P, SWS_BILINEAR, NULL, NULL, NULL); break; default: break; } return 0; } // 编码数据包解析线程函数(从视频文件中解析出音视频编码数据单元，一个AVPacket的data通常对应一个NAL) int parse_thread(void *arg) { VideoState *is = (VideoState *)arg;// 传递用户参数 global_video_state = is;// 传递全局状态参量结构体 /*------------------------- * 打开封装格式 * 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 * 参数一：封装格式上下文 * 参数二：视频路径 * 参数三：指定输入的格式 * 参数四：设置默认参数 --------------------------*/ AVFormatContext *avformat_context = NULL;// 参数一：封装格式上下文 int avformat_open_input_result = avformat_open_input(&avformat_context, is->filename, NULL, NULL); if (avformat_open_input_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"查找音视频流\\n\"); return -1; } is->pFormatCtx = avformat_context;//传递文件容器封装信息及码流参数 /*------------------------- * 查找码流 * 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 * 参数一：封装格式上下文 * 参数二：指定默认配置 -------------------------*/ int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result videoStream = -1;//视频流类型标号初始化为-1 is->audioStream = -1;//音频流类型标号初始化为-1 // 视频流类型标号初始化为-1 int av_video_stream_index = -1; // 音频流类型标号初始化为-1 int av_audio_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 若文件中包含有视频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_video_stream_index = i; } // 若文件中包含有音频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_audio_stream_index = i; } } // 检查文件中是否存在视频流 if (av_video_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到视频流\\n\"); goto fail;//跳转至异常处理 return -1; } // 检查文件中是否存在音频流 if (av_audio_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到音频流\\n\"); goto fail;//跳转至异常处理 return -1; } stream_component_open(is, av_audio_stream_index);// 根据指定类型打开音频流 stream_component_open(is, av_video_stream_index);// 根据指定类型打开视频流 // Main decode loop. for (;;) { if (is->quit) {//检查退出进程标识 break; } // Seek stuff goes here，检查音视频编码数据包队列长度是否溢出 if (is->audioq.size > MAX_AUDIOQ_SIZE || is->videoq.size > MAX_VIDEOQ_SIZE) { SDL_Delay(10); continue; } /*----------------------- * read in a packet and store it in the AVPacket struct * ffmpeg allocates the internal data for us,which is pointed to by packet.data * this is freed by the av_free_packet() -----------------------*/ // 负责保存压缩编码数据相关信息的结构体,每帧图像由一到多个packet包组成 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 if (av_read_frame(is->pFormatCtx, packet) pFormatCtx->pb->error == 0) { SDL_Delay(100); // No error; wait for user input. continue; } else { break; } } // Is this a packet from the video stream? if (packet->stream_index == is->videoStream) {// 检查数据包是否为视频类型 packet_queue_put(&is->videoq, packet);// 向队列中插入数据包 } else if (packet->stream_index == is->audioStream) {// 检查数据包是否为音频类型 packet_queue_put(&is->audioq, packet);// 向队列中插入数据包 } else {// 检查数据包是否为字幕类型 av_packet_unref(packet);// 释放packet中保存的(字幕)编码数据 } } // All done - wait for it. while (!is->quit) { SDL_Delay(100); } fail:// 异常处理 if (1) { SDL_Event event;// SDL事件对象 event.type = FF_QUIT_EVENT;// 指定退出事件类型 event.user.data1 = is;// 传递用户数据 SDL_PushEvent(&event);// 将该事件对象压入SDL后台事件队列 } return 0; } int init_sdl(VideoState *is) { // 初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } // 初始化SDL窗口 SDL_Window* sdl_window = SDL_CreateWindow(\"FFmpeg+SDL播放视频\",// 参数一：窗口名称 SDL_WINDOWPOS_CENTERED,// 参数二：窗口在屏幕上的x坐标 SDL_WINDOWPOS_CENTERED,// 参数三：窗口在屏幕上的y坐标 is->video_width,// 参数四：窗口在屏幕上宽 is->video_height,// 参数五：窗口在屏幕上高 SDL_WINDOW_OPENGL);// 参数六：窗口状态(打开) if (sdl_window == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建渲染器 // 定义渲染器区域 SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window,// 渲染目标创建 -1, // 从那里开始渲染(-1:表示从第一个位置开始) 0);// 渲染类型(软件渲染) if (sdl_renderer == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建纹理 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer,// 渲染器 SDL_PIXELFORMAT_IYUV,// 像素数据格式 SDL_TEXTUREACCESS_STREAMING,// 绘制方式：频繁绘制- is->video_width,// 纹理宽 is->video_height);// 纹理高 if (sdl_texture == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } is->sdl_renderer = sdl_renderer; is->sdl_texture = sdl_texture; return 0; } // SDL入口 extern \"C\" int main(int argc, char *argv[]) { /*------------------------- * 注册组件 * 注册所有ffmpeg支持的多媒体格式及编解码器 -------------------------*/ av_register_all(); // 创建全局状态对象 VideoState *is= (VideoState *)av_mallocz(sizeof(VideoState)); av_strlcpy(is->filename, \"/storage/emulated/0/Download/test.mov\", sizeof(is->filename));// 复制视频文件路径名 is->video_width = 640; is->video_height = 352; is->pictq_lock = SDL_CreateMutex();// 创建编码数据包队列互斥锁对象 is->pictq_ready = SDL_CreateCond();// 创建编码数据包队列就绪条件对象 int init_sdl_result = init_sdl(is); if (init_sdl_result parse_tid = SDL_CreateThread(parse_thread, \"编码数据包解析线程\", is); if (!is->parse_tid) {// 检查线程是否创建成功 av_free(is); return -1; } // SDL事件对象 SDL_Event event; for (;;) {// SDL事件循环 SDL_WaitEvent(&event);// 主线程阻塞，等待事件到来 switch(event.type) {// 事件到来后唤醒主线程，检查事件类型 case FF_QUIT_EVENT: case SDL_QUIT:// 退出进程事件 is->quit = 1; // If the video has finished playing, then both the picture and audio queues are waiting for more data. // Make them stop waiting and terminate normally.. avcodec_close(is->video_st->codec); avformat_free_context(is->pFormatCtx); SDL_CondSignal(is->audioq.qready);// 发出队列就绪信号避免死锁 SDL_CondSignal(is->videoq.qready); SDL_DestroyTexture(is->sdl_texture); SDL_DestroyRenderer(is->sdl_renderer); SDL_Quit(); return 0; case FF_ALLOC_EVENT: alloc_picture(event.user.data1);// 分配视频帧事件响应函数 break; case FF_REFRESH_EVENT:// 视频显示刷新事件 video_refresh_timer(event.user.data1);// 视频显示刷新事件响应函数 break; default: break; } } return 0; } "},"pages/FFmpeg/FFmpeg_SDL同步视频.html":{"url":"pages/FFmpeg/FFmpeg_SDL同步视频.html","title":"FFmpeg+SDL同步视频","keywords":"","body":"FFmpeg+SDL同步视频 Android工程代码 ffmpeg播放器实现详解 - 视频同步控制：https://www.cnblogs.com/breakpointlab/p/15771348.html 源代码一览 #include #include #include #include #include \"SDL.h\" #include #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 #define AV_SYNC_THRESHOLD 0.01//前后两帧间的显示时间间隔的最小值0.01s #define AV_NOSYNC_THRESHOLD 10.0//最小刷新间隔时间10ms #define MAX_AUDIOQ_SIZE (5 * 16 * 1024) #define MAX_VIDEOQ_SIZE (5 * 256 * 1024) #define FF_ALLOC_EVENT (SDL_USEREVENT) #define FF_REFRESH_EVENT (SDL_USEREVENT + 1) #define FF_QUIT_EVENT (SDL_USEREVENT + 2) #define VIDEO_PICTURE_QUEUE_SIZE 1 extern \"C\" { #include #include \"libavformat/avformat.h\" #include #include #include #include } /*-------链表节点结构体-------- typedef struct AVPacketList { AVPacket pkt;//链表数据 struct AVPacketList *next;//链表后继节点 } AVPacketList; ---------------------------*/ // 数据包队列(链表)结构体 typedef struct PacketQueue { AVPacketList *first_pkt, *last_pkt;// 队列首尾节点指针 int nb_packets;// 队列长度 int size;// 保存编码数据的缓存长度，size=packet->size SDL_mutex *qlock;// 队列互斥量，保护队列数据 SDL_cond *qready;// 队列就绪条件变量 } PacketQueue; // 图像帧结构体 typedef struct VideoPicture { AVFrame *avframe_yuv420p; int width, height;//Source height & width. int allocated;//是否分配内存空间，视频帧转换为SDL overlay标识 double pts;//当前图像帧的绝对显示时间戳 } VideoPicture; typedef struct VideoState { AVFormatContext *pFormatCtx;// 保存文件容器封装信息及码流参数的结构体 AVStream *video_st;// 视频流信息结构体 AVStream *audio_st;//音频流信息结构体 struct SwsContext *sws_ctx;// 描述转换器参数的结构体 PacketQueue videoq;// 视频编码数据包队列(编码数据队列，以链表方式实现) VideoPicture pictq[VIDEO_PICTURE_QUEUE_SIZE]; int pictq_size, pictq_rindex, pictq_windex;// 队列长度，读/写位置索引 SDL_mutex *pictq_lock;// 队列读写锁对象，保护图像帧队列数据 SDL_cond *pictq_ready;// 队列就绪条件变量 SDL_Rect sdl_rect; SDL_Renderer* sdl_renderer; SDL_Texture* sdl_texture; PacketQueue audioq;// 音频编码数据包队列(编码数据队列，以链表方式实现) uint8_t audio_buf[(MAX_AUDIO_FRAME_SIZE*3)/2];//保存解码一个packet后的多帧原始音频数据(解码数据队列，以数组方式实现) unsigned int audio_buf_size;//解码后的多帧音频数据长度 unsigned int audio_buf_index;//累计写入stream的长度 uint8_t *audio_pkt_data;//编码数据缓存指针位置 int audio_pkt_size;//缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) AVPacket audio_pkt;//保存从队列中提取的数据包 AVFrame audio_frame;//保存从数据包中解码的音频数据 int video_width; int video_height; char filename[1024];// 输入文件完整路径名 int videoStream, audioStream;// 音视频流类型标号 SDL_Thread *parse_tid;// 编码数据包解析线程id SDL_Thread *decode_tid;// 解码线程id int quit;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 //video/audio_clock save pts of last decoded frame/predicted pts of next decoded frame double video_clock;//keep track of how much time has passed according to the video double audio_clock; double frame_timer;//视频播放到当前帧时的累计已播放时间 double frame_last_pts;//上一帧图像的显示时间戳，用于在video_refersh_timer中保存上一帧的pts值 double frame_last_delay;//上一帧图像的动态刷新延迟时间 } VideoState;// Since we only have one decoding thread, the Big Struct can be global in case we need it. VideoState *global_video_state; /*------取得当前播放音频数据的pts------ * 音视频同步的原理是根据音频的pts来控制视频的播放 * 也就是说在视频解码一帧后，是否显示以及显示多长时间，是通过该帧的PTS与同时正在播放的音频的PTS比较而来的 * 如果音频的PTS较大，则视频准备完毕立即刷新，否则等待 * * 因为pcm数据采用audio_callback回调方式进行播放 * 对于音频播放我们只能得到写入回调函数前缓存音频帧的pts，而无法得到当前播放帧的pts(需要采用当前播放音频帧的pts作为参考时钟) * 考虑到音频的大小与播放时间成正比(相同采样率)，那么当前时刻正在播放的音频帧pts(位于回调函数缓存中) * 就可以根据已送入声卡的pcm数据长度、缓存中剩余pcm数据长度，缓存长度及采样率进行推算了 --------------------------------*/ double get_audio_clock(VideoState *is) { double pts=is->audio_clock;//Maintained in the audio thread，取得解码操作完成时的当前播放时间戳 //还未(送入声卡)播放的剩余原始音频数据长度，等于解码后的多帧原始音频数据长度-累计送入声卡的长度 int hw_buf_size=is->audio_buf_size-is->audio_buf_index;//计算当前音频解码数据缓存索引位置 int bytes_per_sec=0;//每秒的原始音频字节数 int pcm_bytes=is->audio_st->codec->channels*2;//每组原始音频数据字节数=声道数*每声道数据字节数 if (is->audio_st) { bytes_per_sec=is->audio_st->codec->sample_rate*pcm_bytes;//计算每秒的原始音频字节数 } if (bytes_per_sec) {//检查每秒的原始音频字节数 pts-=(double)hw_buf_size/bytes_per_sec;//根据送入声卡缓存的索引位置，往前倒推计算当前时刻的音频播放时间戳pts } return pts;//返回当前正在播放的音频时间戳 } // 定时器触发的回调函数 static Uint32 sdl_refresh_timer_cb(Uint32 interval, void *opaque) { SDL_Event event;//SDL事件对象 event.type = FF_REFRESH_EVENT;//视频显示刷新事件 event.user.data1 = opaque;//传递用户数据 SDL_PushEvent(&event);//发送事件 return 0; // 0 means stop timer. } /*--------------------------- * Schedule a video refresh in 'delay' ms. * 告诉sdl在指定的延时后来推送一个 FF_REFRESH_EVENT 事件 * 这个事件将在事件队列里触发sdl_refresh_timer_cb函数的调用 --------------------------*/ static void schedule_refresh(VideoState *is, int delay) { SDL_AddTimer(delay, sdl_refresh_timer_cb, is);//在指定的时间(ms)后回调用户指定的函数 } // 视频(图像)帧渲染 void video_display(VideoState *is) { SDL_Rect rect;// SDL矩形对象 VideoPicture *vp;// 图像帧结构体指针 vp = &is->pictq[is->pictq_rindex];//从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // 设置纹理数据 SDL_UpdateTexture(is->sdl_texture, // 纹理 NULL,// 渲染区域 vp->avframe_yuv420p->data[0],// 需要渲染数据：视频像素数据帧 vp->avframe_yuv420p->linesize[0]);// 帧宽 // 将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) is->sdl_rect.x = 100; is->sdl_rect.y = 100; is->sdl_rect.w = vp->width; is->sdl_rect.h = vp->height; SDL_RenderClear(is->sdl_renderer); SDL_RenderCopy(is->sdl_renderer, is->sdl_texture, NULL, &is->sdl_rect); // 呈现画面帧 SDL_RenderPresent(is->sdl_renderer); }// end for if }// end for video_display // 显示刷新函数(FF_REFRESH_EVENT响应函数) int video_current_index = 0; void video_refresh_timer(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp;//图像帧对象 //delay-前后帧间的显示时间间隔，diff-图像帧显示与音频帧播放间的时间差 //sync_threshold-前后帧间的最小时间差，actual_delay-当前帧-下已帧的显示时间间隔(动态时间、真实时间、绝对时间) double delay,diff,sync_threshold,actual_delay,ref_clock;//ref_clock-音频时间戳 if (is->video_st) { if (is->pictq_size == 0) {// 检查图像帧队列是否有待显示图像 schedule_refresh(is, 1);//若队列为空，则发送显示刷新事件并再次进入video_refresh_timer函数 } else {// 刷新图像 vp = &is->pictq[is->pictq_rindex];//从显示队列中取得等待显示的图像帧 //计算当前帧和前一帧显示(pts)的间隔时间(显示时间戳的差值) delay = vp->pts - is->frame_last_pts;//The pts from last time，前后帧间的时间差 if (delay = 1.0) {//检查时间间隔是否在合理范围 // If incorrect delay, use previous one delay = is->frame_last_delay;//沿用之前的动态刷新间隔时间 } // Save for next time is->frame_last_delay = delay;//保存上一帧图像的动态刷新延迟时间 is->frame_last_pts = vp->pts;//保存上一帧图像的显示时间戳 // Update delay to sync to audio，取得声音播放时间戳(作为视频同步的参考时间) ref_clock=get_audio_clock(is);//根据Audio clock来判断Video播放的快慢，获取当前播放声音的时间戳 //也就是说在diff这段时间中声音是匀速发生的，但是在delay这段时间frame的显示可能就会有快慢的区别 diff=vp->pts-ref_clock;//计算图像帧显示与音频帧播放间的时间差 //根据时间差调整播放下一帧的延迟时间，以实现同步 Skip or repeat the frame，Take delay into account sync_threshold=(delay>AV_SYNC_THRESHOLD) ? delay : AV_SYNC_THRESHOLD;//比较前后两帧间的显示时间间隔与最小时间间隔 //判断音视频不同步条件，即音视频间的时间差 & 前后帧间的时间差该阈值则为快进模式，不存在音视频同步问题 if (fabs(diff)=sync_threshold) {//比较两帧画面间的显示时间与两帧画面间声音的播放时间，快了，加倍delay delay=2*delay; } }//如果diff(明显)大于AV_NOSYNC_THRESHOLD，即快进的模式了，画面跳动太大，不存在音视频同步的问题了 //更新视频播放到当前帧时的已播放时间值(所有图像帧动态播放累计时间值-真实值)，frame_timer一直累加在播放过程中我们计算的延时 is->frame_timer+=delay; //每次计算frame_timer与系统时间的差值(以系统时间为基准时间)，将frame_timer与系统时间(绝对时间)相关联的目的 actual_delay=is->frame_timer-(av_gettime()/1000000.0);//Computer the REAL delay if (actual_delay pts: %f，ref_clock：%f，actual_delay：%f\", video_current_index, vp->pts, ref_clock, actual_delay); // Update queue for next picture! if (++is->pictq_rindex == VIDEO_PICTURE_QUEUE_SIZE) {// 更新并检查图像帧队列读位置索引 is->pictq_rindex = 0;// 重置读位置索引 } SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 is->pictq_size--;// 更新图像帧队列长度 SDL_CondSignal(is->pictq_ready);// 发送队列就绪信号 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } } else { schedule_refresh(is, 100); } } // 数据包队列初始化函数 void packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue));// 全零初始化队列结构体对象 q->qlock = SDL_CreateMutex();// 创建互斥量对象 q->qready = SDL_CreateCond();// 创建条件变量对象 } // 向队列中插入数据包 int packet_queue_put(PacketQueue *q, AVPacket *pkt) { /*-------准备队列(链表)节点对象------*/ AVPacketList *pktlist=(AVPacketList *)av_malloc(sizeof(AVPacketList));// 在堆上创建链表节点对象 if (!pktlist) {// 检查链表节点对象是否创建成功 return -1; } pktlist->pkt = *pkt;// 将输入数据包赋值给新建链表节点对象中的数据包对象 pktlist->next = NULL;// 链表后继指针为空 // if (av_packet_ref(pkt, pkt) qlock);// 队列互斥量加锁，保护队列数据 if (!q->last_pkt) {// 检查队列尾节点是否存在(检查队列是否为空) q->first_pkt = pktlist;// 若不存在(队列尾空)，则将当前节点作队列为首节点 } else { q->last_pkt->next = pktlist;// 若已存在尾节点，则将当前节点挂到尾节点的后继指针上，并作为新的尾节点 } q->last_pkt = pktlist;// 将当前节点作为新的尾节点 q->nb_packets++;// 队列长度+1 q->size += pktlist->pkt.size;// 更新队列编码数据的缓存长度 SDL_CondSignal(q->qready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(q->qlock);// 释放互斥量 return 0; } // 从队列中提取数据包，并将提取的数据包出队列 static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block) { AVPacketList *pktlist;// 临时链表节点对象指针 int ret;// 操作结果 SDL_LockMutex(q->qlock);// 队列互斥量加锁，保护队列数据 for (;;) { if (global_video_state->quit) {// 检查退出进程标识 ret = -1;// 操作失败 break; }//end for if pktlist = q->first_pkt;// 传递将队列首个数据包指针 if (pktlist) {// 检查数据包是否为空(队列是否有数据) q->first_pkt = pktlist->next;// 队列首节点指针后移 if (!q->first_pkt) {// 检查首节点的后继节点是否存在 q->last_pkt = NULL;// 若不存在，则将尾节点指针置空 } q->nb_packets--;// 队列长度-1 q->size -= pktlist->pkt.size;// 更新队列编码数据的缓存长度 *pkt = pktlist->pkt;// 将队列首节点数据返回 av_free(pktlist);// 清空临时节点数据(清空首节点数据，首节点出队列) ret = 1;// 操作成功 break; } else if (!block) { ret = 0; break; } else {// 队列处于未就绪状态，此时通过SDL_CondWait函数等待qready就绪信号，并暂时对互斥量解锁 /*--------------------- * 等待队列就绪信号qready，并对互斥量暂时解锁 * 此时线程处于阻塞状态，并置于等待条件就绪的线程列表上 * 使得该线程只在临界区资源就绪后才被唤醒，而不至于线程被频繁切换 * 该函数返回时，互斥量再次被锁住，并执行后续操作 --------------------*/ SDL_CondWait(q->qready, q->qlock);// 暂时解锁互斥量并将自己阻塞，等待临界区资源就绪(等待SDL_CondSignal发出临界区资源就绪的信号) } }//end for for-loop SDL_UnlockMutex(q->qlock);// 释放互斥量 return ret; } // 创建/重置图像帧，为图像帧分配内存空间 void alloc_picture(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {// 检查图像帧是否已存在 // We already have one make another, bigger/smaller. av_frame_free(&vp->avframe_yuv420p); } vp->width = is->video_st->codec->width;// 设置图像帧宽度 vp->height = is->video_st->codec->height;// 设置图像帧高度 SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 vp->allocated = 1;// 图像帧像素缓冲区已分配内存 // AV_PIX_FMT_YUV420P格式的视频帧 vp->avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型 int buffer_size =av_image_get_buffer_size(AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 一帧视频像素数据宽 = 视频宽 is->video_st->codec->height,// 一帧视频像素数据高 = 视频高 1);// 字节对齐方式，默认是1 // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 av_image_fill_arrays(vp->avframe_yuv420p->data,// 目标视频帧数据 vp->avframe_yuv420p->linesize,// 目标视频帧行大小 out_buffer,// 原始数据 AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 视频宽 is->video_st->codec->height,//视频高 1);// 字节对齐方式 SDL_CondSignal(is->pictq_ready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } /*--------------------------- * queue_picture：图像帧插入队列等待渲染 * @is：全局状态参数集 * @pFrame：保存图像解码数据的结构体 * 1、首先检查图像帧队列(数组)是否存在空间插入新的图像，若没有足够的空间插入图像则使当前线程休眠等待 * 2、在初始化的条件下，队列(数组)中VideoPicture的bmp对象(YUV overlay)尚未分配空间，通过FF_ALLOC_EVENT事件的方法调用alloc_picture分配空间 * 3、当队列(数组)中所有VideoPicture的bmp对象(YUV overlay)均已分配空间的情况下，直接跳过步骤2向bmp对象拷贝像素数据，像素数据在进行格式转换后执行拷贝操作 ---------------------------*/ int queue_picture(VideoState *is, AVFrame *pFrame, double pts) { /*--------1、检查队列是否有插入空间-------*/ // Wait until we have space for a new pic. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (is->pictq_size >= VIDEO_PICTURE_QUEUE_SIZE && !is->quit) {// 检查队列当前长度 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待pictq_ready信号 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } /*-------2、初始化/重置YUV overlay-------*/ // windex is set to 0 initially. VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列中抽取图像帧对象 // Allocate or resize the buffer，检查YUV overlay是否已存在，否则初始化YUV overlay，分配像素缓存空间 if (!vp->avframe_yuv420p || vp->width!=is->video_st->codec->width || vp->height!=is->video_st->codec->height) { vp->allocated = 0;// 图像帧未分配空间 // We have to do it in the main thread. SDL_Event event;// SDL事件对象 event.type = FF_ALLOC_EVENT;//指定分配图像帧内存事件 event.user.data1 = is;//传递用户数据 SDL_PushEvent(&event);//发送SDL事件 // Wait until we have a picture allocated. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (!vp->allocated && !is->quit) {// 检查当前图像帧是否已初始化 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待alloc_picture发送pictq_ready信号唤醒当前线程 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } }// end for if /*--------3、拷贝视频帧到YUV overlay-------*/ // We have a place to put our picture on the queue. if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // Convert the image into YUV format that SDL uses，将解码后的图像帧转换为AV_PIX_FMT_YUV420P格式，并拷贝到图像帧队列 sws_scale(is->sws_ctx, (uint8_t const * const *)pFrame->data, pFrame->linesize, 0, is->video_st->codec->height, vp->avframe_yuv420p->data, vp->avframe_yuv420p->linesize); vp->pts = pts;//传递当前图像帧的绝对显示时间戳 // Now we inform our display thread that we have a pic ready. if (++is->pictq_windex == VIDEO_PICTURE_QUEUE_SIZE) {//更新并检查当前图像帧队列写入位置 is->pictq_windex = 0;//重置图像帧队列写入位置 } SDL_LockMutex(is->pictq_lock);//锁定队列读写锁，保护队列数据 is->pictq_size++;//更新图像帧队列长度 SDL_UnlockMutex(is->pictq_lock);//释放队列读写锁 }// end for if return 0; } /*--------------------------- * 更新内部视频播放计时器(记录视频已经播时间(video_clock)） * @is：全局状态参数集 * @src_frame：当前(输入的)(待更新的)图像帧对象 * @pts：当前图像帧的显示时间戳 * update the PTS to be in sync ---------------------------*/ double synchronize_video(VideoState *is, AVFrame *src_frame, double pts) { /*----------检查显示时间戳----------*/ if (pts != 0) {//检查显示时间戳是否有效 // If we have pts, set video clock to it. is->video_clock = pts;//用显示时间戳更新已播放时间 } else {//若获取不到显示时间戳 // If we aren't given a pts, set it to the clock. pts = is->video_clock;//用已播放时间更新显示时间戳 } /*--------更新视频已经播时间--------*/ // Update the video clock，若该帧要重复显示(取决于repeat_pict)，则全局视频播放时序video_clock应加上重复显示的数量*帧率 double frame_delay = av_q2d(is->video_st->codec->time_base);//该帧显示完将要花费的时间 // If we are repeating a frame, adjust clock accordingly,若存在重复帧，则在正常播放的前后两帧图像间安排渲染重复帧 frame_delay += src_frame->repeat_pict*(frame_delay*0.5);//计算渲染重复帧的时值(类似于音符时值) is->video_clock += frame_delay;//更新视频播放时间 // printf(\"repeat_pict=%d \\n\",src_frame->repeat_pict); return pts;//此时返回的值即为下一帧将要开始显示的时间戳 } // 视频解码线程函数 int decode_thread(void *arg) { VideoState *is = (VideoState *) arg;// 传递用户数据 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 int frameFinished;// 解码操作是否成功标识 // Allocate video frame，为解码后的视频信息结构体分配空间并完成初始化操作(结构体中的图像缓存按照下面两步手动安装) AVFrame *pFrame = av_frame_alloc(); double pts;//当前桢在整个视频中的(绝对)时间位置 for (;;) { if (packet_queue_get(&is->videoq,packet,1)pts;// Save global pts to be stored in pFrame in first call. /*----------------------- * Decode video frame，解码完整的一帧数据，并将frameFinished设置为true * 可能无法通过只解码一个packet就获得一个完整的视频帧frame，可能需要读取多个packet才行 * avcodec_decode_video2()会在解码到完整的一帧时设置frameFinished为真 * Technically a packet can contain partial frames or other bits of data * ffmpeg's parser ensures that the packets we get contain either complete or multiple frames * convert the packet to a frame for us and set frameFinisned for us when we have the next frame -----------------------*/ avcodec_decode_video2(is->video_st->codec, pFrame, &frameFinished, packet); //取得编码数据包中的显示时间戳PTS(int64_t),并暂时保存在pts(double)中 // if (packet->dts==AV_NOPTS_VALUE && pFrame->opaque && *(uint64_t*)pFrame->opaque!=AV_NOPTS_VALUE) { // pts = *(uint64_t *)pFrame->opaque; // } else if (packet->dts != AV_NOPTS_VALUE) { // pts = packet->dts; // } else { // pts = 0; // } pts=av_frame_get_best_effort_timestamp(pFrame);//取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 /*------------------------- * 在解码线程函数中计算当前图像帧的显示时间戳 * 1、取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 * 2、根据PTS*time_base来计算当前桢在整个视频中的显示时间戳，即PTS*(1/framerate) * av_q2d把AVRatioal结构转换成double的函数， * 用于计算视频源每个图像帧显示的间隔时间(1/framerate),即返回(time_base->num/time_base->den) -------------------------*/ //根据pts=PTS*time_base={numerator=1,denominator=25}计算当前桢在整个视频中的显示时间戳 pts*=av_q2d(is->video_st->time_base);//time_base为AVRational有理数结构体{num=1,den=25}，记录了视频源每个图像帧显示的间隔时间 // Did we get a video frame，检查是否解码出完整一帧图像 if (frameFinished) { pts = synchronize_video(is, pFrame, pts);//检查当前帧的显示时间戳pts并更新内部视频播放计时器(记录视频已经播时间(video_clock)） if (queue_picture(is, pFrame, pts)audio_pkt;// 保存从队列中提取的数据包 double pts;//音频播放时间戳 for (;;) { while (is->audio_pkt_size>0) {// 检查缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) int got_frame = 0;// 解码操作成功标识，成功返回非零值 // 解码一帧音频数据，并返回消耗的编码数据长度 coded_consumed_size = avcodec_decode_audio4(is->audio_st->codec, &is->audio_frame, &got_frame, pkt); if (coded_consumed_size audio_pkt_size = 0;// 更新编码数据缓存长度 break; } if (got_frame) {// 检查解码操作是否成功 // 计算解码后音频数据长度[output] data_size = av_samples_get_buffer_size(NULL, is->audio_st->codec->channels, is->audio_frame.nb_samples, is->audio_st->codec->sample_fmt, 1); memcpy(is->audio_buf, is->audio_frame.data[0], data_size);// 将解码数据复制到输出缓存 } is->audio_pkt_data += coded_consumed_size;// 更新编码数据缓存指针位置 is->audio_pkt_size -= coded_consumed_size;// 更新缓存中剩余的编码数据长度 if (data_size audio_clock;//用每次更新的音频播放时间更新音频PTS *pts_ptr=pts; /*--------------------- * 当一个packet中包含多个音频帧时 * 通过[解码后音频原始数据长度]及[采样率]来推算一个packet中其他音频帧的播放时间戳pts * 采样频率44.1kHz，量化位数16位，意味着每秒采集数据44.1k个，每个数据占2字节 --------------------*/ pcm_bytes=2*is->audio_st->codec->channels;//计算每组音频采样数据的字节数=每个声道音频采样字节数*声道数 /*----更新audio_clock--- * 一个pkt包含多个音频frame，同时一个pkt对应一个pts(pkt->pts) * 因此，该pkt中包含的多个音频帧的时间戳由以下公式推断得出 * bytes_per_sec=pcm_bytes*is->audio_st->codec->sample_rate * 从pkt中不断的解码，推断(一个pkt中)每帧数据的pts并累加到音频播放时钟 --------------------*/ is->audio_clock+=(double)data_size/(double)(pcm_bytes*is->audio_st->codec->sample_rate); // We have data, return it and come back for more later. return data_size;// 返回解码数据缓存长度 } if (pkt->data) {// 检查数据包是否已从队列中提取 av_packet_unref(pkt);// 释放pkt中保存的编码数据 } if (is->quit) {// 检查退出进程标识 return -1; } // Next packet，从队列中提取数据包到pkt if (packet_queue_get(&is->audioq, pkt, 1) audio_pkt_data = pkt->data;// 传递编码数据缓存指针 is->audio_pkt_size = pkt->size;// 传递编码数据缓存长度 // If update, update the audio clock w/pts if (pkt->pts != AV_NOPTS_VALUE) {//检查音频播放时间戳 //获得一个新的packet的时候，更新audio_clock，用packet中的pts更新audio_clock(一个pkt对应一个pts) is->audio_clock=pkt->pts*av_q2d(is->audio_st->time_base);//更新音频已经播的时间 } } } /*------Audio Callback------- * 音频输出回调函数，sdl通过该回调函数将解码后的pcm数据送入声卡播放, * sdl通常一次会准备一组缓存pcm数据，通过该回调送入声卡，声卡根据音频pts依次播放pcm数据 * 待送入缓存的pcm数据完成播放后，再载入一组新的pcm缓存数据(每次音频输出缓存为空时，sdl就调用此函数填充音频输出缓存，并送入声卡播放) * When we begin playing audio, SDL will continually call this callback function * and ask it to fill the audio buffer with a certain number of bytes * The audio function callback takes the following parameters: * stream: A pointer to the audio buffer to be filled，输出音频数据到声卡缓存 * len: The length (in bytes) of the audio buffer,缓存长度wanted_spec.samples=SDL_AUDIO_BUFFER_SIZE(1024) --------------------------*/ void audio_callback(void *userdata, Uint8 *stream, int len) { VideoState *is = (VideoState *) userdata;// 传递用户数据 int wt_stream_len, audio_size;// 每次写入stream的数据长度，解码后的数据长度 double pts;//音频时间戳 while (len > 0) {//检查音频缓存的剩余长度 if (is->audio_buf_index >= is->audio_buf_size) {// 检查是否需要执行解码操作 // We have already sent all our data; get more，从缓存队列中提取数据包、解码，并返回解码后的数据长度，audio_buf缓存中可能包含多帧解码后的音频数据 audio_size = audio_decode_frame(is, &pts); if (audio_size audio_buf_size = 1024; memset(is->audio_buf, 0, is->audio_buf_size);// 全零重置缓冲区 } else { is->audio_buf_size = audio_size;// 返回packet中包含的原始音频数据长度(多帧) } is->audio_buf_index = 0;// 初始化累计写入缓存长度 }//end for if wt_stream_len=is->audio_buf_size-is->audio_buf_index;// 计算解码缓存剩余长度 if (wt_stream_len > len) {// 检查每次写入缓存的数据长度是否超过指定长度(1024) wt_stream_len = len;// 指定长度从解码的缓存中取数据 } // 每次从解码的缓存数据中以指定长度抽取数据并写入stream传递给声卡 memcpy(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, wt_stream_len); len -= wt_stream_len;// 更新解码音频缓存的剩余长度 stream += wt_stream_len;// 更新缓存写入位置 is->audio_buf_index += wt_stream_len;// 更新累计写入缓存数据长度 }//end for while } // 根据指定类型打开流，找到对应的解码器、创建对应的音频配置、保存关键信息到 VideoState、启动音频和视频线程 int stream_component_open(VideoState *is, int stream_index) { AVFormatContext *pFormatCtx = is->pFormatCtx;// 传递文件容器的封装信息及码流参数 AVCodecContext *codecCtx = NULL;// 解码器上下文对象，解码器依赖的相关环境、状态、资源以及参数集的接口指针 AVCodec *codec = NULL;// 保存编解码器信息的结构体，提供编码与解码的公共接口，可以看作是编码器与解码器的一个全局变量 //检查输入的流类型是否在合理范围内 if (stream_index = pFormatCtx->nb_streams) { return -1; } // Get a pointer to the codec context for the video stream. codecCtx = pFormatCtx->streams[stream_index]->codec;// 取得解码器上下文 if (codecCtx->codec_type == AVMEDIA_TYPE_AUDIO) {//检查解码器类型是否为音频解码器 SDL_AudioSpec wanted_spec, spec;//SDL_AudioSpec a structure that contains the audio output format，创建 SDL_AudioSpec 结构体，设置音频播放数据 // Set audio settings from codec info,SDL_AudioSpec a structure that contains the audio output format // 创建SDL_AudioSpec结构体，设置音频播放参数 wanted_spec.freq = codecCtx->sample_rate;//采样频率 DSP frequency -- samples per second wanted_spec.format = AUDIO_S16SYS;//采样格式 Audio data format wanted_spec.channels = codecCtx->channels;//声道数 Number of channels: 1 mono, 2 stereo wanted_spec.silence = 0;//无输出时是否静音 //默认每次读音频缓存的大小，推荐值为 512~8192，ffplay使用的是1024 specifies a unit of audio data refers to the size of the audio buffer in sample frames wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE; wanted_spec.callback = audio_callback;//设置读取音频数据的回调接口函数 the function to call when the audio device needs more data wanted_spec.userdata = is;//传递用户数据 /*--------------------------- * 以指定参数打开音频设备，并返回与指定参数最为接近的参数，该参数为设备实际支持的音频参数 * Opens the audio device with the desired parameters(wanted_spec) * return another specs we actually be using * and not guaranteed to get what we asked for --------------------------*/ if (SDL_OpenAudio(&wanted_spec, &spec) codec_id); AVDictionary *optionsDict = NULL; if (!codec || (avcodec_open2(codecCtx, codec, &optionsDict) name); // 检查解码器类型 switch(codecCtx->codec_type) { case AVMEDIA_TYPE_AUDIO:// 音频解码器 is->audioStream = stream_index;// 音频流类型标号初始化 is->audio_st = pFormatCtx->streams[stream_index]; is->audio_buf_size = 0;// 解码后的多帧音频数据长度 is->audio_buf_index = 0;//累 计写入stream的长度 memset(&is->audio_pkt, 0, sizeof(is->audio_pkt)); packet_queue_init(&is->audioq);// 音频数据包队列初始化 SDL_PauseAudio(0);// audio callback starts running again，开启音频设备，如果这时候没有获得数据那么它就静音 break; case AVMEDIA_TYPE_VIDEO:// 视频解码器 is->videoStream = stream_index;// 视频流类型标号初始化 is->video_st = pFormatCtx->streams[stream_index]; //以系统时间为基准，初始化播放到当前帧的已播放时间值，该值为真实时间值、动态时间值、绝对时间值 is->frame_timer=(double)av_gettime()/1000000.0; is->frame_last_delay = 40e-3;//初始化上一帧图像的动态刷新延迟时间 packet_queue_init(&is->videoq);// 视频数据包队列初始化 is->decode_tid = SDL_CreateThread(decode_thread,\"视频解码线程\" ,is);// 创建视频解码线程 // Initialize SWS context for software scaling，设置图像转换像素格式为AV_PIX_FMT_YUV420P is->sws_ctx = sws_getContext(is->video_st->codec->width, is->video_st->codec->height, is->video_st->codec->pix_fmt, is->video_st->codec->width, is->video_st->codec->height, AV_PIX_FMT_YUV420P, SWS_BILINEAR, NULL, NULL, NULL); break; default: break; } return 0; } // 编码数据包解析线程函数(从视频文件中解析出音视频编码数据单元，一个AVPacket的data通常对应一个NAL) int parse_thread(void *arg) { VideoState *is = (VideoState *)arg;// 传递用户参数 global_video_state = is;// 传递全局状态参量结构体 /*------------------------- * 打开封装格式 * 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 * 参数一：封装格式上下文 * 参数二：视频路径 * 参数三：指定输入的格式 * 参数四：设置默认参数 --------------------------*/ AVFormatContext *avformat_context = NULL;// 参数一：封装格式上下文 int avformat_open_input_result = avformat_open_input(&avformat_context, is->filename, NULL, NULL); if (avformat_open_input_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"查找音视频流\\n\"); return -1; } is->pFormatCtx = avformat_context;//传递文件容器封装信息及码流参数 /*------------------------- * 查找码流 * 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 * 参数一：封装格式上下文 * 参数二：指定默认配置 -------------------------*/ int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result videoStream = -1;//视频流类型标号初始化为-1 is->audioStream = -1;//音频流类型标号初始化为-1 // 视频流类型标号初始化为-1 int av_video_stream_index = -1; // 音频流类型标号初始化为-1 int av_audio_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 若文件中包含有视频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_video_stream_index = i; } // 若文件中包含有音频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_audio_stream_index = i; } } // 检查文件中是否存在视频流 if (av_video_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到视频流\\n\"); goto fail;//跳转至异常处理 return -1; } // 检查文件中是否存在音频流 if (av_audio_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到音频流\\n\"); goto fail;//跳转至异常处理 return -1; } stream_component_open(is, av_audio_stream_index);// 根据指定类型打开音频流 stream_component_open(is, av_video_stream_index);// 根据指定类型打开视频流 // Main decode loop. for (;;) { if (is->quit) {//检查退出进程标识 break; } // Seek stuff goes here，检查音视频编码数据包队列长度是否溢出 if (is->audioq.size > MAX_AUDIOQ_SIZE || is->videoq.size > MAX_VIDEOQ_SIZE) { SDL_Delay(10); continue; } /*----------------------- * read in a packet and store it in the AVPacket struct * ffmpeg allocates the internal data for us,which is pointed to by packet.data * this is freed by the av_free_packet() -----------------------*/ // 负责保存压缩编码数据相关信息的结构体,每帧图像由一到多个packet包组成 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 if (av_read_frame(is->pFormatCtx, packet) pFormatCtx->pb->error == 0) { SDL_Delay(100); // No error; wait for user input. continue; } else { break; } } // Is this a packet from the video stream? if (packet->stream_index == is->videoStream) {// 检查数据包是否为视频类型 packet_queue_put(&is->videoq, packet);// 向队列中插入数据包 } else if (packet->stream_index == is->audioStream) {// 检查数据包是否为音频类型 packet_queue_put(&is->audioq, packet);// 向队列中插入数据包 } else {// 检查数据包是否为字幕类型 av_packet_unref(packet);// 释放packet中保存的(字幕)编码数据 } } // All done - wait for it. while (!is->quit) { SDL_Delay(100); } fail:// 异常处理 if (1) { SDL_Event event;// SDL事件对象 event.type = FF_QUIT_EVENT;// 指定退出事件类型 event.user.data1 = is;// 传递用户数据 SDL_PushEvent(&event);// 将该事件对象压入SDL后台事件队列 } return 0; } int init_sdl(VideoState *is) { // 初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } // 初始化SDL窗口 SDL_Window* sdl_window = SDL_CreateWindow(\"FFmpeg+SDL播放视频\",// 参数一：窗口名称 SDL_WINDOWPOS_CENTERED,// 参数二：窗口在屏幕上的x坐标 SDL_WINDOWPOS_CENTERED,// 参数三：窗口在屏幕上的y坐标 is->video_width,// 参数四：窗口在屏幕上宽 is->video_height,// 参数五：窗口在屏幕上高 SDL_WINDOW_OPENGL);// 参数六：窗口状态(打开) if (sdl_window == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建渲染器 // 定义渲染器区域 SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window,// 渲染目标创建 -1, // 从那里开始渲染(-1:表示从第一个位置开始) 0);// 渲染类型(软件渲染) if (sdl_renderer == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建纹理 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer,// 渲染器 SDL_PIXELFORMAT_IYUV,// 像素数据格式 SDL_TEXTUREACCESS_STREAMING,// 绘制方式：频繁绘制- is->video_width,// 纹理宽 is->video_height);// 纹理高 if (sdl_texture == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } is->sdl_renderer = sdl_renderer; is->sdl_texture = sdl_texture; return 0; } // SDL入口 extern \"C\" int main(int argc, char *argv[]) { /*------------------------- * 注册组件 * 注册所有ffmpeg支持的多媒体格式及编解码器 -------------------------*/ av_register_all(); // 创建全局状态对象 VideoState *is= (VideoState *)av_mallocz(sizeof(VideoState)); av_strlcpy(is->filename, \"/storage/emulated/0/Download/test.mov\", sizeof(is->filename));// 复制视频文件路径名 is->video_width = 640; is->video_height = 352; is->pictq_lock = SDL_CreateMutex();// 创建编码数据包队列互斥锁对象 is->pictq_ready = SDL_CreateCond();// 创建编码数据包队列就绪条件对象 int init_sdl_result = init_sdl(is); if (init_sdl_result parse_tid = SDL_CreateThread(parse_thread, \"编码数据包解析线程\", is); if (!is->parse_tid) {// 检查线程是否创建成功 av_free(is); return -1; } // SDL事件对象 SDL_Event event; for (;;) {// SDL事件循环 SDL_WaitEvent(&event);// 主线程阻塞，等待事件到来 switch(event.type) {// 事件到来后唤醒主线程，检查事件类型 case FF_QUIT_EVENT: case SDL_QUIT:// 退出进程事件 is->quit = 1; // If the video has finished playing, then both the picture and audio queues are waiting for more data. // Make them stop waiting and terminate normally.. avcodec_close(is->video_st->codec); avformat_free_context(is->pFormatCtx); SDL_CondSignal(is->audioq.qready);// 发出队列就绪信号避免死锁 SDL_CondSignal(is->videoq.qready); SDL_DestroyTexture(is->sdl_texture); SDL_DestroyRenderer(is->sdl_renderer); SDL_Quit(); return 0; case FF_ALLOC_EVENT: alloc_picture(event.user.data1);// 分配视频帧事件响应函数 break; case FF_REFRESH_EVENT:// 视频显示刷新事件 video_refresh_timer(event.user.data1);// 视频显示刷新事件响应函数 break; default: break; } } return 0; } "},"pages/FFmpeg/FFmpeg_SDL同步音频.html":{"url":"pages/FFmpeg/FFmpeg_SDL同步音频.html","title":"FFmpeg+SDL同步音频","keywords":"","body":"FFmpeg+SDL同步音频 Android工程代码 iOS工程代码 ffmpeg播放器实现详解 - 音频同步控制：https://www.cnblogs.com/breakpointlab/p/15791998.html 源代码一览 #include #include #include #include #include \"SDL.h\" #include #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 #define AV_SYNC_THRESHOLD 0.01//前后两帧间的显示时间间隔的最小值0.01s #define AV_NOSYNC_THRESHOLD 10.0//最小刷新间隔时间10ms #define MAX_AUDIOQ_SIZE (5 * 16 * 1024) #define MAX_VIDEOQ_SIZE (5 * 256 * 1024) #define FF_ALLOC_EVENT (SDL_USEREVENT) #define FF_REFRESH_EVENT (SDL_USEREVENT + 1) #define FF_QUIT_EVENT (SDL_USEREVENT + 2) #define VIDEO_PICTURE_QUEUE_SIZE 1 #define SAMPLE_CORRECTION_PERCENT_MAX 10 #define AUDIO_DIFF_AVG_NB 20 extern \"C\" { #include #include \"libavformat/avformat.h\" #include #include #include #include #include #include } uint64_t global_video_pkt_pts = AV_NOPTS_VALUE; enum {//同步时钟源 AV_SYNC_AUDIO_MASTER,//音频时钟为主同步源 AV_SYNC_VIDEO_MASTER,//视频时钟为主同步源 AV_SYNC_EXTERNAL_MASTER,//外部时钟为主同步源 }; #define DEFAULT_AV_SYNC_TYPE AV_SYNC_AUDIO_MASTER//指定以视频时钟为主同步源(时间基准) /*-------链表节点结构体-------- typedef struct AVPacketList { AVPacket pkt;//链表数据 struct AVPacketList *next;//链表后继节点 } AVPacketList; ---------------------------*/ // 数据包队列(链表)结构体 typedef struct PacketQueue { AVPacketList *first_pkt, *last_pkt;// 队列首尾节点指针 int nb_packets;// 队列长度 int size;// 保存编码数据的缓存长度，size=packet->size SDL_mutex *qlock;// 队列互斥量，保护队列数据 SDL_cond *qready;// 队列就绪条件变量 } PacketQueue; // 图像帧结构体 typedef struct VideoPicture { AVFrame *avframe_yuv420p; int width, height;//Source height & width. int allocated;//是否分配内存空间，视频帧转换为SDL overlay标识 double pts;//当前图像帧的绝对显示时间戳 } VideoPicture; typedef struct VideoState { AVFormatContext *pFormatCtx;// 保存文件容器封装信息及码流参数的结构体 AVStream *video_st;// 视频流信息结构体 AVStream *audio_st;//音频流信息结构体 struct SwsContext *sws_ctx;// 描述转换器参数的结构体 struct SwsContext *sws_ctx_audio; PacketQueue videoq;// 视频编码数据包队列(编码数据队列，以链表方式实现) VideoPicture pictq[VIDEO_PICTURE_QUEUE_SIZE]; int pictq_size, pictq_rindex, pictq_windex;// 队列长度，读/写位置索引 SDL_mutex *pictq_lock;// 队列读写锁对象，保护图像帧队列数据 SDL_cond *pictq_ready;// 队列就绪条件变量 SDL_Rect sdl_rect; SDL_Renderer* sdl_renderer; SDL_Texture* sdl_texture; PacketQueue audioq;// 音频编码数据包队列(编码数据队列，以链表方式实现) uint8_t audio_buf[(MAX_AUDIO_FRAME_SIZE*3)/2];//保存解码一个packet后的多帧原始音频数据(解码数据队列，以数组方式实现) unsigned int audio_buf_size;//解码后的多帧音频数据长度 unsigned int audio_buf_index;//累计写入stream的长度 uint8_t *audio_pkt_data;//编码数据缓存指针位置 int audio_pkt_size;//缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) AVPacket audio_pkt;//保存从队列中提取的数据包 AVFrame audio_frame;//保存从数据包中解码的音频数据 int video_width; int video_height; char filename[1024];// 输入文件完整路径名 int videoStream, audioStream;// 音视频流类型标号 SDL_Thread *parse_tid;// 编码数据包解析线程id SDL_Thread *decode_tid;// 解码线程id int quit;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 //video/audio_clock save pts of last decoded frame/predicted pts of next decoded frame double video_clock;//keep track of how much time has passed according to the video double audio_clock; double frame_timer;//视频播放到当前帧时的累计已播放时间 double frame_last_pts;//上一帧图像的显示时间戳，用于在video_refersh_timer中保存上一帧的pts值 double frame_last_delay;//上一帧图像的动态刷新延迟时间 int av_sync_type;//主同步源类型 double external_clock;//External clock base int64_t external_clock_time;//外部时钟的绝对时间 double audio_diff_cum;//音频时钟与同步源累计时差，sed for AV difference average computation double audio_diff_avg_coef;//音频时钟与同步源时差均值加权系数 double audio_diff_threshold;//音频时钟与同步源时差均值阈值 int audio_diff_avg_count;//音频不同步计数(音频时钟与主同步源存在不同步的次数) int audio_hw_buf_size; double video_current_pts;//当前帧显示时间戳，Current displayed pts (different from video_clock if frame fifos are used) int64_t video_current_pts_time;//取得video_current_pts的系统时间，time (av_gettime) at which we updated video_current_pts - used to have running video pts } VideoState;// Since we only have one decoding thread, the Big Struct can be global in case we need it. VideoState *global_video_state; /*------取得当前播放音频数据的pts------ * 音视频同步的原理是根据音频的pts来控制视频的播放 * 也就是说在视频解码一帧后，是否显示以及显示多长时间，是通过该帧的PTS与同时正在播放的音频的PTS比较而来的 * 如果音频的PTS较大，则视频准备完毕立即刷新，否则等待 * * 因为pcm数据采用audio_callback回调方式进行播放 * 对于音频播放我们只能得到写入回调函数前缓存音频帧的pts，而无法得到当前播放帧的pts(需要采用当前播放音频帧的pts作为参考时钟) * 考虑到音频的大小与播放时间成正比(相同采样率)，那么当前时刻正在播放的音频帧pts(位于回调函数缓存中) * 就可以根据已送入声卡的pcm数据长度、缓存中剩余pcm数据长度，缓存长度及采样率进行推算了 --------------------------------*/ double get_audio_clock(VideoState *is) { double pts=is->audio_clock;//Maintained in the audio thread，取得解码操作完成时的当前播放时间戳 //还未(送入声卡)播放的剩余原始音频数据长度，等于解码后的多帧原始音频数据长度-累计送入声卡的长度 int hw_buf_size=is->audio_buf_size-is->audio_buf_index;//计算当前音频解码数据缓存索引位置 int bytes_per_sec=0;//每秒的原始音频字节数 int pcm_bytes=is->audio_st->codec->channels*2;//每组原始音频数据字节数=声道数*每声道数据字节数 if (is->audio_st) { bytes_per_sec=is->audio_st->codec->sample_rate*pcm_bytes;//计算每秒的原始音频字节数 } if (bytes_per_sec) {//检查每秒的原始音频字节数 pts-=(double)hw_buf_size/bytes_per_sec;//根据送入声卡缓存的索引位置，往前倒推计算当前时刻的音频播放时间戳pts } return pts;//返回当前正在播放的音频时间戳 } /*-----------取得视频时钟----------- * 即取得当前播放视频帧的pts，以视频时钟pts作为音视频同步基准，return the current time offset of the video currently being played * 该值为当前帧时间戳pts+一个微小的修正值delta * 因为在ms的级别上，在毫秒级别上，若取得视频时钟(即当前帧pts)的时刻，与调用视频时钟的时刻(如将音频同步到该视频pts时刻)存在延迟 * 那么，视频时钟需要在被调用时进行修正，修正值delta为 * delta=[取得视频时钟的时刻值video_current_pts_time] 到 [调用get_video_clock时刻值] 的间隔时间 * 通常情况下，都会选择以外部时钟或音频时钟作为主同步源，以视频同步到音频或外部时钟为首选同步方案 * 以视频时钟作为主同步源的同步方案，属于3种基本的同步方案(同步到音频、同步到视频、同步到外部时钟) * 本利仅为展示同步到视频时钟的方法，一般情况下同步到视频时钟仅作为辅助的同步方案 --------------------------------*/ double get_video_clock(VideoState *is) { double delta=(av_gettime()-is->video_current_pts_time)/1000000.0; //pts_of_last_frame+(Current_time-time_elapsed_since_pts_value_was_set) return is->video_current_pts+delta; } //取得系统时间，以系统时钟作为同步基准 double get_external_clock(VideoState *is) { return av_gettime()/1000000.0;//取得系统当前时间，以1/1000000秒为单位，便于在各个平台移植 } //取得主时钟(基准时钟) double get_master_clock(VideoState *is) { if (is->av_sync_type == AV_SYNC_VIDEO_MASTER) { return get_video_clock(is);//返回视频时钟 } else if (is->av_sync_type == AV_SYNC_AUDIO_MASTER) { return get_audio_clock(is);//返回音频时钟 } else { return get_external_clock(is);//返回系统时钟 } } /*--------------------------- * return the wanted number of samples to get better sync if sync_type is video or external master clock * 通常情况下会以音频或系统时钟为主同步源，只有在音频或系统时钟失效的情况下才以视频为主同步源 * 该函数比对音频时钟与主同步源的时差，通过动态丢帧(或插值)部分音频数据，以起到减少(或增加)音频播放时长，减少与主同步源时差的作用 * 该函数对音频缓存数据进行丢帧(或插值)，返回丢帧(或插值)后的音频数据长度 * 因为音频同步可能带来输出声音不连续等副作用，该函数通过音频不同步次数(audio_diff_avg_count)及时差均值(avg_diff)来约束音频的同步过程 ---------------------------*/ int synchronize_audio(VideoState *is, short *samples, int samples_size, double pts) { double ref_clock;//主同步源(基准时钟) int pcm_bytes=is->audio_st->codec->channels*2;//每组音频数据字节数=声道数*每声道数据字节数 /* if not master, then we try to remove or add samples to correct the clock */ if (is->av_sync_type != AV_SYNC_AUDIO_MASTER) {//检查主同步源，若同步源不是音频时钟的情况下，执行以下代码 double diff, avg_diff;//diff-音频帧播放间与主同步源时差，avg_diff-采样不同步平均值 int wanted_size, min_size, max_size;//经过丢帧(或插值)后的缓存长度，缓存长度最大/最小值 ref_clock = get_master_clock(is);//取得当前主同步源，以主同步源为基准时间 diff = get_audio_clock(is) - ref_clock;//计算音频时钟与当前主同步源的时差 if (diffaudio_diff_cum=diff+is->audio_diff_avg_coef*is->audio_diff_cum; if (is->audio_diff_avg_countaudio_diff_avg_count++;//音频不同步计数更新 } else {//当音频不同步次数超过阈值限定后，触发音频同步操作 avg_diff=is->audio_diff_cum*(1.0-is->audio_diff_avg_coef);//计算时差均值(等比级数几何平均数) if (fabs(avg_diff)>=is->audio_diff_threshold) {//比对时差均值与时差阈值 wanted_size=samples_size+((int)(diff*is->audio_st->codec->sample_rate)*pcm_bytes);//根据时差换算同步后的缓存长度 min_size=samples_size*((100-SAMPLE_CORRECTION_PERCENT_MAX)/100);//同步后的缓存长度最小值 max_size=samples_size*((100+SAMPLE_CORRECTION_PERCENT_MAX)/100);//同步后的缓存长度最大值 if (wanted_sizemax_size) {//若同步后缓存长度>最小缓存长度 wanted_size=max_size;//用最大缓存长度作为同步后的缓存长度 } if (wanted_sizesamples_size) {//若同步后缓存长度大于当前缓存长度 //Add samples by copying final sample，通过复制最后一个音频数据进行插值 //int nb=samples_size-wanted_size; int nb=wanted_size-samples_size;//计算插值后缓存长度与原始缓存长度间的差值(需要插值的音频数据组数) uint8_t *samples_end=(uint8_t*)samples+samples_size-pcm_bytes;//取得缓存末端数据指针 uint8_t *q=samples_end+pcm_bytes;//初始插值位置|||q| while (nb>0) {//检查插值音频组数(每组包括两个声道的pcm数据) memcpy(q,samples_end,pcm_bytes);//在samples原始缓存后追加插值 q += pcm_bytes;//更新插值位置 nb -= pcm_bytes;//更新插值组数 } samples_size=wanted_size;//返回音频同步后的缓存长度 } } } } else { // Difference is too big, reset diff stuff，时差过大，重置时差累计值 is->audio_diff_avg_count = 0;//音频不同步计数重置 is->audio_diff_cum = 0;//音频累计时差重置 } }//end for if (is->av_sync_type != AV_SYNC_AUDIO_MASTER) return samples_size;//返回发送到声卡的音频缓存字节数 } // 定时器触发的回调函数 static Uint32 sdl_refresh_timer_cb(Uint32 interval, void *opaque) { SDL_Event event;//SDL事件对象 event.type = FF_REFRESH_EVENT;//视频显示刷新事件 event.user.data1 = opaque;//传递用户数据 SDL_PushEvent(&event);//发送事件 return 0; // 0 means stop timer. } /*--------------------------- * Schedule a video refresh in 'delay' ms. * 告诉sdl在指定的延时后来推送一个 FF_REFRESH_EVENT 事件 * 这个事件将在事件队列里触发sdl_refresh_timer_cb函数的调用 --------------------------*/ static void schedule_refresh(VideoState *is, int delay) { SDL_AddTimer(delay, sdl_refresh_timer_cb, is);//在指定的时间(ms)后回调用户指定的函数 } // 视频(图像)帧渲染 void video_display(VideoState *is) { SDL_Rect rect;// SDL矩形对象 VideoPicture *vp;// 图像帧结构体指针 vp = &is->pictq[is->pictq_rindex];//从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // 设置纹理数据 SDL_UpdateTexture(is->sdl_texture, // 纹理 NULL,// 渲染区域 vp->avframe_yuv420p->data[0],// 需要渲染数据：视频像素数据帧 vp->avframe_yuv420p->linesize[0]);// 帧宽 // 将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) is->sdl_rect.x = 100; is->sdl_rect.y = 100; is->sdl_rect.w = vp->width; is->sdl_rect.h = vp->height; SDL_RenderClear(is->sdl_renderer); SDL_RenderCopy(is->sdl_renderer, is->sdl_texture, NULL, &is->sdl_rect); // 呈现画面帧 SDL_RenderPresent(is->sdl_renderer); }// end for if }// end for video_display // 显示刷新函数(FF_REFRESH_EVENT响应函数) int video_current_index = 0; void video_refresh_timer(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp;//图像帧对象 //delay-前后帧间的显示时间间隔，diff-图像帧显示与音频帧播放间的时间差 //sync_threshold-前后帧间的最小时间差，actual_delay-当前帧-下已帧的显示时间间隔(动态时间、真实时间、绝对时间) double delay,diff,sync_threshold,actual_delay,ref_clock;//ref_clock-音频时间戳 if (is->video_st) { if (is->pictq_size == 0) {// 检查图像帧队列是否有待显示图像 schedule_refresh(is, 1);//若队列为空，则发送显示刷新事件并再次进入video_refresh_timer函数 } else {// 刷新图像 vp = &is->pictq[is->pictq_rindex];//从显示队列中取得等待显示的图像帧 is->video_current_pts = vp->pts;//取得当前帧的显示时间戳 is->video_current_pts_time = av_gettime();//取得系统时间，作为当前帧播放的时间基准 //计算当前帧和前一帧显示(pts)的间隔时间(显示时间戳的差值) //计算当前帧和前一帧显示(pts)的间隔时间(显示时间戳的差值) delay = vp->pts - is->frame_last_pts;//The pts from last time，前后帧间的时间差 if (delay = 1.0) {//检查时间间隔是否在合理范围 // If incorrect delay, use previous one delay = is->frame_last_delay;//沿用之前的动态刷新间隔时间 } // Save for next time is->frame_last_delay = delay;//保存上一帧图像的动态刷新延迟时间 is->frame_last_pts = vp->pts;//保存上一帧图像的显示时间戳 // Update delay to sync to audio，取得声音播放时间戳(作为视频同步的参考时间) // Update delay to sync to audio，取得声音播放时间戳(作为视频同步的参考时间) if (is->av_sync_type != AV_SYNC_VIDEO_MASTER) {//检查主同步时钟源 ref_clock = get_master_clock(is);//根据Audio clock来判断Video播放的快慢，获取当前播放声音的时间戳 //也就是说在diff这段时间中声音是匀速发生的，但是在delay这段时间frame的显示可能就会有快慢的区别 diff = vp->pts - ref_clock;//计算图像帧显示与音频帧播放间的时间差 //根据时间差调整播放下一帧的延迟时间，以实现同步 Skip or repeat the frame，Take delay into account sync_threshold = (delay > AV_SYNC_THRESHOLD) ? delay : AV_SYNC_THRESHOLD;//比较前后两帧间的显示时间间隔与最小时间间隔 //判断音视频不同步条件，即音视频间的时间差 & 前后帧间的时间差该阈值则为快进模式，不存在音视频同步问题 if (fabs(diff) = sync_threshold) {//比较两帧画面间的显示时间与两帧画面间声音的播放时间，快了，加倍delay delay = 2 * delay; } }//如果diff(明显)大于AV_NOSYNC_THRESHOLD，即快进的模式了，画面跳动太大，不存在音视频同步的问题了 } //更新视频播放到当前帧时的已播放时间值(所有图像帧动态播放累计时间值-真实值)，frame_timer一直累加在播放过程中我们计算的延时 is->frame_timer+=delay; //每次计算frame_timer与系统时间的差值(以系统时间为基准时间)，将frame_timer与系统时间(绝对时间)相关联的目的 actual_delay=is->frame_timer-(av_gettime()/1000000.0);//Computer the REAL delay if (actual_delay pts: %f，ref_clock：%f，actual_delay：%f\", video_current_index, vp->pts, ref_clock, actual_delay); // Update queue for next picture! if (++is->pictq_rindex == VIDEO_PICTURE_QUEUE_SIZE) {// 更新并检查图像帧队列读位置索引 is->pictq_rindex = 0;// 重置读位置索引 } SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 is->pictq_size--;// 更新图像帧队列长度 SDL_CondSignal(is->pictq_ready);// 发送队列就绪信号 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } } else { schedule_refresh(is, 100); } } // 数据包队列初始化函数 void packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue));// 全零初始化队列结构体对象 q->qlock = SDL_CreateMutex();// 创建互斥量对象 q->qready = SDL_CreateCond();// 创建条件变量对象 } // 向队列中插入数据包 int packet_queue_put(PacketQueue *q, AVPacket *pkt) { /*-------准备队列(链表)节点对象------*/ AVPacketList *pktlist=(AVPacketList *)av_malloc(sizeof(AVPacketList));// 在堆上创建链表节点对象 if (!pktlist) {// 检查链表节点对象是否创建成功 return -1; } pktlist->pkt = *pkt;// 将输入数据包赋值给新建链表节点对象中的数据包对象 pktlist->next = NULL;// 链表后继指针为空 // if (av_packet_ref(pkt, pkt) qlock);// 队列互斥量加锁，保护队列数据 if (!q->last_pkt) {// 检查队列尾节点是否存在(检查队列是否为空) q->first_pkt = pktlist;// 若不存在(队列尾空)，则将当前节点作队列为首节点 } else { q->last_pkt->next = pktlist;// 若已存在尾节点，则将当前节点挂到尾节点的后继指针上，并作为新的尾节点 } q->last_pkt = pktlist;// 将当前节点作为新的尾节点 q->nb_packets++;// 队列长度+1 q->size += pktlist->pkt.size;// 更新队列编码数据的缓存长度 SDL_CondSignal(q->qready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(q->qlock);// 释放互斥量 return 0; } // 从队列中提取数据包，并将提取的数据包出队列 static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block) { AVPacketList *pktlist;// 临时链表节点对象指针 int ret;// 操作结果 SDL_LockMutex(q->qlock);// 队列互斥量加锁，保护队列数据 for (;;) { if (global_video_state->quit) {// 检查退出进程标识 ret = -1;// 操作失败 break; }//end for if pktlist = q->first_pkt;// 传递将队列首个数据包指针 if (pktlist) {// 检查数据包是否为空(队列是否有数据) q->first_pkt = pktlist->next;// 队列首节点指针后移 if (!q->first_pkt) {// 检查首节点的后继节点是否存在 q->last_pkt = NULL;// 若不存在，则将尾节点指针置空 } q->nb_packets--;// 队列长度-1 q->size -= pktlist->pkt.size;// 更新队列编码数据的缓存长度 *pkt = pktlist->pkt;// 将队列首节点数据返回 av_free(pktlist);// 清空临时节点数据(清空首节点数据，首节点出队列) ret = 1;// 操作成功 break; } else if (!block) { ret = 0; break; } else {// 队列处于未就绪状态，此时通过SDL_CondWait函数等待qready就绪信号，并暂时对互斥量解锁 /*--------------------- * 等待队列就绪信号qready，并对互斥量暂时解锁 * 此时线程处于阻塞状态，并置于等待条件就绪的线程列表上 * 使得该线程只在临界区资源就绪后才被唤醒，而不至于线程被频繁切换 * 该函数返回时，互斥量再次被锁住，并执行后续操作 --------------------*/ SDL_CondWait(q->qready, q->qlock);// 暂时解锁互斥量并将自己阻塞，等待临界区资源就绪(等待SDL_CondSignal发出临界区资源就绪的信号) } }//end for for-loop SDL_UnlockMutex(q->qlock);// 释放互斥量 return ret; } // 创建/重置图像帧，为图像帧分配内存空间 void alloc_picture(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {// 检查图像帧是否已存在 // We already have one make another, bigger/smaller. av_frame_free(&vp->avframe_yuv420p); } vp->width = is->video_st->codec->width;// 设置图像帧宽度 vp->height = is->video_st->codec->height;// 设置图像帧高度 SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 vp->allocated = 1;// 图像帧像素缓冲区已分配内存 // AV_PIX_FMT_YUV420P格式的视频帧 vp->avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型 int buffer_size =av_image_get_buffer_size(AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 一帧视频像素数据宽 = 视频宽 is->video_st->codec->height,// 一帧视频像素数据高 = 视频高 1);// 字节对齐方式，默认是1 // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 av_image_fill_arrays(vp->avframe_yuv420p->data,// 目标视频帧数据 vp->avframe_yuv420p->linesize,// 目标视频帧行大小 out_buffer,// 原始数据 AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 视频宽 is->video_st->codec->height,//视频高 1);// 字节对齐方式 SDL_CondSignal(is->pictq_ready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } /*--------------------------- * queue_picture：图像帧插入队列等待渲染 * @is：全局状态参数集 * @pFrame：保存图像解码数据的结构体 * 1、首先检查图像帧队列(数组)是否存在空间插入新的图像，若没有足够的空间插入图像则使当前线程休眠等待 * 2、在初始化的条件下，队列(数组)中VideoPicture的bmp对象(YUV overlay)尚未分配空间，通过FF_ALLOC_EVENT事件的方法调用alloc_picture分配空间 * 3、当队列(数组)中所有VideoPicture的bmp对象(YUV overlay)均已分配空间的情况下，直接跳过步骤2向bmp对象拷贝像素数据，像素数据在进行格式转换后执行拷贝操作 ---------------------------*/ int queue_picture(VideoState *is, AVFrame *pFrame, double pts) { /*--------1、检查队列是否有插入空间-------*/ // Wait until we have space for a new pic. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (is->pictq_size >= VIDEO_PICTURE_QUEUE_SIZE && !is->quit) {// 检查队列当前长度 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待pictq_ready信号 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } /*-------2、初始化/重置YUV overlay-------*/ // windex is set to 0 initially. VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列中抽取图像帧对象 // Allocate or resize the buffer，检查YUV overlay是否已存在，否则初始化YUV overlay，分配像素缓存空间 if (!vp->avframe_yuv420p || vp->width!=is->video_st->codec->width || vp->height!=is->video_st->codec->height) { vp->allocated = 0;// 图像帧未分配空间 // We have to do it in the main thread. SDL_Event event;// SDL事件对象 event.type = FF_ALLOC_EVENT;//指定分配图像帧内存事件 event.user.data1 = is;//传递用户数据 SDL_PushEvent(&event);//发送SDL事件 // Wait until we have a picture allocated. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (!vp->allocated && !is->quit) {// 检查当前图像帧是否已初始化 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待alloc_picture发送pictq_ready信号唤醒当前线程 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } }// end for if /*--------3、拷贝视频帧到YUV overlay-------*/ // We have a place to put our picture on the queue. if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // Convert the image into YUV format that SDL uses，将解码后的图像帧转换为AV_PIX_FMT_YUV420P格式，并拷贝到图像帧队列 sws_scale(is->sws_ctx, (uint8_t const * const *)pFrame->data, pFrame->linesize, 0, is->video_st->codec->height, vp->avframe_yuv420p->data, vp->avframe_yuv420p->linesize); vp->pts = pts;//传递当前图像帧的绝对显示时间戳 // Now we inform our display thread that we have a pic ready. if (++is->pictq_windex == VIDEO_PICTURE_QUEUE_SIZE) {//更新并检查当前图像帧队列写入位置 is->pictq_windex = 0;//重置图像帧队列写入位置 } SDL_LockMutex(is->pictq_lock);//锁定队列读写锁，保护队列数据 is->pictq_size++;//更新图像帧队列长度 SDL_UnlockMutex(is->pictq_lock);//释放队列读写锁 }// end for if return 0; } /*--------------------------- * 更新内部视频播放计时器(记录视频已经播时间(video_clock)） * @is：全局状态参数集 * @src_frame：当前(输入的)(待更新的)图像帧对象 * @pts：当前图像帧的显示时间戳 * update the PTS to be in sync ---------------------------*/ double synchronize_video(VideoState *is, AVFrame *src_frame, double pts) { /*----------检查显示时间戳----------*/ if (pts != 0) {//检查显示时间戳是否有效 // If we have pts, set video clock to it. is->video_clock = pts;//用显示时间戳更新已播放时间 } else {//若获取不到显示时间戳 // If we aren't given a pts, set it to the clock. pts = is->video_clock;//用已播放时间更新显示时间戳 } /*--------更新视频已经播时间--------*/ // Update the video clock，若该帧要重复显示(取决于repeat_pict)，则全局视频播放时序video_clock应加上重复显示的数量*帧率 double frame_delay = av_q2d(is->video_st->codec->time_base);//该帧显示完将要花费的时间 // If we are repeating a frame, adjust clock accordingly,若存在重复帧，则在正常播放的前后两帧图像间安排渲染重复帧 frame_delay += src_frame->repeat_pict*(frame_delay*0.5);//计算渲染重复帧的时值(类似于音符时值) is->video_clock += frame_delay;//更新视频播放时间 // printf(\"repeat_pict=%d \\n\",src_frame->repeat_pict); return pts;//此时返回的值即为下一帧将要开始显示的时间戳 } // These are called whenever we allocate a frame buffer. We use this to store the global_pts in a frame at the time it is allocated. int our_get_buffer(struct AVCodecContext *c, AVFrame *pic, int flags) { int ret = avcodec_default_get_buffer2(c, pic, 0); uint64_t *pts = (uint64_t *)av_malloc(sizeof(uint64_t)); *pts = global_video_pkt_pts; pic->opaque = pts; return ret; } // 视频解码线程函数 int decode_thread(void *arg) { VideoState *is = (VideoState *) arg;// 传递用户数据 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 int frameFinished;// 解码操作是否成功标识 // Allocate video frame，为解码后的视频信息结构体分配空间并完成初始化操作(结构体中的图像缓存按照下面两步手动安装) AVFrame *pFrame = av_frame_alloc(); double pts;//当前桢在整个视频中的(绝对)时间位置 for (;;) { if (packet_queue_get(&is->videoq,packet,1)pts;// Save global pts to be stored in pFrame in first call. /*----------------------- * Decode video frame，解码完整的一帧数据，并将frameFinished设置为true * 可能无法通过只解码一个packet就获得一个完整的视频帧frame，可能需要读取多个packet才行 * avcodec_decode_video2()会在解码到完整的一帧时设置frameFinished为真 * Technically a packet can contain partial frames or other bits of data * ffmpeg's parser ensures that the packets we get contain either complete or multiple frames * convert the packet to a frame for us and set frameFinisned for us when we have the next frame -----------------------*/ avcodec_decode_video2(is->video_st->codec, pFrame, &frameFinished, packet); //取得编码数据包中的显示时间戳PTS(int64_t),并暂时保存在pts(double)中 // if (packet->dts==AV_NOPTS_VALUE && pFrame->opaque && *(uint64_t*)pFrame->opaque!=AV_NOPTS_VALUE) { // pts = *(uint64_t *)pFrame->opaque; // } else if (packet->dts != AV_NOPTS_VALUE) { // pts = packet->dts; // } else { // pts = 0; // } pts=av_frame_get_best_effort_timestamp(pFrame);//取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 /*------------------------- * 在解码线程函数中计算当前图像帧的显示时间戳 * 1、取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 * 2、根据PTS*time_base来计算当前桢在整个视频中的显示时间戳，即PTS*(1/framerate) * av_q2d把AVRatioal结构转换成double的函数， * 用于计算视频源每个图像帧显示的间隔时间(1/framerate),即返回(time_base->num/time_base->den) -------------------------*/ //根据pts=PTS*time_base={numerator=1,denominator=25}计算当前桢在整个视频中的显示时间戳 pts*=av_q2d(is->video_st->time_base);//time_base为AVRational有理数结构体{num=1,den=25}，记录了视频源每个图像帧显示的间隔时间 // Did we get a video frame，检查是否解码出完整一帧图像 if (frameFinished) { pts = synchronize_video(is, pFrame, pts);//检查当前帧的显示时间戳pts并更新内部视频播放计时器(记录视频已经播时间(video_clock)） if (queue_picture(is, pFrame, pts)sws_ctx_audio, \"in_channel_layout\", src_ch_layout, 0); av_opt_set_int(is->sws_ctx_audio, \"out_channel_layout\", dst_ch_layout, 0); av_opt_set_int(is->sws_ctx_audio, \"in_sample_rate\", src_rate, 0); av_opt_set_int(is->sws_ctx_audio, \"out_sample_rate\", dst_rate, 0); av_opt_set_sample_fmt(is->sws_ctx_audio, \"in_sample_fmt\", src_sample_fmt, 0); av_opt_set_sample_fmt(is->sws_ctx_audio, \"out_sample_fmt\", dst_sample_fmt, 0); int ret;//返回结果 // Initialize the resampling context. if ((ret = swr_init((struct SwrContext *) is->sws_ctx_audio)) sws_ctx_audio,src_rate)+src_nb_samples,dst_rate,src_rate,AV_ROUND_UP); //Convert to destination format. ret=swr_convert((struct SwrContext*)is->sws_ctx_audio,dst_data,dst_nb_samples,(const uint8_t **)decoded_frame.data,src_nb_samples); if (retaudio_buf, dst_data[0], dst_bufsize); if (src_data) { av_freep(&src_data[0]); } av_freep(&src_data); if (dst_data) { av_freep(&dst_data[0]); } av_freep(&dst_data); return dst_bufsize; } // 音频解码函数，从缓存队列中提取数据包、解码，并返回解码后的数据长度(对一个完整的packet解码，将解码数据写入audio_buf缓存，并返回多帧解码数据的总长度) int audio_decode_frame(VideoState *is, double *pts_ptr) { int coded_consumed_size,data_size=0,pcm_bytes;// 每次消耗的编码数据长度[input](len1)，输出原始音频数据的缓存长度[output]，每组音频采样数据的字节数 AVPacket *pkt = &is->audio_pkt;// 保存从队列中提取的数据包 double pts;//音频播放时间戳 for (;;) { while (is->audio_pkt_size>0) {// 检查缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) int got_frame = 0;// 解码操作成功标识，成功返回非零值 // 解码一帧音频数据，并返回消耗的编码数据长度 coded_consumed_size = avcodec_decode_audio4(is->audio_st->codec, &is->audio_frame, &got_frame, pkt); if (coded_consumed_size audio_pkt_size = 0;// 更新编码数据缓存长度 break; } if (got_frame) {// 检查解码操作是否成功 if (is->audio_frame.format != AV_SAMPLE_FMT_S16) {//检查音频数据格式是否为16位采样格式 //当音频数据不为16位采样格式情况下，采用decode_frame_from_packet计算解码数据长度 data_size=decode_frame_from_packet(is, is->audio_frame); __android_log_print(ANDROID_LOG_INFO, \"main\", \"音频数据格式是采样格式:%d\",is->audio_frame.format); } else {//计算解码后音频数据长度[output] data_size=av_samples_get_buffer_size(NULL,is->audio_st->codec->channels,is->audio_frame.nb_samples,is->audio_st->codec->sample_fmt, 1); memcpy(is->audio_buf,is->audio_frame.data[0],data_size);//将解码数据复制到输出缓存 } } is->audio_pkt_data += coded_consumed_size;// 更新编码数据缓存指针位置 is->audio_pkt_size -= coded_consumed_size;// 更新缓存中剩余的编码数据长度 if (data_size audio_clock;//用每次更新的音频播放时间更新音频PTS *pts_ptr=pts; /*--------------------- * 当一个packet中包含多个音频帧时 * 通过[解码后音频原始数据长度]及[采样率]来推算一个packet中其他音频帧的播放时间戳pts * 采样频率44.1kHz，量化位数16位，意味着每秒采集数据44.1k个，每个数据占2字节 --------------------*/ pcm_bytes=2*is->audio_st->codec->channels;//计算每组音频采样数据的字节数=每个声道音频采样字节数*声道数 /*----更新audio_clock--- * 一个pkt包含多个音频frame，同时一个pkt对应一个pts(pkt->pts) * 因此，该pkt中包含的多个音频帧的时间戳由以下公式推断得出 * bytes_per_sec=pcm_bytes*is->audio_st->codec->sample_rate * 从pkt中不断的解码，推断(一个pkt中)每帧数据的pts并累加到音频播放时钟 --------------------*/ is->audio_clock+=(double)data_size/(double)(pcm_bytes*is->audio_st->codec->sample_rate); // We have data, return it and come back for more later. return data_size;// 返回解码数据缓存长度 } if (pkt->data) {// 检查数据包是否已从队列中提取 av_packet_unref(pkt);// 释放pkt中保存的编码数据 } if (is->quit) {// 检查退出进程标识 return -1; } // Next packet，从队列中提取数据包到pkt if (packet_queue_get(&is->audioq, pkt, 1) audio_pkt_data = pkt->data;// 传递编码数据缓存指针 is->audio_pkt_size = pkt->size;// 传递编码数据缓存长度 // If update, update the audio clock w/pts if (pkt->pts != AV_NOPTS_VALUE) {//检查音频播放时间戳 //获得一个新的packet的时候，更新audio_clock，用packet中的pts更新audio_clock(一个pkt对应一个pts) is->audio_clock=pkt->pts*av_q2d(is->audio_st->time_base);//更新音频已经播的时间 } } } /*------Audio Callback------- * 音频输出回调函数，sdl通过该回调函数将解码后的pcm数据送入声卡播放, * sdl通常一次会准备一组缓存pcm数据，通过该回调送入声卡，声卡根据音频pts依次播放pcm数据 * 待送入缓存的pcm数据完成播放后，再载入一组新的pcm缓存数据(每次音频输出缓存为空时，sdl就调用此函数填充音频输出缓存，并送入声卡播放) * When we begin playing audio, SDL will continually call this callback function * and ask it to fill the audio buffer with a certain number of bytes * The audio function callback takes the following parameters: * stream: A pointer to the audio buffer to be filled，输出音频数据到声卡缓存 * len: The length (in bytes) of the audio buffer,缓存长度wanted_spec.samples=SDL_AUDIO_BUFFER_SIZE(1024) --------------------------*/ void audio_callback(void *userdata, Uint8 *stream, int len) { VideoState *is = (VideoState *) userdata;// 传递用户数据 int wt_stream_len, audio_size;// 每次写入stream的数据长度，解码后的数据长度 double pts;//音频时间戳 while (len > 0) {//检查音频缓存的剩余长度 if (is->audio_buf_index >= is->audio_buf_size) {// 检查是否需要执行解码操作 // We have already sent all our data; get more，从缓存队列中提取数据包、解码，并返回解码后的数据长度，audio_buf缓存中可能包含多帧解码后的音频数据 audio_size = audio_decode_frame(is, &pts); if (audio_size audio_buf_size = 1024; memset(is->audio_buf, 0, is->audio_buf_size);// 全零重置缓冲区 } else { //在回调函数中增加音频同步过程，即对音频数据缓存进行丢帧(或插值)，以起到降低音频时钟与主同步源时差的目的 audio_size=synchronize_audio(is,(int16_t*)is->audio_buf,audio_size,pts);//返回音频同步后的缓存长度 is->audio_buf_size = audio_size;// 返回packet中包含的原始音频数据长度(多帧) } is->audio_buf_index = 0;// 初始化累计写入缓存长度 }//end for if wt_stream_len=is->audio_buf_size-is->audio_buf_index;// 计算解码缓存剩余长度 if (wt_stream_len > len) {// 检查每次写入缓存的数据长度是否超过指定长度(1024) wt_stream_len = len;// 指定长度从解码的缓存中取数据 } // 每次从解码的缓存数据中以指定长度抽取数据并写入stream传递给声卡 memcpy(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, wt_stream_len); len -= wt_stream_len;// 更新解码音频缓存的剩余长度 stream += wt_stream_len;// 更新缓存写入位置 is->audio_buf_index += wt_stream_len;// 更新累计写入缓存数据长度 }//end for while } // 根据指定类型打开流，找到对应的解码器、创建对应的音频配置、保存关键信息到 VideoState、启动音频和视频线程 int stream_component_open(VideoState *is, int stream_index) { AVFormatContext *pFormatCtx = is->pFormatCtx;// 传递文件容器的封装信息及码流参数 AVCodecContext *codecCtx = NULL;// 解码器上下文对象，解码器依赖的相关环境、状态、资源以及参数集的接口指针 AVCodec *codec = NULL;// 保存编解码器信息的结构体，提供编码与解码的公共接口，可以看作是编码器与解码器的一个全局变量 //检查输入的流类型是否在合理范围内 if (stream_index = pFormatCtx->nb_streams) { return -1; } // Get a pointer to the codec context for the video stream. codecCtx = pFormatCtx->streams[stream_index]->codec;// 取得解码器上下文 if (codecCtx->codec_type == AVMEDIA_TYPE_AUDIO) {//检查解码器类型是否为音频解码器 SDL_AudioSpec wanted_spec, spec;//SDL_AudioSpec a structure that contains the audio output format，创建 SDL_AudioSpec 结构体，设置音频播放数据 // Set audio settings from codec info,SDL_AudioSpec a structure that contains the audio output format // 创建SDL_AudioSpec结构体，设置音频播放参数 wanted_spec.freq = codecCtx->sample_rate;//采样频率 DSP frequency -- samples per second wanted_spec.format = AUDIO_S16SYS;//采样格式 Audio data format wanted_spec.channels = codecCtx->channels;//声道数 Number of channels: 1 mono, 2 stereo wanted_spec.silence = 0;//无输出时是否静音 //默认每次读音频缓存的大小，推荐值为 512~8192，ffplay使用的是1024 specifies a unit of audio data refers to the size of the audio buffer in sample frames wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE; wanted_spec.callback = audio_callback;//设置读取音频数据的回调接口函数 the function to call when the audio device needs more data wanted_spec.userdata = is;//传递用户数据 /*--------------------------- * 以指定参数打开音频设备，并返回与指定参数最为接近的参数，该参数为设备实际支持的音频参数 * Opens the audio device with the desired parameters(wanted_spec) * return another specs we actually be using * and not guaranteed to get what we asked for --------------------------*/ if (SDL_OpenAudio(&wanted_spec, &spec) audio_hw_buf_size = spec.size; } /*----------------------- * Find the decoder for the video stream，根据视频流对应的解码器上下文查找对应的解码器，返回对应的解码器(信息结构体) * The stream's information about the codec is in what we call the \"codec context. * This contains all the information about the codec that the stream is using -----------------------*/ codec = avcodec_find_decoder(codecCtx->codec_id); AVDictionary *optionsDict = NULL; if (!codec || (avcodec_open2(codecCtx, codec, &optionsDict) name); // 检查解码器类型 switch(codecCtx->codec_type) { case AVMEDIA_TYPE_AUDIO:// 音频解码器 is->audioStream = stream_index;// 音频流类型标号初始化 is->audio_st = pFormatCtx->streams[stream_index]; is->audio_buf_size = 0;// 解码后的多帧音频数据长度 is->audio_buf_index = 0;//累 计写入stream的长度 // Averaging filter for audio sync. is->audio_diff_avg_coef=exp(log(0.01/AUDIO_DIFF_AVG_NB));//音频时钟与主同步源累计时差加权系数 is->audio_diff_avg_count=0;//音频不同步计数初始化 // Correct audio only if larger error than this. is->audio_diff_threshold=2.0*SDL_AUDIO_BUFFER_SIZE/codecCtx->sample_rate; is->sws_ctx_audio = (struct SwsContext *) swr_alloc(); if (!is->sws_ctx_audio) { fprintf(stderr, \"Could not allocate resampler context\\n\"); return -1; } memset(&is->audio_pkt, 0, sizeof(is->audio_pkt)); packet_queue_init(&is->audioq);// 音频数据包队列初始化 SDL_PauseAudio(0);// audio callback starts running again，开启音频设备，如果这时候没有获得数据那么它就静音 break; case AVMEDIA_TYPE_VIDEO:// 视频解码器 is->videoStream = stream_index;// 视频流类型标号初始化 is->video_st = pFormatCtx->streams[stream_index]; //以系统时间为基准，初始化播放到当前帧的已播放时间值，该值为真实时间值、动态时间值、绝对时间值 is->frame_timer=(double)av_gettime()/1000000.0; is->frame_last_delay = 40e-3;//初始化上一帧图像的动态刷新延迟时间 is->video_current_pts_time = av_gettime();//取得系统当前时间 packet_queue_init(&is->videoq);// 视频数据包队列初始化 is->decode_tid = SDL_CreateThread(decode_thread,\"视频解码线程\" ,is);// 创建视频解码线程 // Initialize SWS context for software scaling，设置图像转换像素格式为AV_PIX_FMT_YUV420P is->sws_ctx = sws_getContext(is->video_st->codec->width, is->video_st->codec->height, is->video_st->codec->pix_fmt, is->video_st->codec->width, is->video_st->codec->height, AV_PIX_FMT_YUV420P, SWS_BILINEAR, NULL, NULL, NULL); codecCtx->get_buffer2 = our_get_buffer; break; default: break; } return 0; } // 编码数据包解析线程函数(从视频文件中解析出音视频编码数据单元，一个AVPacket的data通常对应一个NAL) int parse_thread(void *arg) { VideoState *is = (VideoState *)arg;// 传递用户参数 global_video_state = is;// 传递全局状态参量结构体 /*------------------------- * 打开封装格式 * 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 * 参数一：封装格式上下文 * 参数二：视频路径 * 参数三：指定输入的格式 * 参数四：设置默认参数 --------------------------*/ AVFormatContext *avformat_context = NULL;// 参数一：封装格式上下文 int avformat_open_input_result = avformat_open_input(&avformat_context, is->filename, NULL, NULL); if (avformat_open_input_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"查找音视频流\\n\"); return -1; } is->pFormatCtx = avformat_context;//传递文件容器封装信息及码流参数 /*------------------------- * 查找码流 * 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 * 参数一：封装格式上下文 * 参数二：指定默认配置 -------------------------*/ int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result videoStream = -1;//视频流类型标号初始化为-1 is->audioStream = -1;//音频流类型标号初始化为-1 // 视频流类型标号初始化为-1 int av_video_stream_index = -1; // 音频流类型标号初始化为-1 int av_audio_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 若文件中包含有视频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_video_stream_index = i; } // 若文件中包含有音频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_audio_stream_index = i; } } // 检查文件中是否存在视频流 if (av_video_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到视频流\\n\"); goto fail;//跳转至异常处理 return -1; } // 检查文件中是否存在音频流 if (av_audio_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到音频流\\n\"); goto fail;//跳转至异常处理 return -1; } stream_component_open(is, av_audio_stream_index);// 根据指定类型打开音频流 stream_component_open(is, av_video_stream_index);// 根据指定类型打开视频流 // Main decode loop. for (;;) { if (is->quit) {//检查退出进程标识 break; } // Seek stuff goes here，检查音视频编码数据包队列长度是否溢出 if (is->audioq.size > MAX_AUDIOQ_SIZE || is->videoq.size > MAX_VIDEOQ_SIZE) { SDL_Delay(10); continue; } /*----------------------- * read in a packet and store it in the AVPacket struct * ffmpeg allocates the internal data for us,which is pointed to by packet.data * this is freed by the av_free_packet() -----------------------*/ // 负责保存压缩编码数据相关信息的结构体,每帧图像由一到多个packet包组成 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 if (av_read_frame(is->pFormatCtx, packet) pFormatCtx->pb->error == 0) { SDL_Delay(100); // No error; wait for user input. continue; } else { break; } } // Is this a packet from the video stream? if (packet->stream_index == is->videoStream) {// 检查数据包是否为视频类型 packet_queue_put(&is->videoq, packet);// 向队列中插入数据包 } else if (packet->stream_index == is->audioStream) {// 检查数据包是否为音频类型 packet_queue_put(&is->audioq, packet);// 向队列中插入数据包 } else {// 检查数据包是否为字幕类型 av_packet_unref(packet);// 释放packet中保存的(字幕)编码数据 } } // All done - wait for it. while (!is->quit) { SDL_Delay(100); } fail:// 异常处理 if (1) { SDL_Event event;// SDL事件对象 event.type = FF_QUIT_EVENT;// 指定退出事件类型 event.user.data1 = is;// 传递用户数据 SDL_PushEvent(&event);// 将该事件对象压入SDL后台事件队列 } return 0; } int init_sdl(VideoState *is) { // 初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } // 初始化SDL窗口 SDL_Window* sdl_window = SDL_CreateWindow(\"FFmpeg+SDL播放视频\",// 参数一：窗口名称 SDL_WINDOWPOS_CENTERED,// 参数二：窗口在屏幕上的x坐标 SDL_WINDOWPOS_CENTERED,// 参数三：窗口在屏幕上的y坐标 is->video_width,// 参数四：窗口在屏幕上宽 is->video_height,// 参数五：窗口在屏幕上高 SDL_WINDOW_OPENGL);// 参数六：窗口状态(打开) if (sdl_window == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建渲染器 // 定义渲染器区域 SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window,// 渲染目标创建 -1, // 从那里开始渲染(-1:表示从第一个位置开始) 0);// 渲染类型(软件渲染) if (sdl_renderer == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建纹理 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer,// 渲染器 SDL_PIXELFORMAT_IYUV,// 像素数据格式 SDL_TEXTUREACCESS_STREAMING,// 绘制方式：频繁绘制- is->video_width,// 纹理宽 is->video_height);// 纹理高 if (sdl_texture == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } is->sdl_renderer = sdl_renderer; is->sdl_texture = sdl_texture; return 0; } // SDL入口 extern \"C\" int main(int argc, char *argv[]) { /*------------------------- * 注册组件 * 注册所有ffmpeg支持的多媒体格式及编解码器 -------------------------*/ av_register_all(); // 创建全局状态对象 VideoState *is= (VideoState *)av_mallocz(sizeof(VideoState)); av_strlcpy(is->filename, \"/storage/emulated/0/Download/test.mov\", sizeof(is->filename));// 复制视频文件路径名 is->video_width = 640; is->video_height = 352; av_strlcpy(is->filename, \"/storage/emulated/0/DCIM/Camera/TG-2022-04-13-160703582.mp4\", sizeof(is->filename));// 复制视频文件路径名 is->video_width = 720; is->video_height = 1280; is->pictq_lock = SDL_CreateMutex();// 创建编码数据包队列互斥锁对象 is->pictq_ready = SDL_CreateCond();// 创建编码数据包队列就绪条件对象 int init_sdl_result = init_sdl(is); if (init_sdl_result av_sync_type = DEFAULT_AV_SYNC_TYPE;//指定主同步源 // 创建编码数据包解析线程 is->parse_tid = SDL_CreateThread(parse_thread, \"编码数据包解析线程\", is); if (!is->parse_tid) {// 检查线程是否创建成功 av_free(is); return -1; } // SDL事件对象 SDL_Event event; for (;;) {// SDL事件循环 SDL_WaitEvent(&event);// 主线程阻塞，等待事件到来 switch(event.type) {// 事件到来后唤醒主线程，检查事件类型 case FF_QUIT_EVENT: case SDL_QUIT:// 退出进程事件 is->quit = 1; // If the video has finished playing, then both the picture and audio queues are waiting for more data. // Make them stop waiting and terminate normally.. avcodec_close(is->video_st->codec); avformat_free_context(is->pFormatCtx); SDL_CondSignal(is->audioq.qready);// 发出队列就绪信号避免死锁 SDL_CondSignal(is->videoq.qready); SDL_DestroyTexture(is->sdl_texture); SDL_DestroyRenderer(is->sdl_renderer); SDL_Quit(); return 0; case FF_ALLOC_EVENT: alloc_picture(event.user.data1);// 分配视频帧事件响应函数 break; case FF_REFRESH_EVENT:// 视频显示刷新事件 video_refresh_timer(event.user.data1);// 视频显示刷新事件响应函数 break; default: break; } } return 0; } "},"pages/FFmpeg/FFmpeg_SDL快进快退.html":{"url":"pages/FFmpeg/FFmpeg_SDL快进快退.html","title":"FFmpeg+SDL快进快退","keywords":"","body":"FFmpeg+SDL快进快退 iOS工程代码 ffmpeg播放器实现详解 - 快进快退控制：https://www.cnblogs.com/breakpointlab/p/15807316.html 源代码一览 #import #import \"AppDelegate.h\" #import \"SDL.h\" //#include //#include //#include #include #include \"SDL.h\" #include \"SDL_thread.h\" #include \"SDL_syswm.h\" #import \"ViewController.h\" #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 #define AV_SYNC_THRESHOLD 0.01//前后两帧间的显示时间间隔的最小值0.01s #define AV_NOSYNC_THRESHOLD 10.0//最小刷新间隔时间10ms #define MAX_AUDIOQ_SIZE (5 * 16 * 1024) #define MAX_VIDEOQ_SIZE (5 * 256 * 1024) #define FF_ALLOC_EVENT (SDL_USEREVENT) #define FF_REFRESH_EVENT (SDL_USEREVENT + 1) #define FF_QUIT_EVENT (SDL_USEREVENT + 2) #define VIDEO_PICTURE_QUEUE_SIZE 1 #define SAMPLE_CORRECTION_PERCENT_MAX 10 #define AUDIO_DIFF_AVG_NB 20 //extern \"C\" { #include #include \"libavformat/avformat.h\" #include #include #include #include #include #include //} AVPacket flush_pkt;//在执行[快进]/[快退]操作后，ffmpeg需要执行重置解码器操作 uint64_t global_video_pkt_pts = AV_NOPTS_VALUE; enum {//同步时钟源 AV_SYNC_AUDIO_MASTER,//音频时钟为主同步源 AV_SYNC_VIDEO_MASTER,//视频时钟为主同步源 AV_SYNC_EXTERNAL_MASTER,//外部时钟为主同步源 }; #define DEFAULT_AV_SYNC_TYPE AV_SYNC_AUDIO_MASTER//指定以视频时钟为主同步源(时间基准) SDL_Window *screen;//SDL绘图表面 /*-------链表节点结构体-------- typedef struct AVPacketList { AVPacket pkt;//链表数据 struct AVPacketList *next;//链表后继节点 } AVPacketList; ---------------------------*/ // 数据包队列(链表)结构体 typedef struct PacketQueue { AVPacketList *first_pkt, *last_pkt;// 队列首尾节点指针 int nb_packets;// 队列长度 int size;// 保存编码数据的缓存长度，size=packet->size SDL_mutex *qlock;// 队列互斥量，保护队列数据 SDL_cond *qready;// 队列就绪条件变量 } PacketQueue; // 图像帧结构体 typedef struct VideoPicture { AVFrame *avframe_yuv420p; int width, height;//Source height & width. int allocated;//是否分配内存空间，视频帧转换为SDL overlay标识 double pts;//当前图像帧的绝对显示时间戳 } VideoPicture; typedef struct VideoState { AVFormatContext *pFormatCtx;// 保存文件容器封装信息及码流参数的结构体 AVStream *video_st;// 视频流信息结构体 AVStream *audio_st;//音频流信息结构体 struct SwsContext *sws_ctx;// 描述转换器参数的结构体 struct SwsContext *sws_ctx_audio; PacketQueue videoq;// 视频编码数据包队列(编码数据队列，以链表方式实现) VideoPicture pictq[VIDEO_PICTURE_QUEUE_SIZE]; int pictq_size, pictq_rindex, pictq_windex;// 队列长度，读/写位置索引 SDL_mutex *pictq_lock;// 队列读写锁对象，保护图像帧队列数据 SDL_cond *pictq_ready;// 队列就绪条件变量 SDL_Rect sdl_rect; SDL_Renderer* sdl_renderer; SDL_Texture* sdl_texture; PacketQueue audioq;// 音频编码数据包队列(编码数据队列，以链表方式实现) uint8_t audio_buf[(MAX_AUDIO_FRAME_SIZE*3)/2];//保存解码一个packet后的多帧原始音频数据(解码数据队列，以数组方式实现) unsigned int audio_buf_size;//解码后的多帧音频数据长度 unsigned int audio_buf_index;//累计写入stream的长度 uint8_t *audio_pkt_data;//编码数据缓存指针位置 int audio_pkt_size;//缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) AVPacket audio_pkt;//保存从队列中提取的数据包 AVFrame audio_frame;//保存从数据包中解码的音频数据 int video_width; int video_height; char filename[1024];// 输入文件完整路径名 int videoStream, audioStream;// 音视频流类型标号 SDL_Thread *parse_tid;// 编码数据包解析线程id SDL_Thread *decode_tid;// 解码线程id int quit;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 //video/audio_clock save pts of last decoded frame/predicted pts of next decoded frame double video_clock;//keep track of how much time has passed according to the video double audio_clock; double frame_timer;//视频播放到当前帧时的累计已播放时间 double frame_last_pts;//上一帧图像的显示时间戳，用于在video_refersh_timer中保存上一帧的pts值 double frame_last_delay;//上一帧图像的动态刷新延迟时间 int av_sync_type;//主同步源类型 double external_clock;//External clock base int64_t external_clock_time;//外部时钟的绝对时间 double audio_diff_cum;//音频时钟与同步源累计时差，sed for AV difference average computation double audio_diff_avg_coef;//音频时钟与同步源时差均值加权系数 double audio_diff_threshold;//音频时钟与同步源时差均值阈值 int audio_diff_avg_count;//音频不同步计数(音频时钟与主同步源存在不同步的次数) int audio_hw_buf_size; double video_current_pts;//当前帧显示时间戳，Current displayed pts (different from video_clock if frame fifos are used) int64_t video_current_pts_time;//取得video_current_pts的系统时间，time (av_gettime) at which we updated video_current_pts - used to have running video pts int seek_req;//[快进]/[后退]操作开启标志位 int seek_flags;//[快进]/[后退]操作类型标志位 int64_t seek_pos;//[快进]/[后退]操作后的参考时间戳 } VideoState;// Since we only have one decoding thread, the Big Struct can be global in case we need it. VideoState *global_video_state; /*------取得当前播放音频数据的pts------ * 音视频同步的原理是根据音频的pts来控制视频的播放 * 也就是说在视频解码一帧后，是否显示以及显示多长时间，是通过该帧的PTS与同时正在播放的音频的PTS比较而来的 * 如果音频的PTS较大，则视频准备完毕立即刷新，否则等待 * * 因为pcm数据采用audio_callback回调方式进行播放 * 对于音频播放我们只能得到写入回调函数前缓存音频帧的pts，而无法得到当前播放帧的pts(需要采用当前播放音频帧的pts作为参考时钟) * 考虑到音频的大小与播放时间成正比(相同采样率)，那么当前时刻正在播放的音频帧pts(位于回调函数缓存中) * 就可以根据已送入声卡的pcm数据长度、缓存中剩余pcm数据长度，缓存长度及采样率进行推算了 --------------------------------*/ double get_audio_clock(VideoState *is) { double pts=is->audio_clock;//Maintained in the audio thread，取得解码操作完成时的当前播放时间戳 //还未(送入声卡)播放的剩余原始音频数据长度，等于解码后的多帧原始音频数据长度-累计送入声卡的长度 int hw_buf_size=is->audio_buf_size-is->audio_buf_index;//计算当前音频解码数据缓存索引位置 int bytes_per_sec=0;//每秒的原始音频字节数 int pcm_bytes=is->audio_st->codec->channels*2;//每组原始音频数据字节数=声道数*每声道数据字节数 if (is->audio_st) { bytes_per_sec=is->audio_st->codec->sample_rate*pcm_bytes;//计算每秒的原始音频字节数 } if (bytes_per_sec) {//检查每秒的原始音频字节数 pts-=(double)hw_buf_size/bytes_per_sec;//根据送入声卡缓存的索引位置，往前倒推计算当前时刻的音频播放时间戳pts } return pts;//返回当前正在播放的音频时间戳 } /*-----------取得视频时钟----------- * 即取得当前播放视频帧的pts，以视频时钟pts作为音视频同步基准，return the current time offset of the video currently being played * 该值为当前帧时间戳pts+一个微小的修正值delta * 因为在ms的级别上，在毫秒级别上，若取得视频时钟(即当前帧pts)的时刻，与调用视频时钟的时刻(如将音频同步到该视频pts时刻)存在延迟 * 那么，视频时钟需要在被调用时进行修正，修正值delta为 * delta=[取得视频时钟的时刻值video_current_pts_time] 到 [调用get_video_clock时刻值] 的间隔时间 * 通常情况下，都会选择以外部时钟或音频时钟作为主同步源，以视频同步到音频或外部时钟为首选同步方案 * 以视频时钟作为主同步源的同步方案，属于3种基本的同步方案(同步到音频、同步到视频、同步到外部时钟) * 本利仅为展示同步到视频时钟的方法，一般情况下同步到视频时钟仅作为辅助的同步方案 --------------------------------*/ double get_video_clock(VideoState *is) { double delta=(av_gettime()-is->video_current_pts_time)/1000000.0; //pts_of_last_frame+(Current_time-time_elapsed_since_pts_value_was_set) return is->video_current_pts+delta; } //取得系统时间，以系统时钟作为同步基准 double get_external_clock(VideoState *is) { return av_gettime()/1000000.0;//取得系统当前时间，以1/1000000秒为单位，便于在各个平台移植 } //取得主时钟(基准时钟) double get_master_clock(VideoState *is) { if (is->av_sync_type == AV_SYNC_VIDEO_MASTER) { return get_video_clock(is);//返回视频时钟 } else if (is->av_sync_type == AV_SYNC_AUDIO_MASTER) { return get_audio_clock(is);//返回音频时钟 } else { return get_external_clock(is);//返回系统时钟 } } /*--------------------------- * return the wanted number of samples to get better sync if sync_type is video or external master clock * 通常情况下会以音频或系统时钟为主同步源，只有在音频或系统时钟失效的情况下才以视频为主同步源 * 该函数比对音频时钟与主同步源的时差，通过动态丢帧(或插值)部分音频数据，以起到减少(或增加)音频播放时长，减少与主同步源时差的作用 * 该函数对音频缓存数据进行丢帧(或插值)，返回丢帧(或插值)后的音频数据长度 * 因为音频同步可能带来输出声音不连续等副作用，该函数通过音频不同步次数(audio_diff_avg_count)及时差均值(avg_diff)来约束音频的同步过程 ---------------------------*/ int synchronize_audio(VideoState *is, short *samples, int samples_size, double pts) { double ref_clock;//主同步源(基准时钟) int pcm_bytes=is->audio_st->codec->channels*2;//每组音频数据字节数=声道数*每声道数据字节数 /* if not master, then we try to remove or add samples to correct the clock */ if (is->av_sync_type != AV_SYNC_AUDIO_MASTER) {//检查主同步源，若同步源不是音频时钟的情况下，执行以下代码 double diff, avg_diff;//diff-音频帧播放间与主同步源时差，avg_diff-采样不同步平均值 int wanted_size, min_size, max_size;//经过丢帧(或插值)后的缓存长度，缓存长度最大/最小值 ref_clock = get_master_clock(is);//取得当前主同步源，以主同步源为基准时间 diff = get_audio_clock(is) - ref_clock;//计算音频时钟与当前主同步源的时差 if (diffaudio_diff_cum=diff+is->audio_diff_avg_coef*is->audio_diff_cum; if (is->audio_diff_avg_countaudio_diff_avg_count++;//音频不同步计数更新 } else {//当音频不同步次数超过阈值限定后，触发音频同步操作 avg_diff=is->audio_diff_cum*(1.0-is->audio_diff_avg_coef);//计算时差均值(等比级数几何平均数) if (fabs(avg_diff)>=is->audio_diff_threshold) {//比对时差均值与时差阈值 wanted_size=samples_size+((int)(diff*is->audio_st->codec->sample_rate)*pcm_bytes);//根据时差换算同步后的缓存长度 min_size=samples_size*((100-SAMPLE_CORRECTION_PERCENT_MAX)/100);//同步后的缓存长度最小值 max_size=samples_size*((100+SAMPLE_CORRECTION_PERCENT_MAX)/100);//同步后的缓存长度最大值 if (wanted_sizemax_size) {//若同步后缓存长度>最小缓存长度 wanted_size=max_size;//用最大缓存长度作为同步后的缓存长度 } if (wanted_sizesamples_size) {//若同步后缓存长度大于当前缓存长度 //Add samples by copying final sample，通过复制最后一个音频数据进行插值 //int nb=samples_size-wanted_size; int nb=wanted_size-samples_size;//计算插值后缓存长度与原始缓存长度间的差值(需要插值的音频数据组数) uint8_t *samples_end=(uint8_t*)samples+samples_size-pcm_bytes;//取得缓存末端数据指针 uint8_t *q=samples_end+pcm_bytes;//初始插值位置|||q| while (nb>0) {//检查插值音频组数(每组包括两个声道的pcm数据) memcpy(q,samples_end,pcm_bytes);//在samples原始缓存后追加插值 q += pcm_bytes;//更新插值位置 nb -= pcm_bytes;//更新插值组数 } samples_size=wanted_size;//返回音频同步后的缓存长度 } } } } else { // Difference is too big, reset diff stuff，时差过大，重置时差累计值 is->audio_diff_avg_count = 0;//音频不同步计数重置 is->audio_diff_cum = 0;//音频累计时差重置 } }//end for if (is->av_sync_type != AV_SYNC_AUDIO_MASTER) return samples_size;//返回发送到声卡的音频缓存字节数 } // 定时器触发的回调函数 static Uint32 sdl_refresh_timer_cb(Uint32 interval, void *opaque) { SDL_Event event;//SDL事件对象 event.type = FF_REFRESH_EVENT;//视频显示刷新事件 event.user.data1 = opaque;//传递用户数据 SDL_PushEvent(&event);//发送事件 return 0; // 0 means stop timer. } /*--------------------------- * Schedule a video refresh in 'delay' ms. * 告诉sdl在指定的延时后来推送一个 FF_REFRESH_EVENT 事件 * 这个事件将在事件队列里触发sdl_refresh_timer_cb函数的调用 --------------------------*/ static void schedule_refresh(VideoState *is, int delay) { SDL_AddTimer(delay, sdl_refresh_timer_cb, is);//在指定的时间(ms)后回调用户指定的函数 } // 视频(图像)帧渲染 void video_display(VideoState *is) { SDL_Rect rect;// SDL矩形对象 VideoPicture *vp;// 图像帧结构体指针 float aspect_ratio;//宽度/高度比 int w, h, x, y;//窗口尺寸及起始位置 vp = &is->pictq[is->pictq_rindex];//从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 if (is->video_st->codec->sample_aspect_ratio.num == 0) { aspect_ratio = 0; } else { aspect_ratio = av_q2d(is->video_st->codec->sample_aspect_ratio) * is->video_st->codec->width / is->video_st->codec->height; } if (aspect_ratio video_st->codec->width / (float) is->video_st->codec->height; } // SDL中获取屏幕尺寸 SDL_DisplayMode DM; SDL_GetCurrentDisplayMode(0, &DM); h = DM.h; w = ((int) rint(h * aspect_ratio)) & -3; if (w > DM.w) { w = DM.w; h = ((int) rint(w / aspect_ratio)) & -3; } x = (DM.w - w) / 2; y = (DM.h - h) / 2; // 设置纹理数据 SDL_UpdateTexture(is->sdl_texture, // 纹理 NULL,// 渲染区域 vp->avframe_yuv420p->data[0],// 需要渲染数据：视频像素数据帧 vp->avframe_yuv420p->linesize[0]);// 帧宽 // 将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) is->sdl_rect.x = x; is->sdl_rect.y = y; is->sdl_rect.w = w; is->sdl_rect.h = h; SDL_RenderClear(is->sdl_renderer); SDL_RenderCopy(is->sdl_renderer, is->sdl_texture, NULL, &is->sdl_rect); // 呈现画面帧 SDL_RenderPresent(is->sdl_renderer); }// end for if }// end for video_display // 显示刷新函数(FF_REFRESH_EVENT响应函数) int video_current_index = 0; void video_refresh_timer(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp;//图像帧对象 //delay-前后帧间的显示时间间隔，diff-图像帧显示与音频帧播放间的时间差 //sync_threshold-前后帧间的最小时间差，actual_delay-当前帧-下已帧的显示时间间隔(动态时间、真实时间、绝对时间) double delay,diff,sync_threshold,actual_delay,ref_clock;//ref_clock-音频时间戳 if (is->video_st) { if (is->pictq_size == 0) {// 检查图像帧队列是否有待显示图像 schedule_refresh(is, 1);//若队列为空，则发送显示刷新事件并再次进入video_refresh_timer函数 } else {// 刷新图像 vp = &is->pictq[is->pictq_rindex];//从显示队列中取得等待显示的图像帧 is->video_current_pts = vp->pts;//取得当前帧的显示时间戳 is->video_current_pts_time = av_gettime();//取得系统时间，作为当前帧播放的时间基准 //计算当前帧和前一帧显示(pts)的间隔时间(显示时间戳的差值) //计算当前帧和前一帧显示(pts)的间隔时间(显示时间戳的差值) delay = vp->pts - is->frame_last_pts;//The pts from last time，前后帧间的时间差 if (delay = 1.0) {//检查时间间隔是否在合理范围 // If incorrect delay, use previous one delay = is->frame_last_delay;//沿用之前的动态刷新间隔时间 } // Save for next time is->frame_last_delay = delay;//保存上一帧图像的动态刷新延迟时间 is->frame_last_pts = vp->pts;//保存上一帧图像的显示时间戳 // Update delay to sync to audio，取得声音播放时间戳(作为视频同步的参考时间) // Update delay to sync to audio，取得声音播放时间戳(作为视频同步的参考时间) if (is->av_sync_type != AV_SYNC_VIDEO_MASTER) {//检查主同步时钟源 ref_clock = get_master_clock(is);//根据Audio clock来判断Video播放的快慢，获取当前播放声音的时间戳 //也就是说在diff这段时间中声音是匀速发生的，但是在delay这段时间frame的显示可能就会有快慢的区别 diff = vp->pts - ref_clock;//计算图像帧显示与音频帧播放间的时间差 //根据时间差调整播放下一帧的延迟时间，以实现同步 Skip or repeat the frame，Take delay into account sync_threshold = (delay > AV_SYNC_THRESHOLD) ? delay : AV_SYNC_THRESHOLD;//比较前后两帧间的显示时间间隔与最小时间间隔 //判断音视频不同步条件，即音视频间的时间差 & 前后帧间的时间差该阈值则为快进模式，不存在音视频同步问题 if (fabs(diff) = sync_threshold) {//比较两帧画面间的显示时间与两帧画面间声音的播放时间，快了，加倍delay delay = 2 * delay; } }//如果diff(明显)大于AV_NOSYNC_THRESHOLD，即快进的模式了，画面跳动太大，不存在音视频同步的问题了 } //更新视频播放到当前帧时的已播放时间值(所有图像帧动态播放累计时间值-真实值)，frame_timer一直累加在播放过程中我们计算的延时 is->frame_timer+=delay; //每次计算frame_timer与系统时间的差值(以系统时间为基准时间)，将frame_timer与系统时间(绝对时间)相关联的目的 actual_delay=is->frame_timer-(av_gettime()/1000000.0);//Computer the REAL delay if (actual_delay pts: %f，ref_clock：%f，actual_delay：%f\", video_current_index, vp->pts, ref_clock, actual_delay); // Update queue for next picture! if (++is->pictq_rindex == VIDEO_PICTURE_QUEUE_SIZE) {// 更新并检查图像帧队列读位置索引 is->pictq_rindex = 0;// 重置读位置索引 } SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 is->pictq_size--;// 更新图像帧队列长度 SDL_CondSignal(is->pictq_ready);// 发送队列就绪信号 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } } else { schedule_refresh(is, 100); } } // 数据包队列初始化函数 void packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue));// 全零初始化队列结构体对象 q->qlock = SDL_CreateMutex();// 创建互斥量对象 q->qready = SDL_CreateCond();// 创建条件变量对象 } // 向队列中插入数据包 int packet_queue_put(PacketQueue *q, AVPacket *pkt) { /*-------准备队列(链表)节点对象------*/ AVPacketList *pktlist=(AVPacketList *)av_malloc(sizeof(AVPacketList));// 在堆上创建链表节点对象 if (!pktlist) {// 检查链表节点对象是否创建成功 return -1; } pktlist->pkt = *pkt;// 将输入数据包赋值给新建链表节点对象中的数据包对象 pktlist->next = NULL;// 链表后继指针为空 // if (av_packet_ref(pkt, pkt) qlock);// 队列互斥量加锁，保护队列数据 if (!q->last_pkt) {// 检查队列尾节点是否存在(检查队列是否为空) q->first_pkt = pktlist;// 若不存在(队列尾空)，则将当前节点作队列为首节点 } else { q->last_pkt->next = pktlist;// 若已存在尾节点，则将当前节点挂到尾节点的后继指针上，并作为新的尾节点 } q->last_pkt = pktlist;// 将当前节点作为新的尾节点 q->nb_packets++;// 队列长度+1 q->size += pktlist->pkt.size;// 更新队列编码数据的缓存长度 SDL_CondSignal(q->qready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(q->qlock);// 释放互斥量 return 0; } // 从队列中提取数据包，并将提取的数据包出队列 static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block) { AVPacketList *pktlist;// 临时链表节点对象指针 int ret;// 操作结果 SDL_LockMutex(q->qlock);// 队列互斥量加锁，保护队列数据 for (;;) { if (global_video_state->quit) {// 检查退出进程标识 ret = -1;// 操作失败 break; }//end for if pktlist = q->first_pkt;// 传递将队列首个数据包指针 if (pktlist) {// 检查数据包是否为空(队列是否有数据) q->first_pkt = pktlist->next;// 队列首节点指针后移 if (!q->first_pkt) {// 检查首节点的后继节点是否存在 q->last_pkt = NULL;// 若不存在，则将尾节点指针置空 } q->nb_packets--;// 队列长度-1 q->size -= pktlist->pkt.size;// 更新队列编码数据的缓存长度 *pkt = pktlist->pkt;// 将队列首节点数据返回 av_free(pktlist);// 清空临时节点数据(清空首节点数据，首节点出队列) ret = 1;// 操作成功 break; } else if (!block) { ret = 0; break; } else {// 队列处于未就绪状态，此时通过SDL_CondWait函数等待qready就绪信号，并暂时对互斥量解锁 /*--------------------- * 等待队列就绪信号qready，并对互斥量暂时解锁 * 此时线程处于阻塞状态，并置于等待条件就绪的线程列表上 * 使得该线程只在临界区资源就绪后才被唤醒，而不至于线程被频繁切换 * 该函数返回时，互斥量再次被锁住，并执行后续操作 --------------------*/ SDL_CondWait(q->qready, q->qlock);// 暂时解锁互斥量并将自己阻塞，等待临界区资源就绪(等待SDL_CondSignal发出临界区资源就绪的信号) } }//end for for-loop SDL_UnlockMutex(q->qlock);// 释放互斥量 return ret; } // 创建/重置图像帧，为图像帧分配内存空间 void alloc_picture(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {// 检查图像帧是否已存在 // We already have one make another, bigger/smaller. av_frame_free(&vp->avframe_yuv420p); } vp->width = is->video_st->codec->width;// 设置图像帧宽度 vp->height = is->video_st->codec->height;// 设置图像帧高度 SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 vp->allocated = 1;// 图像帧像素缓冲区已分配内存 // AV_PIX_FMT_YUV420P格式的视频帧 vp->avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型 int buffer_size =av_image_get_buffer_size(AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 一帧视频像素数据宽 = 视频宽 is->video_st->codec->height,// 一帧视频像素数据高 = 视频高 1);// 字节对齐方式，默认是1 // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 av_image_fill_arrays(vp->avframe_yuv420p->data,// 目标视频帧数据 vp->avframe_yuv420p->linesize,// 目标视频帧行大小 out_buffer,// 原始数据 AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 视频宽 is->video_st->codec->height,//视频高 1);// 字节对齐方式 SDL_CondSignal(is->pictq_ready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } /*--------------------------- * queue_picture：图像帧插入队列等待渲染 * @is：全局状态参数集 * @pFrame：保存图像解码数据的结构体 * 1、首先检查图像帧队列(数组)是否存在空间插入新的图像，若没有足够的空间插入图像则使当前线程休眠等待 * 2、在初始化的条件下，队列(数组)中VideoPicture的bmp对象(YUV overlay)尚未分配空间，通过FF_ALLOC_EVENT事件的方法调用alloc_picture分配空间 * 3、当队列(数组)中所有VideoPicture的bmp对象(YUV overlay)均已分配空间的情况下，直接跳过步骤2向bmp对象拷贝像素数据，像素数据在进行格式转换后执行拷贝操作 ---------------------------*/ int queue_picture(VideoState *is, AVFrame *pFrame, double pts) { /*--------1、检查队列是否有插入空间-------*/ // Wait until we have space for a new pic. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (is->pictq_size >= VIDEO_PICTURE_QUEUE_SIZE && !is->quit) {// 检查队列当前长度 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待pictq_ready信号 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } /*-------2、初始化/重置YUV overlay-------*/ // windex is set to 0 initially. VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列中抽取图像帧对象 // Allocate or resize the buffer，检查YUV overlay是否已存在，否则初始化YUV overlay，分配像素缓存空间 if (!vp->avframe_yuv420p || vp->width!=is->video_st->codec->width || vp->height!=is->video_st->codec->height) { vp->allocated = 0;// 图像帧未分配空间 // We have to do it in the main thread. SDL_Event event;// SDL事件对象 event.type = FF_ALLOC_EVENT;//指定分配图像帧内存事件 event.user.data1 = is;//传递用户数据 SDL_PushEvent(&event);//发送SDL事件 // Wait until we have a picture allocated. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (!vp->allocated && !is->quit) {// 检查当前图像帧是否已初始化 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待alloc_picture发送pictq_ready信号唤醒当前线程 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } }// end for if /*--------3、拷贝视频帧到YUV overlay-------*/ // We have a place to put our picture on the queue. if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // Convert the image into YUV format that SDL uses，将解码后的图像帧转换为AV_PIX_FMT_YUV420P格式，并拷贝到图像帧队列 sws_scale(is->sws_ctx, (uint8_t const * const *)pFrame->data, pFrame->linesize, 0, is->video_st->codec->height, vp->avframe_yuv420p->data, vp->avframe_yuv420p->linesize); vp->pts = pts;//传递当前图像帧的绝对显示时间戳 // Now we inform our display thread that we have a pic ready. if (++is->pictq_windex == VIDEO_PICTURE_QUEUE_SIZE) {//更新并检查当前图像帧队列写入位置 is->pictq_windex = 0;//重置图像帧队列写入位置 } SDL_LockMutex(is->pictq_lock);//锁定队列读写锁，保护队列数据 is->pictq_size++;//更新图像帧队列长度 SDL_UnlockMutex(is->pictq_lock);//释放队列读写锁 }// end for if return 0; } /*--------------------------- * 更新内部视频播放计时器(记录视频已经播时间(video_clock)） * @is：全局状态参数集 * @src_frame：当前(输入的)(待更新的)图像帧对象 * @pts：当前图像帧的显示时间戳 * update the PTS to be in sync ---------------------------*/ double synchronize_video(VideoState *is, AVFrame *src_frame, double pts) { /*----------检查显示时间戳----------*/ if (pts != 0) {//检查显示时间戳是否有效 // If we have pts, set video clock to it. is->video_clock = pts;//用显示时间戳更新已播放时间 } else {//若获取不到显示时间戳 // If we aren't given a pts, set it to the clock. pts = is->video_clock;//用已播放时间更新显示时间戳 } /*--------更新视频已经播时间--------*/ // Update the video clock，若该帧要重复显示(取决于repeat_pict)，则全局视频播放时序video_clock应加上重复显示的数量*帧率 double frame_delay = av_q2d(is->video_st->codec->time_base);//该帧显示完将要花费的时间 // If we are repeating a frame, adjust clock accordingly,若存在重复帧，则在正常播放的前后两帧图像间安排渲染重复帧 frame_delay += src_frame->repeat_pict*(frame_delay*0.5);//计算渲染重复帧的时值(类似于音符时值) is->video_clock += frame_delay;//更新视频播放时间 // printf(\"repeat_pict=%d \\n\",src_frame->repeat_pict); return pts;//此时返回的值即为下一帧将要开始显示的时间戳 } // These are called whenever we allocate a frame buffer. We use this to store the global_pts in a frame at the time it is allocated. int our_get_buffer(struct AVCodecContext *c, AVFrame *pic, int flags) { int ret = avcodec_default_get_buffer2(c, pic, 0); uint64_t *pts = (uint64_t *)av_malloc(sizeof(uint64_t)); *pts = global_video_pkt_pts; pic->opaque = pts; return ret; } // 视频解码线程函数 int decode_thread(void *arg) { VideoState *is = (VideoState *) arg;// 传递用户数据 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 int frameFinished;// 解码操作是否成功标识 // Allocate video frame，为解码后的视频信息结构体分配空间并完成初始化操作(结构体中的图像缓存按照下面两步手动安装) AVFrame *pFrame = av_frame_alloc(); double pts;//当前桢在整个视频中的(绝对)时间位置 for (;;) { if (packet_queue_get(&is->videoq,packet,1)data == flush_pkt.data) {//检查是否需要重新解码 avcodec_flush_buffers(is->video_st->codec);//重新解码前需要重置解码器 continue; } pts = 0;//(绝对)显示时间戳初始化 global_video_pkt_pts = packet->pts;// Save global pts to be stored in pFrame in first call. /*----------------------- * Decode video frame，解码完整的一帧数据，并将frameFinished设置为true * 可能无法通过只解码一个packet就获得一个完整的视频帧frame，可能需要读取多个packet才行 * avcodec_decode_video2()会在解码到完整的一帧时设置frameFinished为真 * Technically a packet can contain partial frames or other bits of data * ffmpeg's parser ensures that the packets we get contain either complete or multiple frames * convert the packet to a frame for us and set frameFinisned for us when we have the next frame -----------------------*/ avcodec_decode_video2(is->video_st->codec, pFrame, &frameFinished, packet); //取得编码数据包中的显示时间戳PTS(int64_t),并暂时保存在pts(double)中 // if (packet->dts==AV_NOPTS_VALUE && pFrame->opaque && *(uint64_t*)pFrame->opaque!=AV_NOPTS_VALUE) { // pts = *(uint64_t *)pFrame->opaque; // } else if (packet->dts != AV_NOPTS_VALUE) { // pts = packet->dts; // } else { // pts = 0; // } pts=av_frame_get_best_effort_timestamp(pFrame);//取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 /*------------------------- * 在解码线程函数中计算当前图像帧的显示时间戳 * 1、取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 * 2、根据PTS*time_base来计算当前桢在整个视频中的显示时间戳，即PTS*(1/framerate) * av_q2d把AVRatioal结构转换成double的函数， * 用于计算视频源每个图像帧显示的间隔时间(1/framerate),即返回(time_base->num/time_base->den) -------------------------*/ //根据pts=PTS*time_base={numerator=1,denominator=25}计算当前桢在整个视频中的显示时间戳 pts*=av_q2d(is->video_st->time_base);//time_base为AVRational有理数结构体{num=1,den=25}，记录了视频源每个图像帧显示的间隔时间 // Did we get a video frame，检查是否解码出完整一帧图像 if (frameFinished) { pts = synchronize_video(is, pFrame, pts);//检查当前帧的显示时间戳pts并更新内部视频播放计时器(记录视频已经播时间(video_clock)） if (queue_picture(is, pFrame, pts)sws_ctx_audio, \"in_channel_layout\", src_ch_layout, 0); av_opt_set_int(is->sws_ctx_audio, \"out_channel_layout\", dst_ch_layout, 0); av_opt_set_int(is->sws_ctx_audio, \"in_sample_rate\", src_rate, 0); av_opt_set_int(is->sws_ctx_audio, \"out_sample_rate\", dst_rate, 0); av_opt_set_sample_fmt(is->sws_ctx_audio, \"in_sample_fmt\", src_sample_fmt, 0); av_opt_set_sample_fmt(is->sws_ctx_audio, \"out_sample_fmt\", dst_sample_fmt, 0); int ret;//返回结果 // Initialize the resampling context. if ((ret = swr_init((struct SwrContext *) is->sws_ctx_audio)) sws_ctx_audio,src_rate)+src_nb_samples,dst_rate,src_rate,AV_ROUND_UP); //Convert to destination format. ret=swr_convert((struct SwrContext*)is->sws_ctx_audio,dst_data,dst_nb_samples,(const uint8_t **)decoded_frame.data,src_nb_samples); if (retaudio_buf, dst_data[0], dst_bufsize); if (src_data) { av_freep(&src_data[0]); } av_freep(&src_data); if (dst_data) { av_freep(&dst_data[0]); } av_freep(&dst_data); return dst_bufsize; } // 音频解码函数，从缓存队列中提取数据包、解码，并返回解码后的数据长度(对一个完整的packet解码，将解码数据写入audio_buf缓存，并返回多帧解码数据的总长度) int audio_decode_frame(VideoState *is, double *pts_ptr) { int coded_consumed_size,data_size=0,pcm_bytes;// 每次消耗的编码数据长度[input](len1)，输出原始音频数据的缓存长度[output]，每组音频采样数据的字节数 AVPacket *pkt = &is->audio_pkt;// 保存从队列中提取的数据包 double pts;//音频播放时间戳 for (;;) { while (is->audio_pkt_size>0) {// 检查缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) int got_frame = 0;// 解码操作成功标识，成功返回非零值 // 解码一帧音频数据，并返回消耗的编码数据长度 coded_consumed_size = avcodec_decode_audio4(is->audio_st->codec, &is->audio_frame, &got_frame, pkt); if (coded_consumed_size audio_pkt_size = 0;// 更新编码数据缓存长度 break; } if (got_frame) {// 检查解码操作是否成功 if (is->audio_frame.format != AV_SAMPLE_FMT_S16) {//检查音频数据格式是否为16位采样格式 //当音频数据不为16位采样格式情况下，采用decode_frame_from_packet计算解码数据长度 data_size=decode_frame_from_packet(is, is->audio_frame); } else {//计算解码后音频数据长度[output] data_size=av_samples_get_buffer_size(NULL,is->audio_st->codec->channels,is->audio_frame.nb_samples,is->audio_st->codec->sample_fmt, 1); memcpy(is->audio_buf,is->audio_frame.data[0],data_size);//将解码数据复制到输出缓存 } } is->audio_pkt_data += coded_consumed_size;// 更新编码数据缓存指针位置 is->audio_pkt_size -= coded_consumed_size;// 更新缓存中剩余的编码数据长度 if (data_size audio_clock;//用每次更新的音频播放时间更新音频PTS *pts_ptr=pts; /*--------------------- * 当一个packet中包含多个音频帧时 * 通过[解码后音频原始数据长度]及[采样率]来推算一个packet中其他音频帧的播放时间戳pts * 采样频率44.1kHz，量化位数16位，意味着每秒采集数据44.1k个，每个数据占2字节 --------------------*/ pcm_bytes=2*is->audio_st->codec->channels;//计算每组音频采样数据的字节数=每个声道音频采样字节数*声道数 /*----更新audio_clock--- * 一个pkt包含多个音频frame，同时一个pkt对应一个pts(pkt->pts) * 因此，该pkt中包含的多个音频帧的时间戳由以下公式推断得出 * bytes_per_sec=pcm_bytes*is->audio_st->codec->sample_rate * 从pkt中不断的解码，推断(一个pkt中)每帧数据的pts并累加到音频播放时钟 --------------------*/ is->audio_clock+=(double)data_size/(double)(pcm_bytes*is->audio_st->codec->sample_rate); // We have data, return it and come back for more later. return data_size;// 返回解码数据缓存长度 } if (pkt->data) {// 检查数据包是否已从队列中提取 av_packet_unref(pkt);// 释放pkt中保存的编码数据 } if (is->quit) {// 检查退出进程标识 return -1; } // Next packet，从队列中提取数据包到pkt if (packet_queue_get(&is->audioq, pkt, 1) data == flush_pkt.data) {//检查是否需要重新解码 avcodec_flush_buffers(is->audio_st->codec);//重新解码前需要重置解码器 continue; } is->audio_pkt_data = pkt->data;// 传递编码数据缓存指针 is->audio_pkt_size = pkt->size;// 传递编码数据缓存长度 // If update, update the audio clock w/pts if (pkt->pts != AV_NOPTS_VALUE) {//检查音频播放时间戳 //获得一个新的packet的时候，更新audio_clock，用packet中的pts更新audio_clock(一个pkt对应一个pts) is->audio_clock=pkt->pts*av_q2d(is->audio_st->time_base);//更新音频已经播的时间 } } } /*------Audio Callback------- * 音频输出回调函数，sdl通过该回调函数将解码后的pcm数据送入声卡播放, * sdl通常一次会准备一组缓存pcm数据，通过该回调送入声卡，声卡根据音频pts依次播放pcm数据 * 待送入缓存的pcm数据完成播放后，再载入一组新的pcm缓存数据(每次音频输出缓存为空时，sdl就调用此函数填充音频输出缓存，并送入声卡播放) * When we begin playing audio, SDL will continually call this callback function * and ask it to fill the audio buffer with a certain number of bytes * The audio function callback takes the following parameters: * stream: A pointer to the audio buffer to be filled，输出音频数据到声卡缓存 * len: The length (in bytes) of the audio buffer,缓存长度wanted_spec.samples=SDL_AUDIO_BUFFER_SIZE(1024) --------------------------*/ void audio_callback(void *userdata, Uint8 *stream, int len) { VideoState *is = (VideoState *) userdata;// 传递用户数据 int wt_stream_len, audio_size;// 每次写入stream的数据长度，解码后的数据长度 double pts;//音频时间戳 while (len > 0) {//检查音频缓存的剩余长度 if (is->audio_buf_index >= is->audio_buf_size) {// 检查是否需要执行解码操作 // We have already sent all our data; get more，从缓存队列中提取数据包、解码，并返回解码后的数据长度，audio_buf缓存中可能包含多帧解码后的音频数据 audio_size = audio_decode_frame(is, &pts); if (audio_size audio_buf_size = 1024; memset(is->audio_buf, 0, is->audio_buf_size);// 全零重置缓冲区 } else { //在回调函数中增加音频同步过程，即对音频数据缓存进行丢帧(或插值)，以起到降低音频时钟与主同步源时差的目的 audio_size=synchronize_audio(is,(int16_t*)is->audio_buf,audio_size,pts);//返回音频同步后的缓存长度 is->audio_buf_size = audio_size;// 返回packet中包含的原始音频数据长度(多帧) } is->audio_buf_index = 0;// 初始化累计写入缓存长度 }//end for if wt_stream_len=is->audio_buf_size-is->audio_buf_index;// 计算解码缓存剩余长度 if (wt_stream_len > len) {// 检查每次写入缓存的数据长度是否超过指定长度(1024) wt_stream_len = len;// 指定长度从解码的缓存中取数据 } // 每次从解码的缓存数据中以指定长度抽取数据并写入stream传递给声卡 memcpy(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, wt_stream_len); len -= wt_stream_len;// 更新解码音频缓存的剩余长度 stream += wt_stream_len;// 更新缓存写入位置 is->audio_buf_index += wt_stream_len;// 更新累计写入缓存数据长度 }//end for while } // 根据指定类型打开流，找到对应的解码器、创建对应的音频配置、保存关键信息到 VideoState、启动音频和视频线程 int stream_component_open(VideoState *is, int stream_index) { AVFormatContext *pFormatCtx = is->pFormatCtx;// 传递文件容器的封装信息及码流参数 AVCodecContext *codecCtx = NULL;// 解码器上下文对象，解码器依赖的相关环境、状态、资源以及参数集的接口指针 AVCodec *codec = NULL;// 保存编解码器信息的结构体，提供编码与解码的公共接口，可以看作是编码器与解码器的一个全局变量 //检查输入的流类型是否在合理范围内 if (stream_index = pFormatCtx->nb_streams) { return -1; } // Get a pointer to the codec context for the video stream. codecCtx = pFormatCtx->streams[stream_index]->codec;// 取得解码器上下文 if (codecCtx->codec_type == AVMEDIA_TYPE_AUDIO) {//检查解码器类型是否为音频解码器 SDL_AudioSpec wanted_spec, spec;//SDL_AudioSpec a structure that contains the audio output format，创建 SDL_AudioSpec 结构体，设置音频播放数据 // Set audio settings from codec info,SDL_AudioSpec a structure that contains the audio output format // 创建SDL_AudioSpec结构体，设置音频播放参数 wanted_spec.freq = codecCtx->sample_rate;//采样频率 DSP frequency -- samples per second wanted_spec.format = AUDIO_S16SYS;//采样格式 Audio data format wanted_spec.channels = codecCtx->channels;//声道数 Number of channels: 1 mono, 2 stereo wanted_spec.silence = 0;//无输出时是否静音 //默认每次读音频缓存的大小，推荐值为 512~8192，ffplay使用的是1024 specifies a unit of audio data refers to the size of the audio buffer in sample frames wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE; wanted_spec.callback = audio_callback;//设置读取音频数据的回调接口函数 the function to call when the audio device needs more data wanted_spec.userdata = is;//传递用户数据 /*--------------------------- * 以指定参数打开音频设备，并返回与指定参数最为接近的参数，该参数为设备实际支持的音频参数 * Opens the audio device with the desired parameters(wanted_spec) * return another specs we actually be using * and not guaranteed to get what we asked for --------------------------*/ if (SDL_OpenAudio(&wanted_spec, &spec) audio_hw_buf_size = spec.size; } /*----------------------- * Find the decoder for the video stream，根据视频流对应的解码器上下文查找对应的解码器，返回对应的解码器(信息结构体) * The stream's information about the codec is in what we call the \"codec context. * This contains all the information about the codec that the stream is using -----------------------*/ codec = avcodec_find_decoder(codecCtx->codec_id); AVDictionary *optionsDict = NULL; if (!codec || (avcodec_open2(codecCtx, codec, &optionsDict) name); // 检查解码器类型 switch(codecCtx->codec_type) { case AVMEDIA_TYPE_AUDIO:// 音频解码器 is->audioStream = stream_index;// 音频流类型标号初始化 is->audio_st = pFormatCtx->streams[stream_index]; is->audio_buf_size = 0;// 解码后的多帧音频数据长度 is->audio_buf_index = 0;//累 计写入stream的长度 // Averaging filter for audio sync. is->audio_diff_avg_coef=exp(log(0.01/AUDIO_DIFF_AVG_NB));//音频时钟与主同步源累计时差加权系数 is->audio_diff_avg_count=0;//音频不同步计数初始化 // Correct audio only if larger error than this. is->audio_diff_threshold=2.0*SDL_AUDIO_BUFFER_SIZE/codecCtx->sample_rate; is->sws_ctx_audio = (struct SwsContext *) swr_alloc(); if (!is->sws_ctx_audio) { fprintf(stderr, \"Could not allocate resampler context\\n\"); return -1; } memset(&is->audio_pkt, 0, sizeof(is->audio_pkt)); packet_queue_init(&is->audioq);// 音频数据包队列初始化 SDL_PauseAudio(0);// audio callback starts running again，开启音频设备，如果这时候没有获得数据那么它就静音 break; case AVMEDIA_TYPE_VIDEO:// 视频解码器 is->videoStream = stream_index;// 视频流类型标号初始化 is->video_st = pFormatCtx->streams[stream_index]; //以系统时间为基准，初始化播放到当前帧的已播放时间值，该值为真实时间值、动态时间值、绝对时间值 is->frame_timer=(double)av_gettime()/1000000.0; is->frame_last_delay = 40e-3;//初始化上一帧图像的动态刷新延迟时间 is->video_current_pts_time = av_gettime();//取得系统当前时间 packet_queue_init(&is->videoq);// 视频数据包队列初始化 is->decode_tid = SDL_CreateThread(decode_thread,\"视频解码线程\" ,is);// 创建视频解码线程 // Initialize SWS context for software scaling，设置图像转换像素格式为AV_PIX_FMT_YUV420P is->sws_ctx = sws_getContext(is->video_st->codec->width, is->video_st->codec->height, is->video_st->codec->pix_fmt, is->video_st->codec->width, is->video_st->codec->height, AV_PIX_FMT_YUV420P, SWS_BILINEAR, NULL, NULL, NULL); codecCtx->get_buffer2 = our_get_buffer; break; default: break; } return 0; } //清除队列缓存，释放队列中所有动态分配的内存 static void packet_queue_flush(PacketQueue *q) { AVPacketList *pkt, *pkttmp;//队列当前节点，临时节点 SDL_LockMutex(q->qlock);//锁定互斥量 for (pkt = q->first_pkt; pkt != NULL; pkt = pkttmp) {//遍历队列所有节点 pkttmp = pkt->next;//队列头节点后移 av_packet_unref(&pkt->pkt);//当前节点引用计数-1 av_freep(&pkt);//释放当前节点缓存 } q->last_pkt = NULL;//队列尾节点指针置零 q->first_pkt = NULL;//队列头节点指针置零 q->nb_packets = 0;//队列长度置零 q->size = 0;//队列编码数据的缓存长度置零 SDL_UnlockMutex(q->qlock);//互斥量解锁 } // 编码数据包解析线程函数(从视频文件中解析出音视频编码数据单元，一个AVPacket的data通常对应一个NAL) int parse_thread(void *arg) { VideoState *is = (VideoState *)arg;// 传递用户参数 global_video_state = is;// 传递全局状态参量结构体 /*------------------------- * 打开封装格式 * 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 * 参数一：封装格式上下文 * 参数二：视频路径 * 参数三：指定输入的格式 * 参数四：设置默认参数 --------------------------*/ AVFormatContext *avformat_context = NULL;// 参数一：封装格式上下文 int avformat_open_input_result = avformat_open_input(&avformat_context, is->filename, NULL, NULL); if (avformat_open_input_result != 0){ // __android_log_print(ANDROID_LOG_INFO, \"main\", \"查找音视频流\\n\"); return -1; } is->pFormatCtx = avformat_context;//传递文件容器封装信息及码流参数 /*------------------------- * 查找码流 * 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 * 参数一：封装格式上下文 * 参数二：指定默认配置 -------------------------*/ int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result videoStream = -1;//视频流类型标号初始化为-1 is->audioStream = -1;//音频流类型标号初始化为-1 // 视频流类型标号初始化为-1 int av_video_stream_index = -1; // 音频流类型标号初始化为-1 int av_audio_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 若文件中包含有视频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_video_stream_index = i; } // 若文件中包含有音频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_audio_stream_index = i; } } // 检查文件中是否存在视频流 if (av_video_stream_index == -1) { // __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到视频流\\n\"); goto fail;//跳转至异常处理 return -1; } // 检查文件中是否存在音频流 if (av_audio_stream_index == -1) { // __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到音频流\\n\"); goto fail;//跳转至异常处理 return -1; } stream_component_open(is, av_audio_stream_index);// 根据指定类型打开音频流 stream_component_open(is, av_video_stream_index);// 根据指定类型打开视频流 // Main decode loop. for (;;) { if (is->quit) {//检查退出进程标识 break; } // Seek stuff goes here if (is->seek_req) {//检查[快进]/[快退]操作标志位是否开启 int stream_index= -1;//初始化音视频流类型标号 int64_t seek_target = is->seek_pos;//取得[快进]/[快退]操作后的参考时间戳 if (is->videoStream >= 0) {//检查是否取得视频流类型标号 stream_index = is->videoStream;//取得视频流类型标号 } else if (is->audioStream >= 0) {//检查是否取得音频流类型标号 stream_index = is->audioStream;//取得音频流类型标号 } if (stream_index >= 0){//检查是否取得音视频流类型标号 //时间单位转换，将seek_target的单位由AV_TIME_BASE_Q转换为time_base seek_target= av_rescale_q(seek_target, AV_TIME_BASE_Q, avformat_context->streams[stream_index]->time_base); } //根据[快进]/[快退]操作后的时间戳，跳到指定帧(该函数只能跳到离指定帧最近的关键帧) if (av_seek_frame(is->pFormatCtx, stream_index, seek_target, is->seek_flags) pFormatCtx->filename); } else {//在执行[快进]/[快退]操作后，立刻清空缓存队列，并重置音视频解码器 if (is->audioStream >= 0) {//检查是否取得音频流类型标号 packet_queue_flush(&is->audioq);//清除音频队列缓存，释放队列中所有动态分配的内存 packet_queue_put(&is->audioq, &flush_pkt);//将flush_pkt插入音频数据包队列，执行重置音频解码器操作avcodec_flush_buffers } if (is->videoStream >= 0) {//取得视频流类型标号 packet_queue_flush(&is->videoq);//清除视频队列缓存，释放队列中所有动态分配的内存 packet_queue_put(&is->videoq, &flush_pkt);//将flush_pkt插入视频数据包队列，执行重置视频解码器操作avcodec_flush_buffers } } is->seek_req = 0;//关闭[快进]/[快退]操作标志位 }//end for if (is->seek_req) // Seek stuff goes here，检查音视频编码数据包队列长度是否溢出 if (is->audioq.size > MAX_AUDIOQ_SIZE || is->videoq.size > MAX_VIDEOQ_SIZE) { SDL_Delay(10); continue; } /*----------------------- * read in a packet and store it in the AVPacket struct * ffmpeg allocates the internal data for us,which is pointed to by packet.data * this is freed by the av_free_packet() -----------------------*/ // 负责保存压缩编码数据相关信息的结构体,每帧图像由一到多个packet包组成 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 if (av_read_frame(is->pFormatCtx, packet) pFormatCtx->pb->error == 0) { SDL_Delay(100); // No error; wait for user input. continue; } else { break; } } // Is this a packet from the video stream? if (packet->stream_index == is->videoStream) {// 检查数据包是否为视频类型 packet_queue_put(&is->videoq, packet);// 向队列中插入数据包 } else if (packet->stream_index == is->audioStream) {// 检查数据包是否为音频类型 packet_queue_put(&is->audioq, packet);// 向队列中插入数据包 } else {// 检查数据包是否为字幕类型 av_packet_unref(packet);// 释放packet中保存的(字幕)编码数据 } } // All done - wait for it. while (!is->quit) { SDL_Delay(100); } fail:// 异常处理 if (1) { SDL_Event event;// SDL事件对象 event.type = FF_QUIT_EVENT;// 指定退出事件类型 event.user.data1 = is;// 传递用户数据 SDL_PushEvent(&event);// 将该事件对象压入SDL后台事件队列 } return 0; } int init_sdl(VideoState *is) { // 初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { // __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } // SDL中获取屏幕尺寸 SDL_DisplayMode DM; SDL_GetCurrentDisplayMode(0, &DM); // 初始化SDL窗口 SDL_Window* sdl_window = SDL_CreateWindow(\"FFmpeg+SDL播放视频\",// 参数一：窗口名称 SDL_WINDOWPOS_CENTERED,// 参数二：窗口在屏幕上的x坐标 SDL_WINDOWPOS_CENTERED,// 参数三：窗口在屏幕上的y坐标 DM.w,// 参数四：窗口在屏幕上宽 DM.h,// 参数五：窗口在屏幕上高 SDL_WINDOW_OPENGL);// 参数六：窗口状态(打开) if (sdl_window == NULL){ // __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } screen = sdl_window; // 创建渲染器 // 定义渲染器区域 SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window,// 渲染目标创建 -1, // 从那里开始渲染(-1:表示从第一个位置开始) 0);// 渲染类型(软件渲染) if (sdl_renderer == NULL){ // __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建纹理 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer,// 渲染器 SDL_PIXELFORMAT_IYUV,// 像素数据格式 SDL_TEXTUREACCESS_STREAMING,// 绘制方式：频繁绘制- is->video_width,// 纹理宽 is->video_height);// 纹理高 if (sdl_texture == NULL) { // __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } is->sdl_renderer = sdl_renderer; is->sdl_texture = sdl_texture; return 0; } //设置[快进]/[快退]状态参数 void stream_seek(VideoState *is, int64_t pos, int rel) { if (!is->seek_req) {//检查[快进]/[快退]操作标志位是否开启 is->seek_pos = pos;//更新[快进]/[快退]后的参考时间戳 is->seek_flags = rel seek_req = 1;//开启[快进]/[快退]标志位 } } int init_controller(VideoState *is) { SDL_Window *sdl_window2 = SDL_GL_GetCurrentWindow(); SDL_SysWMinfo systemWindowInfo; SDL_VERSION(&systemWindowInfo.version); if (!SDL_GetWindowWMInfo(sdl_window2, &systemWindowInfo)) { return -1; } ViewController *vc = [ViewController new]; vc.tapFastForwardButtonBlock = ^{ printf(\"前进10秒\\n\"); if (global_video_state) { double incr, pos; incr = 10.0; pos = get_master_clock(global_video_state);//取得当前主同步源时间戳 pos += incr;//根据键盘操作更新主同步源时间戳(AV_TIME_BASE为时间戳基准值) stream_seek(global_video_state,(int64_t)(pos*AV_TIME_BASE),incr);//根据主同步源时间戳设置查找位置 } }; vc.tapFastBackwardButtonBlock = ^{ printf(\"后退10秒\\n\"); if (global_video_state) { double incr, pos; incr = -10.0; pos = get_master_clock(global_video_state);//取得当前主同步源时间戳 pos += incr;//根据键盘操作更新主同步源时间戳(AV_TIME_BASE为时间戳基准值) stream_seek(global_video_state,(int64_t)(pos*AV_TIME_BASE),incr);//根据主同步源时间戳设置查找位置 } }; UIWindow *customWindow = [[UIWindow alloc] initWithFrame:[UIScreen mainScreen].bounds]; customWindow.rootViewController = vc; [customWindow makeKeyAndVisible]; [systemWindowInfo.info.uikit.window addSubview:customWindow]; return 0; } // SDL入口 //extern \"C\" int main(int argc, char *argv[]) { /*------------------------- * 注册组件 * 注册所有ffmpeg支持的多媒体格式及编解码器 -------------------------*/ av_register_all(); // 创建全局状态对象 VideoState *is= (VideoState *)av_mallocz(sizeof(VideoState)); NSString* inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"mov\"]; av_strlcpy(is->filename, [inPath UTF8String], sizeof(is->filename));// 复制视频文件路径名 is->video_width = 640; is->video_height = 352; // av_strlcpy(is->filename, \"/storage/emulated/0/DCIM/Camera/TG-2022-04-13-160703582.mp4\", sizeof(is->filename));// 复制视频文件路径名 // is->video_width = 720; // is->video_height = 1280; is->pictq_lock = SDL_CreateMutex();// 创建编码数据包队列互斥锁对象 is->pictq_ready = SDL_CreateCond();// 创建编码数据包队列就绪条件对象 int init_sdl_result = init_sdl(is); if (init_sdl_result av_sync_type = DEFAULT_AV_SYNC_TYPE;//指定主同步源 // 创建编码数据包解析线程 is->parse_tid = SDL_CreateThread(parse_thread, \"编码数据包解析线程\", is); if (!is->parse_tid) {// 检查线程是否创建成功 av_free(is); return -1; } av_init_packet(&flush_pkt);//初始化flush_pkt //将flush_pkt的data成员指定为\"FLUSH\"，当数据包队列中某个包的data成员取值为\"FLUSH\"，执行重置解码器操作 flush_pkt.data = (unsigned char *) \"FLUSH\"; // SDL事件对象 SDL_Event event; for (;;) {// SDL事件循环 SDL_WaitEvent(&event);// 主线程阻塞，等待事件到来 switch(event.type) {// 事件到来后唤醒主线程，检查事件类型 case FF_QUIT_EVENT: case SDL_QUIT:// 退出进程事件 is->quit = 1; // If the video has finished playing, then both the picture and audio queues are waiting for more data. // Make them stop waiting and terminate normally.. avcodec_close(is->video_st->codec); avformat_free_context(is->pFormatCtx); SDL_CondSignal(is->audioq.qready);// 发出队列就绪信号避免死锁 SDL_CondSignal(is->videoq.qready); SDL_DestroyTexture(is->sdl_texture); SDL_DestroyRenderer(is->sdl_renderer); SDL_Quit(); return 0; case FF_ALLOC_EVENT: alloc_picture(event.user.data1);// 分配视频帧事件响应函数 break; case FF_REFRESH_EVENT:// 视频显示刷新事件 video_refresh_timer(event.user.data1);// 视频显示刷新事件响应函数 break; default: break; } } return 0; } "},"pages/FFmpeg/ffplay源码.html":{"url":"pages/FFmpeg/ffplay源码.html","title":"ffplay源码","keywords":"","body":"ffplay源码 源代码一览 /* * Copyright (c) 2003 Fabrice Bellard * * This file is part of FFmpeg. * * FFmpeg is free software; you can redistribute it and/or * modify it under the terms of the GNU Lesser General Public * License as published by the Free Software Foundation; either * version 2.1 of the License, or (at your option) any later version. * * FFmpeg is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU * Lesser General Public License for more details. * * You should have received a copy of the GNU Lesser General Public * License along with FFmpeg; if not, write to the Free Software * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA */ /** * @file * simple media player based on the FFmpeg libraries */ #include \"config.h\" #include #include #include #include #include #include \"libavutil/avstring.h\" #include \"libavutil/eval.h\" #include \"libavutil/mathematics.h\" #include \"libavutil/pixdesc.h\" #include \"libavutil/imgutils.h\" #include \"libavutil/dict.h\" #include \"libavutil/parseutils.h\" #include \"libavutil/samplefmt.h\" #include \"libavutil/avassert.h\" #include \"libavutil/time.h\" #include \"libavformat/avformat.h\" #include \"libavdevice/avdevice.h\" #include \"libswscale/swscale.h\" #include \"libavutil/opt.h\" #include \"libavcodec/avfft.h\" #include \"libswresample/swresample.h\" #if CONFIG_AVFILTER # include \"libavfilter/avfilter.h\" # include \"libavfilter/buffersink.h\" # include \"libavfilter/buffersrc.h\" #endif #include #include #include \"cmdutils.h\" #include const char program_name[] = \"ffplay\"; const int program_birth_year = 2003; #define MAX_QUEUE_SIZE (15 * 1024 * 1024) #define MIN_FRAMES 25 #define EXTERNAL_CLOCK_MIN_FRAMES 2 #define EXTERNAL_CLOCK_MAX_FRAMES 10 /* Minimum SDL audio buffer size, in samples. */ #define SDL_AUDIO_MIN_BUFFER_SIZE 512 /* Calculate actual buffer size keeping in mind not cause too frequent audio callbacks */ #define SDL_AUDIO_MAX_CALLBACKS_PER_SEC 30 /* Step size for volume control in dB */ #define SDL_VOLUME_STEP (0.75) /* no AV sync correction is done if below the minimum AV sync threshold */ #define AV_SYNC_THRESHOLD_MIN 0.04 /* AV sync correction is done if above the maximum AV sync threshold */ #define AV_SYNC_THRESHOLD_MAX 0.1 /* If a frame duration is longer than this, it will not be duplicated to compensate AV sync */ #define AV_SYNC_FRAMEDUP_THRESHOLD 0.1 /* no AV correction is done if too big error */ #define AV_NOSYNC_THRESHOLD 10.0 /* maximum audio speed change to get correct sync */ #define SAMPLE_CORRECTION_PERCENT_MAX 10 /* external clock speed adjustment constants for realtime sources based on buffer fullness */ #define EXTERNAL_CLOCK_SPEED_MIN 0.900 #define EXTERNAL_CLOCK_SPEED_MAX 1.010 #define EXTERNAL_CLOCK_SPEED_STEP 0.001 /* we use about AUDIO_DIFF_AVG_NB A-V differences to make the average */ #define AUDIO_DIFF_AVG_NB 20 /* polls for possible required screen refresh at least this often, should be less than 1/fps */ #define REFRESH_RATE 0.01 /* NOTE: the size must be big enough to compensate the hardware audio buffersize size */ /* TODO: We assume that a decoded and resampled frame fits into this buffer */ #define SAMPLE_ARRAY_SIZE (8 * 65536) #define CURSOR_HIDE_DELAY 1000000 #define USE_ONEPASS_SUBTITLE_RENDER 1 static unsigned sws_flags = SWS_BICUBIC; typedef struct MyAVPacketList { AVPacket pkt; struct MyAVPacketList *next; int serial; } MyAVPacketList; typedef struct PacketQueue { MyAVPacketList *first_pkt, *last_pkt; int nb_packets; int size; int64_t duration; int abort_request; int serial; SDL_mutex *mutex; SDL_cond *cond; } PacketQueue; #define VIDEO_PICTURE_QUEUE_SIZE 3 #define SUBPICTURE_QUEUE_SIZE 16 #define SAMPLE_QUEUE_SIZE 9 #define FRAME_QUEUE_SIZE FFMAX(SAMPLE_QUEUE_SIZE, FFMAX(VIDEO_PICTURE_QUEUE_SIZE, SUBPICTURE_QUEUE_SIZE)) typedef struct AudioParams { int freq; int channels; int64_t channel_layout; enum AVSampleFormat fmt; int frame_size; int bytes_per_sec; } AudioParams; typedef struct Clock { double pts; /* clock base */ double pts_drift; /* clock base minus time at which we updated the clock */ double last_updated; double speed; int serial; /* clock is based on a packet with this serial */ int paused; int *queue_serial; /* pointer to the current packet queue serial, used for obsolete clock detection */ } Clock; /* Common struct for handling all types of decoded data and allocated render buffers. */ typedef struct Frame { AVFrame *frame; AVSubtitle sub; int serial; double pts; /* presentation timestamp for the frame */ double duration; /* estimated duration of the frame */ int64_t pos; /* byte position of the frame in the input file */ int width; int height; int format; AVRational sar; int uploaded; int flip_v; } Frame; typedef struct FrameQueue { Frame queue[FRAME_QUEUE_SIZE]; int rindex; int windex; int size; int max_size; int keep_last; int rindex_shown; SDL_mutex *mutex; SDL_cond *cond; PacketQueue *pktq; } FrameQueue; enum { AV_SYNC_AUDIO_MASTER, /* default choice */ AV_SYNC_VIDEO_MASTER, AV_SYNC_EXTERNAL_CLOCK, /* synchronize to an external clock */ }; typedef struct Decoder { AVPacket pkt; PacketQueue *queue; AVCodecContext *avctx; int pkt_serial; int finished; int packet_pending; SDL_cond *empty_queue_cond; int64_t start_pts; AVRational start_pts_tb; int64_t next_pts; AVRational next_pts_tb; SDL_Thread *decoder_tid; } Decoder; typedef struct VideoState { SDL_Thread *read_tid; AVInputFormat *iformat; int abort_request; int force_refresh; int paused; int last_paused; int queue_attachments_req; int seek_req; int seek_flags; int64_t seek_pos; int64_t seek_rel; int read_pause_return; AVFormatContext *ic; int realtime; Clock audclk; Clock vidclk; Clock extclk; FrameQueue pictq; FrameQueue subpq; FrameQueue sampq; Decoder auddec; Decoder viddec; Decoder subdec; int audio_stream; int av_sync_type; double audio_clock; int audio_clock_serial; double audio_diff_cum; /* used for AV difference average computation */ double audio_diff_avg_coef; double audio_diff_threshold; int audio_diff_avg_count; AVStream *audio_st; PacketQueue audioq; int audio_hw_buf_size; uint8_t *audio_buf; uint8_t *audio_buf1; unsigned int audio_buf_size; /* in bytes */ unsigned int audio_buf1_size; int audio_buf_index; /* in bytes */ int audio_write_buf_size; int audio_volume; int muted; struct AudioParams audio_src; #if CONFIG_AVFILTER struct AudioParams audio_filter_src; #endif struct AudioParams audio_tgt; struct SwrContext *swr_ctx; int frame_drops_early; int frame_drops_late; enum ShowMode { SHOW_MODE_NONE = -1, SHOW_MODE_VIDEO = 0, SHOW_MODE_WAVES, SHOW_MODE_RDFT, SHOW_MODE_NB } show_mode; int16_t sample_array[SAMPLE_ARRAY_SIZE]; int sample_array_index; int last_i_start; RDFTContext *rdft; int rdft_bits; FFTSample *rdft_data; int xpos; double last_vis_time; SDL_Texture *vis_texture; SDL_Texture *sub_texture; SDL_Texture *vid_texture; int subtitle_stream; AVStream *subtitle_st; PacketQueue subtitleq; double frame_timer; double frame_last_returned_time; double frame_last_filter_delay; int video_stream; AVStream *video_st; PacketQueue videoq; double max_frame_duration; // maximum duration of a frame - above this, we consider the jump a timestamp discontinuity struct SwsContext *img_convert_ctx; struct SwsContext *sub_convert_ctx; int eof; char *filename; int width, height, xleft, ytop; int step; #if CONFIG_AVFILTER int vfilter_idx; AVFilterContext *in_video_filter; // the first filter in the video chain AVFilterContext *out_video_filter; // the last filter in the video chain AVFilterContext *in_audio_filter; // the first filter in the audio chain AVFilterContext *out_audio_filter; // the last filter in the audio chain AVFilterGraph *agraph; // audio filter graph #endif int last_video_stream, last_audio_stream, last_subtitle_stream; SDL_cond *continue_read_thread; } VideoState; /* options specified by the user */ static AVInputFormat *file_iformat; static const char *input_filename; static const char *window_title; static int default_width = 640; static int default_height = 480; static int screen_width = 0; static int screen_height = 0; static int screen_left = SDL_WINDOWPOS_CENTERED; static int screen_top = SDL_WINDOWPOS_CENTERED; static int audio_disable; static int video_disable; static int subtitle_disable; static const char* wanted_stream_spec[AVMEDIA_TYPE_NB] = {0}; static int seek_by_bytes = -1; static float seek_interval = 10; static int display_disable; static int borderless; static int alwaysontop; static int startup_volume = 100; static int show_status = 1; static int av_sync_type = AV_SYNC_AUDIO_MASTER; static int64_t start_time = AV_NOPTS_VALUE; static int64_t duration = AV_NOPTS_VALUE; static int fast = 0; static int genpts = 0; static int lowres = 0; static int decoder_reorder_pts = -1; static int autoexit; static int exit_on_keydown; static int exit_on_mousedown; static int loop = 1; static int framedrop = -1; static int infinite_buffer = -1; static enum ShowMode show_mode = SHOW_MODE_NONE; static const char *audio_codec_name; static const char *subtitle_codec_name; static const char *video_codec_name; double rdftspeed = 0.02; static int64_t cursor_last_shown; static int cursor_hidden = 0; #if CONFIG_AVFILTER static const char **vfilters_list = NULL; static int nb_vfilters = 0; static char *afilters = NULL; #endif static int autorotate = 1; static int find_stream_info = 1; static int filter_nbthreads = 0; /* current context */ static int is_full_screen; static int64_t audio_callback_time; static AVPacket flush_pkt; #define FF_QUIT_EVENT (SDL_USEREVENT + 2) static SDL_Window *window; static SDL_Renderer *renderer; static SDL_RendererInfo renderer_info = {0}; static SDL_AudioDeviceID audio_dev; static const struct TextureFormatEntry { enum AVPixelFormat format; int texture_fmt; } sdl_texture_format_map[] = { { AV_PIX_FMT_RGB8, SDL_PIXELFORMAT_RGB332 }, { AV_PIX_FMT_RGB444, SDL_PIXELFORMAT_RGB444 }, { AV_PIX_FMT_RGB555, SDL_PIXELFORMAT_RGB555 }, { AV_PIX_FMT_BGR555, SDL_PIXELFORMAT_BGR555 }, { AV_PIX_FMT_RGB565, SDL_PIXELFORMAT_RGB565 }, { AV_PIX_FMT_BGR565, SDL_PIXELFORMAT_BGR565 }, { AV_PIX_FMT_RGB24, SDL_PIXELFORMAT_RGB24 }, { AV_PIX_FMT_BGR24, SDL_PIXELFORMAT_BGR24 }, { AV_PIX_FMT_0RGB32, SDL_PIXELFORMAT_RGB888 }, { AV_PIX_FMT_0BGR32, SDL_PIXELFORMAT_BGR888 }, { AV_PIX_FMT_NE(RGB0, 0BGR), SDL_PIXELFORMAT_RGBX8888 }, { AV_PIX_FMT_NE(BGR0, 0RGB), SDL_PIXELFORMAT_BGRX8888 }, { AV_PIX_FMT_RGB32, SDL_PIXELFORMAT_ARGB8888 }, { AV_PIX_FMT_RGB32_1, SDL_PIXELFORMAT_RGBA8888 }, { AV_PIX_FMT_BGR32, SDL_PIXELFORMAT_ABGR8888 }, { AV_PIX_FMT_BGR32_1, SDL_PIXELFORMAT_BGRA8888 }, { AV_PIX_FMT_YUV420P, SDL_PIXELFORMAT_IYUV }, { AV_PIX_FMT_YUYV422, SDL_PIXELFORMAT_YUY2 }, { AV_PIX_FMT_UYVY422, SDL_PIXELFORMAT_UYVY }, { AV_PIX_FMT_NONE, SDL_PIXELFORMAT_UNKNOWN }, }; #if CONFIG_AVFILTER static int opt_add_vfilter(void *optctx, const char *opt, const char *arg) { GROW_ARRAY(vfilters_list, nb_vfilters); vfilters_list[nb_vfilters - 1] = arg; return 0; } #endif static inline int cmp_audio_fmts(enum AVSampleFormat fmt1, int64_t channel_count1, enum AVSampleFormat fmt2, int64_t channel_count2) { /* If channel count == 1, planar and non-planar formats are the same */ if (channel_count1 == 1 && channel_count2 == 1) return av_get_packed_sample_fmt(fmt1) != av_get_packed_sample_fmt(fmt2); else return channel_count1 != channel_count2 || fmt1 != fmt2; } static inline int64_t get_valid_channel_layout(int64_t channel_layout, int channels) { if (channel_layout && av_get_channel_layout_nb_channels(channel_layout) == channels) return channel_layout; else return 0; } static int packet_queue_put_private(PacketQueue *q, AVPacket *pkt) { MyAVPacketList *pkt1; if (q->abort_request) return -1; pkt1 = av_malloc(sizeof(MyAVPacketList)); if (!pkt1) return -1; pkt1->pkt = *pkt; pkt1->next = NULL; if (pkt == &flush_pkt) q->serial++; pkt1->serial = q->serial; if (!q->last_pkt) q->first_pkt = pkt1; else q->last_pkt->next = pkt1; q->last_pkt = pkt1; q->nb_packets++; q->size += pkt1->pkt.size + sizeof(*pkt1); q->duration += pkt1->pkt.duration; /* XXX: should duplicate packet data in DV case */ SDL_CondSignal(q->cond); return 0; } static int packet_queue_put(PacketQueue *q, AVPacket *pkt) { int ret; SDL_LockMutex(q->mutex); ret = packet_queue_put_private(q, pkt); SDL_UnlockMutex(q->mutex); if (pkt != &flush_pkt && ret data = NULL; pkt->size = 0; pkt->stream_index = stream_index; return packet_queue_put(q, pkt); } /* packet queue handling */ static int packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue)); q->mutex = SDL_CreateMutex(); if (!q->mutex) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateMutex(): %s\\n\", SDL_GetError()); return AVERROR(ENOMEM); } q->cond = SDL_CreateCond(); if (!q->cond) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateCond(): %s\\n\", SDL_GetError()); return AVERROR(ENOMEM); } q->abort_request = 1; return 0; } static void packet_queue_flush(PacketQueue *q) { MyAVPacketList *pkt, *pkt1; SDL_LockMutex(q->mutex); for (pkt = q->first_pkt; pkt; pkt = pkt1) { pkt1 = pkt->next; av_packet_unref(&pkt->pkt); av_freep(&pkt); } q->last_pkt = NULL; q->first_pkt = NULL; q->nb_packets = 0; q->size = 0; q->duration = 0; SDL_UnlockMutex(q->mutex); } static void packet_queue_destroy(PacketQueue *q) { packet_queue_flush(q); SDL_DestroyMutex(q->mutex); SDL_DestroyCond(q->cond); } static void packet_queue_abort(PacketQueue *q) { SDL_LockMutex(q->mutex); q->abort_request = 1; SDL_CondSignal(q->cond); SDL_UnlockMutex(q->mutex); } static void packet_queue_start(PacketQueue *q) { SDL_LockMutex(q->mutex); q->abort_request = 0; packet_queue_put_private(q, &flush_pkt); SDL_UnlockMutex(q->mutex); } /* return 0 if packet. */ static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block, int *serial) { MyAVPacketList *pkt1; int ret; SDL_LockMutex(q->mutex); for (;;) { if (q->abort_request) { ret = -1; break; } pkt1 = q->first_pkt; if (pkt1) { q->first_pkt = pkt1->next; if (!q->first_pkt) q->last_pkt = NULL; q->nb_packets--; q->size -= pkt1->pkt.size + sizeof(*pkt1); q->duration -= pkt1->pkt.duration; *pkt = pkt1->pkt; if (serial) *serial = pkt1->serial; av_free(pkt1); ret = 1; break; } else if (!block) { ret = 0; break; } else { SDL_CondWait(q->cond, q->mutex); } } SDL_UnlockMutex(q->mutex); return ret; } static void decoder_init(Decoder *d, AVCodecContext *avctx, PacketQueue *queue, SDL_cond *empty_queue_cond) { memset(d, 0, sizeof(Decoder)); d->avctx = avctx; d->queue = queue; d->empty_queue_cond = empty_queue_cond; d->start_pts = AV_NOPTS_VALUE; d->pkt_serial = -1; } static int decoder_decode_frame(Decoder *d, AVFrame *frame, AVSubtitle *sub) { int ret = AVERROR(EAGAIN); for (;;) { AVPacket pkt; if (d->queue->serial == d->pkt_serial) { do { if (d->queue->abort_request) return -1; switch (d->avctx->codec_type) { case AVMEDIA_TYPE_VIDEO: ret = avcodec_receive_frame(d->avctx, frame); if (ret >= 0) { if (decoder_reorder_pts == -1) { frame->pts = frame->best_effort_timestamp; } else if (!decoder_reorder_pts) { frame->pts = frame->pkt_dts; } } break; case AVMEDIA_TYPE_AUDIO: ret = avcodec_receive_frame(d->avctx, frame); if (ret >= 0) { AVRational tb = (AVRational){1, frame->sample_rate}; if (frame->pts != AV_NOPTS_VALUE) frame->pts = av_rescale_q(frame->pts, d->avctx->pkt_timebase, tb); else if (d->next_pts != AV_NOPTS_VALUE) frame->pts = av_rescale_q(d->next_pts, d->next_pts_tb, tb); if (frame->pts != AV_NOPTS_VALUE) { d->next_pts = frame->pts + frame->nb_samples; d->next_pts_tb = tb; } } break; } if (ret == AVERROR_EOF) { d->finished = d->pkt_serial; avcodec_flush_buffers(d->avctx); return 0; } if (ret >= 0) return 1; } while (ret != AVERROR(EAGAIN)); } do { if (d->queue->nb_packets == 0) SDL_CondSignal(d->empty_queue_cond); if (d->packet_pending) { av_packet_move_ref(&pkt, &d->pkt); d->packet_pending = 0; } else { if (packet_queue_get(d->queue, &pkt, 1, &d->pkt_serial) queue->serial != d->pkt_serial); if (pkt.data == flush_pkt.data) { avcodec_flush_buffers(d->avctx); d->finished = 0; d->next_pts = d->start_pts; d->next_pts_tb = d->start_pts_tb; } else { if (d->avctx->codec_type == AVMEDIA_TYPE_SUBTITLE) { int got_frame = 0; ret = avcodec_decode_subtitle2(d->avctx, sub, &got_frame, &pkt); if (ret packet_pending = 1; av_packet_move_ref(&d->pkt, &pkt); } ret = got_frame ? 0 : (pkt.data ? AVERROR(EAGAIN) : AVERROR_EOF); } } else { if (avcodec_send_packet(d->avctx, &pkt) == AVERROR(EAGAIN)) { av_log(d->avctx, AV_LOG_ERROR, \"Receive_frame and send_packet both returned EAGAIN, which is an API violation.\\n\"); d->packet_pending = 1; av_packet_move_ref(&d->pkt, &pkt); } } av_packet_unref(&pkt); } } } static void decoder_destroy(Decoder *d) { av_packet_unref(&d->pkt); avcodec_free_context(&d->avctx); } static void frame_queue_unref_item(Frame *vp) { av_frame_unref(vp->frame); avsubtitle_free(&vp->sub); } static int frame_queue_init(FrameQueue *f, PacketQueue *pktq, int max_size, int keep_last) { int i; memset(f, 0, sizeof(FrameQueue)); if (!(f->mutex = SDL_CreateMutex())) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateMutex(): %s\\n\", SDL_GetError()); return AVERROR(ENOMEM); } if (!(f->cond = SDL_CreateCond())) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateCond(): %s\\n\", SDL_GetError()); return AVERROR(ENOMEM); } f->pktq = pktq; f->max_size = FFMIN(max_size, FRAME_QUEUE_SIZE); f->keep_last = !!keep_last; for (i = 0; i max_size; i++) if (!(f->queue[i].frame = av_frame_alloc())) return AVERROR(ENOMEM); return 0; } static void frame_queue_destory(FrameQueue *f) { int i; for (i = 0; i max_size; i++) { Frame *vp = &f->queue[i]; frame_queue_unref_item(vp); av_frame_free(&vp->frame); } SDL_DestroyMutex(f->mutex); SDL_DestroyCond(f->cond); } static void frame_queue_signal(FrameQueue *f) { SDL_LockMutex(f->mutex); SDL_CondSignal(f->cond); SDL_UnlockMutex(f->mutex); } static Frame *frame_queue_peek(FrameQueue *f) { return &f->queue[(f->rindex + f->rindex_shown) % f->max_size]; } static Frame *frame_queue_peek_next(FrameQueue *f) { return &f->queue[(f->rindex + f->rindex_shown + 1) % f->max_size]; } static Frame *frame_queue_peek_last(FrameQueue *f) { return &f->queue[f->rindex]; } static Frame *frame_queue_peek_writable(FrameQueue *f) { /* wait until we have space to put a new frame */ SDL_LockMutex(f->mutex); while (f->size >= f->max_size && !f->pktq->abort_request) { SDL_CondWait(f->cond, f->mutex); } SDL_UnlockMutex(f->mutex); if (f->pktq->abort_request) return NULL; return &f->queue[f->windex]; } static Frame *frame_queue_peek_readable(FrameQueue *f) { /* wait until we have a readable a new frame */ SDL_LockMutex(f->mutex); while (f->size - f->rindex_shown pktq->abort_request) { SDL_CondWait(f->cond, f->mutex); } SDL_UnlockMutex(f->mutex); if (f->pktq->abort_request) return NULL; return &f->queue[(f->rindex + f->rindex_shown) % f->max_size]; } static void frame_queue_push(FrameQueue *f) { if (++f->windex == f->max_size) f->windex = 0; SDL_LockMutex(f->mutex); f->size++; SDL_CondSignal(f->cond); SDL_UnlockMutex(f->mutex); } static void frame_queue_next(FrameQueue *f) { if (f->keep_last && !f->rindex_shown) { f->rindex_shown = 1; return; } frame_queue_unref_item(&f->queue[f->rindex]); if (++f->rindex == f->max_size) f->rindex = 0; SDL_LockMutex(f->mutex); f->size--; SDL_CondSignal(f->cond); SDL_UnlockMutex(f->mutex); } /* return the number of undisplayed frames in the queue */ static int frame_queue_nb_remaining(FrameQueue *f) { return f->size - f->rindex_shown; } /* return last shown position */ static int64_t frame_queue_last_pos(FrameQueue *f) { Frame *fp = &f->queue[f->rindex]; if (f->rindex_shown && fp->serial == f->pktq->serial) return fp->pos; else return -1; } static void decoder_abort(Decoder *d, FrameQueue *fq) { packet_queue_abort(d->queue); frame_queue_signal(fq); SDL_WaitThread(d->decoder_tid, NULL); d->decoder_tid = NULL; packet_queue_flush(d->queue); } static inline void fill_rectangle(int x, int y, int w, int h) { SDL_Rect rect; rect.x = x; rect.y = y; rect.w = w; rect.h = h; if (w && h) SDL_RenderFillRect(renderer, &rect); } static int realloc_texture(SDL_Texture **texture, Uint32 new_format, int new_width, int new_height, SDL_BlendMode blendmode, int init_texture) { Uint32 format; int access, w, h; if (!*texture || SDL_QueryTexture(*texture, &format, &access, &w, &h) scr_width) { width = scr_width; height = av_rescale(width, aspect_ratio.den, aspect_ratio.num) & ~1; } x = (scr_width - width) / 2; y = (scr_height - height) / 2; rect->x = scr_xleft + x; rect->y = scr_ytop + y; rect->w = FFMAX((int)width, 1); rect->h = FFMAX((int)height, 1); } static void get_sdl_pix_fmt_and_blendmode(int format, Uint32 *sdl_pix_fmt, SDL_BlendMode *sdl_blendmode) { int i; *sdl_blendmode = SDL_BLENDMODE_NONE; *sdl_pix_fmt = SDL_PIXELFORMAT_UNKNOWN; if (format == AV_PIX_FMT_RGB32 || format == AV_PIX_FMT_RGB32_1 || format == AV_PIX_FMT_BGR32 || format == AV_PIX_FMT_BGR32_1) *sdl_blendmode = SDL_BLENDMODE_BLEND; for (i = 0; i format, &sdl_pix_fmt, &sdl_blendmode); if (realloc_texture(tex, sdl_pix_fmt == SDL_PIXELFORMAT_UNKNOWN ? SDL_PIXELFORMAT_ARGB8888 : sdl_pix_fmt, frame->width, frame->height, sdl_blendmode, 0) width, frame->height, frame->format, frame->width, frame->height, AV_PIX_FMT_BGRA, sws_flags, NULL, NULL, NULL); if (*img_convert_ctx != NULL) { uint8_t *pixels[4]; int pitch[4]; if (!SDL_LockTexture(*tex, NULL, (void **)pixels, pitch)) { sws_scale(*img_convert_ctx, (const uint8_t * const *)frame->data, frame->linesize, 0, frame->height, pixels, pitch); SDL_UnlockTexture(*tex); } } else { av_log(NULL, AV_LOG_FATAL, \"Cannot initialize the conversion context\\n\"); ret = -1; } break; case SDL_PIXELFORMAT_IYUV: if (frame->linesize[0] > 0 && frame->linesize[1] > 0 && frame->linesize[2] > 0) { ret = SDL_UpdateYUVTexture(*tex, NULL, frame->data[0], frame->linesize[0], frame->data[1], frame->linesize[1], frame->data[2], frame->linesize[2]); } else if (frame->linesize[0] linesize[1] linesize[2] data[0] + frame->linesize[0] * (frame->height - 1), -frame->linesize[0], frame->data[1] + frame->linesize[1] * (AV_CEIL_RSHIFT(frame->height, 1) - 1), -frame->linesize[1], frame->data[2] + frame->linesize[2] * (AV_CEIL_RSHIFT(frame->height, 1) - 1), -frame->linesize[2]); } else { av_log(NULL, AV_LOG_ERROR, \"Mixed negative and positive linesizes are not supported.\\n\"); return -1; } break; default: if (frame->linesize[0] data[0] + frame->linesize[0] * (frame->height - 1), -frame->linesize[0]); } else { ret = SDL_UpdateTexture(*tex, NULL, frame->data[0], frame->linesize[0]); } break; } return ret; } static void set_sdl_yuv_conversion_mode(AVFrame *frame) { #if SDL_VERSION_ATLEAST(2,0,8) SDL_YUV_CONVERSION_MODE mode = SDL_YUV_CONVERSION_AUTOMATIC; if (frame && (frame->format == AV_PIX_FMT_YUV420P || frame->format == AV_PIX_FMT_YUYV422 || frame->format == AV_PIX_FMT_UYVY422)) { if (frame->color_range == AVCOL_RANGE_JPEG) mode = SDL_YUV_CONVERSION_JPEG; else if (frame->colorspace == AVCOL_SPC_BT709) mode = SDL_YUV_CONVERSION_BT709; else if (frame->colorspace == AVCOL_SPC_BT470BG || frame->colorspace == AVCOL_SPC_SMPTE170M || frame->colorspace == AVCOL_SPC_SMPTE240M) mode = SDL_YUV_CONVERSION_BT601; } SDL_SetYUVConversionMode(mode); #endif } static void video_image_display(VideoState *is) { Frame *vp; Frame *sp = NULL; SDL_Rect rect; vp = frame_queue_peek_last(&is->pictq); if (is->subtitle_st) { if (frame_queue_nb_remaining(&is->subpq) > 0) { sp = frame_queue_peek(&is->subpq); if (vp->pts >= sp->pts + ((float) sp->sub.start_display_time / 1000)) { if (!sp->uploaded) { uint8_t* pixels[4]; int pitch[4]; int i; if (!sp->width || !sp->height) { sp->width = vp->width; sp->height = vp->height; } if (realloc_texture(&is->sub_texture, SDL_PIXELFORMAT_ARGB8888, sp->width, sp->height, SDL_BLENDMODE_BLEND, 1) sub.num_rects; i++) { AVSubtitleRect *sub_rect = sp->sub.rects[i]; sub_rect->x = av_clip(sub_rect->x, 0, sp->width ); sub_rect->y = av_clip(sub_rect->y, 0, sp->height); sub_rect->w = av_clip(sub_rect->w, 0, sp->width - sub_rect->x); sub_rect->h = av_clip(sub_rect->h, 0, sp->height - sub_rect->y); is->sub_convert_ctx = sws_getCachedContext(is->sub_convert_ctx, sub_rect->w, sub_rect->h, AV_PIX_FMT_PAL8, sub_rect->w, sub_rect->h, AV_PIX_FMT_BGRA, 0, NULL, NULL, NULL); if (!is->sub_convert_ctx) { av_log(NULL, AV_LOG_FATAL, \"Cannot initialize the conversion context\\n\"); return; } if (!SDL_LockTexture(is->sub_texture, (SDL_Rect *)sub_rect, (void **)pixels, pitch)) { sws_scale(is->sub_convert_ctx, (const uint8_t * const *)sub_rect->data, sub_rect->linesize, 0, sub_rect->h, pixels, pitch); SDL_UnlockTexture(is->sub_texture); } } sp->uploaded = 1; } } else sp = NULL; } } calculate_display_rect(&rect, is->xleft, is->ytop, is->width, is->height, vp->width, vp->height, vp->sar); if (!vp->uploaded) { if (upload_texture(&is->vid_texture, vp->frame, &is->img_convert_ctx) uploaded = 1; vp->flip_v = vp->frame->linesize[0] frame); SDL_RenderCopyEx(renderer, is->vid_texture, NULL, &rect, 0, NULL, vp->flip_v ? SDL_FLIP_VERTICAL : 0); set_sdl_yuv_conversion_mode(NULL); if (sp) { #if USE_ONEPASS_SUBTITLE_RENDER SDL_RenderCopy(renderer, is->sub_texture, NULL, &rect); #else int i; double xratio = (double)rect.w / (double)sp->width; double yratio = (double)rect.h / (double)sp->height; for (i = 0; i sub.num_rects; i++) { SDL_Rect *sub_rect = (SDL_Rect*)sp->sub.rects[i]; SDL_Rect target = {.x = rect.x + sub_rect->x * xratio, .y = rect.y + sub_rect->y * yratio, .w = sub_rect->w * xratio, .h = sub_rect->h * yratio}; SDL_RenderCopy(renderer, is->sub_texture, sub_rect, &target); } #endif } } static inline int compute_mod(int a, int b) { return a height; rdft_bits++) ; nb_freq = 1 audio_tgt.channels; nb_display_channels = channels; if (!s->paused) { int data_used= s->show_mode == SHOW_MODE_WAVES ? s->width : (2*nb_freq); n = 2 * channels; delay = s->audio_write_buf_size; delay /= n; /* to be more precise, we take into account the time spent since the last buffer computation */ if (audio_callback_time) { time_diff = av_gettime_relative() - audio_callback_time; delay -= (time_diff * s->audio_tgt.freq) / 1000000; } delay += 2 * data_used; if (delay sample_array_index - delay * channels, SAMPLE_ARRAY_SIZE); if (s->show_mode == SHOW_MODE_WAVES) { h = INT_MIN; for (i = 0; i sample_array[idx]; int b = s->sample_array[(idx + 4 * channels) % SAMPLE_ARRAY_SIZE]; int c = s->sample_array[(idx + 5 * channels) % SAMPLE_ARRAY_SIZE]; int d = s->sample_array[(idx + 9 * channels) % SAMPLE_ARRAY_SIZE]; int score = a - d; if (h last_i_start = i_start; } else { i_start = s->last_i_start; } if (s->show_mode == SHOW_MODE_WAVES) { SDL_SetRenderDrawColor(renderer, 255, 255, 255, 255); /* total height for one channel */ h = s->height / nb_display_channels; /* graph height / 2 */ h2 = (h * 9) / 20; for (ch = 0; ch ytop + ch * h + (h / 2); /* position of center line */ for (x = 0; x width; x++) { y = (s->sample_array[i] * h2) >> 15; if (y xleft + x, ys, 1, y); i += channels; if (i >= SAMPLE_ARRAY_SIZE) i -= SAMPLE_ARRAY_SIZE; } } SDL_SetRenderDrawColor(renderer, 0, 0, 255, 255); for (ch = 1; ch ytop + ch * h; fill_rectangle(s->xleft, y, s->width, 1); } } else { if (realloc_texture(&s->vis_texture, SDL_PIXELFORMAT_ARGB8888, s->width, s->height, SDL_BLENDMODE_NONE, 1) rdft_bits) { av_rdft_end(s->rdft); av_free(s->rdft_data); s->rdft = av_rdft_init(rdft_bits, DFT_R2C); s->rdft_bits = rdft_bits; s->rdft_data = av_malloc_array(nb_freq, 4 *sizeof(*s->rdft_data)); } if (!s->rdft || !s->rdft_data){ av_log(NULL, AV_LOG_ERROR, \"Failed to allocate buffers for RDFT, switching to waves display\\n\"); s->show_mode = SHOW_MODE_WAVES; } else { FFTSample *data[2]; SDL_Rect rect = {.x = s->xpos, .y = 0, .w = 1, .h = s->height}; uint32_t *pixels; int pitch; for (ch = 0; ch rdft_data + 2 * nb_freq * ch; i = i_start + ch; for (x = 0; x sample_array[i] * (1.0 - w * w); i += channels; if (i >= SAMPLE_ARRAY_SIZE) i -= SAMPLE_ARRAY_SIZE; } av_rdft_calc(s->rdft, data[ch]); } /* Least efficient way to do this, we should of course * directly access it but it is more than fast enough. */ if (!SDL_LockTexture(s->vis_texture, &rect, (void **)&pixels, &pitch)) { pitch >>= 2; pixels += pitch * s->height; for (y = 0; y height; y++) { double w = 1 / sqrt(nb_freq); int a = sqrt(w * sqrt(data[0][2 * y + 0] * data[0][2 * y + 0] + data[0][2 * y + 1] * data[0][2 * y + 1])); int b = (nb_display_channels == 2 ) ? sqrt(w * hypot(data[1][2 * y + 0], data[1][2 * y + 1])) : a; a = FFMIN(a, 255); b = FFMIN(b, 255); pixels -= pitch; *pixels = (a > 1); } SDL_UnlockTexture(s->vis_texture); } SDL_RenderCopy(renderer, s->vis_texture, NULL, NULL); } if (!s->paused) s->xpos++; if (s->xpos >= s->width) s->xpos= s->xleft; } } static void stream_component_close(VideoState *is, int stream_index) { AVFormatContext *ic = is->ic; AVCodecParameters *codecpar; if (stream_index = ic->nb_streams) return; codecpar = ic->streams[stream_index]->codecpar; switch (codecpar->codec_type) { case AVMEDIA_TYPE_AUDIO: decoder_abort(&is->auddec, &is->sampq); SDL_CloseAudioDevice(audio_dev); decoder_destroy(&is->auddec); swr_free(&is->swr_ctx); av_freep(&is->audio_buf1); is->audio_buf1_size = 0; is->audio_buf = NULL; if (is->rdft) { av_rdft_end(is->rdft); av_freep(&is->rdft_data); is->rdft = NULL; is->rdft_bits = 0; } break; case AVMEDIA_TYPE_VIDEO: decoder_abort(&is->viddec, &is->pictq); decoder_destroy(&is->viddec); break; case AVMEDIA_TYPE_SUBTITLE: decoder_abort(&is->subdec, &is->subpq); decoder_destroy(&is->subdec); break; default: break; } ic->streams[stream_index]->discard = AVDISCARD_ALL; switch (codecpar->codec_type) { case AVMEDIA_TYPE_AUDIO: is->audio_st = NULL; is->audio_stream = -1; break; case AVMEDIA_TYPE_VIDEO: is->video_st = NULL; is->video_stream = -1; break; case AVMEDIA_TYPE_SUBTITLE: is->subtitle_st = NULL; is->subtitle_stream = -1; break; default: break; } } static void stream_close(VideoState *is) { /* XXX: use a special url_shutdown call to abort parse cleanly */ is->abort_request = 1; SDL_WaitThread(is->read_tid, NULL); /* close each stream */ if (is->audio_stream >= 0) stream_component_close(is, is->audio_stream); if (is->video_stream >= 0) stream_component_close(is, is->video_stream); if (is->subtitle_stream >= 0) stream_component_close(is, is->subtitle_stream); avformat_close_input(&is->ic); packet_queue_destroy(&is->videoq); packet_queue_destroy(&is->audioq); packet_queue_destroy(&is->subtitleq); /* free all pictures */ frame_queue_destory(&is->pictq); frame_queue_destory(&is->sampq); frame_queue_destory(&is->subpq); SDL_DestroyCond(is->continue_read_thread); sws_freeContext(is->img_convert_ctx); sws_freeContext(is->sub_convert_ctx); av_free(is->filename); if (is->vis_texture) SDL_DestroyTexture(is->vis_texture); if (is->vid_texture) SDL_DestroyTexture(is->vid_texture); if (is->sub_texture) SDL_DestroyTexture(is->sub_texture); av_free(is); } static void do_exit(VideoState *is) { if (is) { stream_close(is); } if (renderer) SDL_DestroyRenderer(renderer); if (window) SDL_DestroyWindow(window); uninit_opts(); #if CONFIG_AVFILTER av_freep(&vfilters_list); #endif avformat_network_deinit(); if (show_status) printf(\"\\n\"); SDL_Quit(); av_log(NULL, AV_LOG_QUIET, \"%s\", \"\"); exit(0); } static void sigterm_handler(int sig) { exit(123); } static void set_default_window_size(int width, int height, AVRational sar) { SDL_Rect rect; int max_width = screen_width ? screen_width : INT_MAX; int max_height = screen_height ? screen_height : INT_MAX; if (max_width == INT_MAX && max_height == INT_MAX) max_height = height; calculate_display_rect(&rect, 0, 0, max_width, max_height, width, height, sar); default_width = rect.w; default_height = rect.h; } static int video_open(VideoState *is) { int w,h; w = screen_width ? screen_width : default_width; h = screen_height ? screen_height : default_height; if (!window_title) window_title = input_filename; SDL_SetWindowTitle(window, window_title); SDL_SetWindowSize(window, w, h); SDL_SetWindowPosition(window, screen_left, screen_top); if (is_full_screen) SDL_SetWindowFullscreen(window, SDL_WINDOW_FULLSCREEN_DESKTOP); SDL_ShowWindow(window); is->width = w; is->height = h; return 0; } /* display the current picture, if any */ static void video_display(VideoState *is) { if (!is->width) video_open(is); SDL_SetRenderDrawColor(renderer, 0, 0, 0, 255); SDL_RenderClear(renderer); if (is->audio_st && is->show_mode != SHOW_MODE_VIDEO) video_audio_display(is); else if (is->video_st) video_image_display(is); SDL_RenderPresent(renderer); } static double get_clock(Clock *c) { if (*c->queue_serial != c->serial) return NAN; if (c->paused) { return c->pts; } else { double time = av_gettime_relative() / 1000000.0; return c->pts_drift + time - (time - c->last_updated) * (1.0 - c->speed); } } static void set_clock_at(Clock *c, double pts, int serial, double time) { c->pts = pts; c->last_updated = time; c->pts_drift = c->pts - time; c->serial = serial; } static void set_clock(Clock *c, double pts, int serial) { double time = av_gettime_relative() / 1000000.0; set_clock_at(c, pts, serial, time); } static void set_clock_speed(Clock *c, double speed) { set_clock(c, get_clock(c), c->serial); c->speed = speed; } static void init_clock(Clock *c, int *queue_serial) { c->speed = 1.0; c->paused = 0; c->queue_serial = queue_serial; set_clock(c, NAN, -1); } static void sync_clock_to_slave(Clock *c, Clock *slave) { double clock = get_clock(c); double slave_clock = get_clock(slave); if (!isnan(slave_clock) && (isnan(clock) || fabs(clock - slave_clock) > AV_NOSYNC_THRESHOLD)) set_clock(c, slave_clock, slave->serial); } static int get_master_sync_type(VideoState *is) { if (is->av_sync_type == AV_SYNC_VIDEO_MASTER) { if (is->video_st) return AV_SYNC_VIDEO_MASTER; else return AV_SYNC_AUDIO_MASTER; } else if (is->av_sync_type == AV_SYNC_AUDIO_MASTER) { if (is->audio_st) return AV_SYNC_AUDIO_MASTER; else return AV_SYNC_EXTERNAL_CLOCK; } else { return AV_SYNC_EXTERNAL_CLOCK; } } /* get the current master clock value */ static double get_master_clock(VideoState *is) { double val; switch (get_master_sync_type(is)) { case AV_SYNC_VIDEO_MASTER: val = get_clock(&is->vidclk); break; case AV_SYNC_AUDIO_MASTER: val = get_clock(&is->audclk); break; default: val = get_clock(&is->extclk); break; } return val; } static void check_external_clock_speed(VideoState *is) { if (is->video_stream >= 0 && is->videoq.nb_packets audio_stream >= 0 && is->audioq.nb_packets extclk, FFMAX(EXTERNAL_CLOCK_SPEED_MIN, is->extclk.speed - EXTERNAL_CLOCK_SPEED_STEP)); } else if ((is->video_stream videoq.nb_packets > EXTERNAL_CLOCK_MAX_FRAMES) && (is->audio_stream audioq.nb_packets > EXTERNAL_CLOCK_MAX_FRAMES)) { set_clock_speed(&is->extclk, FFMIN(EXTERNAL_CLOCK_SPEED_MAX, is->extclk.speed + EXTERNAL_CLOCK_SPEED_STEP)); } else { double speed = is->extclk.speed; if (speed != 1.0) set_clock_speed(&is->extclk, speed + EXTERNAL_CLOCK_SPEED_STEP * (1.0 - speed) / fabs(1.0 - speed)); } } /* seek in the stream */ static void stream_seek(VideoState *is, int64_t pos, int64_t rel, int seek_by_bytes) { if (!is->seek_req) { is->seek_pos = pos; is->seek_rel = rel; is->seek_flags &= ~AVSEEK_FLAG_BYTE; if (seek_by_bytes) is->seek_flags |= AVSEEK_FLAG_BYTE; is->seek_req = 1; SDL_CondSignal(is->continue_read_thread); } } /* pause or resume the video */ static void stream_toggle_pause(VideoState *is) { if (is->paused) { is->frame_timer += av_gettime_relative() / 1000000.0 - is->vidclk.last_updated; if (is->read_pause_return != AVERROR(ENOSYS)) { is->vidclk.paused = 0; } set_clock(&is->vidclk, get_clock(&is->vidclk), is->vidclk.serial); } set_clock(&is->extclk, get_clock(&is->extclk), is->extclk.serial); is->paused = is->audclk.paused = is->vidclk.paused = is->extclk.paused = !is->paused; } static void toggle_pause(VideoState *is) { stream_toggle_pause(is); is->step = 0; } static void toggle_mute(VideoState *is) { is->muted = !is->muted; } static void update_volume(VideoState *is, int sign, double step) { double volume_level = is->audio_volume ? (20 * log(is->audio_volume / (double)SDL_MIX_MAXVOLUME) / log(10)) : -1000.0; int new_volume = lrint(SDL_MIX_MAXVOLUME * pow(10.0, (volume_level + sign * step) / 20.0)); is->audio_volume = av_clip(is->audio_volume == new_volume ? (is->audio_volume + sign) : new_volume, 0, SDL_MIX_MAXVOLUME); } static void step_to_next_frame(VideoState *is) { /* if the stream is paused unpause it, then step */ if (is->paused) stream_toggle_pause(is); is->step = 1; } static double compute_target_delay(double delay, VideoState *is) { double sync_threshold, diff = 0; /* update delay to follow master synchronisation source */ if (get_master_sync_type(is) != AV_SYNC_VIDEO_MASTER) { /* if video is slave, we try to correct big delays by duplicating or deleting a frame */ diff = get_clock(&is->vidclk) - get_master_clock(is); /* skip or repeat frame. We take into account the delay to compute the threshold. I still don't know if it is the best guess */ sync_threshold = FFMAX(AV_SYNC_THRESHOLD_MIN, FFMIN(AV_SYNC_THRESHOLD_MAX, delay)); if (!isnan(diff) && fabs(diff) max_frame_duration) { if (diff = sync_threshold && delay > AV_SYNC_FRAMEDUP_THRESHOLD) delay = delay + diff; else if (diff >= sync_threshold) delay = 2 * delay; } } av_log(NULL, AV_LOG_TRACE, \"video: delay=%0.3f A-V=%f\\n\", delay, -diff); return delay; } static double vp_duration(VideoState *is, Frame *vp, Frame *nextvp) { if (vp->serial == nextvp->serial) { double duration = nextvp->pts - vp->pts; if (isnan(duration) || duration is->max_frame_duration) return vp->duration; else return duration; } else { return 0.0; } } static void update_video_pts(VideoState *is, double pts, int64_t pos, int serial) { /* update current video pts */ set_clock(&is->vidclk, pts, serial); sync_clock_to_slave(&is->extclk, &is->vidclk); } /* called to display each frame */ static void video_refresh(void *opaque, double *remaining_time) { VideoState *is = opaque; double time; Frame *sp, *sp2; if (!is->paused && get_master_sync_type(is) == AV_SYNC_EXTERNAL_CLOCK && is->realtime) check_external_clock_speed(is); if (!display_disable && is->show_mode != SHOW_MODE_VIDEO && is->audio_st) { time = av_gettime_relative() / 1000000.0; if (is->force_refresh || is->last_vis_time + rdftspeed last_vis_time = time; } *remaining_time = FFMIN(*remaining_time, is->last_vis_time + rdftspeed - time); } if (is->video_st) { retry: if (frame_queue_nb_remaining(&is->pictq) == 0) { // nothing to do, no picture to display in the queue } else { double last_duration, duration, delay; Frame *vp, *lastvp; /* dequeue the picture */ lastvp = frame_queue_peek_last(&is->pictq); vp = frame_queue_peek(&is->pictq); if (vp->serial != is->videoq.serial) { frame_queue_next(&is->pictq); goto retry; } if (lastvp->serial != vp->serial) is->frame_timer = av_gettime_relative() / 1000000.0; if (is->paused) goto display; /* compute nominal last_duration */ last_duration = vp_duration(is, lastvp, vp); delay = compute_target_delay(last_duration, is); time= av_gettime_relative()/1000000.0; if (time frame_timer + delay) { *remaining_time = FFMIN(is->frame_timer + delay - time, *remaining_time); goto display; } is->frame_timer += delay; if (delay > 0 && time - is->frame_timer > AV_SYNC_THRESHOLD_MAX) is->frame_timer = time; SDL_LockMutex(is->pictq.mutex); if (!isnan(vp->pts)) update_video_pts(is, vp->pts, vp->pos, vp->serial); SDL_UnlockMutex(is->pictq.mutex); if (frame_queue_nb_remaining(&is->pictq) > 1) { Frame *nextvp = frame_queue_peek_next(&is->pictq); duration = vp_duration(is, vp, nextvp); if(!is->step && (framedrop>0 || (framedrop && get_master_sync_type(is) != AV_SYNC_VIDEO_MASTER)) && time > is->frame_timer + duration){ is->frame_drops_late++; frame_queue_next(&is->pictq); goto retry; } } if (is->subtitle_st) { while (frame_queue_nb_remaining(&is->subpq) > 0) { sp = frame_queue_peek(&is->subpq); if (frame_queue_nb_remaining(&is->subpq) > 1) sp2 = frame_queue_peek_next(&is->subpq); else sp2 = NULL; if (sp->serial != is->subtitleq.serial || (is->vidclk.pts > (sp->pts + ((float) sp->sub.end_display_time / 1000))) || (sp2 && is->vidclk.pts > (sp2->pts + ((float) sp2->sub.start_display_time / 1000)))) { if (sp->uploaded) { int i; for (i = 0; i sub.num_rects; i++) { AVSubtitleRect *sub_rect = sp->sub.rects[i]; uint8_t *pixels; int pitch, j; if (!SDL_LockTexture(is->sub_texture, (SDL_Rect *)sub_rect, (void **)&pixels, &pitch)) { for (j = 0; j h; j++, pixels += pitch) memset(pixels, 0, sub_rect->w sub_texture); } } } frame_queue_next(&is->subpq); } else { break; } } } frame_queue_next(&is->pictq); is->force_refresh = 1; if (is->step && !is->paused) stream_toggle_pause(is); } display: /* display picture */ if (!display_disable && is->force_refresh && is->show_mode == SHOW_MODE_VIDEO && is->pictq.rindex_shown) video_display(is); } is->force_refresh = 0; if (show_status) { static int64_t last_time; int64_t cur_time; int aqsize, vqsize, sqsize; double av_diff; cur_time = av_gettime_relative(); if (!last_time || (cur_time - last_time) >= 30000) { aqsize = 0; vqsize = 0; sqsize = 0; if (is->audio_st) aqsize = is->audioq.size; if (is->video_st) vqsize = is->videoq.size; if (is->subtitle_st) sqsize = is->subtitleq.size; av_diff = 0; if (is->audio_st && is->video_st) av_diff = get_clock(&is->audclk) - get_clock(&is->vidclk); else if (is->video_st) av_diff = get_master_clock(is) - get_clock(&is->vidclk); else if (is->audio_st) av_diff = get_master_clock(is) - get_clock(&is->audclk); av_log(NULL, AV_LOG_INFO, \"%7.2f %s:%7.3f fd=%4d aq=%5dKB vq=%5dKB sq=%5dB f=%\"PRId64\"/%\"PRId64\" \\r\", get_master_clock(is), (is->audio_st && is->video_st) ? \"A-V\" : (is->video_st ? \"M-V\" : (is->audio_st ? \"M-A\" : \" \")), av_diff, is->frame_drops_early + is->frame_drops_late, aqsize / 1024, vqsize / 1024, sqsize, is->video_st ? is->viddec.avctx->pts_correction_num_faulty_dts : 0, is->video_st ? is->viddec.avctx->pts_correction_num_faulty_pts : 0); fflush(stdout); last_time = cur_time; } } } static int queue_picture(VideoState *is, AVFrame *src_frame, double pts, double duration, int64_t pos, int serial) { Frame *vp; #if defined(DEBUG_SYNC) printf(\"frame_type=%c pts=%0.3f\\n\", av_get_picture_type_char(src_frame->pict_type), pts); #endif if (!(vp = frame_queue_peek_writable(&is->pictq))) return -1; vp->sar = src_frame->sample_aspect_ratio; vp->uploaded = 0; vp->width = src_frame->width; vp->height = src_frame->height; vp->format = src_frame->format; vp->pts = pts; vp->duration = duration; vp->pos = pos; vp->serial = serial; set_default_window_size(vp->width, vp->height, vp->sar); av_frame_move_ref(vp->frame, src_frame); frame_queue_push(&is->pictq); return 0; } static int get_video_frame(VideoState *is, AVFrame *frame) { int got_picture; if ((got_picture = decoder_decode_frame(&is->viddec, frame, NULL)) pts != AV_NOPTS_VALUE) dpts = av_q2d(is->video_st->time_base) * frame->pts; frame->sample_aspect_ratio = av_guess_sample_aspect_ratio(is->ic, is->video_st, frame); if (framedrop>0 || (framedrop && get_master_sync_type(is) != AV_SYNC_VIDEO_MASTER)) { if (frame->pts != AV_NOPTS_VALUE) { double diff = dpts - get_master_clock(is); if (!isnan(diff) && fabs(diff) frame_last_filter_delay viddec.pkt_serial == is->vidclk.serial && is->videoq.nb_packets) { is->frame_drops_early++; av_frame_unref(frame); got_picture = 0; } } } } return got_picture; } #if CONFIG_AVFILTER static int configure_filtergraph(AVFilterGraph *graph, const char *filtergraph, AVFilterContext *source_ctx, AVFilterContext *sink_ctx) { int ret, i; int nb_filters = graph->nb_filters; AVFilterInOut *outputs = NULL, *inputs = NULL; if (filtergraph) { outputs = avfilter_inout_alloc(); inputs = avfilter_inout_alloc(); if (!outputs || !inputs) { ret = AVERROR(ENOMEM); goto fail; } outputs->name = av_strdup(\"in\"); outputs->filter_ctx = source_ctx; outputs->pad_idx = 0; outputs->next = NULL; inputs->name = av_strdup(\"out\"); inputs->filter_ctx = sink_ctx; inputs->pad_idx = 0; inputs->next = NULL; if ((ret = avfilter_graph_parse_ptr(graph, filtergraph, &inputs, &outputs, NULL)) nb_filters - nb_filters; i++) FFSWAP(AVFilterContext*, graph->filters[i], graph->filters[i + nb_filters]); ret = avfilter_graph_config(graph, NULL); fail: avfilter_inout_free(&outputs); avfilter_inout_free(&inputs); return ret; } static int configure_video_filters(AVFilterGraph *graph, VideoState *is, const char *vfilters, AVFrame *frame) { enum AVPixelFormat pix_fmts[FF_ARRAY_ELEMS(sdl_texture_format_map)]; char sws_flags_str[512] = \"\"; char buffersrc_args[256]; int ret; AVFilterContext *filt_src = NULL, *filt_out = NULL, *last_filter = NULL; AVCodecParameters *codecpar = is->video_st->codecpar; AVRational fr = av_guess_frame_rate(is->ic, is->video_st, NULL); AVDictionaryEntry *e = NULL; int nb_pix_fmts = 0; int i, j; for (i = 0; i key, \"sws_flags\")) { av_strlcatf(sws_flags_str, sizeof(sws_flags_str), \"%s=%s:\", \"flags\", e->value); } else av_strlcatf(sws_flags_str, sizeof(sws_flags_str), \"%s=%s:\", e->key, e->value); } if (strlen(sws_flags_str)) sws_flags_str[strlen(sws_flags_str)-1] = '\\0'; graph->scale_sws_opts = av_strdup(sws_flags_str); snprintf(buffersrc_args, sizeof(buffersrc_args), \"video_size=%dx%d:pix_fmt=%d:time_base=%d/%d:pixel_aspect=%d/%d\", frame->width, frame->height, frame->format, is->video_st->time_base.num, is->video_st->time_base.den, codecpar->sample_aspect_ratio.num, FFMAX(codecpar->sample_aspect_ratio.den, 1)); if (fr.num && fr.den) av_strlcatf(buffersrc_args, sizeof(buffersrc_args), \":frame_rate=%d/%d\", fr.num, fr.den); if ((ret = avfilter_graph_create_filter(&filt_src, avfilter_get_by_name(\"buffer\"), \"ffplay_buffer\", buffersrc_args, NULL, graph)) video_st); if (fabs(theta - 90) 1.0) { char rotate_buf[64]; snprintf(rotate_buf, sizeof(rotate_buf), \"%f*PI/180\", theta); INSERT_FILT(\"rotate\", rotate_buf); } } if ((ret = configure_filtergraph(graph, vfilters, filt_src, last_filter)) in_video_filter = filt_src; is->out_video_filter = filt_out; fail: return ret; } static int configure_audio_filters(VideoState *is, const char *afilters, int force_output_format) { static const enum AVSampleFormat sample_fmts[] = { AV_SAMPLE_FMT_S16, AV_SAMPLE_FMT_NONE }; int sample_rates[2] = { 0, -1 }; int64_t channel_layouts[2] = { 0, -1 }; int channels[2] = { 0, -1 }; AVFilterContext *filt_asrc = NULL, *filt_asink = NULL; char aresample_swr_opts[512] = \"\"; AVDictionaryEntry *e = NULL; char asrc_args[256]; int ret; avfilter_graph_free(&is->agraph); if (!(is->agraph = avfilter_graph_alloc())) return AVERROR(ENOMEM); is->agraph->nb_threads = filter_nbthreads; while ((e = av_dict_get(swr_opts, \"\", e, AV_DICT_IGNORE_SUFFIX))) av_strlcatf(aresample_swr_opts, sizeof(aresample_swr_opts), \"%s=%s:\", e->key, e->value); if (strlen(aresample_swr_opts)) aresample_swr_opts[strlen(aresample_swr_opts)-1] = '\\0'; av_opt_set(is->agraph, \"aresample_swr_opts\", aresample_swr_opts, 0); ret = snprintf(asrc_args, sizeof(asrc_args), \"sample_rate=%d:sample_fmt=%s:channels=%d:time_base=%d/%d\", is->audio_filter_src.freq, av_get_sample_fmt_name(is->audio_filter_src.fmt), is->audio_filter_src.channels, 1, is->audio_filter_src.freq); if (is->audio_filter_src.channel_layout) snprintf(asrc_args + ret, sizeof(asrc_args) - ret, \":channel_layout=0x%\"PRIx64, is->audio_filter_src.channel_layout); ret = avfilter_graph_create_filter(&filt_asrc, avfilter_get_by_name(\"abuffer\"), \"ffplay_abuffer\", asrc_args, NULL, is->agraph); if (ret agraph); if (ret audio_tgt.channel_layout; channels [0] = is->audio_tgt.channels; sample_rates [0] = is->audio_tgt.freq; if ((ret = av_opt_set_int(filt_asink, \"all_channel_counts\", 0, AV_OPT_SEARCH_CHILDREN)) agraph, afilters, filt_asrc, filt_asink)) in_audio_filter = filt_asrc; is->out_audio_filter = filt_asink; end: if (ret agraph); return ret; } #endif /* CONFIG_AVFILTER */ static int audio_thread(void *arg) { VideoState *is = arg; AVFrame *frame = av_frame_alloc(); Frame *af; #if CONFIG_AVFILTER int last_serial = -1; int64_t dec_channel_layout; int reconfigure; #endif int got_frame = 0; AVRational tb; int ret = 0; if (!frame) return AVERROR(ENOMEM); do { if ((got_frame = decoder_decode_frame(&is->auddec, frame, NULL)) sample_rate}; #if CONFIG_AVFILTER dec_channel_layout = get_valid_channel_layout(frame->channel_layout, frame->channels); reconfigure = cmp_audio_fmts(is->audio_filter_src.fmt, is->audio_filter_src.channels, frame->format, frame->channels) || is->audio_filter_src.channel_layout != dec_channel_layout || is->audio_filter_src.freq != frame->sample_rate || is->auddec.pkt_serial != last_serial; if (reconfigure) { char buf1[1024], buf2[1024]; av_get_channel_layout_string(buf1, sizeof(buf1), -1, is->audio_filter_src.channel_layout); av_get_channel_layout_string(buf2, sizeof(buf2), -1, dec_channel_layout); av_log(NULL, AV_LOG_DEBUG, \"Audio frame changed from rate:%d ch:%d fmt:%s layout:%s serial:%d to rate:%d ch:%d fmt:%s layout:%s serial:%d\\n\", is->audio_filter_src.freq, is->audio_filter_src.channels, av_get_sample_fmt_name(is->audio_filter_src.fmt), buf1, last_serial, frame->sample_rate, frame->channels, av_get_sample_fmt_name(frame->format), buf2, is->auddec.pkt_serial); is->audio_filter_src.fmt = frame->format; is->audio_filter_src.channels = frame->channels; is->audio_filter_src.channel_layout = dec_channel_layout; is->audio_filter_src.freq = frame->sample_rate; last_serial = is->auddec.pkt_serial; if ((ret = configure_audio_filters(is, afilters, 1)) in_audio_filter, frame)) out_audio_filter, frame, 0)) >= 0) { tb = av_buffersink_get_time_base(is->out_audio_filter); #endif if (!(af = frame_queue_peek_writable(&is->sampq))) goto the_end; af->pts = (frame->pts == AV_NOPTS_VALUE) ? NAN : frame->pts * av_q2d(tb); af->pos = frame->pkt_pos; af->serial = is->auddec.pkt_serial; af->duration = av_q2d((AVRational){frame->nb_samples, frame->sample_rate}); av_frame_move_ref(af->frame, frame); frame_queue_push(&is->sampq); #if CONFIG_AVFILTER if (is->audioq.serial != is->auddec.pkt_serial) break; } if (ret == AVERROR_EOF) is->auddec.finished = is->auddec.pkt_serial; #endif } } while (ret >= 0 || ret == AVERROR(EAGAIN) || ret == AVERROR_EOF); the_end: #if CONFIG_AVFILTER avfilter_graph_free(&is->agraph); #endif av_frame_free(&frame); return ret; } static int decoder_start(Decoder *d, int (*fn)(void *), const char *thread_name, void* arg) { packet_queue_start(d->queue); d->decoder_tid = SDL_CreateThread(fn, thread_name, arg); if (!d->decoder_tid) { av_log(NULL, AV_LOG_ERROR, \"SDL_CreateThread(): %s\\n\", SDL_GetError()); return AVERROR(ENOMEM); } return 0; } static int video_thread(void *arg) { VideoState *is = arg; AVFrame *frame = av_frame_alloc(); double pts; double duration; int ret; AVRational tb = is->video_st->time_base; AVRational frame_rate = av_guess_frame_rate(is->ic, is->video_st, NULL); #if CONFIG_AVFILTER AVFilterGraph *graph = NULL; AVFilterContext *filt_out = NULL, *filt_in = NULL; int last_w = 0; int last_h = 0; enum AVPixelFormat last_format = -2; int last_serial = -1; int last_vfilter_idx = 0; #endif if (!frame) return AVERROR(ENOMEM); for (;;) { ret = get_video_frame(is, frame); if (ret width || last_h != frame->height || last_format != frame->format || last_serial != is->viddec.pkt_serial || last_vfilter_idx != is->vfilter_idx) { av_log(NULL, AV_LOG_DEBUG, \"Video frame changed from size:%dx%d format:%s serial:%d to size:%dx%d format:%s serial:%d\\n\", last_w, last_h, (const char *)av_x_if_null(av_get_pix_fmt_name(last_format), \"none\"), last_serial, frame->width, frame->height, (const char *)av_x_if_null(av_get_pix_fmt_name(frame->format), \"none\"), is->viddec.pkt_serial); avfilter_graph_free(&graph); graph = avfilter_graph_alloc(); if (!graph) { ret = AVERROR(ENOMEM); goto the_end; } graph->nb_threads = filter_nbthreads; if ((ret = configure_video_filters(graph, is, vfilters_list ? vfilters_list[is->vfilter_idx] : NULL, frame)) in_video_filter; filt_out = is->out_video_filter; last_w = frame->width; last_h = frame->height; last_format = frame->format; last_serial = is->viddec.pkt_serial; last_vfilter_idx = is->vfilter_idx; frame_rate = av_buffersink_get_frame_rate(filt_out); } ret = av_buffersrc_add_frame(filt_in, frame); if (ret = 0) { is->frame_last_returned_time = av_gettime_relative() / 1000000.0; ret = av_buffersink_get_frame_flags(filt_out, frame, 0); if (ret viddec.finished = is->viddec.pkt_serial; ret = 0; break; } is->frame_last_filter_delay = av_gettime_relative() / 1000000.0 - is->frame_last_returned_time; if (fabs(is->frame_last_filter_delay) > AV_NOSYNC_THRESHOLD / 10.0) is->frame_last_filter_delay = 0; tb = av_buffersink_get_time_base(filt_out); #endif duration = (frame_rate.num && frame_rate.den ? av_q2d((AVRational){frame_rate.den, frame_rate.num}) : 0); pts = (frame->pts == AV_NOPTS_VALUE) ? NAN : frame->pts * av_q2d(tb); ret = queue_picture(is, frame, pts, duration, frame->pkt_pos, is->viddec.pkt_serial); av_frame_unref(frame); #if CONFIG_AVFILTER if (is->videoq.serial != is->viddec.pkt_serial) break; } #endif if (ret subpq))) return 0; if ((got_subtitle = decoder_decode_frame(&is->subdec, NULL, &sp->sub)) sub.format == 0) { if (sp->sub.pts != AV_NOPTS_VALUE) pts = sp->sub.pts / (double)AV_TIME_BASE; sp->pts = pts; sp->serial = is->subdec.pkt_serial; sp->width = is->subdec.avctx->width; sp->height = is->subdec.avctx->height; sp->uploaded = 0; /* now we can update the picture count */ frame_queue_push(&is->subpq); } else if (got_subtitle) { avsubtitle_free(&sp->sub); } } return 0; } /* copy samples for viewing in editor window */ static void update_sample_display(VideoState *is, short *samples, int samples_size) { int size, len; size = samples_size / sizeof(short); while (size > 0) { len = SAMPLE_ARRAY_SIZE - is->sample_array_index; if (len > size) len = size; memcpy(is->sample_array + is->sample_array_index, samples, len * sizeof(short)); samples += len; is->sample_array_index += len; if (is->sample_array_index >= SAMPLE_ARRAY_SIZE) is->sample_array_index = 0; size -= len; } } /* return the wanted number of samples to get better sync if sync_type is video * or external master clock */ static int synchronize_audio(VideoState *is, int nb_samples) { int wanted_nb_samples = nb_samples; /* if not master, then we try to remove or add samples to correct the clock */ if (get_master_sync_type(is) != AV_SYNC_AUDIO_MASTER) { double diff, avg_diff; int min_nb_samples, max_nb_samples; diff = get_clock(&is->audclk) - get_master_clock(is); if (!isnan(diff) && fabs(diff) audio_diff_cum = diff + is->audio_diff_avg_coef * is->audio_diff_cum; if (is->audio_diff_avg_count audio_diff_avg_count++; } else { /* estimate the A-V difference */ avg_diff = is->audio_diff_cum * (1.0 - is->audio_diff_avg_coef); if (fabs(avg_diff) >= is->audio_diff_threshold) { wanted_nb_samples = nb_samples + (int)(diff * is->audio_src.freq); min_nb_samples = ((nb_samples * (100 - SAMPLE_CORRECTION_PERCENT_MAX) / 100)); max_nb_samples = ((nb_samples * (100 + SAMPLE_CORRECTION_PERCENT_MAX) / 100)); wanted_nb_samples = av_clip(wanted_nb_samples, min_nb_samples, max_nb_samples); } av_log(NULL, AV_LOG_TRACE, \"diff=%f adiff=%f sample_diff=%d apts=%0.3f %f\\n\", diff, avg_diff, wanted_nb_samples - nb_samples, is->audio_clock, is->audio_diff_threshold); } } else { /* too big difference : may be initial PTS errors, so reset A-V filter */ is->audio_diff_avg_count = 0; is->audio_diff_cum = 0; } } return wanted_nb_samples; } /** * Decode one audio frame and return its uncompressed size. * * The processed audio frame is decoded, converted if required, and * stored in is->audio_buf, with size in bytes given by the return * value. */ static int audio_decode_frame(VideoState *is) { int data_size, resampled_data_size; int64_t dec_channel_layout; av_unused double audio_clock0; int wanted_nb_samples; Frame *af; if (is->paused) return -1; do { #if defined(_WIN32) while (frame_queue_nb_remaining(&is->sampq) == 0) { if ((av_gettime_relative() - audio_callback_time) > 1000000LL * is->audio_hw_buf_size / is->audio_tgt.bytes_per_sec / 2) return -1; av_usleep (1000); } #endif if (!(af = frame_queue_peek_readable(&is->sampq))) return -1; frame_queue_next(&is->sampq); } while (af->serial != is->audioq.serial); data_size = av_samples_get_buffer_size(NULL, af->frame->channels, af->frame->nb_samples, af->frame->format, 1); dec_channel_layout = (af->frame->channel_layout && af->frame->channels == av_get_channel_layout_nb_channels(af->frame->channel_layout)) ? af->frame->channel_layout : av_get_default_channel_layout(af->frame->channels); wanted_nb_samples = synchronize_audio(is, af->frame->nb_samples); if (af->frame->format != is->audio_src.fmt || dec_channel_layout != is->audio_src.channel_layout || af->frame->sample_rate != is->audio_src.freq || (wanted_nb_samples != af->frame->nb_samples && !is->swr_ctx)) { swr_free(&is->swr_ctx); is->swr_ctx = swr_alloc_set_opts(NULL, is->audio_tgt.channel_layout, is->audio_tgt.fmt, is->audio_tgt.freq, dec_channel_layout, af->frame->format, af->frame->sample_rate, 0, NULL); if (!is->swr_ctx || swr_init(is->swr_ctx) frame->sample_rate, av_get_sample_fmt_name(af->frame->format), af->frame->channels, is->audio_tgt.freq, av_get_sample_fmt_name(is->audio_tgt.fmt), is->audio_tgt.channels); swr_free(&is->swr_ctx); return -1; } is->audio_src.channel_layout = dec_channel_layout; is->audio_src.channels = af->frame->channels; is->audio_src.freq = af->frame->sample_rate; is->audio_src.fmt = af->frame->format; } if (is->swr_ctx) { const uint8_t **in = (const uint8_t **)af->frame->extended_data; uint8_t **out = &is->audio_buf1; int out_count = (int64_t)wanted_nb_samples * is->audio_tgt.freq / af->frame->sample_rate + 256; int out_size = av_samples_get_buffer_size(NULL, is->audio_tgt.channels, out_count, is->audio_tgt.fmt, 0); int len2; if (out_size frame->nb_samples) { if (swr_set_compensation(is->swr_ctx, (wanted_nb_samples - af->frame->nb_samples) * is->audio_tgt.freq / af->frame->sample_rate, wanted_nb_samples * is->audio_tgt.freq / af->frame->sample_rate) audio_buf1, &is->audio_buf1_size, out_size); if (!is->audio_buf1) return AVERROR(ENOMEM); len2 = swr_convert(is->swr_ctx, out, out_count, in, af->frame->nb_samples); if (len2 swr_ctx) swr_ctx); } is->audio_buf = is->audio_buf1; resampled_data_size = len2 * is->audio_tgt.channels * av_get_bytes_per_sample(is->audio_tgt.fmt); } else { is->audio_buf = af->frame->data[0]; resampled_data_size = data_size; } audio_clock0 = is->audio_clock; /* update the audio clock with the pts */ if (!isnan(af->pts)) is->audio_clock = af->pts + (double) af->frame->nb_samples / af->frame->sample_rate; else is->audio_clock = NAN; is->audio_clock_serial = af->serial; #ifdef DEBUG { static double last_clock; printf(\"audio: delay=%0.3f clock=%0.3f clock0=%0.3f\\n\", is->audio_clock - last_clock, is->audio_clock, audio_clock0); last_clock = is->audio_clock; } #endif return resampled_data_size; } /* prepare a new audio buffer */ static void sdl_audio_callback(void *opaque, Uint8 *stream, int len) { VideoState *is = opaque; int audio_size, len1; audio_callback_time = av_gettime_relative(); while (len > 0) { if (is->audio_buf_index >= is->audio_buf_size) { audio_size = audio_decode_frame(is); if (audio_size audio_buf = NULL; is->audio_buf_size = SDL_AUDIO_MIN_BUFFER_SIZE / is->audio_tgt.frame_size * is->audio_tgt.frame_size; } else { if (is->show_mode != SHOW_MODE_VIDEO) update_sample_display(is, (int16_t *)is->audio_buf, audio_size); is->audio_buf_size = audio_size; } is->audio_buf_index = 0; } len1 = is->audio_buf_size - is->audio_buf_index; if (len1 > len) len1 = len; if (!is->muted && is->audio_buf && is->audio_volume == SDL_MIX_MAXVOLUME) memcpy(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, len1); else { memset(stream, 0, len1); if (!is->muted && is->audio_buf) SDL_MixAudioFormat(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, AUDIO_S16SYS, len1, is->audio_volume); } len -= len1; stream += len1; is->audio_buf_index += len1; } is->audio_write_buf_size = is->audio_buf_size - is->audio_buf_index; /* Let's assume the audio driver that is used by SDL has two periods. */ if (!isnan(is->audio_clock)) { set_clock_at(&is->audclk, is->audio_clock - (double)(2 * is->audio_hw_buf_size + is->audio_write_buf_size) / is->audio_tgt.bytes_per_sec, is->audio_clock_serial, audio_callback_time / 1000000.0); sync_clock_to_slave(&is->extclk, &is->audclk); } } static int audio_open(void *opaque, int64_t wanted_channel_layout, int wanted_nb_channels, int wanted_sample_rate, struct AudioParams *audio_hw_params) { SDL_AudioSpec wanted_spec, spec; const char *env; static const int next_nb_channels[] = {0, 0, 1, 6, 2, 6, 4, 6}; static const int next_sample_rates[] = {0, 44100, 48000, 96000, 192000}; int next_sample_rate_idx = FF_ARRAY_ELEMS(next_sample_rates) - 1; env = SDL_getenv(\"SDL_AUDIO_CHANNELS\"); if (env) { wanted_nb_channels = atoi(env); wanted_channel_layout = av_get_default_channel_layout(wanted_nb_channels); } if (!wanted_channel_layout || wanted_nb_channels != av_get_channel_layout_nb_channels(wanted_channel_layout)) { wanted_channel_layout = av_get_default_channel_layout(wanted_nb_channels); wanted_channel_layout &= ~AV_CH_LAYOUT_STEREO_DOWNMIX; } wanted_nb_channels = av_get_channel_layout_nb_channels(wanted_channel_layout); wanted_spec.channels = wanted_nb_channels; wanted_spec.freq = wanted_sample_rate; if (wanted_spec.freq = wanted_spec.freq) next_sample_rate_idx--; wanted_spec.format = AUDIO_S16SYS; wanted_spec.silence = 0; wanted_spec.samples = FFMAX(SDL_AUDIO_MIN_BUFFER_SIZE, 2 fmt = AV_SAMPLE_FMT_S16; audio_hw_params->freq = spec.freq; audio_hw_params->channel_layout = wanted_channel_layout; audio_hw_params->channels = spec.channels; audio_hw_params->frame_size = av_samples_get_buffer_size(NULL, audio_hw_params->channels, 1, audio_hw_params->fmt, 1); audio_hw_params->bytes_per_sec = av_samples_get_buffer_size(NULL, audio_hw_params->channels, audio_hw_params->freq, audio_hw_params->fmt, 1); if (audio_hw_params->bytes_per_sec frame_size ic; AVCodecContext *avctx; AVCodec *codec; const char *forced_codec_name = NULL; AVDictionary *opts = NULL; AVDictionaryEntry *t = NULL; int sample_rate, nb_channels; int64_t channel_layout; int ret = 0; int stream_lowres = lowres; if (stream_index = ic->nb_streams) return -1; avctx = avcodec_alloc_context3(NULL); if (!avctx) return AVERROR(ENOMEM); ret = avcodec_parameters_to_context(avctx, ic->streams[stream_index]->codecpar); if (ret pkt_timebase = ic->streams[stream_index]->time_base; codec = avcodec_find_decoder(avctx->codec_id); switch(avctx->codec_type){ case AVMEDIA_TYPE_AUDIO : is->last_audio_stream = stream_index; forced_codec_name = audio_codec_name; break; case AVMEDIA_TYPE_SUBTITLE: is->last_subtitle_stream = stream_index; forced_codec_name = subtitle_codec_name; break; case AVMEDIA_TYPE_VIDEO : is->last_video_stream = stream_index; forced_codec_name = video_codec_name; break; } if (forced_codec_name) codec = avcodec_find_decoder_by_name(forced_codec_name); if (!codec) { if (forced_codec_name) av_log(NULL, AV_LOG_WARNING, \"No codec could be found with name '%s'\\n\", forced_codec_name); else av_log(NULL, AV_LOG_WARNING, \"No decoder could be found for codec %s\\n\", avcodec_get_name(avctx->codec_id)); ret = AVERROR(EINVAL); goto fail; } avctx->codec_id = codec->id; if (stream_lowres > codec->max_lowres) { av_log(avctx, AV_LOG_WARNING, \"The maximum value for lowres supported by the decoder is %d\\n\", codec->max_lowres); stream_lowres = codec->max_lowres; } avctx->lowres = stream_lowres; if (fast) avctx->flags2 |= AV_CODEC_FLAG2_FAST; opts = filter_codec_opts(codec_opts, avctx->codec_id, ic, ic->streams[stream_index], codec); if (!av_dict_get(opts, \"threads\", NULL, 0)) av_dict_set(&opts, \"threads\", \"auto\", 0); if (stream_lowres) av_dict_set_int(&opts, \"lowres\", stream_lowres, 0); if (avctx->codec_type == AVMEDIA_TYPE_VIDEO || avctx->codec_type == AVMEDIA_TYPE_AUDIO) av_dict_set(&opts, \"refcounted_frames\", \"1\", 0); if ((ret = avcodec_open2(avctx, codec, &opts)) key); ret = AVERROR_OPTION_NOT_FOUND; goto fail; } is->eof = 0; ic->streams[stream_index]->discard = AVDISCARD_DEFAULT; switch (avctx->codec_type) { case AVMEDIA_TYPE_AUDIO: #if CONFIG_AVFILTER { AVFilterContext *sink; is->audio_filter_src.freq = avctx->sample_rate; is->audio_filter_src.channels = avctx->channels; is->audio_filter_src.channel_layout = get_valid_channel_layout(avctx->channel_layout, avctx->channels); is->audio_filter_src.fmt = avctx->sample_fmt; if ((ret = configure_audio_filters(is, afilters, 0)) out_audio_filter; sample_rate = av_buffersink_get_sample_rate(sink); nb_channels = av_buffersink_get_channels(sink); channel_layout = av_buffersink_get_channel_layout(sink); } #else sample_rate = avctx->sample_rate; nb_channels = avctx->channels; channel_layout = avctx->channel_layout; #endif /* prepare audio output */ if ((ret = audio_open(is, channel_layout, nb_channels, sample_rate, &is->audio_tgt)) audio_hw_buf_size = ret; is->audio_src = is->audio_tgt; is->audio_buf_size = 0; is->audio_buf_index = 0; /* init averaging filter */ is->audio_diff_avg_coef = exp(log(0.01) / AUDIO_DIFF_AVG_NB); is->audio_diff_avg_count = 0; /* since we do not have a precise anough audio FIFO fullness, we correct audio sync only if larger than this threshold */ is->audio_diff_threshold = (double)(is->audio_hw_buf_size) / is->audio_tgt.bytes_per_sec; is->audio_stream = stream_index; is->audio_st = ic->streams[stream_index]; decoder_init(&is->auddec, avctx, &is->audioq, is->continue_read_thread); if ((is->ic->iformat->flags & (AVFMT_NOBINSEARCH | AVFMT_NOGENSEARCH | AVFMT_NO_BYTE_SEEK)) && !is->ic->iformat->read_seek) { is->auddec.start_pts = is->audio_st->start_time; is->auddec.start_pts_tb = is->audio_st->time_base; } if ((ret = decoder_start(&is->auddec, audio_thread, \"audio_decoder\", is)) video_stream = stream_index; is->video_st = ic->streams[stream_index]; decoder_init(&is->viddec, avctx, &is->videoq, is->continue_read_thread); if ((ret = decoder_start(&is->viddec, video_thread, \"video_decoder\", is)) queue_attachments_req = 1; break; case AVMEDIA_TYPE_SUBTITLE: is->subtitle_stream = stream_index; is->subtitle_st = ic->streams[stream_index]; decoder_init(&is->subdec, avctx, &is->subtitleq, is->continue_read_thread); if ((ret = decoder_start(&is->subdec, subtitle_thread, \"subtitle_decoder\", is)) abort_request; } static int stream_has_enough_packets(AVStream *st, int stream_id, PacketQueue *queue) { return stream_id abort_request || (st->disposition & AV_DISPOSITION_ATTACHED_PIC) || queue->nb_packets > MIN_FRAMES && (!queue->duration || av_q2d(st->time_base) * queue->duration > 1.0); } static int is_realtime(AVFormatContext *s) { if( !strcmp(s->iformat->name, \"rtp\") || !strcmp(s->iformat->name, \"rtsp\") || !strcmp(s->iformat->name, \"sdp\") ) return 1; if(s->pb && ( !strncmp(s->url, \"rtp:\", 4) || !strncmp(s->url, \"udp:\", 4) ) ) return 1; return 0; } /* this thread gets the stream from the disk or the network */ static int read_thread(void *arg) { VideoState *is = arg; AVFormatContext *ic = NULL; int err, i, ret; int st_index[AVMEDIA_TYPE_NB]; AVPacket pkt1, *pkt = &pkt1; int64_t stream_start_time; int pkt_in_play_range = 0; AVDictionaryEntry *t; SDL_mutex *wait_mutex = SDL_CreateMutex(); int scan_all_pmts_set = 0; int64_t pkt_ts; if (!wait_mutex) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateMutex(): %s\\n\", SDL_GetError()); ret = AVERROR(ENOMEM); goto fail; } memset(st_index, -1, sizeof(st_index)); is->eof = 0; ic = avformat_alloc_context(); if (!ic) { av_log(NULL, AV_LOG_FATAL, \"Could not allocate context.\\n\"); ret = AVERROR(ENOMEM); goto fail; } ic->interrupt_callback.callback = decode_interrupt_cb; ic->interrupt_callback.opaque = is; if (!av_dict_get(format_opts, \"scan_all_pmts\", NULL, AV_DICT_MATCH_CASE)) { av_dict_set(&format_opts, \"scan_all_pmts\", \"1\", AV_DICT_DONT_OVERWRITE); scan_all_pmts_set = 1; } err = avformat_open_input(&ic, is->filename, is->iformat, &format_opts); if (err filename, err); ret = -1; goto fail; } if (scan_all_pmts_set) av_dict_set(&format_opts, \"scan_all_pmts\", NULL, AV_DICT_MATCH_CASE); if ((t = av_dict_get(format_opts, \"\", NULL, AV_DICT_IGNORE_SUFFIX))) { av_log(NULL, AV_LOG_ERROR, \"Option %s not found.\\n\", t->key); ret = AVERROR_OPTION_NOT_FOUND; goto fail; } is->ic = ic; if (genpts) ic->flags |= AVFMT_FLAG_GENPTS; av_format_inject_global_side_data(ic); if (find_stream_info) { AVDictionary **opts = setup_find_stream_info_opts(ic, codec_opts); int orig_nb_streams = ic->nb_streams; err = avformat_find_stream_info(ic, opts); for (i = 0; i filename); ret = -1; goto fail; } } if (ic->pb) ic->pb->eof_reached = 0; // FIXME hack, ffplay maybe should not use avio_feof() to test for the end if (seek_by_bytes iformat->flags & AVFMT_TS_DISCONT) && strcmp(\"ogg\", ic->iformat->name); is->max_frame_duration = (ic->iformat->flags & AVFMT_TS_DISCONT) ? 10.0 : 3600.0; if (!window_title && (t = av_dict_get(ic->metadata, \"title\", NULL, 0))) window_title = av_asprintf(\"%s - %s\", t->value, input_filename); /* if seeking requested, we execute it */ if (start_time != AV_NOPTS_VALUE) { int64_t timestamp; timestamp = start_time; /* add the stream start time */ if (ic->start_time != AV_NOPTS_VALUE) timestamp += ic->start_time; ret = avformat_seek_file(ic, -1, INT64_MIN, timestamp, INT64_MAX, 0); if (ret filename, (double)timestamp / AV_TIME_BASE); } } is->realtime = is_realtime(ic); if (show_status) av_dump_format(ic, 0, is->filename, 0); for (i = 0; i nb_streams; i++) { AVStream *st = ic->streams[i]; enum AVMediaType type = st->codecpar->codec_type; st->discard = AVDISCARD_ALL; if (type >= 0 && wanted_stream_spec[type] && st_index[type] == -1) if (avformat_match_stream_specifier(ic, st, wanted_stream_spec[type]) > 0) st_index[type] = i; } for (i = 0; i = 0 ? st_index[AVMEDIA_TYPE_AUDIO] : st_index[AVMEDIA_TYPE_VIDEO]), NULL, 0); is->show_mode = show_mode; if (st_index[AVMEDIA_TYPE_VIDEO] >= 0) { AVStream *st = ic->streams[st_index[AVMEDIA_TYPE_VIDEO]]; AVCodecParameters *codecpar = st->codecpar; AVRational sar = av_guess_sample_aspect_ratio(ic, st, NULL); if (codecpar->width) set_default_window_size(codecpar->width, codecpar->height, sar); } /* open the streams */ if (st_index[AVMEDIA_TYPE_AUDIO] >= 0) { stream_component_open(is, st_index[AVMEDIA_TYPE_AUDIO]); } ret = -1; if (st_index[AVMEDIA_TYPE_VIDEO] >= 0) { ret = stream_component_open(is, st_index[AVMEDIA_TYPE_VIDEO]); } if (is->show_mode == SHOW_MODE_NONE) is->show_mode = ret >= 0 ? SHOW_MODE_VIDEO : SHOW_MODE_RDFT; if (st_index[AVMEDIA_TYPE_SUBTITLE] >= 0) { stream_component_open(is, st_index[AVMEDIA_TYPE_SUBTITLE]); } if (is->video_stream audio_stream filename); ret = -1; goto fail; } if (infinite_buffer realtime) infinite_buffer = 1; for (;;) { if (is->abort_request) break; if (is->paused != is->last_paused) { is->last_paused = is->paused; if (is->paused) is->read_pause_return = av_read_pause(ic); else av_read_play(ic); } #if CONFIG_RTSP_DEMUXER || CONFIG_MMSH_PROTOCOL if (is->paused && (!strcmp(ic->iformat->name, \"rtsp\") || (ic->pb && !strncmp(input_filename, \"mmsh:\", 5)))) { /* wait 10 ms to avoid trying to get another packet */ /* XXX: horrible */ SDL_Delay(10); continue; } #endif if (is->seek_req) { int64_t seek_target = is->seek_pos; int64_t seek_min = is->seek_rel > 0 ? seek_target - is->seek_rel + 2: INT64_MIN; int64_t seek_max = is->seek_rel seek_rel - 2: INT64_MAX; // FIXME the +-2 is due to rounding being not done in the correct direction in generation // of the seek_pos/seek_rel variables ret = avformat_seek_file(is->ic, -1, seek_min, seek_target, seek_max, is->seek_flags); if (ret ic->url); } else { if (is->audio_stream >= 0) { packet_queue_flush(&is->audioq); packet_queue_put(&is->audioq, &flush_pkt); } if (is->subtitle_stream >= 0) { packet_queue_flush(&is->subtitleq); packet_queue_put(&is->subtitleq, &flush_pkt); } if (is->video_stream >= 0) { packet_queue_flush(&is->videoq); packet_queue_put(&is->videoq, &flush_pkt); } if (is->seek_flags & AVSEEK_FLAG_BYTE) { set_clock(&is->extclk, NAN, 0); } else { set_clock(&is->extclk, seek_target / (double)AV_TIME_BASE, 0); } } is->seek_req = 0; is->queue_attachments_req = 1; is->eof = 0; if (is->paused) step_to_next_frame(is); } if (is->queue_attachments_req) { if (is->video_st && is->video_st->disposition & AV_DISPOSITION_ATTACHED_PIC) { AVPacket copy = { 0 }; if ((ret = av_packet_ref(&copy, &is->video_st->attached_pic)) videoq, &copy); packet_queue_put_nullpacket(&is->videoq, is->video_stream); } is->queue_attachments_req = 0; } /* if the queue are full, no need to read more */ if (infinite_bufferaudioq.size + is->videoq.size + is->subtitleq.size > MAX_QUEUE_SIZE || (stream_has_enough_packets(is->audio_st, is->audio_stream, &is->audioq) && stream_has_enough_packets(is->video_st, is->video_stream, &is->videoq) && stream_has_enough_packets(is->subtitle_st, is->subtitle_stream, &is->subtitleq)))) { /* wait 10 ms */ SDL_LockMutex(wait_mutex); SDL_CondWaitTimeout(is->continue_read_thread, wait_mutex, 10); SDL_UnlockMutex(wait_mutex); continue; } if (!is->paused && (!is->audio_st || (is->auddec.finished == is->audioq.serial && frame_queue_nb_remaining(&is->sampq) == 0)) && (!is->video_st || (is->viddec.finished == is->videoq.serial && frame_queue_nb_remaining(&is->pictq) == 0))) { if (loop != 1 && (!loop || --loop)) { stream_seek(is, start_time != AV_NOPTS_VALUE ? start_time : 0, 0, 0); } else if (autoexit) { ret = AVERROR_EOF; goto fail; } } ret = av_read_frame(ic, pkt); if (ret pb)) && !is->eof) { if (is->video_stream >= 0) packet_queue_put_nullpacket(&is->videoq, is->video_stream); if (is->audio_stream >= 0) packet_queue_put_nullpacket(&is->audioq, is->audio_stream); if (is->subtitle_stream >= 0) packet_queue_put_nullpacket(&is->subtitleq, is->subtitle_stream); is->eof = 1; } if (ic->pb && ic->pb->error) break; SDL_LockMutex(wait_mutex); SDL_CondWaitTimeout(is->continue_read_thread, wait_mutex, 10); SDL_UnlockMutex(wait_mutex); continue; } else { is->eof = 0; } /* check if packet is in play range specified by user, then queue, otherwise discard */ stream_start_time = ic->streams[pkt->stream_index]->start_time; pkt_ts = pkt->pts == AV_NOPTS_VALUE ? pkt->dts : pkt->pts; pkt_in_play_range = duration == AV_NOPTS_VALUE || (pkt_ts - (stream_start_time != AV_NOPTS_VALUE ? stream_start_time : 0)) * av_q2d(ic->streams[pkt->stream_index]->time_base) - (double)(start_time != AV_NOPTS_VALUE ? start_time : 0) / 1000000 stream_index == is->audio_stream && pkt_in_play_range) { packet_queue_put(&is->audioq, pkt); } else if (pkt->stream_index == is->video_stream && pkt_in_play_range && !(is->video_st->disposition & AV_DISPOSITION_ATTACHED_PIC)) { packet_queue_put(&is->videoq, pkt); } else if (pkt->stream_index == is->subtitle_stream && pkt_in_play_range) { packet_queue_put(&is->subtitleq, pkt); } else { av_packet_unref(pkt); } } ret = 0; fail: if (ic && !is->ic) avformat_close_input(&ic); if (ret != 0) { SDL_Event event; event.type = FF_QUIT_EVENT; event.user.data1 = is; SDL_PushEvent(&event); } SDL_DestroyMutex(wait_mutex); return 0; } static VideoState *stream_open(const char *filename, AVInputFormat *iformat) { VideoState *is; is = av_mallocz(sizeof(VideoState)); if (!is) return NULL; is->last_video_stream = is->video_stream = -1; is->last_audio_stream = is->audio_stream = -1; is->last_subtitle_stream = is->subtitle_stream = -1; is->filename = av_strdup(filename); if (!is->filename) goto fail; is->iformat = iformat; is->ytop = 0; is->xleft = 0; /* start video display */ if (frame_queue_init(&is->pictq, &is->videoq, VIDEO_PICTURE_QUEUE_SIZE, 1) subpq, &is->subtitleq, SUBPICTURE_QUEUE_SIZE, 0) sampq, &is->audioq, SAMPLE_QUEUE_SIZE, 1) videoq) audioq) subtitleq) continue_read_thread = SDL_CreateCond())) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateCond(): %s\\n\", SDL_GetError()); goto fail; } init_clock(&is->vidclk, &is->videoq.serial); init_clock(&is->audclk, &is->audioq.serial); init_clock(&is->extclk, &is->extclk.serial); is->audio_clock_serial = -1; if (startup_volume 100) av_log(NULL, AV_LOG_WARNING, \"-volume=%d > 100, setting to 100\\n\", startup_volume); startup_volume = av_clip(startup_volume, 0, 100); startup_volume = av_clip(SDL_MIX_MAXVOLUME * startup_volume / 100, 0, SDL_MIX_MAXVOLUME); is->audio_volume = startup_volume; is->muted = 0; is->av_sync_type = av_sync_type; is->read_tid = SDL_CreateThread(read_thread, \"read_thread\", is); if (!is->read_tid) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateThread(): %s\\n\", SDL_GetError()); fail: stream_close(is); return NULL; } return is; } static void stream_cycle_channel(VideoState *is, int codec_type) { AVFormatContext *ic = is->ic; int start_index, stream_index; int old_index; AVStream *st; AVProgram *p = NULL; int nb_streams = is->ic->nb_streams; if (codec_type == AVMEDIA_TYPE_VIDEO) { start_index = is->last_video_stream; old_index = is->video_stream; } else if (codec_type == AVMEDIA_TYPE_AUDIO) { start_index = is->last_audio_stream; old_index = is->audio_stream; } else { start_index = is->last_subtitle_stream; old_index = is->subtitle_stream; } stream_index = start_index; if (codec_type != AVMEDIA_TYPE_VIDEO && is->video_stream != -1) { p = av_find_program_from_stream(ic, NULL, is->video_stream); if (p) { nb_streams = p->nb_stream_indexes; for (start_index = 0; start_index stream_index[start_index] == stream_index) break; if (start_index == nb_streams) start_index = -1; stream_index = start_index; } } for (;;) { if (++stream_index >= nb_streams) { if (codec_type == AVMEDIA_TYPE_SUBTITLE) { stream_index = -1; is->last_subtitle_stream = -1; goto the_end; } if (start_index == -1) return; stream_index = 0; } if (stream_index == start_index) return; st = is->ic->streams[p ? p->stream_index[stream_index] : stream_index]; if (st->codecpar->codec_type == codec_type) { /* check that parameters are OK */ switch (codec_type) { case AVMEDIA_TYPE_AUDIO: if (st->codecpar->sample_rate != 0 && st->codecpar->channels != 0) goto the_end; break; case AVMEDIA_TYPE_VIDEO: case AVMEDIA_TYPE_SUBTITLE: goto the_end; default: break; } } } the_end: if (p && stream_index != -1) stream_index = p->stream_index[stream_index]; av_log(NULL, AV_LOG_INFO, \"Switch %s stream from #%d to #%d\\n\", av_get_media_type_string(codec_type), old_index, stream_index); stream_component_close(is, old_index); stream_component_open(is, stream_index); } static void toggle_full_screen(VideoState *is) { is_full_screen = !is_full_screen; SDL_SetWindowFullscreen(window, is_full_screen ? SDL_WINDOW_FULLSCREEN_DESKTOP : 0); } static void toggle_audio_display(VideoState *is) { int next = is->show_mode; do { next = (next + 1) % SHOW_MODE_NB; } while (next != is->show_mode && (next == SHOW_MODE_VIDEO && !is->video_st || next != SHOW_MODE_VIDEO && !is->audio_st)); if (is->show_mode != next) { is->force_refresh = 1; is->show_mode = next; } } static void refresh_loop_wait_event(VideoState *is, SDL_Event *event) { double remaining_time = 0.0; SDL_PumpEvents(); while (!SDL_PeepEvents(event, 1, SDL_GETEVENT, SDL_FIRSTEVENT, SDL_LASTEVENT)) { if (!cursor_hidden && av_gettime_relative() - cursor_last_shown > CURSOR_HIDE_DELAY) { SDL_ShowCursor(0); cursor_hidden = 1; } if (remaining_time > 0.0) av_usleep((int64_t)(remaining_time * 1000000.0)); remaining_time = REFRESH_RATE; if (is->show_mode != SHOW_MODE_NONE && (!is->paused || is->force_refresh)) video_refresh(is, &remaining_time); SDL_PumpEvents(); } } static void seek_chapter(VideoState *is, int incr) { int64_t pos = get_master_clock(is) * AV_TIME_BASE; int i; if (!is->ic->nb_chapters) return; /* find the current chapter */ for (i = 0; i ic->nb_chapters; i++) { AVChapter *ch = is->ic->chapters[i]; if (av_compare_ts(pos, AV_TIME_BASE_Q, ch->start, ch->time_base) = is->ic->nb_chapters) return; av_log(NULL, AV_LOG_VERBOSE, \"Seeking to chapter %d.\\n\", i); stream_seek(is, av_rescale_q(is->ic->chapters[i]->start, is->ic->chapters[i]->time_base, AV_TIME_BASE_Q), 0, 0); } /* handle an event sent by the GUI */ static void event_loop(VideoState *cur_stream) { SDL_Event event; double incr, pos, frac; for (;;) { double x; refresh_loop_wait_event(cur_stream, &event); switch (event.type) { case SDL_KEYDOWN: if (exit_on_keydown || event.key.keysym.sym == SDLK_ESCAPE || event.key.keysym.sym == SDLK_q) { do_exit(cur_stream); break; } // If we don't yet have a window, skip all key events, because read_thread might still be initializing... if (!cur_stream->width) continue; switch (event.key.keysym.sym) { case SDLK_f: toggle_full_screen(cur_stream); cur_stream->force_refresh = 1; break; case SDLK_p: case SDLK_SPACE: toggle_pause(cur_stream); break; case SDLK_m: toggle_mute(cur_stream); break; case SDLK_KP_MULTIPLY: case SDLK_0: update_volume(cur_stream, 1, SDL_VOLUME_STEP); break; case SDLK_KP_DIVIDE: case SDLK_9: update_volume(cur_stream, -1, SDL_VOLUME_STEP); break; case SDLK_s: // S: Step to next frame step_to_next_frame(cur_stream); break; case SDLK_a: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_AUDIO); break; case SDLK_v: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_VIDEO); break; case SDLK_c: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_VIDEO); stream_cycle_channel(cur_stream, AVMEDIA_TYPE_AUDIO); stream_cycle_channel(cur_stream, AVMEDIA_TYPE_SUBTITLE); break; case SDLK_t: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_SUBTITLE); break; case SDLK_w: #if CONFIG_AVFILTER if (cur_stream->show_mode == SHOW_MODE_VIDEO && cur_stream->vfilter_idx vfilter_idx >= nb_vfilters) cur_stream->vfilter_idx = 0; } else { cur_stream->vfilter_idx = 0; toggle_audio_display(cur_stream); } #else toggle_audio_display(cur_stream); #endif break; case SDLK_PAGEUP: if (cur_stream->ic->nb_chapters ic->nb_chapters video_stream >= 0) pos = frame_queue_last_pos(&cur_stream->pictq); if (pos audio_stream >= 0) pos = frame_queue_last_pos(&cur_stream->sampq); if (pos ic->pb); if (cur_stream->ic->bit_rate) incr *= cur_stream->ic->bit_rate / 8.0; else incr *= 180000.0; pos += incr; stream_seek(cur_stream, pos, incr, 1); } else { pos = get_master_clock(cur_stream); if (isnan(pos)) pos = (double)cur_stream->seek_pos / AV_TIME_BASE; pos += incr; if (cur_stream->ic->start_time != AV_NOPTS_VALUE && pos ic->start_time / (double)AV_TIME_BASE) pos = cur_stream->ic->start_time / (double)AV_TIME_BASE; stream_seek(cur_stream, (int64_t)(pos * AV_TIME_BASE), (int64_t)(incr * AV_TIME_BASE), 0); } break; default: break; } break; case SDL_MOUSEBUTTONDOWN: if (exit_on_mousedown) { do_exit(cur_stream); break; } if (event.button.button == SDL_BUTTON_LEFT) { static int64_t last_mouse_left_click = 0; if (av_gettime_relative() - last_mouse_left_click force_refresh = 1; last_mouse_left_click = 0; } else { last_mouse_left_click = av_gettime_relative(); } } case SDL_MOUSEMOTION: if (cursor_hidden) { SDL_ShowCursor(1); cursor_hidden = 0; } cursor_last_shown = av_gettime_relative(); if (event.type == SDL_MOUSEBUTTONDOWN) { if (event.button.button != SDL_BUTTON_RIGHT) break; x = event.button.x; } else { if (!(event.motion.state & SDL_BUTTON_RMASK)) break; x = event.motion.x; } if (seek_by_bytes || cur_stream->ic->duration ic->pb); stream_seek(cur_stream, size*x/cur_stream->width, 0, 1); } else { int64_t ts; int ns, hh, mm, ss; int tns, thh, tmm, tss; tns = cur_stream->ic->duration / 1000000LL; thh = tns / 3600; tmm = (tns % 3600) / 60; tss = (tns % 60); frac = x / cur_stream->width; ns = frac * tns; hh = ns / 3600; mm = (ns % 3600) / 60; ss = (ns % 60); av_log(NULL, AV_LOG_INFO, \"Seek to %2.0f%% (%2d:%02d:%02d) of total duration (%2d:%02d:%02d) \\n\", frac*100, hh, mm, ss, thh, tmm, tss); ts = frac * cur_stream->ic->duration; if (cur_stream->ic->start_time != AV_NOPTS_VALUE) ts += cur_stream->ic->start_time; stream_seek(cur_stream, ts, 0, 0); } break; case SDL_WINDOWEVENT: switch (event.window.event) { case SDL_WINDOWEVENT_SIZE_CHANGED: screen_width = cur_stream->width = event.window.data1; screen_height = cur_stream->height = event.window.data2; if (cur_stream->vis_texture) { SDL_DestroyTexture(cur_stream->vis_texture); cur_stream->vis_texture = NULL; } case SDL_WINDOWEVENT_EXPOSED: cur_stream->force_refresh = 1; } break; case SDL_QUIT: case FF_QUIT_EVENT: do_exit(cur_stream); break; default: break; } } } static int opt_frame_size(void *optctx, const char *opt, const char *arg) { av_log(NULL, AV_LOG_WARNING, \"Option -s is deprecated, use -video_size.\\n\"); return opt_default(NULL, \"video_size\", arg); } static int opt_width(void *optctx, const char *opt, const char *arg) { screen_width = parse_number_or_die(opt, arg, OPT_INT64, 1, INT_MAX); return 0; } static int opt_height(void *optctx, const char *opt, const char *arg) { screen_height = parse_number_or_die(opt, arg, OPT_INT64, 1, INT_MAX); return 0; } static int opt_format(void *optctx, const char *opt, const char *arg) { file_iformat = av_find_input_format(arg); if (!file_iformat) { av_log(NULL, AV_LOG_FATAL, \"Unknown input format: %s\\n\", arg); return AVERROR(EINVAL); } return 0; } static int opt_frame_pix_fmt(void *optctx, const char *opt, const char *arg) { av_log(NULL, AV_LOG_WARNING, \"Option -pix_fmt is deprecated, use -pixel_format.\\n\"); return opt_default(NULL, \"pixel_format\", arg); } static int opt_sync(void *optctx, const char *opt, const char *arg) { if (!strcmp(arg, \"audio\")) av_sync_type = AV_SYNC_AUDIO_MASTER; else if (!strcmp(arg, \"video\")) av_sync_type = AV_SYNC_VIDEO_MASTER; else if (!strcmp(arg, \"ext\")) av_sync_type = AV_SYNC_EXTERNAL_CLOCK; else { av_log(NULL, AV_LOG_ERROR, \"Unknown value for %s: %s\\n\", opt, arg); exit(1); } return 0; } static int opt_seek(void *optctx, const char *opt, const char *arg) { start_time = parse_time_or_die(opt, arg, 1); return 0; } static int opt_duration(void *optctx, const char *opt, const char *arg) { duration = parse_time_or_die(opt, arg, 1); return 0; } static int opt_show_mode(void *optctx, const char *opt, const char *arg) { show_mode = !strcmp(arg, \"video\") ? SHOW_MODE_VIDEO : !strcmp(arg, \"waves\") ? SHOW_MODE_WAVES : !strcmp(arg, \"rdft\" ) ? SHOW_MODE_RDFT : parse_number_or_die(opt, arg, OPT_INT, 0, SHOW_MODE_NB-1); return 0; } static void opt_input_file(void *optctx, const char *filename) { if (input_filename) { av_log(NULL, AV_LOG_FATAL, \"Argument '%s' provided as input filename, but '%s' was already specified.\\n\", filename, input_filename); exit(1); } if (!strcmp(filename, \"-\")) filename = \"pipe:\"; input_filename = filename; } static int opt_codec(void *optctx, const char *opt, const char *arg) { const char *spec = strchr(opt, ':'); if (!spec) { av_log(NULL, AV_LOG_ERROR, \"No media specifier was specified in '%s' in option '%s'\\n\", arg, opt); return AVERROR(EINVAL); } spec++; switch (spec[0]) { case 'a' : audio_codec_name = arg; break; case 's' : subtitle_codec_name = arg; break; case 'v' : video_codec_name = arg; break; default: av_log(NULL, AV_LOG_ERROR, \"Invalid media specifier '%s' in option '%s'\\n\", spec, opt); return AVERROR(EINVAL); } return 0; } static int dummy; static const OptionDef options[] = { CMDUTILS_COMMON_OPTIONS { \"x\", HAS_ARG, { .func_arg = opt_width }, \"force displayed width\", \"width\" }, { \"y\", HAS_ARG, { .func_arg = opt_height }, \"force displayed height\", \"height\" }, { \"s\", HAS_ARG | OPT_VIDEO, { .func_arg = opt_frame_size }, \"set frame size (WxH or abbreviation)\", \"size\" }, { \"fs\", OPT_BOOL, { &is_full_screen }, \"force full screen\" }, { \"an\", OPT_BOOL, { &audio_disable }, \"disable audio\" }, { \"vn\", OPT_BOOL, { &video_disable }, \"disable video\" }, { \"sn\", OPT_BOOL, { &subtitle_disable }, \"disable subtitling\" }, { \"ast\", OPT_STRING | HAS_ARG | OPT_EXPERT, { &wanted_stream_spec[AVMEDIA_TYPE_AUDIO] }, \"select desired audio stream\", \"stream_specifier\" }, { \"vst\", OPT_STRING | HAS_ARG | OPT_EXPERT, { &wanted_stream_spec[AVMEDIA_TYPE_VIDEO] }, \"select desired video stream\", \"stream_specifier\" }, { \"sst\", OPT_STRING | HAS_ARG | OPT_EXPERT, { &wanted_stream_spec[AVMEDIA_TYPE_SUBTITLE] }, \"select desired subtitle stream\", \"stream_specifier\" }, { \"ss\", HAS_ARG, { .func_arg = opt_seek }, \"seek to a given position in seconds\", \"pos\" }, { \"t\", HAS_ARG, { .func_arg = opt_duration }, \"play \\\"duration\\\" seconds of audio/video\", \"duration\" }, { \"bytes\", OPT_INT | HAS_ARG, { &seek_by_bytes }, \"seek by bytes 0=off 1=on -1=auto\", \"val\" }, { \"seek_interval\", OPT_FLOAT | HAS_ARG, { &seek_interval }, \"set seek interval for left/right keys, in seconds\", \"seconds\" }, { \"nodisp\", OPT_BOOL, { &display_disable }, \"disable graphical display\" }, { \"noborder\", OPT_BOOL, { &borderless }, \"borderless window\" }, { \"alwaysontop\", OPT_BOOL, { &alwaysontop }, \"window always on top\" }, { \"volume\", OPT_INT | HAS_ARG, { &startup_volume}, \"set startup volume 0=min 100=max\", \"volume\" }, { \"f\", HAS_ARG, { .func_arg = opt_format }, \"force format\", \"fmt\" }, { \"pix_fmt\", HAS_ARG | OPT_EXPERT | OPT_VIDEO, { .func_arg = opt_frame_pix_fmt }, \"set pixel format\", \"format\" }, { \"stats\", OPT_BOOL | OPT_EXPERT, { &show_status }, \"show status\", \"\" }, { \"fast\", OPT_BOOL | OPT_EXPERT, { &fast }, \"non spec compliant optimizations\", \"\" }, { \"genpts\", OPT_BOOL | OPT_EXPERT, { &genpts }, \"generate pts\", \"\" }, { \"drp\", OPT_INT | HAS_ARG | OPT_EXPERT, { &decoder_reorder_pts }, \"let decoder reorder pts 0=off 1=on -1=auto\", \"\"}, { \"lowres\", OPT_INT | HAS_ARG | OPT_EXPERT, { &lowres }, \"\", \"\" }, { \"sync\", HAS_ARG | OPT_EXPERT, { .func_arg = opt_sync }, \"set audio-video sync. type (type=audio/video/ext)\", \"type\" }, { \"autoexit\", OPT_BOOL | OPT_EXPERT, { &autoexit }, \"exit at the end\", \"\" }, { \"exitonkeydown\", OPT_BOOL | OPT_EXPERT, { &exit_on_keydown }, \"exit on key down\", \"\" }, { \"exitonmousedown\", OPT_BOOL | OPT_EXPERT, { &exit_on_mousedown }, \"exit on mouse down\", \"\" }, { \"loop\", OPT_INT | HAS_ARG | OPT_EXPERT, { &loop }, \"set number of times the playback shall be looped\", \"loop count\" }, { \"framedrop\", OPT_BOOL | OPT_EXPERT, { &framedrop }, \"drop frames when cpu is too slow\", \"\" }, { \"infbuf\", OPT_BOOL | OPT_EXPERT, { &infinite_buffer }, \"don't limit the input buffer size (useful with realtime streams)\", \"\" }, { \"window_title\", OPT_STRING | HAS_ARG, { &window_title }, \"set window title\", \"window title\" }, { \"left\", OPT_INT | HAS_ARG | OPT_EXPERT, { &screen_left }, \"set the x position for the left of the window\", \"x pos\" }, { \"top\", OPT_INT | HAS_ARG | OPT_EXPERT, { &screen_top }, \"set the y position for the top of the window\", \"y pos\" }, #if CONFIG_AVFILTER { \"vf\", OPT_EXPERT | HAS_ARG, { .func_arg = opt_add_vfilter }, \"set video filters\", \"filter_graph\" }, { \"af\", OPT_STRING | HAS_ARG, { &afilters }, \"set audio filters\", \"filter_graph\" }, #endif { \"rdftspeed\", OPT_INT | HAS_ARG| OPT_AUDIO | OPT_EXPERT, { &rdftspeed }, \"rdft speed\", \"msecs\" }, { \"showmode\", HAS_ARG, { .func_arg = opt_show_mode}, \"select show mode (0 = video, 1 = waves, 2 = RDFT)\", \"mode\" }, { \"default\", HAS_ARG | OPT_AUDIO | OPT_VIDEO | OPT_EXPERT, { .func_arg = opt_default }, \"generic catch all option\", \"\" }, { \"i\", OPT_BOOL, { &dummy}, \"read specified file\", \"input_file\"}, { \"codec\", HAS_ARG, { .func_arg = opt_codec}, \"force decoder\", \"decoder_name\" }, { \"acodec\", HAS_ARG | OPT_STRING | OPT_EXPERT, { &audio_codec_name }, \"force audio decoder\", \"decoder_name\" }, { \"scodec\", HAS_ARG | OPT_STRING | OPT_EXPERT, { &subtitle_codec_name }, \"force subtitle decoder\", \"decoder_name\" }, { \"vcodec\", HAS_ARG | OPT_STRING | OPT_EXPERT, { &video_codec_name }, \"force video decoder\", \"decoder_name\" }, { \"autorotate\", OPT_BOOL, { &autorotate }, \"automatically rotate video\", \"\" }, { \"find_stream_info\", OPT_BOOL | OPT_INPUT | OPT_EXPERT, { &find_stream_info }, \"read and decode the streams to fill missing information with heuristics\" }, { \"filter_threads\", HAS_ARG | OPT_INT | OPT_EXPERT, { &filter_nbthreads }, \"number of filter threads per graph\" }, { NULL, }, }; static void show_usage(void) { av_log(NULL, AV_LOG_INFO, \"Simple media player\\n\"); av_log(NULL, AV_LOG_INFO, \"usage: %s [options] input_file\\n\", program_name); av_log(NULL, AV_LOG_INFO, \"\\n\"); } void show_help_default(const char *opt, const char *arg) { av_log_set_callback(log_callback_help); show_usage(); show_help_options(options, \"Main options:\", 0, OPT_EXPERT, 0); show_help_options(options, \"Advanced options:\", OPT_EXPERT, 0, 0); printf(\"\\n\"); show_help_children(avcodec_get_class(), AV_OPT_FLAG_DECODING_PARAM); show_help_children(avformat_get_class(), AV_OPT_FLAG_DECODING_PARAM); #if !CONFIG_AVFILTER show_help_children(sws_get_class(), AV_OPT_FLAG_ENCODING_PARAM); #else show_help_children(avfilter_get_class(), AV_OPT_FLAG_FILTERING_PARAM); #endif printf(\"\\nWhile playing:\\n\" \"q, ESC quit\\n\" \"f toggle full screen\\n\" \"p, SPC pause\\n\" \"m toggle mute\\n\" \"9, 0 decrease and increase volume respectively\\n\" \"/, * decrease and increase volume respectively\\n\" \"a cycle audio channel in the current program\\n\" \"v cycle video channel\\n\" \"t cycle subtitle channel in the current program\\n\" \"c cycle program\\n\" \"w cycle video filters or show modes\\n\" \"s activate frame-step mode\\n\" \"left/right seek backward/forward 10 seconds or to custom interval if -seek_interval is set\\n\" \"down/up seek backward/forward 1 minute\\n\" \"page down/page up seek backward/forward 10 minutes\\n\" \"right mouse click seek to percentage in file corresponding to fraction of width\\n\" \"left double-click toggle full screen\\n\" ); } /* Called from the main */ int main(int argc, char **argv) { int flags; VideoState *is; init_dynload(); av_log_set_flags(AV_LOG_SKIP_REPEATED); parse_loglevel(argc, argv, options); /* register all codecs, demux and protocols */ #if CONFIG_AVDEVICE avdevice_register_all(); #endif avformat_network_init(); init_opts(); signal(SIGINT , sigterm_handler); /* Interrupt (ANSI). */ signal(SIGTERM, sigterm_handler); /* Termination (ANSI). */ show_banner(argc, argv, options); parse_options(NULL, argc, argv, options, opt_input_file); if (!input_filename) { show_usage(); av_log(NULL, AV_LOG_FATAL, \"An input file must be specified\\n\"); av_log(NULL, AV_LOG_FATAL, \"Use -h to get full help or, even better, run 'man %s'\\n\", program_name); exit(1); } if (display_disable) { video_disable = 1; } flags = SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER; if (audio_disable) flags &= ~SDL_INIT_AUDIO; else { /* Try to work around an occasional ALSA buffer underflow issue when the * period size is NPOT due to ALSA resampling by forcing the buffer size. */ if (!SDL_getenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\")) SDL_setenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\",\"1\", 1); } if (display_disable) flags &= ~SDL_INIT_VIDEO; if (SDL_Init (flags)) { av_log(NULL, AV_LOG_FATAL, \"Could not initialize SDL - %s\\n\", SDL_GetError()); av_log(NULL, AV_LOG_FATAL, \"(Did you set the DISPLAY variable?)\\n\"); exit(1); } SDL_EventState(SDL_SYSWMEVENT, SDL_IGNORE); SDL_EventState(SDL_USEREVENT, SDL_IGNORE); av_init_packet(&flush_pkt); flush_pkt.data = (uint8_t *)&flush_pkt; if (!display_disable) { int flags = SDL_WINDOW_HIDDEN; if (alwaysontop) #if SDL_VERSION_ATLEAST(2,0,5) flags |= SDL_WINDOW_ALWAYS_ON_TOP; #else av_log(NULL, AV_LOG_WARNING, \"Your SDL version doesn't support SDL_WINDOW_ALWAYS_ON_TOP. Feature will be inactive.\\n\"); #endif if (borderless) flags |= SDL_WINDOW_BORDERLESS; else flags |= SDL_WINDOW_RESIZABLE; window = SDL_CreateWindow(program_name, SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED, default_width, default_height, flags); SDL_SetHint(SDL_HINT_RENDER_SCALE_QUALITY, \"linear\"); if (window) { renderer = SDL_CreateRenderer(window, -1, SDL_RENDERER_ACCELERATED | SDL_RENDERER_PRESENTVSYNC); if (!renderer) { av_log(NULL, AV_LOG_WARNING, \"Failed to initialize a hardware accelerated renderer: %s\\n\", SDL_GetError()); renderer = SDL_CreateRenderer(window, -1, 0); } if (renderer) { if (!SDL_GetRendererInfo(renderer, &renderer_info)) av_log(NULL, AV_LOG_VERBOSE, \"Initialized %s renderer.\\n\", renderer_info.name); } } if (!window || !renderer || !renderer_info.num_texture_formats) { av_log(NULL, AV_LOG_FATAL, \"Failed to create window or renderer: %s\", SDL_GetError()); do_exit(NULL); } } is = stream_open(input_filename, file_iformat); if (!is) { av_log(NULL, AV_LOG_FATAL, \"Failed to initialize VideoState!\\n\"); do_exit(NULL); } event_loop(is); /* never returns */ return 0; } "},"pages/FFmpeg/ffplay源码_main函数_do_exit函数.html":{"url":"pages/FFmpeg/ffplay源码_main函数_do_exit函数.html","title":"ffplay源码-main函数，do_exit函数","keywords":"","body":"ffplay源码-main函数，do_exit函数 iOS工程代码 源代码一览 #import #include \"libavutil/avstring.h\" #include \"libavutil/eval.h\" #include \"libavutil/mathematics.h\" #include \"libavutil/pixdesc.h\" #include \"libavutil/imgutils.h\" #include \"libavutil/dict.h\" #include \"libavutil/parseutils.h\" #include \"libavutil/samplefmt.h\" #include \"libavutil/avassert.h\" #include \"libavutil/time.h\" #include \"libavformat/avformat.h\" #include \"libavdevice/avdevice.h\" #include \"libswscale/swscale.h\" #include \"libavutil/opt.h\" #include \"libavcodec/avfft.h\" #include \"libswresample/swresample.h\" #include #include const char program_name[] = \"ffplay\"; /* options specified by the user */ static AVInputFormat *file_iformat; static const char *input_filename; static int default_width = 640; static int default_height = 480; /* current context */ static AVPacket flush_pkt; static SDL_Window *window; static SDL_Renderer *renderer; static SDL_RendererInfo renderer_info = {0}; typedef struct VideoState { } VideoState; static void stream_close(VideoState *is) { // TODO: stream_close } static void do_exit(VideoState *is) { if (is) { stream_close(is); } if (renderer) SDL_DestroyRenderer(renderer); if (window) SDL_DestroyWindow(window); // uninit_opts(); //#if CONFIG_AVFILTER // av_freep(&vfilters_list); //#endif // avformat_network_deinit(); // if (show_status) // printf(\"\\n\"); SDL_Quit(); av_log(NULL, AV_LOG_QUIET, \"%s\", \"\"); exit(0); } static VideoState *stream_open(const char *filename, AVInputFormat *iformat) { // TODO: stream_open return NULL; } /* handle an event sent by the GUI */ static void event_loop(VideoState *cur_stream) { // TODO: event_loop } int main(int argc, char *argv[]) { int flags; VideoState *is; // 动态加载的初始化，这是Windows平台的dll库相关处理； // https://blog.csdn.net/ericbar/article/details/79541420 // init_dynload(); // 设置打印的标记，AV_LOG_SKIP_REPEATED表示对于重复打印的语句，不重复输出； // https://blog.csdn.net/ericbar/article/details/79541420 av_log_set_flags(AV_LOG_SKIP_REPEATED); // 使命令行'-loglevel'生效 // parse_loglevel(argc, argv, options); /* register all codecs, demux and protocols */ //#if CONFIG_AVDEVICE // 在使用libavdevice之前，必须先运行avdevice_register_all()对设备进行注册，否则就会出错 // https://blog.csdn.net/leixiaohua1020/article/details/41211121 avdevice_register_all(); //#endif // 打开网络流的话，前面要加上函数 // avformat_network_init(); // Initialize the cmdutils option system, in particular allocate the *_opts contexts. // 初始化 cmdutils 选项系统，特别是分配 *_opts 上下文。 // init_opts(); // signal(SIGINT , sigterm_handler); /* Interrupt (ANSI). */ // signal(SIGTERM, sigterm_handler); /* Termination (ANSI). */ // 将程序横幅打印到 stderr。 横幅内容取决于当前版本的存储库和程序使用的 libav* 库。 // Print the program banner to stderr. The banner contents depend // on the current version of the repository and of the libav* libraries used by // the program. // show_banner(argc, argv, options); // parse_options(NULL, argc, argv, options, opt_input_file); // input_filename：命令行 -i 指定，视频路径 NSString *inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"mov\"]; input_filename = [inPath UTF8String]; if (!input_filename) { // show_usage(); // av_log(NULL, AV_LOG_FATAL, \"An input file must be specified\\n\"); // av_log(NULL, AV_LOG_FATAL, // \"Use -h to get full help or, even better, run 'man %s'\\n\", program_name); exit(1); } // display_disable：命令行 -nodisp 指定，不渲染画面不播放声音 // if (display_disable) { // video_disable = 1; // } flags = SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER; // audio_disable：命令行 -an 指定，渲染画面不播放声音 // if (audio_disable) // flags &= ~SDL_INIT_AUDIO; // else { // /* Try to work around an occasional ALSA buffer underflow issue when the // * period size is NPOT due to ALSA resampling by forcing the buffer size. */ // if (!SDL_getenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\")) // SDL_setenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\",\"1\", 1); // } // if (display_disable) // flags &= ~SDL_INIT_VIDEO; // 指定flags，SDL初始化 if (SDL_Init (flags)) { av_log(NULL, AV_LOG_FATAL, \"Could not initialize SDL - %s\\n\", SDL_GetError()); av_log(NULL, AV_LOG_FATAL, \"(Did you set the DISPLAY variable?)\\n\"); exit(1); } // 禁用一些事件 SDL_EventState(SDL_SYSWMEVENT, SDL_IGNORE); SDL_EventState(SDL_USEREVENT, SDL_IGNORE); av_init_packet(&flush_pkt); flush_pkt.data = (uint8_t *)&flush_pkt; if (1/**!display_disable*/) { int flags = SDL_WINDOW_HIDDEN; // if (alwaysontop) #if SDL_VERSION_ATLEAST(2,0,5) flags |= SDL_WINDOW_ALWAYS_ON_TOP; #else av_log(NULL, AV_LOG_WARNING, \"Your SDL version doesn't support SDL_WINDOW_ALWAYS_ON_TOP. Feature will be inactive.\\n\"); #endif // borderless：命令行 -noborder 指定，没有边框 // if (borderless) // flags |= SDL_WINDOW_BORDERLESS; // else // 可以自由拉伸 flags |= SDL_WINDOW_RESIZABLE; window = SDL_CreateWindow(program_name, SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED, default_width, default_height, flags); // \"0\" or \"nearest\" - Nearest pixel sampling // \"1\" or \"linear\" - Linear filtering (supported by OpenGL and Direct3D) // \"2\" or \"best\" - Currently this is the same as \"linear\" SDL_SetHint(SDL_HINT_RENDER_SCALE_QUALITY, \"linear\"); if (window) { renderer = SDL_CreateRenderer(window, -1, SDL_RENDERER_ACCELERATED | SDL_RENDERER_PRESENTVSYNC); if (!renderer) { av_log(NULL, AV_LOG_WARNING, \"Failed to initialize a hardware accelerated renderer: %s\\n\", SDL_GetError()); renderer = SDL_CreateRenderer(window, -1, 0); } if (renderer) { if (!SDL_GetRendererInfo(renderer, &renderer_info)) av_log(NULL, AV_LOG_VERBOSE, \"Initialized %s renderer.\\n\", renderer_info.name); } } if (!window || !renderer || !renderer_info.num_texture_formats) { av_log(NULL, AV_LOG_FATAL, \"Failed to create window or renderer: %s\", SDL_GetError()); do_exit(NULL); } } is = stream_open(input_filename, file_iformat); if (!is) { av_log(NULL, AV_LOG_FATAL, \"Failed to initialize VideoState!\\n\"); do_exit(NULL); } event_loop(is); /* never returns */ return 0; } "},"pages/FFmpeg/ffplay源码_event_loop函数_refresh_loop_wait_event函数.html":{"url":"pages/FFmpeg/ffplay源码_event_loop函数_refresh_loop_wait_event函数.html","title":"ffplay源码-event_loop函数、refresh_loop_wait_event函数","keywords":"","body":"ffplay源码-event_loop函数、refresh_loop_wait_event函数 iOS工程代码 源代码一览 // // main.m // iOSFFmpegSDLFastForwardAndBackward // // Created by 陈长青 on 2022/5/8. // #import #include \"libavutil/avstring.h\" #include \"libavutil/eval.h\" #include \"libavutil/mathematics.h\" #include \"libavutil/pixdesc.h\" #include \"libavutil/imgutils.h\" #include \"libavutil/dict.h\" #include \"libavutil/parseutils.h\" #include \"libavutil/samplefmt.h\" #include \"libavutil/avassert.h\" #include \"libavutil/time.h\" #include \"libavformat/avformat.h\" #include \"libavdevice/avdevice.h\" #include \"libswscale/swscale.h\" #include \"libavutil/opt.h\" #include \"libavcodec/avfft.h\" #include \"libswresample/swresample.h\" #include #include const char program_name[] = \"ffplay\"; /* Step size for volume control in dB */ #define SDL_VOLUME_STEP (0.75) /* polls for possible required screen refresh at least this often, should be less than 1/fps */ #define REFRESH_RATE 0.01 #define CURSOR_HIDE_DELAY 1000000 /* options specified by the user */ static AVInputFormat *file_iformat; static const char *input_filename; static int default_width = 640; static int default_height = 480; static int screen_width = 0; static int screen_height = 0; static int cursor_hidden = 0; static int64_t cursor_last_shown; /* current context */ // 命令行 -fs 指定，控制是否全屏显示 static int is_full_screen; static AVPacket flush_pkt; #define FF_QUIT_EVENT (SDL_USEREVENT + 2) static SDL_Window *window; static SDL_Renderer *renderer; static SDL_RendererInfo renderer_info = {0}; // MARK: 视频状态 typedef struct VideoState { int force_refresh; int paused; int seek_req; int seek_flags; int64_t seek_pos; int64_t seek_rel; AVFormatContext *ic; int muted; int width, height, xleft, ytop; int step; // 命令行 -showmode 指定 enum ShowMode { SHOW_MODE_NONE = -1, SHOW_MODE_VIDEO = 0, SHOW_MODE_WAVES, SHOW_MODE_RDFT, SHOW_MODE_NB } show_mode; SDL_Texture *vis_texture; SDL_cond *continue_read_thread; } VideoState; // MARK: 关闭码流 static void stream_close(VideoState *is) { // TODO: stream_close } // MARK: 打开码流 static VideoState *stream_open(const char *filename, AVInputFormat *iformat) { // TODO: stream_open VideoState *is; is = av_mallocz(sizeof(VideoState)); if (!is) return NULL; return is; } // MARK: 切换码流 static void stream_cycle_channel(VideoState *is, int codec_type) { // TODO: stream_cycle_channel } // MARK: 退出 static void do_exit(VideoState *is) { if (is) { stream_close(is); } if (renderer) SDL_DestroyRenderer(renderer); if (window) SDL_DestroyWindow(window); // uninit_opts(); //#if CONFIG_AVFILTER // av_freep(&vfilters_list); //#endif // avformat_network_deinit(); // if (show_status) // printf(\"\\n\"); SDL_Quit(); av_log(NULL, AV_LOG_QUIET, \"%s\", \"\"); exit(0); } // MARK: 刷新视频 /* called to display each frame */ static void video_refresh(void *opaque, double *remaining_time) { // TODO: video_refresh // av_log(NULL, AV_LOG_INFO, \"player，刷新视频\\n\"); } static void toggle_full_screen(VideoState *is) { is_full_screen = !is_full_screen; SDL_SetWindowFullscreen(window, is_full_screen ? SDL_WINDOW_FULLSCREEN_DESKTOP : 0); } // MARK: 主时钟 /* get the current master clock value */ static double get_master_clock(VideoState *is) { // TODO: get_master_clock return 0; } // MARK: Seek /* seek in the stream */ static void stream_seek(VideoState *is, int64_t pos, int64_t rel, int seek_by_bytes) { if (!is->seek_req) { is->seek_pos = pos; is->seek_rel = rel; is->seek_flags &= ~AVSEEK_FLAG_BYTE; if (seek_by_bytes) is->seek_flags |= AVSEEK_FLAG_BYTE; is->seek_req = 1; SDL_CondSignal(is->continue_read_thread); } } // MARK: 暂停2 /* pause or resume the video */ static void stream_toggle_pause(VideoState *is) { // TODO: stream_toggle_pause } // MARK: 暂停 static void toggle_pause(VideoState *is) { stream_toggle_pause(is); is->step = 0; } // MARK: 禁音 static void toggle_mute(VideoState *is) { is->muted = !is->muted; } // MARK: 调声音 static void update_volume(VideoState *is, int sign, double step) { // TODO: update_volume } // MARK: 进入下一帧 static void step_to_next_frame(VideoState *is) { /* if the stream is paused unpause it, then step */ if (is->paused) stream_toggle_pause(is); is->step = 1; } // MARK: SDL事件 /** * SDL 事件 * * 循环检测并优先处理用户输入事件 * 内置刷新率控制，约10ms刷新一次 * https://blog.csdn.net/qq_36783046/article/details/88706162 */ static void refresh_loop_wait_event(VideoState *is, SDL_Event *event) { double remaining_time = 0.0; /* 从输入设备收集事件并放到事件队列中 */ SDL_PumpEvents(); /** * SDL_PeepEvents * 从事件队列中提取事件，由于这里使用的是SDL_GETEVENT, 所以获取事件时会从队列中移除 * 如果有事件发生，返回事件数量，则while循环不执行。 * 如果出错，返回负数的错误码，则while循环不执行。 * 如果当前没有事件发生，且没有出错，返回0，进入while循环。 */ while (!SDL_PeepEvents(event, 1, SDL_GETEVENT, SDL_FIRSTEVENT, SDL_LASTEVENT)) { /* 隐藏鼠标指针， CURSOR_HIDE_DELAY = 1s */ if (!cursor_hidden && av_gettime_relative() - cursor_last_shown > CURSOR_HIDE_DELAY) { SDL_ShowCursor(0); cursor_hidden = 1; } /* 默认屏幕刷新率控制，REFRESH_RATE = 10ms */ if (remaining_time > 0.0) av_usleep((int64_t)(remaining_time * 1000000.0)); remaining_time = REFRESH_RATE; /* 显示视频 */ if (is->show_mode != SHOW_MODE_NONE && (!is->paused || is->force_refresh)) video_refresh(is, &remaining_time); /* 再次检测输入事件 */ SDL_PumpEvents(); } } // MARK: 事件循环 /* handle an event sent by the GUI */ static void event_loop(VideoState *cur_stream) { SDL_Event event; double incr, pos, frac; for (;;) { double x; refresh_loop_wait_event(cur_stream, &event); switch (event.type) { // 按键按下事件 case SDL_KEYDOWN: // 按esc,q退出 if (1/**exit_on_keydown*/ || event.key.keysym.sym == SDLK_ESCAPE || event.key.keysym.sym == SDLK_q) { do_exit(cur_stream); break; } // If we don't yet have a window, skip all key events, because read_thread might still be initializing... if (!cur_stream->width) continue; switch (event.key.keysym.sym) { // 按F键，全屏 case SDLK_f: toggle_full_screen(cur_stream); // 调用video_refresh()刷新视频 cur_stream->force_refresh = 1; break; // 按P、SPACE键，暂停 case SDLK_p: case SDLK_SPACE: toggle_pause(cur_stream); break; // 按M键，静音 case SDLK_m: toggle_mute(cur_stream); break; // 按+、0键，增加音量 // https://blog.csdn.net/huzhifei/article/details/112682390 case SDLK_KP_MULTIPLY: case SDLK_0: update_volume(cur_stream, 1, SDL_VOLUME_STEP); break; // 按-、9键，减小音量 case SDLK_KP_DIVIDE: case SDLK_9: update_volume(cur_stream, -1, SDL_VOLUME_STEP); break; // 按S键，下一帧 case SDLK_s: // S: Step to next frame step_to_next_frame(cur_stream); break; // 按A键，切换音频流 case SDLK_a: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_AUDIO); break; // 按V键，切换视频流 case SDLK_v: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_VIDEO); break; // 按C键，循环切换节目（切换音频、视频、字幕流） case SDLK_c: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_VIDEO); stream_cycle_channel(cur_stream, AVMEDIA_TYPE_AUDIO); stream_cycle_channel(cur_stream, AVMEDIA_TYPE_SUBTITLE); break; // 按T键，切换字幕流 case SDLK_t: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_SUBTITLE); break; // 按W键，循环切换过滤器或显示模式 case SDLK_w: //#if CONFIG_AVFILTER // if (cur_stream->show_mode == SHOW_MODE_VIDEO && cur_stream->vfilter_idx vfilter_idx >= nb_vfilters) // cur_stream->vfilter_idx = 0; // } else { // cur_stream->vfilter_idx = 0; // toggle_audio_display(cur_stream); // } //#else // toggle_audio_display(cur_stream); //#endif break; // mac上好像没找到这个键 case SDLK_PAGEUP: // 如果只有一个视频则向前10分钟 // if (cur_stream->ic->nb_chapters ic->nb_chapters video_stream >= 0) // pos = frame_queue_last_pos(&cur_stream->pictq); // if (pos audio_stream >= 0) // pos = frame_queue_last_pos(&cur_stream->sampq); // if (pos ic->pb); // if (cur_stream->ic->bit_rate) // incr *= cur_stream->ic->bit_rate / 8.0; // else // incr *= 180000.0; // pos += incr; // stream_seek(cur_stream, pos, incr, 1); // } else { pos = get_master_clock(cur_stream); if (isnan(pos)) pos = (double)cur_stream->seek_pos / AV_TIME_BASE; pos += incr; if (cur_stream->ic->start_time != AV_NOPTS_VALUE && pos ic->start_time / (double)AV_TIME_BASE) pos = cur_stream->ic->start_time / (double)AV_TIME_BASE; stream_seek(cur_stream, (int64_t)(pos * AV_TIME_BASE), (int64_t)(incr * AV_TIME_BASE), 0); // } break; default: break; } break; case SDL_MOUSEBUTTONDOWN: // exit_on_mousedown：命令行 -exitonmousedown 指定，鼠标单击左键退出 // if (exit_on_mousedown) { // do_exit(cur_stream); // break; // } // 双击左键全屏 if (event.button.button == SDL_BUTTON_LEFT) { static int64_t last_mouse_left_click = 0; if (av_gettime_relative() - last_mouse_left_click force_refresh = 1; last_mouse_left_click = 0; } else { last_mouse_left_click = av_gettime_relative(); } } // 鼠标移动事件，执行Seek（暂时不会用） case SDL_MOUSEMOTION: // if (cursor_hidden) { // SDL_ShowCursor(1); // cursor_hidden = 0; // } // cursor_last_shown = av_gettime_relative(); // if (event.type == SDL_MOUSEBUTTONDOWN) { // if (event.button.button != SDL_BUTTON_RIGHT) // break; // x = event.button.x; // } else { // if (!(event.motion.state & SDL_BUTTON_RMASK)) // break; // x = event.motion.x; // } // if (seek_by_bytes || cur_stream->ic->duration ic->pb); // stream_seek(cur_stream, size*x/cur_stream->width, 0, 1); // } else { // int64_t ts; // int ns, hh, mm, ss; // int tns, thh, tmm, tss; // tns = cur_stream->ic->duration / 1000000LL; // thh = tns / 3600; // tmm = (tns % 3600) / 60; // tss = (tns % 60); // frac = x / cur_stream->width; // ns = frac * tns; // hh = ns / 3600; // mm = (ns % 3600) / 60; // ss = (ns % 60); // av_log(NULL, AV_LOG_INFO, // \"Seek to %2.0f%% (%2d:%02d:%02d) of total duration (%2d:%02d:%02d) \\n\", frac*100, // hh, mm, ss, thh, tmm, tss); // ts = frac * cur_stream->ic->duration; // if (cur_stream->ic->start_time != AV_NOPTS_VALUE) // ts += cur_stream->ic->start_time; // stream_seek(cur_stream, ts, 0, 0); // } break; case SDL_WINDOWEVENT: switch (event.window.event) { case SDL_WINDOWEVENT_SIZE_CHANGED: screen_width = cur_stream->width = event.window.data1; screen_height = cur_stream->height = event.window.data2; if (cur_stream->vis_texture) { SDL_DestroyTexture(cur_stream->vis_texture); cur_stream->vis_texture = NULL; } case SDL_WINDOWEVENT_EXPOSED: cur_stream->force_refresh = 1; } break; case SDL_QUIT: case FF_QUIT_EVENT: do_exit(cur_stream); break; default: break; } } } // MARK: 入口函数 int main(int argc, char *argv[]) { int flags; VideoState *is; // 动态加载的初始化，这是Windows平台的dll库相关处理； // https://blog.csdn.net/ericbar/article/details/79541420 // init_dynload(); // 设置打印的标记，AV_LOG_SKIP_REPEATED表示对于重复打印的语句，不重复输出； // https://blog.csdn.net/ericbar/article/details/79541420 av_log_set_flags(AV_LOG_SKIP_REPEATED); // 使命令行'-loglevel'生效 // parse_loglevel(argc, argv, options); /* register all codecs, demux and protocols */ //#if CONFIG_AVDEVICE // 在使用libavdevice之前，必须先运行avdevice_register_all()对设备进行注册，否则就会出错 // https://blog.csdn.net/leixiaohua1020/article/details/41211121 avdevice_register_all(); //#endif // 打开网络流的话，前面要加上函数 // avformat_network_init(); // Initialize the cmdutils option system, in particular allocate the *_opts contexts. // 初始化 cmdutils 选项系统，特别是分配 *_opts 上下文。 // init_opts(); // signal(SIGINT , sigterm_handler); /* Interrupt (ANSI). */ // signal(SIGTERM, sigterm_handler); /* Termination (ANSI). */ // 将程序横幅打印到 stderr。 横幅内容取决于当前版本的存储库和程序使用的 libav* 库。 // Print the program banner to stderr. The banner contents depend // on the current version of the repository and of the libav* libraries used by // the program. // show_banner(argc, argv, options); // parse_options(NULL, argc, argv, options, opt_input_file); // input_filename：命令行 -i 指定，视频路径 NSString *inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"mov\"]; input_filename = [inPath UTF8String]; if (!input_filename) { // show_usage(); // av_log(NULL, AV_LOG_FATAL, \"An input file must be specified\\n\"); // av_log(NULL, AV_LOG_FATAL, // \"Use -h to get full help or, even better, run 'man %s'\\n\", program_name); exit(1); } // display_disable：命令行 -nodisp 指定，不渲染画面不播放声音 // if (display_disable) { // video_disable = 1; // } flags = SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER; // audio_disable：命令行 -an 指定，渲染画面不播放声音 // if (audio_disable) // flags &= ~SDL_INIT_AUDIO; // else { // /* Try to work around an occasional ALSA buffer underflow issue when the // * period size is NPOT due to ALSA resampling by forcing the buffer size. */ // if (!SDL_getenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\")) // SDL_setenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\",\"1\", 1); // } // if (display_disable) // flags &= ~SDL_INIT_VIDEO; // 指定flags，SDL初始化 if (SDL_Init (flags)) { av_log(NULL, AV_LOG_FATAL, \"Could not initialize SDL - %s\\n\", SDL_GetError()); av_log(NULL, AV_LOG_FATAL, \"(Did you set the DISPLAY variable?)\\n\"); exit(1); } // 禁用一些事件 SDL_EventState(SDL_SYSWMEVENT, SDL_IGNORE); SDL_EventState(SDL_USEREVENT, SDL_IGNORE); av_init_packet(&flush_pkt); flush_pkt.data = (uint8_t *)&flush_pkt; if (1/**!display_disable*/) { int flags = SDL_WINDOW_HIDDEN; // if (alwaysontop) #if SDL_VERSION_ATLEAST(2,0,5) flags |= SDL_WINDOW_ALWAYS_ON_TOP; #else av_log(NULL, AV_LOG_WARNING, \"Your SDL version doesn't support SDL_WINDOW_ALWAYS_ON_TOP. Feature will be inactive.\\n\"); #endif // borderless：命令行 -noborder 指定，没有边框 // if (borderless) // flags |= SDL_WINDOW_BORDERLESS; // else // 可以自由拉伸 flags |= SDL_WINDOW_RESIZABLE; window = SDL_CreateWindow(program_name, SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED, default_width, default_height, flags); // \"0\" or \"nearest\" - Nearest pixel sampling // \"1\" or \"linear\" - Linear filtering (supported by OpenGL and Direct3D) // \"2\" or \"best\" - Currently this is the same as \"linear\" SDL_SetHint(SDL_HINT_RENDER_SCALE_QUALITY, \"linear\"); if (window) { renderer = SDL_CreateRenderer(window, -1, SDL_RENDERER_ACCELERATED | SDL_RENDERER_PRESENTVSYNC); if (!renderer) { av_log(NULL, AV_LOG_WARNING, \"Failed to initialize a hardware accelerated renderer: %s\\n\", SDL_GetError()); renderer = SDL_CreateRenderer(window, -1, 0); } if (renderer) { if (!SDL_GetRendererInfo(renderer, &renderer_info)) av_log(NULL, AV_LOG_VERBOSE, \"Initialized %s renderer.\\n\", renderer_info.name); } } if (!window || !renderer || !renderer_info.num_texture_formats) { av_log(NULL, AV_LOG_FATAL, \"Failed to create window or renderer: %s\", SDL_GetError()); do_exit(NULL); } } is = stream_open(input_filename, file_iformat); if (!is) { av_log(NULL, AV_LOG_FATAL, \"Failed to initialize VideoState!\\n\"); do_exit(NULL); } event_loop(is); /* never returns */ return 0; } "},"pages/OpenGL系列/01_OpenGL渲染结构与常用图元.html":{"url":"pages/OpenGL系列/01_OpenGL渲染结构与常用图元.html","title":"1.OpenGL渲染结构与常用图元","keywords":"","body":"1.OpenGL渲染结构与常用图元 学习内容 掌握OpenGL渲染基础架构 如何使用7种OpenGL几何图元 如何使⽤存储着⾊器 如何使用Uniform属性 如何使用GLBatch帮助类创建⼏何图形 一、OpenGL 与 着色器 在OpenGL 3.0之前，OpenGL 包含一个固定功能的管线，它可以在不使用 着⾊器的情况下处理理⼏何与像素数据。在3.1版本开始，固定管线从核心 模式去掉。因此现在需要使用着色器来完成⼯作。 使⽤用OpenGL 来说，我们会使用GLSL,(OpenGL Shading Langruage，它是 在OpenGL 2.0版本发布的)。 语法与“C、C++”类似。 二、基础图形管线 OpenGL 中的图元只不过是顶点的集合以预定义的方式结合一起罢了。 例如:⼀个单独的点就是⼀个图元。它只需要⼀个顶点 2.1 OpenGL 渲染管线简化版本 客户机、服务器 管线分为上下2部分，上部分是客户端，而下半部分则是服务端。 客户端是存储在CPU存储器中的，并且在应用程序中执行，或者在主系 统内存的驱动程序中执⾏。驱动程序会将渲染命令和数组组合起来，发送给服务器器执⾏!(在⼀台典型的个人计算机上，服务器就是实际上就 是图形加速卡上的硬件和内存) 服务器 和 客户机在功能上也是异步的。 它们是各⾃独立的软件块或硬件块。我们是希望它们2个端都尽量在不停的工作。客户端不断的把数据块和命令块组合在一起输送到缓冲区，然后缓冲区就会发送到服务器 执行。 如果服务器停止⼯作等待客户机，或者客户机停止工作来等待服务器做 好接受更多的命令和准备，我们把这种情况成为管线停滞。 着色器 上图的Vertex Shader(顶点着⾊器) 和 Fragment Shader(⽚段着色器)。 着⾊器是使⽤用GLSL编写的程序，看起来与C语⾔非常类似。 着⾊器必 须从源代码中编译和链接在一起。最终准备就绪的着⾊器程序 顶点着⾊器-->处理从客户机输⼊的数据、应⽤变换、进行其他的类型 的数学运算来计算关照效果、位移、颜色值等。(**为了渲染共有3个顶点的三⻆形，顶点着⾊器将执行3次，也就是为了每个顶点执行⼀次)在⽬前的硬件上有多个执行单元同时运行，就意味着所有的3个顶点可以同时进⾏处理! 图上(primitive Assembly 说明的是:3个顶点已经组合在一起，⽽三⻆ 形已经逐个片段的进⾏了光栅化。每个⽚段通过执⾏⽚元着⾊器进行 填充。⽚元着⾊器会输出我们将屏幕上看到的最终颜色值。 重点! 我们必须在这之前为着⾊色器器提供数据，否则什什么都⽆无法实现! 有3种向OpenGL 着⾊色器器传递渲染数据的⽅方法可供我们选择 1.属性 2.uniform 值 3.纹理理 三、属性、uniform值、纹理理、输出 3.1 属性 属性:就是对每⼀个顶点都要作改变的数据元素。实际上，顶点位置本身 就是⼀个属性。属性值可以是浮点数、整数、布尔数据。 属性总是以四维向量量的形式进行内部存储的，即使我们不会使用所有的 4个分量量。⼀个顶点位置可能存储(x,y,z)，将占有4个分量中的3个。 实际上如果是在平⾯情况下:只要在xy平⾯上就能绘制，那么Z分量就 会自动设置为0; 属性还可以是:纹理坐标、颜⾊值、光照计算表⾯法线 在顶点程序(shader渲染)可以代表你想要的任何意义。因为都是你设 定的。 属性会从本地客户机内存中复制存储在图形硬件中的⼀个缓冲区上。这些属性只提供给顶点着⾊器使用，对于⽚元着色器⽊有太大意义。 声明:这些属性对每个顶点都要做改变，但并不意味着它们的值不能重复。通常情况下，它们都是不一样的，但有可能整个数组都是同一值的 情况。 3.2 Uniform值 属性是⼀种对整个批次属性都取统一值的单⼀值。它是不变的。通过设置uniform变量就紧接着发送一个图元批次命令，Uniform变量实际上可以无数次限制地使⽤用，设置一个应用于整个表⾯的单个颜色值，还可以设置⼀ 个时间值。在每次渲染某种类型的顶点动画时修改它。 注意:这⾥的uniform变量每个批次改变一次，⽽不是每个顶点改变一次。 uniform变量最常见的应⽤是在顶点渲染中设置变换矩阵 后⾯的课程会详细讲解 与属性相同点:可以是浮点值、整数、布尔值 与属性不同点:顶点着⾊器和片元着色器都可以使用uniform变量。uniform 变量还可以是标量类型、⽮量类型、uniform矩阵。 3.3 纹理 传递给着⾊器的第三种数据类型:纹理数据 现在就教大家如果处理纹理数据并将其传递给着⾊器的细节还为时过早。 我们先在前⾯的课程中，了解什么叫做纹理! 在顶点着⾊器、⽚段着⾊器中都可以对纹理数据进⾏采样和筛选。 典型的应⽤场景:⽚段着⾊器对⼀个纹理值进行采样，然后在⼀个三⻆形表⾯应⽤渲染纹理数据。 纹理数据，不仅仅表现在图形，很多图形⽂件格式都是以无符号字节(每个颜⾊通道8位)形式对颜⾊分量进⾏存储的。 3.4 输出 在图表中第四种数据类型是输出(out);输出数据是作为⼀个阶段着⾊ 器的输出定义的，⼆后续阶段的着⾊器则作为输⼊定义。 输出数据可以简单的从⼀个阶段传递到下⼀个阶段，也可以⽤不同的⽅式插⼊。 客户端的代码接触不到这些内部变量 我们的OpenGL开发暂时接触不到。 四、创建坐标系 4.1 正投影 这就是⼀个正投影的例子，在所在3个轴(X,Y,Z)中，它们的范围都是 从-100到+100。这个视景体将包括所有的⼏何图形。 如果你指定了视景体外的⼏何图形，就会被裁减掉!(它将沿着视景体的边界进行剪切) 在正投影中，所有在这个空间范围内的所有东⻄都将被呈现在屏幕上。⽽ 不存在照相机或视点坐标系的概念。 4.2 透视投影 透视投影会进行透视除法对距离观察者很远的对象进⾏缩短和收缩。在投 影到屏幕之后，视景体背⾯与视景体正⾯的宽度测量标准不同。 上图所示:平截头体(frustum)的⼏何体，它的观察⽅向是从⾦字塔的 尖端到宽阔端。观察者的视点与⾦字塔的尖端拉开⼀定距离。 GLFrustum类通过setPerspective⽅法为我们构建⼀个平截头体。 CLFrustum::SetPerspective(float fFov,float fAspect,float fNear ,float fFar); 参数: fFov:垂直⽅向上的视场⻆度 fAspect:窗⼝的宽度与⾼度的纵横⽐ fNear:近裁剪⾯距离 fFar:远裁剪⾯距离 纵横⽐比 = 宽(w)/⾼(h) 五、使⽤存储着⾊器 5.1 使⽤背景 在OpenGL核⼼框架中，并没有提供任何内建渲染管线，在提交一个⼏何图形进行渲染之前，必须实现⼀个着⾊器。 在前⾯的课程可以使用存储着⾊器。这些存储着⾊器由GLTools的C++类 GLShaderManager管理。它们能够满⾜进行基本渲染的基本要求。要求不高的程序员，这些存储着⾊器已经⾜以满足他们的需求。但是，随着时间和经验的提升，⼤部分开发者可能不满⾜于此。 会开始⾃自己着⼿去写着⾊器。 5.2 存储着⾊器的使⽤ GLShaderManager 的初始化 // GLShaderManager 的初始化 GLShaderManager shaderManager; shaderManager.InitializeStockShaders() GLShaderManager 属性 存储着⾊器为每⼀个变量都使用一致的内部变量命名规则和相同的属性 槽。以上就是存储着色器的属性列列表 GLShanderManager 的 uniform值 ⼀般情况，要对⼏何图形进行渲染，我们需要给对象递属性矩阵，⾸ 先要绑定我们想要使⽤的着⾊程序上，并提供程序的uniform值。但是GLShanderManager 类可以暂时为我们完成工作。 userStockShader函数会选择⼀个存储着⾊器并提供这个着⾊器的 uniform值。 GLShaderManager::UserStockShader(GLeunm shader...); 单位(Identity 着⾊器) GLShaderManager::UserStockShader(GLT_ATTRIBUTE_VERTEX,GLfloat vColor[4]); 单位着⾊器:只是简单地使⽤默认笛卡尔坐标系(坐标范围(-1.0， 1.0))。所有的⽚段都应⽤同一种颜色，几何图形为实⼼和未渲染的。 需要设置存储着色器⼀个属性: GLT_ATTRIBUTE_VERTEX(顶点分量) 参数2:vColor[4],你需要的颜色。 平⾯着⾊器 GLShaderManager::UserStockShader(GLT_SHADER_FLAT,GLfloat mvp[1 6],GLfloat vColor[4]); 参数1:平⾯着⾊器 参数2:允许变化的4*4矩阵 参数3:颜⾊ 它将统⼀着⾊器器进行了拓展。允许为⼏何图形变换指定一个 4 * 4 变换矩 阵。经常被称为“模型视图投影矩阵” 上⾊着⾊器 GLShaderManager::UserStockShader(GLT_SHADER_SHADED,GLfloat mvp [16]); 在⼏几何图形中应⽤用的变换矩阵。 需要设置存储着色器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_COLOR(颜⾊色分量量) 2个属性。颜⾊值将被平滑地插⼊顶点之间(平滑着色) 默认光源着⾊器 GLShaderManager::UserStockShader(GLT_SHADER_DEFAULT_LIGHT,GLfloat mvMatrix[16],GLfloat pMatrix[16],GLfloat vColor[4]); 参数1:默认光源着色器 参数2:模型视图矩阵 参数3:投影矩阵 参数4:颜色值 这种着⾊器，是对象产⽣阴影和光照的效果。需要设置存储着⾊器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_NORMAL(表⾯面法线) 点光源着⾊色器器 GLShaderManager::UserStockShader(GLT_SHADER_DEFAULT_LIGHT_DIEF ,GLfloat mvMatrix[16],GLfloat pMatrix[16],GLfloat vLightPos[3] ,GLfloat vColor[4]); 参数1:点光源着⾊器 参数2:模型视图矩阵 参数3:投影矩阵 参数4:视点坐标光源位置 参数5:颜⾊值 点光源着⾊器和默认光源着色器很相似，区别在于:光源位置是特定的。 同样需要设置存储着色器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_NORMAL(表⾯法线) 纹理替换矩阵 GLShaderManager::UserStockShader(GLT_SHADER_TEXTURE_REPLACE,GL float mvMatrix[16],GLint nTextureUnit); 着⾊器通过给定的模型视图投影矩阵，使⽤绑定到 nTextureUnit (纹理单元) 指定纹理单元的纹理对几何图形进行变化。 ⽚段颜色:是直接从纹理样本中直接获取的。 需要设置存储着⾊器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_NORMAL(表⾯法线) 纹理调整着⾊器 GLShaderManager::UserStockShader(GLT_SHADER_TEXTURE_MODULATE,GLfloat mvMatrix[16],GLfloat vColor[4],GLint nTextureUnit); 将⼀个基本⾊乘以一个取⾃纹理单元 nTextureUnit 的纹理。 需要设置存储着色器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_TEXTURE0(纹理坐标) 纹理光源着⾊器 GLShaderManager::UserStockShader(GLT_SHADER_TEXTURE_POINT_LIGH T_DIEF,GLfloat mvMatrix[16],GLfloat pMatrix[16],GLfloat vLight Pos[3],GLfloat vBaseColor[4],GLint nTextureUnit); 参数1:纹理光源着⾊器 参数2:投影矩阵 参数3:视觉空间中的光源位置 参数4:⼏何图形的基本⾊ 参数5:将要使用的纹理单元 将⼀个纹理通过漫反射照明计算机进行调整(相乘)。光线在视觉空间中的位置是给定的。 需要设置存储着⾊器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_TEXTURE0(纹理坐标)、GLT_ATTRIBUTE_NORMAL(表⾯法线) "},"pages/OpenGL系列/02_OpenGL基础渲染.html":{"url":"pages/OpenGL系列/02_OpenGL基础渲染.html","title":"2.OpenGL基础渲染","keywords":"","body":"2.OpenGL基础渲染 This browser does not support PDFs. Please download the PDF to view it: Download PDF. "},"pages/OpenGL系列/03_OpenGL渲染问题的处理方法.html":{"url":"pages/OpenGL系列/03_OpenGL渲染问题的处理方法.html","title":"3.OpenGL渲染问题的处理方法","keywords":"","body":"3.OpenGL渲染问题的处理方法 学习内容 渲染过程中可能产⽣的问题 油画渲染 正⾯面&背⾯面剔除 深度测试 多边形模型 多边形偏移 裁剪 一、在渲染过程中可能产⽣的问题 在绘制3D场景的时候,我们需要决定哪些部分是对观察者 可⻅的,或者哪些部分是对观察者不可⻅的.对于不可⻅的 部分,应该及早丢弃.例如在⼀个不透明的墙壁后,就不应该 渲染.这种情况叫做“隐藏⾯消除”(Hidden surface elimination). 二、油画算法 2.1 解释 先绘制场景中的离观察者较远的物体,再绘制较近的物体. 例如下⾯的图例：先绘制红⾊部分,再绘制⻩⾊部分,最后再绘制灰色部分,即可解决隐藏⾯消除的 问题。 2.2 弊端 使⽤油画算法,只要将场景按照物理距离观察者的距离远近排序,由远及近的绘制即可.那么会出现什么问题? 如果三个三角形是叠加的情况,油画算法将⽆法处理. 三、正背⾯剔除(Face Culling) 3.1 思考 ⼀个3D图形,你从任何⼀个⽅向去观察,最多可以看到几个面? 答案：最多3⾯. 从⼀个⽴⽅体的任意位置和⽅向上看,你用过不可能看到多于3个面. 那么思考? 我们为何要多余的去绘制那根本看不到的3个面? 如果我们能以某种方式去丢弃这部分数据,OpenGL在渲染的性能即可提⾼超过50%. 如何知道某个面在观察者的视􏰀中不会出现? 任何平⾯都有2个面,正面/背面.意味着你⼀个时刻只能看到一⾯.OpenGL 可以做到检查所有正面朝向观察者的面,并渲染它们.从⽽丢弃背⾯朝向的⾯. 这样可以 节约⽚元着⾊器的性能. 如果告诉OpenGL你绘制的图形,哪个面是正面,哪个面是背面? 答案：通过分析顶点数据的顺序 3.2 分析顶点顺序 正⾯: 按照逆时针顶点连接顺序的三⻆形⾯ 背⾯: 按照顺时针顶点连接顺序的三⻆形⾯ 3.3 分析⽴方体中的正背⾯ 分析 左侧三⻆形顶点顺序为: 1—> 2—> 3; 右侧三⻆形的顶点顺序为: 1—> 2—> 3 . 当观察者在右侧时,则右边的三⻆形⽅向为逆时针⽅方向判定为正⾯,⽽左侧的三⻆形为顺时针则为背面 当观察者在左侧时,则左边的三角形⽅向为逆时针⽅方向判定为正⾯,⽽右侧的三⻆形为顺时针则为背面 总结 正⾯和背面是由三⻆形的顶点定义顺序和观察者方向共同决定的.随着观察者的⻆度⽅向的改变,正面背面也 会跟着改变。 3.4 开启正背面剔除函数 开启表⾯剔除(默认背⾯剔除) void glEnable(GL_CULL_FACE); 关闭表面剔除(默认背⾯剔除) void glDisable(GL_CULL_FACE); ⽤户选择剔除那个面(正面/背面) // mode参数为: GL_FRONT,GL_BACK,GL_FRONT_AND_BACK ,默认GL_BACK void glCullFace(GLenum mode); ⽤户指定绕序哪个为正⾯ // mode参数为: GL_CW,GL_CCW,默认值:GL_CCW oid glFrontFace(GLenum mode); 例如,剔除正面实现(1) glCullFace(GL_BACK); glFrontFace(GL_CW); 例如,剔除正面实现(1) glCullFace(GL_FRONT); 四、深度测试 深度缓冲区(DepthBuffer)和颜⾊缓存区(ColorBuffer)是对应的.颜⾊缓存区存储像素的颜色信息,⽽深度缓冲区存储像素的深度信息.在决定是否绘制一个物体表⾯时,首先要将表⾯面对应的像素的深度值与当前深度缓冲区中的值进⾏比较.如果⼤于深度缓冲区中的值,则丢弃这部分.否则利用这个像素对应的深度值和颜⾊值.分别更新深度缓冲区和颜色缓存区. 这个过程称为”深度测 试”。 4.1 什么是深度？ 深度其实就是该像素点在3D世界中距离摄像机的距离,Z值 4.2 什么是深度缓冲区? 深度缓存区,就是一块内存区域,专⻔存储着每个像素点(绘制在屏幕上的)深度值.深度值(Z值)越⼤, 则离摄像机就越远. 4.3 为什么需要深度缓冲区? 在不使⽤深度测试的时候,如果我们先绘制一个距离⽐较近的物体,再绘制距离较远的物体,则距离远的位图因为后绘制,会把距离近的物体覆盖掉.有了深度缓冲区后,绘制物体的顺序就不那么􏰀重要的. 实际上,只要存在深度缓冲区,OpenGL都会把像素的深度值写⼊到缓冲区中. 除非调用 glDepthMask(GL_FALSE).来禁⽌写入. 4.4 使⽤深度测试 深度缓冲区,⼀般由窗⼝管理理系统,GLFW创建.深度值⼀般由16位,24位,32位值表示.通常是24位.数越高,深度精确度更好. 开启深度测试glEnable(GL_DEPTH_TEST); 在绘制场景前,清除颜⾊缓存区,深度缓冲glClearColor(0.0f,0.0f,0.0f,1.0f); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); 清除深度缓冲区默认值为1.0,表示最大的深度值,深度值的范围为(0,1)之间.值越小表示越靠近观察者,值越大表示越远离观察者 指定深度测试判断模式 打开/阻断深度缓存区写⼊//value : GL_TURE 开启深度缓冲区写⼊入; GL_FALSE 关闭深度缓冲区写⼊入 void glDepthMask(GLBool value); 4.5 ZFighting闪烁问题出现 因为开启深度测试后,OpenGL就不会再去绘制模型被遮挡的部分.这样实现的显示更加真实.但是 由于深度缓冲区精度的限制对于深度相差⾮常小的情况下.(例如在同一平⾯上进行2次绘制),OpenGL就可能出现不能正确判断两者的深度值,会导致深度测试的结果不可预测.显示出来的现象时交错闪烁的前面2个画面,交错出现. 4.6 ZFighting闪烁问题解决 第⼀步: 启⽤用 Polygon Offset ⽅式解决 让深度值之间产⽣生间隔.如果2个图形之间有间隔,是不是意味着就不会产⽣干涉.可以理解为在执⾏深度测试前将⽴方体的深度值做⼀些细微的增加.于是就能将􏰀叠的2个图形深度值之间有所区分. // 启⽤用Polygon Offset⽅式 // 参数列列表: // GL_POLYGON_OFFSET_POINT GL_POLYGON_OFFSET_LINE GL_POLYGON_OFFSET_FILL // 对应光栅化模式: GL_POINT // 对应光栅化模式: GL_LINE // 对应光栅化模式: GL_FILL glEnable(GL_POLYGON_OFFSET_FILL) 第⼆步: 指定偏移量 通过glPolygonOffset来指定.glPolygonOffset需要2个参数: factor , unitsvoid glPolygonOffset(Glfloat factor,Glfloat units); 每个Fragment的深度值都会增加如下所示的偏移量Offset = ( m * factor ) + ( r * units) m(深度值(Z值)) : 多边形的深度的斜率的最⼤值,理解一个多边形越是与近裁剪面平⾏,m 就越接近于0. r(使得深度缓冲区产⽣生变化的最小值) : 能产⽣于窗⼝坐标系的深度值中可分辨的差异最小值.r是由具体OpenGL平台指定的⼀个常量. ⼀个⼤于0的Offset 会把模型推到离你(摄像机)更远的位置,相应的一个⼩于0的Offset会把模型拉近 ⼀般⽽言,只需要将-1.0 和 0.0 这样简单赋值给glPolygonOffset基本可以满⾜需求. 第三步: 关闭Polygon Offset glDisable(GL_POLYGON_OFFSET_FILL) 4.7 ZFighting闪烁问题预防 不要将两个物体靠的太近，避免渲染时三⻆形叠在⼀起。这种方式要求对场景中物体插入⼀个少量的偏移，那么就可能避免ZFighting现象。例如上面的⽴方体和平⾯问题中，将平⾯下移0.001f就可以解决这个问题。当然⼿动去插⼊这个⼩的偏移是要付出代价的。 尽可能将近裁剪⾯设置得离观察者远⼀些。上⾯我们看到，在近裁剪平面附近，深度的精确度是很高的，因此尽可能让近裁剪⾯远⼀些的话，会使整个裁剪范围内的精确度变高⼀些。但是这种⽅式会使离观察者较近的物体被裁减掉，因此需要调试好裁剪⾯参数。 使⽤更⾼位数的深度缓冲区，通常使⽤的深度缓冲区是24位的，现在有一些硬件使用32位的缓冲区，使精确度得到提⾼ 五、裁剪 在OpenGL中提高渲染的一种⽅式.只刷新屏幕上发生变化的部分.OpenGL允许将要进行渲染的窗口只 去指定一个裁剪框. 基本原理:⽤于渲染时限制绘制区域，通过此技术可以再屏幕(帧缓冲)指定一个矩形区域。启⽤剪裁测试之后，不在此矩形区域内的⽚元被丢弃，只有在此矩形区域内的⽚元才有可能进入帧缓冲。因此实际达到的效果就是在屏幕上开辟了了一个⼩窗口，可以再其中进⾏指定内容的绘制。 //1 开启裁剪测试 glEnable(GL_SCISSOR_TEST); //2.关闭裁剪测试 glDisable(GL_SCISSOR_TEST); //3.指定裁剪窗⼝口 void glScissor(Glint x,Glint y,GLSize width,GLSize height); x,y:指定裁剪框左下⻆角位置; width , height:指定裁剪尺⼨ 5.1 理解窗口 就是显示界⾯ 5.2 视口 就是窗⼝中⽤来显示图形的一块矩形区域，它可以和窗口等大，也可以⽐窗口⼤或者小。只有绘制在视口区域中的图形才能被显示，如果图形有一部分超出了视口区域，那么那一部分是看不到的。 5.3 裁剪区域（平行投影） 就是视⼝矩形区域的最小最大x坐标(left,right)和最小最大y坐标 (bottom,top），⽽不是窗口的最小最大x坐标和y坐标。通过glOrtho()函数设置，这个函数还需指定最近 最远z坐标，形成⼀个立体的裁剪区域。 六、混合 我们把OpenGL渲染时会把颜色值存在颜⾊缓存区中，每个⽚段的深度值也是放在深度缓冲区。当深度缓冲区被关闭时，新的颜色将简单的覆盖原来颜色缓存区存在的颜色值，当深度缓冲区再次打开时，新的颜色⽚段只是当它们⽐原来的值更接近邻近的裁剪平⾯才会替换原来的颜色⽚段。 glEnable(GL_BlEND); 6.1 组合颜⾊ ⽬标颜色:已经存储在颜⾊缓存区的颜⾊值 源颜⾊:作为当前渲染命令结果进⼊颜色缓存区的颜⾊值 当混合功能被启动时，源颜⾊和⽬标颜⾊的组合⽅式是混合方程式控制的。在默认情况下， 混合方程式如下所示: // Cf: 最终计算参数的颜⾊色 // Cs: 源颜⾊色 // Cd:⽬目标颜⾊色 // S: 源混合因⼦子 // D: ⽬目标混合因⼦子 Cf = (Cs * S) + (Cd * D) 6.2 设置混合因⼦ 函数介绍 设置混合因⼦子，需要⽤用到glBlendFun函数 // S:源混合因⼦子 // D:⽬目标混合因⼦子 glBlendFunc(GLenum S,GLenum D); 表中R、G、B、A 分别代表 红、绿、蓝、alpha。 表中下标S、D，分别代表源、⽬目标。 表中C 代表常量量颜⾊色(默认⿊黑⾊色)。 函数使用 下⾯面通过⼀个常⻅的混合函数组合来说明问题: glBlendFunc(GL_SRC_ALPHA,GL_ONE_MINUS_SRC_ALPHA); 如果颜⾊缓存区已经有一种颜色红色(1.0f,0.0f,0.0f,0.0f),这个目标颜色Cd，如果在这上面用⼀种alpha为0.6的蓝色(0.0f,0.0f,1.0f,0.6f) Cd (⽬标颜色) = (1.0f,0.0f,0.0f,0.0f); Cs (源颜⾊色) = (0.0f,0.0f,1.0f,0.6f); S = 源alpha值 = 0.6f D = 1 - 源alpha值= 1-0.6f = 0.4f 方程式Cf = (Cs * S) + (Cd * D) 等价于 = (Blue 0.6f) + (Red 0.4f) 总结 最终颜色是以原先的红色(目标颜色)与 后来的蓝色(源颜色)进行组合。源颜⾊的alpha值越⾼，添加的蓝⾊色颜色成分越高，⽬标颜⾊所保留的成分就会越少。混合函数经常⽤于实现在其他一些不透明的物体前⾯绘制一个透明物体的效果。 6.3 改变组合⽅程式 默认混合⽅方程式: Cf = (CsS)+(CdD) 实际上远不止这一种混合⽅程式，我们可以从5个不同的方程式中进行选择 选择混合⽅方程式的函数: glbBlendEquation(GLenum mode); 6.4 glBlendFuncSeparate 函数 除了能使⽤glBlendFunc来设置混合因子，还可以有更灵活的选择。 // strRGB: 源颜⾊的混合因⼦ // dstRGB: ⽬标颜⾊的混合因⼦ // strAlpha: 源颜色的Alpha因⼦ // dstAlpha: ⽬标颜色的Alpha因⼦ void glBlendFuncSeparate(GLenum strRGB,GLenum dstRGB ,GLenum strAlpha,GLenum dstAlpha); glBlendFunc 指定源和⽬标RGBA值的混合函数;但是glBlendFuncSeparate函数则允许为RGB 和 Alpha 成分单独指定混合函数。 在混合因⼦子表中，GL_CONSTANT_COLOR,GL_ONE_MINUS_CONSTANT_COLOR,GL_CONSTANT_ALPHA,GL_ONE_MINUS_CONSTANT值允许混合方程式中引⼊一个常量混合颜⾊。 6.5 常量混合颜⾊ 常量混合颜⾊色，默认初始化为⿊⾊(0.0f,0.0f,0.0f,0.0f)，但是还是可以修改这个常量混合颜色。 void glBlendColor(GLclampf red ,GLclampf green ,GLclampf blue ,GLclampf alpha); 七、案例 源码 7.1 深度测试+正背面剔除 //演示了OpenGL背面剔除，深度测试，和多边形模式 #include \"GLTools.h\" #include \"GLMatrixStack.h\" #include \"GLFrame.h\" #include \"GLFrustum.h\" #include \"GLGeometryTransform.h\" #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif ////设置角色帧，作为相机 GLFrame viewFrame; //使用GLFrustum类来设置透视投影 GLFrustum viewFrustum; GLTriangleBatch torusBatch; GLMatrixStack modelViewMatix; GLMatrixStack projectionMatrix; GLGeometryTransform transformPipeline; GLShaderManager shaderManager; //标记：背面剔除、深度测试 int iCull = 0; int iDepth = 0; //右键菜单栏选项 void ProcessMenu(int value) { switch(value) { case 1: iDepth = !iDepth; break; case 2: iCull = !iCull; break; case 3: glPolygonMode(GL_FRONT_AND_BACK, GL_FILL); break; case 4: glPolygonMode(GL_FRONT_AND_BACK, GL_LINE); break; case 5: glPolygonMode(GL_FRONT_AND_BACK, GL_POINT); break; } glutPostRedisplay(); } // 召唤场景 void RenderScene(void) { //清除窗口和深度缓冲区 glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); //根据设置iCull标记来判断是否开启背面剔除 if(iCull) { glEnable(GL_CULL_FACE); glFrontFace(GL_CCW); glCullFace(GL_BACK); } else glDisable(GL_CULL_FACE); //根据设置iDepth标记来判断是否开启深度测试 if(iDepth) glEnable(GL_DEPTH_TEST); else glDisable(GL_DEPTH_TEST); //把摄像机矩阵压入模型矩阵中 modelViewMatix.PushMatrix(viewFrame); GLfloat vRed[] = { 1.0f, 0.0f, 0.0f, 1.0f }; //使用平面着色器 //参数1：平面着色器 //参数2：模型视图投影矩阵 //参数3：颜色 //shaderManager.UseStockShader(GLT_SHADER_FLAT, transformPipeline.GetModelViewProjectionMatrix(), vRed); //使用默认光源着色器 //通过光源、阴影效果跟提现立体效果 //参数1：GLT_SHADER_DEFAULT_LIGHT 默认光源着色器 //参数2：模型视图矩阵 //参数3：投影矩阵 //参数4：基本颜色值 shaderManager.UseStockShader(GLT_SHADER_DEFAULT_LIGHT, transformPipeline.GetModelViewMatrix(), transformPipeline.GetProjectionMatrix(), vRed); //绘制 torusBatch.Draw(); //出栈 modelViewMatix.PopMatrix(); glutSwapBuffers(); } // 这个函数不需要初始化渲染 // context. 图像上下文 void SetupRC() { // 设置背景颜色 glClearColor(0.3f, 0.3f, 0.3f, 1.0f ); //初始化着色器管理器 shaderManager.InitializeStockShaders(); //将相机向后移动7个单元：肉眼到物体之间的距离 viewFrame.MoveForward(7.0); //创建一个甜甜圈 //void gltMakeTorus(GLTriangleBatch& torusBatch, GLfloat majorRadius, GLfloat minorRadius, GLint numMajor, GLint numMinor); //参数1：GLTriangleBatch 容器帮助类 //参数2：外边缘半径 //参数3：内边缘半径 //参数4、5：主半径和从半径的细分单元数量 gltMakeTorus(torusBatch, 1.0f, 0.3f, 52, 26); //点的大小 glPointSize(4.0f); } //键位设置，通过不同的键位对其进行设置 //控制Camera的移动，从而改变视口 void SpecialKeys(int key, int x, int y) { if(key == GLUT_KEY_UP) viewFrame.RotateWorld(m3dDegToRad(-5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_DOWN) viewFrame.RotateWorld(m3dDegToRad(5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_LEFT) viewFrame.RotateWorld(m3dDegToRad(-5.0), 0.0f, 1.0f, 0.0f); if(key == GLUT_KEY_RIGHT) viewFrame.RotateWorld(m3dDegToRad(5.0), 0.0f, 1.0f, 0.0f); //重新刷新window glutPostRedisplay(); } void ChangeSize(int w, int h) { //防止h变为0 if(h == 0) h = 1; //设置视口窗口尺寸 glViewport(0, 0, w, h); //setPerspective函数的参数是一个从顶点方向看去的视场角度（用角度值表示） // 设置透视模式，初始化其透视矩阵 viewFrustum.SetPerspective(35.0f, float(w)/float(h), 1.0f, 100.0f); //把透视矩阵加载到透视矩阵对阵中 projectionMatrix.LoadMatrix(viewFrustum.GetProjectionMatrix()); // 初始化渲染管线 transformPipeline.SetMatrixStacks(modelViewMatix, projectionMatrix); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH | GLUT_STENCIL); glutInitWindowSize(800, 600); glutCreateWindow(\"Geometry Test Program\"); glutReshapeFunc(ChangeSize); glutSpecialFunc(SpecialKeys); glutDisplayFunc(RenderScene); // Create the Menu glutCreateMenu(ProcessMenu); glutAddMenuEntry(\"Toggle depth test\",1); glutAddMenuEntry(\"Toggle cull backface\",2); glutAddMenuEntry(\"Set Fill Mode\", 3); glutAddMenuEntry(\"Set Line Mode\", 4); glutAddMenuEntry(\"Set Point Mode\", 5); glutAttachMenu(GLUT_RIGHT_BUTTON); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"GLEW Error: %s\\n\", glewGetErrorString(err)); return 1; } SetupRC(); glutMainLoop(); return 0; } 7.2 裁剪 //demo OpenGL 裁剪 #include \"GLTools.h\" #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif //召唤场景 void RenderScene(void) { //设置清屏颜色为蓝色 glClearColor(0.0f, 0.0f, 1.0f, 0.0f); glClear(GL_COLOR_BUFFER_BIT); //1.现在剪成小红色分区 //(1)设置裁剪区颜色为红色 glClearColor(1.0f, 0.0f, 0.0f, 0.0f); //(2)设置裁剪尺寸 glScissor(100, 100, 600, 400); //(3)开启裁剪测试 glEnable(GL_SCISSOR_TEST); //(4)开启清屏，执行裁剪 glClear(GL_COLOR_BUFFER_BIT); // 2.裁剪一个绿色的小矩形 //(1).设置清屏颜色为绿色 glClearColor(0.0f, 1.0f, 0.0f, 0.0f); //(2).设置裁剪尺寸 glScissor(200, 200, 400, 200); //(3).开始清屏执行裁剪 glClear(GL_COLOR_BUFFER_BIT); //关闭裁剪测试 glDisable(GL_SCISSOR_TEST); //强制执行缓存区 glutSwapBuffers(); } void ChangeSize(int w, int h) { //保证高度不能为0 if(h == 0) h = 1; // 将视口设置为窗口尺寸 glViewport(0, 0, w, h); } //程序入口 int main(int argc, char* argv[]) { glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGB); glutInitWindowSize(800,600); glutCreateWindow(\"OpenGL Scissor\"); glutReshapeFunc(ChangeSize); glutDisplayFunc(RenderScene); glutMainLoop(); return 0; } 7.3 颜色混合 //颜色组合 #include \"GLTools.h\" #include \"GLShaderManager.h\" #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLBatch squareBatch; GLBatch greenBatch; GLBatch redBatch; GLBatch blueBatch; GLBatch blackBatch; GLShaderManager shaderManager; GLfloat blockSize = 0.2f; GLfloat vVerts[] = { -blockSize, -blockSize, 0.0f, blockSize, -blockSize, 0.0f, blockSize, blockSize, 0.0f, -blockSize, blockSize, 0.0f}; void SetupRC() { glClearColor(1.0f, 1.0f, 1.0f, 1.0f ); shaderManager.InitializeStockShaders(); //绘制1个移动矩形 squareBatch.Begin(GL_TRIANGLE_FAN, 4); squareBatch.CopyVertexData3f(vVerts); squareBatch.End(); //绘制4个固定矩形 GLfloat vBlock[] = { 0.25f, 0.25f, 0.0f, 0.75f, 0.25f, 0.0f, 0.75f, 0.75f, 0.0f, 0.25f, 0.75f, 0.0f}; greenBatch.Begin(GL_TRIANGLE_FAN, 4); greenBatch.CopyVertexData3f(vBlock); greenBatch.End(); GLfloat vBlock2[] = { -0.75f, 0.25f, 0.0f, -0.25f, 0.25f, 0.0f, -0.25f, 0.75f, 0.0f, -0.75f, 0.75f, 0.0f}; redBatch.Begin(GL_TRIANGLE_FAN, 4); redBatch.CopyVertexData3f(vBlock2); redBatch.End(); GLfloat vBlock3[] = { -0.75f, -0.75f, 0.0f, -0.25f, -0.75f, 0.0f, -0.25f, -0.25f, 0.0f, -0.75f, -0.25f, 0.0f}; blueBatch.Begin(GL_TRIANGLE_FAN, 4); blueBatch.CopyVertexData3f(vBlock3); blueBatch.End(); GLfloat vBlock4[] = { 0.25f, -0.75f, 0.0f, 0.75f, -0.75f, 0.0f, 0.75f, -0.25f, 0.0f, 0.25f, -0.25f, 0.0f}; blackBatch.Begin(GL_TRIANGLE_FAN, 4); blackBatch.CopyVertexData3f(vBlock4); blackBatch.End(); } //上下左右键位控制移动 void SpecialKeys(int key, int x, int y) { GLfloat stepSize = 0.025f; GLfloat blockX = vVerts[0]; GLfloat blockY = vVerts[7]; if(key == GLUT_KEY_UP) blockY += stepSize; if(key == GLUT_KEY_DOWN) blockY -= stepSize; if(key == GLUT_KEY_LEFT) blockX -= stepSize; if(key == GLUT_KEY_RIGHT) blockX += stepSize; if(blockX (1.0f - blockSize * 2)) blockX = 1.0f - blockSize * 2;; if(blockY 1.0f) blockY = 1.0f; vVerts[0] = blockX; vVerts[1] = blockY - blockSize*2; vVerts[3] = blockX + blockSize*2; vVerts[4] = blockY - blockSize*2; vVerts[6] = blockX + blockSize*2; vVerts[7] = blockY; vVerts[9] = blockX; vVerts[10] = blockY; squareBatch.CopyVertexData3f(vVerts); glutPostRedisplay(); } //召唤场景 void RenderScene(void) { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT | GL_STENCIL_BUFFER_BIT); //定义4种颜色 GLfloat vRed[] = { 1.0f, 0.0f, 0.0f, 0.5f }; GLfloat vGreen[] = { 0.0f, 1.0f, 0.0f, 1.0f }; GLfloat vBlue[] = { 0.0f, 0.0f, 1.0f, 1.0f }; GLfloat vBlack[] = { 0.0f, 0.0f, 0.0f, 1.0f }; //召唤场景的时候，将4个固定矩形绘制好 //使用 单位着色器 //参数1：简单的使用默认笛卡尔坐标系（-1，1），所有片段都应用一种颜色。GLT_SHADER_IDENTITY //参数2：着色器颜色 shaderManager.UseStockShader(GLT_SHADER_IDENTITY, vGreen); greenBatch.Draw(); shaderManager.UseStockShader(GLT_SHADER_IDENTITY, vRed); redBatch.Draw(); shaderManager.UseStockShader(GLT_SHADER_IDENTITY, vBlue); blueBatch.Draw(); shaderManager.UseStockShader(GLT_SHADER_IDENTITY, vBlack); blackBatch.Draw(); //组合核心代码 //1.开启混合 glEnable(GL_BLEND); //2.开启组合函数 计算混合颜色因子 glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); //3.使用着色器管理器 //*使用 单位着色器 //参数1：简单的使用默认笛卡尔坐标系（-1，1），所有片段都应用一种颜色。GLT_SHADER_IDENTITY //参数2：着色器颜色 shaderManager.UseStockShader(GLT_SHADER_IDENTITY, vRed); //4.容器类开始绘制 squareBatch.Draw(); //5.关闭混合功能 glDisable(GL_BLEND); //同步绘制命令 glutSwapBuffers(); } void ChangeSize(int w, int h) { glViewport(0, 0, w, h); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH); glutInitWindowSize(800, 600); glutCreateWindow(\"移动矩形，观察颜色\"); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"Error: %s\\n\", glewGetErrorString(err)); return 1; } glutReshapeFunc(ChangeSize); glutDisplayFunc(RenderScene); glutSpecialFunc(SpecialKeys); SetupRC(); glutMainLoop(); return 0; } "},"pages/OpenGL系列/04_OpenGL渲染技巧.html":{"url":"pages/OpenGL系列/04_OpenGL渲染技巧.html","title":"4.OpenGL渲染技巧","keywords":"","body":"4.OpenGL渲染技巧 学习内容 抗锯齿 多重采样 OpenGL基本变化 投影矩阵-正投影 投影矩阵-透视投影 一、抗锯齿 抗锯齿混合的2大功能:颜色组合、抗锯齿。 //开启混合处理理 glEnable(GL_BLEND); //指定混合因⼦子 GLBlendFunc(GL_SRC_ALPHA,GL_ONE_MINUS_SRC_ALPHA); //指定混合⽅方程式 glBlendEquation(GL_FUNC_ADD); //对点进⾏行行抗锯⻮齿处理理 glEnable(GL_POINT_SMOOTH); //对线进⾏行行抗锯⻮齿处理理 glEnable(GL_LINE_SMOOTH); //对多边形进⾏行行抗锯⻮齿处理理 glEnable(GL_POLYGON_SMOOTH); 未开启抗锯齿 开启抗锯齿 二、多重采样 // 1.可以调⽤用 glutInitDisplayMode 添加采样缓存区 glutInitDisplayMode(GLUT_MULTISAMPLE); // 2.可以使⽤用glEnable| glDisable组合使⽤用GLUT_MULTISAMPLE 打开|关闭 多重采 glEnable(GLUT_MULTISAMPLE); glDisable(GLUT_MULTISAMPLE); // 3. 多重采样只正对多边形，点和线使用混合 glEnable(GL_POINT_SMOOTH); glDisable(GL_POINT_SMOOTH); glEnable(GL_LINE_SMOOTH); glDisable(GL_LINE_SMOOTH); 多重采样缓存区在默认情况下使用⽚段RGB值，并不包含颜色的alpha成分，我们可以通过调⽤用glEnable来修改这个行为: GL_SAMPLE_ALPHA_TO_COVERAGE 使⽤用alpha值 GL_SAMPLE_ALPHA_TO_ON 使⽤alpha值并设为1，并使⽤它。 GL_SAMPLE_COVERAGE 使⽤glSampleCoverage所设置的值。 当启⽤ GL_SAMPLE_COVERAGE 时，可以使⽤glSampleCoverage函数允许指定一个特定的值，它是与⽚段覆盖值进⾏按位与操作的结果。 三、OpenGL基础变化 This browser does not support PDFs. Please download the PDF to view it: Download PDF. 四、数学知识 M3DVector3f,三维向量(x,y,z) M3DVector4f,思维向量(x,y,z,w).w = 1.0 //定义2个向量V1,V2 M3DVector3f v1 = {1.0,0.0,0.0}; M3DVector3f v2 = {0.0,1.0,0.0}; //方法1：获取V1,V2的点积，获取夹角的cos值 GLfloat value1 = m3dDotProduct3(v1, v2); printf(\"V1V2 余弦值：%f\\n\",value1); //通过acos()，获取value1的弧度值 GLfloat value2 = acos(value1); printf(\"V1V2 弧度：%f\\n\",value2); //方法2：获取V1，V2的夹角弧度值 GLfloat value3 = m3dGetAngleBetweenVectors3(v1, v2); printf(\"V1V2 弧度：%f\\n\",value3); //m3dRadToDeg 弧度->度数 //m3dDegToRad 度数->弧度 GLfloat value4 = m3dRadToDeg(value3); GLfloat value5 = m3dDegToRad(90); printf(\"V1V2角度：%f\\n\",value4); printf(\"弧度：%f\\n\",value5); //定义向量vector2 M3DVector3f vector2 = {0.0f,0.0f,0.0f}; //实现矩阵叉乘：结果，向量1，向量2 // 获得一个与v1v2所在平面垂直的新向量 // 叉乘的结果与v1v2的相乘顺序有关 m3dCrossProduct3(vector2, v1, v2); printf(\"%f,%f,%f\",vector2[0],vector2[1],vector2[2]); 四、案例 源码 4.1 抗锯齿+多重采样 #include \"GLTools.h\" #include \"GLFrustum.h\" #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLShaderManager shaderManager; GLFrustum viewFrustum; GLBatch smallStarBatch; GLBatch mediumStarBatch; GLBatch largeStarBatch; GLBatch mountainRangeBatch; GLBatch moonBatch; #define SMALL_STARS 100 #define MEDIUM_STARS 40 #define LARGE_STARS 15 #define SCREEN_X 800 #define SCREEN_Y 600 // 选择菜单 void ProcessMenu(int value) { switch(value) { case 1: //打开抗锯齿，并给出关于尽可能进行最佳的处理提示 //设置混合因子 glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); glEnable(GL_BLEND); glEnable(GL_POINT_SMOOTH); glHint(GL_POINT_SMOOTH_HINT, GL_NICEST); glEnable(GL_LINE_SMOOTH); glHint(GL_LINE_SMOOTH_HINT, GL_NICEST); glEnable(GL_POLYGON_SMOOTH); glHint(GL_POLYGON_SMOOTH_HINT, GL_NICEST); break; case 2: //关闭抗锯齿 glDisable(GL_BLEND); glDisable(GL_LINE_SMOOTH); glDisable(GL_POINT_SMOOTH); glDisable(GL_POLYGON_SMOOTH); break; default: break; } // 触发重新绘制 glutPostRedisplay(); } //场景召唤 void RenderScene(void) { //执行clear（颜色缓存区、深度缓冲区） glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); //定义白色 GLfloat vWhite [] = { 1.0f, 1.0f, 1.0f, 1.0f }; //使用存储着色管理器中的平面着色器 //参数1：平面着色器 //参数2: 模型视图投影矩阵 //参数3: 颜色，白色 shaderManager.UseStockShader(GLT_SHADER_FLAT, viewFrustum.GetProjectionMatrix(), vWhite); //针对多边形开始多重采样 glEnable(GLUT_MULTISAMPLE); //绘制小星星 //点的大小 glPointSize(1.0f); smallStarBatch.Draw(); //绘制中星星 glPointSize(4.0f); mediumStarBatch.Draw(); //绘制大星星 glPointSize(8.0f); largeStarBatch.Draw(); // 绘制遥远的地平线 glLineWidth(3.5); mountainRangeBatch.Draw(); //绘制月亮 moonBatch.Draw(); //关闭多重采样 glDisable(GLUT_MULTISAMPLE); // 交换缓冲区 glutSwapBuffers(); } //对渲染进行必要的初始化。 void SetupRC() { M3DVector3f vVerts[SMALL_STARS]; int i; shaderManager.InitializeStockShaders(); // 小星星 smallStarBatch.Begin(GL_POINTS, SMALL_STARS); for(i = 0; i 4.2 图形移动（矩阵变换） /* 案例：实现矩阵的移动，利用矩阵的平移、旋转、综合变化等 */ #include \"GLTools.h\" #include \"GLShaderManager.h\" #include \"math3d.h\" #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLBatch squareBatch; GLShaderManager shaderManager; GLfloat blockSize = 0.1f; GLfloat vVerts[] = { -blockSize, -blockSize, 0.0f, blockSize, -blockSize, 0.0f, blockSize, blockSize, 0.0f, -blockSize, blockSize, 0.0f}; GLfloat xPos = 0.0f; GLfloat yPos = 0.0f; void SetupRC() { //背景颜色 glClearColor(0.0f, 0.0f, 1.0f, 1.0f ); shaderManager.InitializeStockShaders(); // 加载三角形 squareBatch.Begin(GL_TRIANGLE_FAN, 4); squareBatch.CopyVertexData3f(vVerts); squareBatch.End(); } //移动（移动只是计算了X,Y移动的距离，以及碰撞检测） void SpecialKeys(int key, int x, int y) { GLfloat stepSize = 0.025f; if(key == GLUT_KEY_UP) yPos += stepSize; if(key == GLUT_KEY_DOWN) yPos -= stepSize; if(key == GLUT_KEY_LEFT) xPos -= stepSize; if(key == GLUT_KEY_RIGHT) xPos += stepSize; // 碰撞检测 if(xPos (1.0f - blockSize)) xPos = 1.0f - blockSize; if(yPos (1.0f - blockSize)) yPos = 1.0f - blockSize; glutPostRedisplay(); } void RenderScene(void) { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT | GL_STENCIL_BUFFER_BIT); GLfloat vRed[] = { 1.0f, 0.0f, 0.0f, 1.0f }; M3DMatrix44f mFinalTransform, mTranslationMatrix, mRotationMatrix; //平移 xPos,yPos m3dTranslationMatrix44(mTranslationMatrix, xPos, yPos, 0.0f); // 每次重绘时，旋转5度 static float yRot = 0.0f; yRot += 5.0f; m3dRotationMatrix44(mRotationMatrix, m3dDegToRad(yRot), 0.0f, 0.0f, 1.0f); //将旋转和移动的结果合并到mFinalTransform 中 m3dMatrixMultiply44(mFinalTransform, mTranslationMatrix, mRotationMatrix); //将矩阵结果提交到固定着色器（平面着色器）中。 shaderManager.UseStockShader(GLT_SHADER_FLAT, mFinalTransform, vRed); squareBatch.Draw(); // 执行缓冲区交换 glutSwapBuffers(); } void ChangeSize(int w, int h) { glViewport(0, 0, w, h); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH); glutInitWindowSize(600, 600); glutCreateWindow(\"Move Block with Arrow Keys\"); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"Error: %s\\n\", glewGetErrorString(err)); return 1; } glutReshapeFunc(ChangeSize); glutDisplayFunc(RenderScene); glutSpecialFunc(SpecialKeys); SetupRC(); glutMainLoop(); return 0; } 4.3 正交投影 #include \"GLTools.h\" #include \"GLMatrixStack.h\" #include \"GLFrame.h\" #include \"GLFrustum.h\" #include \"GLGeometryTransform.h\" #include \"GLBatch.h\" #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLFrame viewFrame; GLFrustum viewFrustum; GLBatch tubeBatch; GLBatch innerBatch; //GLMatrixStack 堆栈矩阵 GLMatrixStack modelViewMatix; GLMatrixStack projectionMatrix; //几何变换的管道 GLGeometryTransform transformPipeline; GLShaderManager shaderManager; // 召唤场景 void RenderScene(void) { // 清屏、深度缓存 glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); //glEnable(GL_CULL_FACE); //开启深度测试 glEnable(GL_DEPTH_TEST); //绘制前压栈，将数据保存进去 modelViewMatix.PushMatrix(viewFrame); GLfloat vRed[] = { 1.0f, 0.0f, 0.0f, 1.0f }; GLfloat vGray[] = { 0.75f, 0.75f, 0.75f, 1.0f }; //默认光源着色器 //参数1：类型 //参数2：模型视图矩阵 //参数3：投影矩阵 //参数4：颜色 shaderManager.UseStockShader(GLT_SHADER_DEFAULT_LIGHT, transformPipeline.GetModelViewMatrix(), transformPipeline.GetProjectionMatrix(), vRed); tubeBatch.Draw(); //默认光源着色器 //参数1：类型 //参数2：模型视图矩阵 //参数3：投影矩阵 //参数4：颜色 shaderManager.UseStockShader(GLT_SHADER_DEFAULT_LIGHT, transformPipeline.GetModelViewMatrix(), transformPipeline.GetProjectionMatrix(), vGray); innerBatch.Draw(); //绘制完出栈，还原开始的数据 modelViewMatix.PopMatrix(); glutSwapBuffers(); } //对图形上下文初始化 void SetupRC() { //设置清屏颜色 glClearColor(0.0f, 0.0f, 0.75f, 1.0f ); // glEnable(GL_CULL_FACE); glEnable(GL_DEPTH_TEST); //初始化着色器管理器 shaderManager.InitializeStockShaders(); tubeBatch.Begin(GL_QUADS, 200); float fZ = 100.0f; float bZ = -100.0f; //左面板的颜色、顶点、光照数据 //颜色值 tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); //光照法线 //接受3个表示坐标的值，指定一条垂直于三角形表面的法线向量 tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); //顶点数据 tubeBatch.Vertex3f(-50.0f, 50.0f, 100.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f,50.0f,bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f,50.0f,bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, bZ); tubeBatch.End(); //内壁 innerBatch.Begin(GL_QUADS, 40); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f,35.0f,bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, bZ); innerBatch.End(); } void SpecialKeys(int key, int x, int y) { if(key == GLUT_KEY_UP) viewFrame.RotateWorld(m3dDegToRad(-5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_DOWN) viewFrame.RotateWorld(m3dDegToRad(5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_LEFT) viewFrame.RotateWorld(m3dDegToRad(-5.0), 0.0f, 1.0f, 0.0f); if(key == GLUT_KEY_RIGHT) viewFrame.RotateWorld(m3dDegToRad(5.0), 0.0f, 1.0f, 0.0f); //刷新窗口 glutPostRedisplay(); } //启动demo，就会调用这个方法 void ChangeSize(int w, int h) { if(h == 0) h = 1; // 设置视图窗口的尺寸 glViewport(0, 0, w, h); //设置正投影矩阵 /* void SetOrthographic(GLfloat xMin, GLfloat xMax, GLfloat yMin, GLfloat yMax, GLfloat zMin, GLfloat zMax) */ viewFrustum.SetOrthographic(-130.0f, 130.0f, -130.0f, 130.0f, -130.0f, 130.0f); //1.获取投影矩阵 viewFrustum.GetProjectionMatrix() //2.将投影矩阵加载到projectionMatrix中 projectionMatrix.LoadMatrix(viewFrustum.GetProjectionMatrix()); //设置变换管线以使用两个矩阵堆栈 transformPipeline.SetMatrixStacks(modelViewMatix, projectionMatrix); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH | GLUT_STENCIL); glutInitWindowSize(800, 600); glutCreateWindow(\"Orthographic Projection Example\"); glutReshapeFunc(ChangeSize); glutSpecialFunc(SpecialKeys); glutDisplayFunc(RenderScene); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"GLEW Error: %s\\n\", glewGetErrorString(err)); return 1; } SetupRC(); glutMainLoop(); return 0; } 4.4 透视投影 #include \"GLTools.h\" #include \"GLMatrixStack.h\" #include \"GLFrame.h\" #include \"GLFrustum.h\" #include \"GLGeometryTransform.h\" #include \"GLBatch.h\" #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLFrame viewFrame; GLFrustum viewFrustum; GLBatch tubeBatch; GLBatch innerBatch; GLMatrixStack modelViewMatix; GLMatrixStack projectionMatrix; GLGeometryTransform transformPipeline; GLShaderManager shaderManager; void RenderScene(void) { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glEnable(GL_DEPTH_TEST); modelViewMatix.PushMatrix(viewFrame); GLfloat vRed[] = { 1.0f, 0.0f, 0.0f, 1.0f }; GLfloat vGray[] = { 0.75f, 0.75f, 0.75f, 1.0f }; shaderManager.UseStockShader(GLT_SHADER_DEFAULT_LIGHT, transformPipeline.GetModelViewMatrix(), transformPipeline.GetProjectionMatrix(), vRed); tubeBatch.Draw(); shaderManager.UseStockShader(GLT_SHADER_DEFAULT_LIGHT, transformPipeline.GetModelViewMatrix(), transformPipeline.GetProjectionMatrix(), vGray); innerBatch.Draw(); modelViewMatix.PopMatrix(); glutSwapBuffers(); } void SetupRC() { glClearColor(0.0f, 0.0f, 0.75f, 1.0f ); glEnable(GL_DEPTH_TEST); shaderManager.InitializeStockShaders(); viewFrame.MoveForward(450.0f); tubeBatch.Begin(GL_QUADS, 200); float fZ = 100.0f; float bZ = -100.0f; tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, 100.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,fZ); // Right Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,fZ); // Top Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f,fZ); // Bottom Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,fZ); // Top length section //////////////////////////// // Normal points up Y axis tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f,50.0f,bZ); // Bottom section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); // Left section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); // Right Section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); // Pointing straight out Z // Left Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,fZ); // Right Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,fZ); // Top Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f,fZ); // Bottom Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,fZ); // Top length section //////////////////////////// // Normal points up Y axis tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f,50.0f,bZ); // Bottom section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); // Left section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); // Right Section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); // Left Panel tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); // Right Panel tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); // Top Panel tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, bZ); // Bottom Panel tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, bZ); tubeBatch.End(); innerBatch.Begin(GL_QUADS, 40); // Insides ///////////////////////////// // Normal points up Y axis innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f,35.0f,bZ); // Bottom section innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, fZ); // Left section innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, fZ); // Right Section innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, bZ); innerBatch.End(); } void SpecialKeys(int key, int x, int y) { if(key == GLUT_KEY_UP) viewFrame.RotateWorld(m3dDegToRad(-5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_DOWN) viewFrame.RotateWorld(m3dDegToRad(5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_LEFT) viewFrame.RotateWorld(m3dDegToRad(-5.0), 0.0f, 1.0f, 0.0f); if(key == GLUT_KEY_RIGHT) viewFrame.RotateWorld(m3dDegToRad(5.0), 0.0f, 1.0f, 0.0f); // Refresh the Window glutPostRedisplay(); } void ChangeSize(int w, int h) { if(h == 0) h = 1; glViewport(0, 0, w, h); //设置正投影矩阵 //viewFrustum.SetOrthographic(-130.0f, 130.0f, -130.0f, 130.0f, -130.0f, 130.0f); viewFrustum.SetPerspective(35.0f, float(w)/float(h), 1.0f, 1000.0f); projectionMatrix.LoadMatrix(viewFrustum.GetProjectionMatrix()); transformPipeline.SetMatrixStacks(modelViewMatix, projectionMatrix); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH | GLUT_STENCIL); glutInitWindowSize(800, 600); glutCreateWindow(\"Perspective Projection Example\"); glutReshapeFunc(ChangeSize); glutSpecialFunc(SpecialKeys); glutDisplayFunc(RenderScene); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"GLEW Error: %s\\n\", glewGetErrorString(err)); return 1; } SetupRC(); glutMainLoop(); return 0; } "},"pages/OpenGL系列/05_OpenGL三角形批次类绘制案例.html":{"url":"pages/OpenGL系列/05_OpenGL三角形批次类绘制案例.html","title":"5.OpenGL三角形批次类绘制案例","keywords":"","body":"5.OpenGL三角形批次类绘制案例 源码 // Objects.cpp // OpenGL SuperBible, Chapter 4 // Demonstrates GLTools built-in objects // Program by Richard S. Wright Jr. #include \"GLTools.h\" // OpenGL toolkit #include \"GLMatrixStack.h\" #include \"GLFrame.h\" #include \"GLFrustum.h\" #include \"GLBatch.h\" #include \"GLGeometryTransform.h\" #include \"StopWatch.h\" #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLShaderManager shaderManager; GLMatrixStack modelViewMatrix; GLMatrixStack projectionMatrix; //观察者位置 GLFrame cameraFrame; //世界坐标位置 GLFrame objectFrame; //视景体，用来构造投影矩阵 GLFrustum viewFrustum; //三角形批次类 GLTriangleBatch CC_Triangle; //球 GLTriangleBatch sphereBatch; //环 GLTriangleBatch torusBatch; //圆柱 GLTriangleBatch cylinderBatch; //锥 GLTriangleBatch coneBatch; //磁盘 GLTriangleBatch diskBatch; GLGeometryTransform transformPipeline; M3DMatrix44f shadowMatrix; GLfloat vGreen[] = { 0.0f, 1.0f, 0.0f, 1.0f }; GLfloat vBlack[] = { 0.0f, 0.0f, 0.0f, 1.0f }; int nStep = 0; // 将上下文中，进行必要的初始化 void SetupRC() { // Black background glClearColor(0.7f, 0.7f, 0.7f, 1.0f ); //初始化固定着色管理器 shaderManager.InitializeStockShaders(); //开启深度测试 glEnable(GL_DEPTH_TEST); //通过GLGeometryTransform管理矩阵堆栈 //使用transformPipeline 管道管理模型视图矩阵堆栈 和 投影矩阵堆栈 transformPipeline.SetMatrixStacks(modelViewMatrix, projectionMatrix); //将观察者坐标位置Z移动往屏幕里移动15个单位位置 //表示离屏幕之间的距离 负数，是往屏幕后面移动；正数，往屏幕前面移动 cameraFrame.MoveForward(-15.0f); //利用三角形批次类构造图形对象 // 球 /* gltMakeSphere(GLTriangleBatch& sphereBatch, GLfloat fRadius, GLint iSlices, GLint iStacks); 参数1：sphereBatch，三角形批次类对象 参数2：fRadius，球体半径 参数3：iSlices，从球体底部堆叠到顶部的三角形带的数量；其实球体是一圈一圈三角形带组成 参数4：iStacks，围绕球体一圈排列的三角形对数 建议：一个对称性较好的球体的片段数量是堆叠数量的2倍，就是iStacks = 2 * iSlices; 绘制球体都是围绕Z轴，这样+z就是球体的顶点，-z就是球体的底部。 */ gltMakeSphere(sphereBatch, 3.0, 10, 20); // 环面 /* gltMakeTorus(GLTriangleBatch& torusBatch, GLfloat majorRadius, GLfloat minorRadius, GLint numMajor, GLint numMinor); 参数1：torusBatch，三角形批次类对象 参数2：majorRadius,甜甜圈中心到外边缘的半径 参数3：minorRadius,甜甜圈中心到内边缘的半径 参数4：numMajor,沿着主半径的三角形数量 参数5：numMinor,沿着内部较小半径的三角形数量 */ gltMakeTorus(torusBatch, 3.0f, 0.75f, 15, 15); // 圆柱 /* void gltMakeCylinder(GLTriangleBatch& cylinderBatch, GLfloat baseRadius, GLfloat topRadius, GLfloat fLength, GLint numSlices, GLint numStacks); 参数1：cylinderBatch，三角形批次类对象 参数2：baseRadius,底部半径 参数3：topRadius,头部半径 参数4：fLength,圆形长度 参数5：numSlices,围绕Z轴的三角形对的数量 参数6：numStacks,圆柱底部堆叠到顶部圆环的三角形数量 */ gltMakeCylinder(cylinderBatch, 2.0f, 2.0f, 3.0f, 15, 2); //锥 /* void gltMakeCylinder(GLTriangleBatch& cylinderBatch, GLfloat baseRadius, GLfloat topRadius, GLfloat fLength, GLint numSlices, GLint numStacks); 参数1：cylinderBatch，三角形批次类对象 参数2：baseRadius,底部半径 参数3：topRadius,头部半径 参数4：fLength,圆形长度 参数5：numSlices,围绕Z轴的三角形对的数量 参数6：numStacks,圆柱底部堆叠到顶部圆环的三角形数量 */ //圆柱体，从0开始向Z轴正方向延伸。 //圆锥体，是一端的半径为0，另一端半径可指定。 gltMakeCylinder(coneBatch, 2.0f, 0.0f, 3.0f, 13, 2); // 磁盘 /* void gltMakeDisk(GLTriangleBatch& diskBatch, GLfloat innerRadius, GLfloat outerRadius, GLint nSlices, GLint nStacks); 参数1:diskBatch，三角形批次类对象 参数2:innerRadius,内圆半径 参数3:outerRadius,外圆半径 参数4:nSlices,圆盘围绕Z轴的三角形对的数量 参数5:nStacks,圆盘外网到内围的三角形数量 */ gltMakeDisk(diskBatch, 1.5f, 3.0f, 13, 3); } void DrawWireFramedBatch(GLTriangleBatch* pBatch) { //平面着色器，绘制三角形 shaderManager.UseStockShader(GLT_SHADER_FLAT, transformPipeline.GetModelViewProjectionMatrix(), vGreen); //传过来的参数，对应不同的图形Batch pBatch->Draw(); // 画出黑色轮廓 glPolygonOffset(-1.0f, -1.0f); //开启处理线 glEnable(GL_LINE_SMOOTH); //开启混合功能 glEnable(GL_BLEND); //颜色混合 //表示源颜色乘以自身的alpha 值，目标颜色乘以1.0减去源颜色的alpha值，这样一来，源颜色的alpha值越大，则产生的新颜色中源颜色所占比例就越大，而目标颜色所占比例则减 小。这种情况下，我们可以简单的将源颜色的alpha值理解为“不透明度”。这也是混合时最常用的方式。 glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); //通过程序点大小模式来设置点的大小 glEnable(GL_POLYGON_OFFSET_LINE); //多边形模型(背面、线) 将多边形背面设为线框模式 glPolygonMode(GL_FRONT_AND_BACK, GL_LINE); //线条宽度 glLineWidth(2.5f); //平面着色器绘制线条 shaderManager.UseStockShader(GLT_SHADER_FLAT, transformPipeline.GetModelViewProjectionMatrix(), vBlack); pBatch->Draw(); // 恢复多边形模式和深度测试 glPolygonMode(GL_FRONT_AND_BACK, GL_FILL); glDisable(GL_POLYGON_OFFSET_LINE); glLineWidth(1.0f); glDisable(GL_BLEND); glDisable(GL_LINE_SMOOTH); } //召唤场景 void RenderScene(void) { //用当前清除颜色清除窗口背景 glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT | GL_STENCIL_BUFFER_BIT); //模型视图矩阵栈堆，压栈 modelViewMatrix.PushMatrix(); //获取摄像头矩阵 M3DMatrix44f mCamera; //从camereaFrame中获取矩阵到mCamera cameraFrame.GetCameraMatrix(mCamera); //模型视图堆栈的 矩阵与mCamera矩阵 相乘之后，存储到modelViewMatrix矩阵堆栈中 modelViewMatrix.MultMatrix(mCamera); //创建矩阵mObjectFrame M3DMatrix44f mObjectFrame; //从ObjectFrame 获取矩阵到mOjectFrame中 objectFrame.GetMatrix(mObjectFrame); //将modelViewMatrix 的堆栈中的矩阵 与 mOjbectFrame 矩阵相乘，存储到modelViewMatrix矩阵堆栈中 modelViewMatrix.MultMatrix(mObjectFrame); //使用平面着色器 //参数1：类型 //参数2：通过transformPipeline获取模型视图矩阵 //参数3：颜色 shaderManager.UseStockShader(GLT_SHADER_FLAT, transformPipeline.GetModelViewProjectionMatrix(), vBlack); //判断你目前是绘制第几个图形 switch(nStep) { case 0: DrawWireFramedBatch(&sphereBatch); break; case 1: DrawWireFramedBatch(&torusBatch); break; case 2: DrawWireFramedBatch(&cylinderBatch); break; case 3: DrawWireFramedBatch(&coneBatch); break; case 4: DrawWireFramedBatch(&diskBatch); break; } modelViewMatrix.PopMatrix(); // Flush drawing commands glutSwapBuffers(); } //上下左右，移动图形 void SpecialKeys(int key, int x, int y) { if(key == GLUT_KEY_UP) //移动世界坐标系，而不是去移动物体。 //将世界坐标系在X方向移动-5.0 objectFrame.RotateWorld(m3dDegToRad(-5.0f), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_DOWN) objectFrame.RotateWorld(m3dDegToRad(5.0f), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_LEFT) objectFrame.RotateWorld(m3dDegToRad(-5.0f), 0.0f, 1.0f, 0.0f); if(key == GLUT_KEY_RIGHT) objectFrame.RotateWorld(m3dDegToRad(5.0f), 0.0f, 1.0f, 0.0f); glutPostRedisplay(); } //点击空格，切换渲染图形 void KeyPressFunc(unsigned char key, int x, int y) { if(key == 32) { nStep++; if(nStep > 4) nStep = 0; } switch(nStep) { case 0: glutSetWindowTitle(\"Sphere\"); break; case 1: glutSetWindowTitle(\"Torus\"); break; case 2: glutSetWindowTitle(\"Cylinder\"); break; case 3: glutSetWindowTitle(\"Cone\"); break; case 4: glutSetWindowTitle(\"Disk\"); break; } glutPostRedisplay(); } void ChangeSize(int w, int h) { glViewport(0, 0, w, h); //透视投影 viewFrustum.SetPerspective(35.0f, float(w) / float(h), 1.0f, 500.0f); //projectionMatrix 矩阵堆栈 加载透视投影矩阵 projectionMatrix.LoadMatrix(viewFrustum.GetProjectionMatrix()); //modelViewMatrix 矩阵堆栈 加载单元矩阵 modelViewMatrix.LoadIdentity(); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH | GLUT_STENCIL); glutInitWindowSize(800, 600); glutCreateWindow(\"Sphere\"); glutReshapeFunc(ChangeSize); glutKeyboardFunc(KeyPressFunc); glutSpecialFunc(SpecialKeys); glutDisplayFunc(RenderScene); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"GLEW Error: %s\\n\", glewGetErrorString(err)); return 1; } SetupRC(); glutMainLoop(); return 0; } "},"pages/OpenGL系列/06_OpenGL模型视图矩阵绘制案例.html":{"url":"pages/OpenGL系列/06_OpenGL模型视图矩阵绘制案例.html","title":"6.OpenGL模型视图矩阵绘制案例","keywords":"","body":"6.OpenGL模型视图矩阵绘制案例 源码 #include \"GLTools.h\" #include \"GLMatrixStack.h\" #include \"GLFrame.h\" #include \"GLFrustum.h\" #include \"GLGeometryTransform.h\" #include \"GLBatch.h\" #include \"StopWatch.h\" #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLFrustum viewFrustum; GLShaderManager shaderManager; GLTriangleBatch torusBatch; // 设置视口和投影矩阵 void ChangeSize(int w, int h) { //防止除以零 if(h == 0) h = 1; //将视口设置为窗口尺寸 glViewport(0, 0, w, h); //设置透视投影 viewFrustum.SetPerspective(35.0f, float(w)/float(h), 1.0f, 1000.0f); } //召唤场景 void RenderScene(void) { //建立基于时间变化的动画 static CStopWatch rotTimer; //当前时间 * 60s float yRot = rotTimer.GetElapsedSeconds() * 60.0f; //清除屏幕、深度缓存区 glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); // 矩阵变量 M3DMatrix44f mTranslate, mRotate, mModelview, mModelViewProjection; //创建一个4*4矩阵变量，将花托沿着Z轴负方向移动2.5个单位长度 m3dTranslationMatrix44(mTranslate, 0.0f, 0.0f, -2.5f); //创建一个4*4矩阵变量，将花托在Y轴上渲染yRot度，yRot根据经过时间设置动画帧率 m3dRotationMatrix44(mRotate, m3dDegToRad(yRot), 0.0f, 1.0f, 0.0f); // m3dRotationMatrix44(mRotate, m3dDegToRad(yRot), 1.0f, 0.0f, 0.0f); //为mModerView 通过矩阵旋转矩阵、移动矩阵相乘，将结果添加到mModerView上 m3dMatrixMultiply44(mModelview, mTranslate, mRotate); // 将模型视图矩阵的投影矩阵， // 将投影矩阵乘以模型视图矩阵，将变化结果通过矩阵乘法应用到mModelViewProjection矩阵上 m3dMatrixMultiply44(mModelViewProjection, viewFrustum.GetProjectionMatrix(),mModelview); // 将这个已完成的矩阵传递给着色器，并渲染这个圆环面。 //绘图颜色 GLfloat vBlack[] = { 0.0f, 0.0f, 0.0f, 1.0f }; //通过平面着色器提交矩阵，和颜色。 //平面着色器的工作只是使用提供矩阵来顶点来进行转换，并且使用指定的颜色对几何图形进行着色以得到实心几何图形 shaderManager.UseStockShader(GLT_SHADER_FLAT, mModelViewProjection, vBlack); //开始绘图 torusBatch.Draw(); // 交换缓冲区，并立即刷新 glutSwapBuffers(); glutPostRedisplay(); } void SetupRC() { glClearColor(0.8f, 0.8f, 0.8f, 1.0f ); glEnable(GL_DEPTH_TEST); shaderManager.InitializeStockShaders(); // 形成一个圆环 gltMakeTorus(torusBatch, 0.4f, 0.15f, 30, 30); //形成一个球 gltMakeSphere(torusBatch, 0.4f, 10, 20); glPolygonMode(GL_FRONT_AND_BACK, GL_LINE); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH | GLUT_STENCIL); glutInitWindowSize(800, 600); glutCreateWindow(\"ModelViewProjection Example\"); glutReshapeFunc(ChangeSize); glutDisplayFunc(RenderScene); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"GLEW Error: %s\\n\", glewGetErrorString(err)); return 1; } SetupRC(); glutMainLoop(); return 0; } "},"pages/OpenGL系列/07_OpenGL顶点变化管线详解.html":{"url":"pages/OpenGL系列/07_OpenGL顶点变化管线详解.html","title":"7.OpenGL顶点变化管线详解","keywords":"","body":"7.OpenGL顶点变化管线详解 一、顶点变化管线 二、使用矩阵堆栈 // 类型 GLMatrixStack::GLMatrixStack(int iStackDepth = 64); // 在堆栈顶部载⼊一个单元矩阵 void GLMatrixStack::LoadIdentity(void); // 在堆栈顶部载⼊任何矩阵 // 参数:4*4矩阵 void GLMatrixStack::LoadMatrix(const M3DMatrix44f m); // 矩阵乘以矩阵堆栈顶部矩阵，相乘结果存储到堆栈的顶部 void GLMatrixStack::MultMatrix(const M3DMatrix44f); // 获取矩阵堆栈顶部的值 GetMatrix 函数 // 为了适应GLShaderMananger的使⽤，或者获取顶部矩阵的副本 const M3DMatrix44f & GLMatrixStack::GetMatrix(void); void GLMatrixStack::GetMatrix(M3DMatrix44f mMatrix); 三、压栈、出栈 压栈: 存储⼀个状态 出栈: 恢复⼀个状态 // 将当前矩阵压⼊堆栈 void GLMatrixStack::PushMatrix(void); // 将M3DMatrix44f 矩阵对象压入当前矩阵堆栈 void PushMatrix(const M3DMatrix44f mMatrix); // 将GLFame对象压入矩阵对象 void PushMatrix(GLFame &frame); // 出栈(出栈指的是移除顶部的矩阵对象) void GLMatrixStack::PopMatrix(void); 四、仿射变换 //Rotate 函数angle参数是传递的度数，⽽不是弧度 void MatrixStack::Rotate(GLfloat angle,GLfloat x,GLfloat y,GLfloat z); void MatrixStack::Translate(GLfloat x,GLfloat y,GLfloat z); void MatrixStack::Scale(GLfloat x,GLfloat y,GLfloat z); 五、GLFrame 使⽤照相机(摄像机) 和 ⻆色帧 进⾏移动 class GLFrame { protected: // Where am I? M3DVector3f vOrigin; // Where am I going? M3DVector3f vForward; // Which way is up? M3DVector3f vUp; } // 将堆栈的顶部压入任何矩阵 void GLMatrixStack::LoadMatrix(GLFrame &frame); // 矩阵乘以矩阵堆栈顶部的矩阵。相乘结果存储在堆栈的顶部 void GLMatrixStack::MultMatrix(GLFrame &frame); // 将当前的矩阵压栈 void GLMatrixStack::PushMatrix(GLFrame &frame); // GLFrame函数，这个函数⽤来检索条件适合的照相矩阵 void GetCameraMatrix(M3DMatrix44f m,bool bRotationOnly = flase); "},"pages/OpenGL系列/08_OpenGL公转自转案例.html":{"url":"pages/OpenGL系列/08_OpenGL公转自转案例.html","title":"8.OpenGL公转自转案例","keywords":"","body":"8.OpenGL公转自转案例 源码 #include \"GLTools.h\" #include \"GLShaderManager.h\" #include \"GLFrustum.h\" #include \"GLBatch.h\" #include \"GLMatrixStack.h\" #include \"GLGeometryTransform.h\" #include \"StopWatch.h\" #include #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif //**4、添加附加随机球 #define NUM_SPHERES 50 GLFrame spheres[NUM_SPHERES]; GLShaderManager shaderManager; // 着色器管理器 GLMatrixStack modelViewMatrix; // 模型视图矩阵 GLMatrixStack projectionMatrix; // 投影矩阵 GLFrustum viewFrustum; // 视景体 GLGeometryTransform transformPipeline; // 几何图形变换管道 GLTriangleBatch torusBatch; // 花托批处理 GLBatch floorBatch; // 地板批处理 //**2、定义公转球的批处理（公转自转）** GLTriangleBatch sphereBatch; //球批处理 //**3、角色帧 照相机角色帧（全局照相机实例） GLFrame cameraFrame; void SetupRC() { // 初始化着色器管理器 shaderManager.InitializeStockShaders(); //开启深度测试 glEnable(GL_DEPTH_TEST); //开启多边形模型 //**4、关闭线框渲染模式 //glPolygonMode(GL_FRONT_AND_BACK, GL_LINE); //设置清屏颜色到颜色缓存区 glClearColor(0.0f, 0.0f, 0.0f, 1.0f); //绘制甜甜圈 gltMakeTorus(torusBatch, 0.4f, 0.15f, 30, 30); //**2、 绘制球(公转自转)** gltMakeSphere(sphereBatch, 0.1f, 26, 13); //往地板floorBatch批处理中添加顶点数据 floorBatch.Begin(GL_LINES, 324); for(GLfloat x = -20.0; x "},"pages/RxSwift系列/01_Observable_可监听序列.html":{"url":"pages/RxSwift系列/01_Observable_可监听序列.html","title":"1.Observable - 可监听序列","keywords":"","body":"1.Observable - 可监听序列 一、Observable 介绍 Observable 作为 Rx 的根基，我们首先对它要有一些基本的了解。 iOS-RxSwift-Tutorials 1.1 Observable Observable 这个类就是 Rx 框架的基础，我们可以称它为可观察序列。它的作用就是可以异步地产生一系列的 Event（事件），即一个 Observable 对象会随着时间推移不定期地发出 event(element : T) 这样一个东西。 而且这些 Event 还可以携带数据，它的泛型 就是用来指定这个 Event 携带的数据的类型。 有了可观察序列，我们还需要有一个 Observer（订阅者）来订阅它，这样这个订阅者才能收到 Observable 不时发出的 Event。 1.2 Event 查看 RxSwift 源码可以发现，事件 Event 的定义如下： public enum Event { /// Next element is produced. case next(Element) /// Sequence terminated with an error. case error(Swift.Error) /// Sequence completed successfully. case completed } 可以看到 Event 就是一个枚举，也就是说一个 Observable 是可以发出 3 种不同类型的 Event 事件： next：next 事件就是那个可以携带数据 的事件，可以说它就是一个“最正常”的事件。 参考文章 原文出自：www.hangge.com 转载请保留原文链接：https://www.hangge.com/blog/cache/detail_1922.html RxSwift中文文档 "},"pages/RxSwift系列/02_Observable_订阅_事件监听_订阅销毁.html":{"url":"pages/RxSwift系列/02_Observable_订阅_事件监听_订阅销毁.html","title":"2.Observable - 订阅、事件监听、订阅销毁","keywords":"","body":"2.Observable - 订阅、事件监听、订阅销毁 参考文章 Swift - RxSwift的使用详解1（基本介绍、安装配置） RxSwift中文文档 "},"pages/RxSwift系列/03_Observer_观察者.html":{"url":"pages/RxSwift系列/03_Observer_观察者.html","title":"3.Observer - 观察者","keywords":"","body":"3.Observer - 观察者 参考文章 Swift - RxSwift的使用详解1（基本介绍、安装配置） RxSwift中文文档 "},"pages/RxSwift系列/04_Observer_自定义可绑定属性.html":{"url":"pages/RxSwift系列/04_Observer_自定义可绑定属性.html","title":"4.Observer - 自定义可绑定属性","keywords":"","body":"4.Observer - 自定义可绑定属性 "},"pages/RxSwift系列/05_Subjects、Variables.html":{"url":"pages/RxSwift系列/05_Subjects、Variables.html","title":"5.Subjects、Variables","keywords":"","body":"5.Subjects、Variables "},"pages/RxSwift系列/06_变换操作符_buffer_map_flatMap_scan等.html":{"url":"pages/RxSwift系列/06_变换操作符_buffer_map_flatMap_scan等.html","title":"6.变换操作符：buffer、map、flatMap、scan等","keywords":"","body":"6.变换操作符：buffer、map、flatMap、scan等 参考文章 为什么RxSwift也需要flatMap "},"pages/RxSwift系列/07_过滤操作符_filter_take_skip等.html":{"url":"pages/RxSwift系列/07_过滤操作符_filter_take_skip等.html","title":"7.过滤操作符：filter、take、skip等","keywords":"","body":"7.过滤操作符：filter、take、skip等 参考文章 RxSwift 操作符 "},"pages/RxSwift系列/08_条件和布尔操作符_amb_takeWhile_skipWhile等.html":{"url":"pages/RxSwift系列/08_条件和布尔操作符_amb_takeWhile_skipWhile等.html","title":"8.条件和布尔操作符：amb、takeWhile、skipWhile等","keywords":"","body":"8.条件和布尔操作符：amb、takeWhile、skipWhile等 "},"pages/RxSwift系列/09_结合操作符_startWith_merge_zip等.html":{"url":"pages/RxSwift系列/09_结合操作符_startWith_merge_zip等.html","title":"9.结合操作符：startWith、merge、zip等","keywords":"","body":"9.结合操作符：startWith、merge、zip等 "},"pages/RxSwift系列/10_算数_聚合操作符_toArray_reduce_concat.html":{"url":"pages/RxSwift系列/10_算数_聚合操作符_toArray_reduce_concat.html","title":"10.算数&聚合操作符：toArray、reduce、concat","keywords":"","body":"10.算数&聚合操作符：toArray、reduce、concat "},"pages/RxSwift系列/11_连接操作符_connect_publish_replay_multicast.html":{"url":"pages/RxSwift系列/11_连接操作符_connect_publish_replay_multicast.html","title":"11.连接操作符：connect、publish、replay、multicast","keywords":"","body":"11.连接操作符：connect、publish、replay、multicast "},"pages/RxSwift系列/12_其他操作符_delay_materialize_timeout等.html":{"url":"pages/RxSwift系列/12_其他操作符_delay_materialize_timeout等.html","title":"12.其他操作符：delay、materialize、timeout等","keywords":"","body":"12.其他操作符：delay、materialize、timeout等 "},"pages/RxSwift系列/13_错误处理操作.html":{"url":"pages/RxSwift系列/13_错误处理操作.html","title":"13.错误处理操作","keywords":"","body":"13.错误处理操作 "},"pages/RxSwift系列/14_调试操作.html":{"url":"pages/RxSwift系列/14_调试操作.html","title":"14.调试操作","keywords":"","body":"14.调试操作 "},"pages/RxSwift系列/15_特征序列1_Single_Completable_Maybe.html":{"url":"pages/RxSwift系列/15_特征序列1_Single_Completable_Maybe.html","title":"15.特征序列1：Single、Completable、Maybe","keywords":"","body":"15.特征序列1：Single、Completable、Maybe "},"pages/RxSwift系列/16_特征序列2_Driver.html":{"url":"pages/RxSwift系列/16_特征序列2_Driver.html","title":"16.特征序列2：Driver","keywords":"","body":"16.特征序列2：Driver "},"pages/RxSwift系列/17_特征序列3_ControlProperty_ControlEvent.html":{"url":"pages/RxSwift系列/17_特征序列3_ControlProperty_ControlEvent.html","title":"17.特征序列3：ControlProperty、ControlEvent","keywords":"","body":"17.特征序列3：ControlProperty、ControlEvent "},"pages/RxSwift系列/18_调度器_subscribeOn_observeOn.html":{"url":"pages/RxSwift系列/18_调度器_subscribeOn_observeOn.html","title":"18.调度器、subscribeOn、observeOn","keywords":"","body":"18.调度器、subscribeOn、observeOn "},"pages/RxSwift系列/19_UI控件扩展1_UILabel.html":{"url":"pages/RxSwift系列/19_UI控件扩展1_UILabel.html","title":"19.UI控件扩展1：UILabel","keywords":"","body":"19.UI控件扩展1：UILabel "},"pages/RxSwift系列/20_UI控件扩展2_UITextField_UITextView.html":{"url":"pages/RxSwift系列/20_UI控件扩展2_UITextField_UITextView.html","title":"20.UI控件扩展2：UITextField、UITextView","keywords":"","body":"20.UI控件扩展2：UITextField、UITextView "},"pages/RxSwift系列/21_UI控件扩展3_UIButton_UIBarButtonItem.html":{"url":"pages/RxSwift系列/21_UI控件扩展3_UIButton_UIBarButtonItem.html","title":"21.UI控件扩展3：UIButton、UIBarButtonItem","keywords":"","body":"21.UI控件扩展3：UIButton、UIBarButtonItem "},"pages/RxSwift系列/22_UI控件扩展4_UISwitch_UISegmentedControl.html":{"url":"pages/RxSwift系列/22_UI控件扩展4_UISwitch_UISegmentedControl.html","title":"22.UI控件扩展4：UISwitch、UISegmentedControl","keywords":"","body":"22.UI控件扩展4：UISwitch、UISegmentedControl "},"pages/RxSwift系列/23_UI控件扩展5_UIActivityIndicatorView_UIApplication.html":{"url":"pages/RxSwift系列/23_UI控件扩展5_UIActivityIndicatorView_UIApplication.html","title":"23.UI控件扩展5：UIActivityIndicatorView、UIApplication","keywords":"","body":"23.UI控件扩展5：UIActivityIndicatorView、UIApplication "},"pages/RxSwift系列/24_UI控件扩展6_UISlider_UIStepper.html":{"url":"pages/RxSwift系列/24_UI控件扩展6_UISlider_UIStepper.html","title":"24.UI控件扩展6：UISlider、UIStepper","keywords":"","body":"24.UI控件扩展6：UISlider、UIStepper "},"pages/RxSwift系列/25_双向绑定_<->.html":{"url":"pages/RxSwift系列/25_双向绑定_<->.html","title":"25.双向绑定：<->","keywords":"","body":"25.双向绑定： "},"pages/RxSwift系列/26_UI控件扩展7_UIGestureRecognizer.html":{"url":"pages/RxSwift系列/26_UI控件扩展7_UIGestureRecognizer.html","title":"26.UI控件扩展7：UIGestureRecognizer","keywords":"","body":"26.UI控件扩展7：UIGestureRecognizer "},"pages/RxSwift系列/27_UI控件扩展8_UIDatePicker.html":{"url":"pages/RxSwift系列/27_UI控件扩展8_UIDatePicker.html","title":"27.UI控件扩展8：UIDatePicker","keywords":"","body":"27.UI控件扩展8：UIDatePicker "},"pages/RxSwift系列/28_UITableView的使用1_基本用法.html":{"url":"pages/RxSwift系列/28_UITableView的使用1_基本用法.html","title":"28.UITableView的使用1：基本用法","keywords":"","body":"28.UITableView的使用1：基本用法 "},"pages/RxSwift系列/29_UITableView的使用2_RxDataSources.html":{"url":"pages/RxSwift系列/29_UITableView的使用2_RxDataSources.html","title":"29.UITableView的使用2：RxDataSources","keywords":"","body":"29.UITableView的使用2：RxDataSources "},"pages/RxSwift系列/30_UITableView的使用3_刷新表格数据.html":{"url":"pages/RxSwift系列/30_UITableView的使用3_刷新表格数据.html","title":"30.UITableView的使用3：刷新表格数据","keywords":"","body":"30.UITableView的使用3：刷新表格数据 "},"pages/RxSwift系列/31_UITableView的使用5_可编辑表格.html":{"url":"pages/RxSwift系列/31_UITableView的使用5_可编辑表格.html","title":"31.UITableView的使用5：可编辑表格","keywords":"","body":"31.UITableView的使用5：可编辑表格 "},"pages/RxSwift系列/32_UITableView的使用6_不同类型的单元格混用.html":{"url":"pages/RxSwift系列/32_UITableView的使用6_不同类型的单元格混用.html","title":"32.UITableView的使用6：不同类型的单元格混用","keywords":"","body":"32.UITableView的使用6：不同类型的单元格混用 "},"pages/RxSwift系列/33_UITableView的使用7_样式修改.html":{"url":"pages/RxSwift系列/33_UITableView的使用7_样式修改.html","title":"33.UITableView的使用7：样式修改","keywords":"","body":"33.UITableView的使用7：样式修改 "},"pages/RxSwift系列/34_UICollectionView的使用1_基本用法.html":{"url":"pages/RxSwift系列/34_UICollectionView的使用1_基本用法.html","title":"34.UICollectionView的使用1：基本用法","keywords":"","body":"34.UICollectionView的使用1：基本用法 "},"pages/RxSwift系列/35_UICollectionView的使用2_RxDataSources.html":{"url":"pages/RxSwift系列/35_UICollectionView的使用2_RxDataSources.html","title":"35.UICollectionView的使用2：RxDataSources","keywords":"","body":"35.UICollectionView的使用2：RxDataSources "},"pages/RxSwift系列/36_UICollectionView的使用3_刷新集合数据.html":{"url":"pages/RxSwift系列/36_UICollectionView的使用3_刷新集合数据.html","title":"36.UICollectionView的使用3：刷新集合数据","keywords":"","body":"36.UICollectionView的使用3：刷新集合数据 "},"pages/RxSwift系列/37_UICollectionView的使用4_样式修改.html":{"url":"pages/RxSwift系列/37_UICollectionView的使用4_样式修改.html","title":"37.UICollectionView的使用4：样式修改","keywords":"","body":"37.UICollectionView的使用4：样式修改 "},"pages/RxSwift系列/38_UIPickerView的使用.html":{"url":"pages/RxSwift系列/38_UIPickerView的使用.html","title":"38.UIPickerView的使用","keywords":"","body":"38.UIPickerView的使用 "},"pages/RxSwift系列/39_URLSession的使用1_请求数据.html":{"url":"pages/RxSwift系列/39_URLSession的使用1_请求数据.html","title":"39.URLSession的使用1：请求数据","keywords":"","body":"39.URLSession的使用1：请求数据 "},"pages/RxSwift系列/40_URLSession的使用2_结果处理_模型转换.html":{"url":"pages/RxSwift系列/40_URLSession的使用2_结果处理_模型转换.html","title":"40.URLSession的使用2：结果处理、模型转换","keywords":"","body":"40.URLSession的使用2：结果处理、模型转换 "},"pages/RxSwift系列/41_结合RxAlamofire使用1_数据请求.html":{"url":"pages/RxSwift系列/41_结合RxAlamofire使用1_数据请求.html","title":"41.结合RxAlamofire使用1：数据请求","keywords":"","body":"41.结合RxAlamofire使用1：数据请求 "},"pages/RxSwift系列/42_结合RxAlamofire使用2_结果处理_模型转换.html":{"url":"pages/RxSwift系列/42_结合RxAlamofire使用2_结果处理_模型转换.html","title":"42.结合RxAlamofire使用2：结果处理、模型转换","keywords":"","body":"42.结合RxAlamofire使用2：结果处理、模型转换 "},"pages/RxSwift系列/43_结合RxAlamofire使用3_文件上传.html":{"url":"pages/RxSwift系列/43_结合RxAlamofire使用3_文件上传.html","title":"43.结合RxAlamofire使用3：文件上传","keywords":"","body":"43.结合RxAlamofire使用3：文件上传 "},"pages/RxSwift系列/44_结合RxAlamofire使用4_文件下载.html":{"url":"pages/RxSwift系列/44_结合RxAlamofire使用4_文件下载.html","title":"44.结合RxAlamofire使用4：文件下载","keywords":"","body":"44.结合RxAlamofire使用4：文件下载 "},"pages/RxSwift系列/45_结合Moya使用1_数据请求.html":{"url":"pages/RxSwift系列/45_结合Moya使用1_数据请求.html","title":"45.结合Moya使用1：数据请求","keywords":"","body":"45.结合Moya使用1：数据请求 "},"pages/RxSwift系列/46_结合Moya使用2_结果处理_模型转换.html":{"url":"pages/RxSwift系列/46_结合Moya使用2_结果处理_模型转换.html","title":"46.结合Moya使用2：结果处理、模型转换","keywords":"","body":"46.结合Moya使用2：结果处理、模型转换 "},"pages/RxSwift系列/47_MVVM架构演示2_使用Observable样例.html":{"url":"pages/RxSwift系列/47_MVVM架构演示2_使用Observable样例.html","title":"47.MVVM架构演示2：使用Observable样例","keywords":"","body":"47.MVVM架构演示2：使用Observable样例 "},"pages/RxSwift系列/48_MVVM架构演示3_使用Driver样例.html":{"url":"pages/RxSwift系列/48_MVVM架构演示3_使用Driver样例.html","title":"48.MVVM架构演示3：使用Driver样例","keywords":"","body":"48.MVVM架构演示3：使用Driver样例 "},"pages/RxSwift系列/49_一个用户注册样例1_基本功能实现.html":{"url":"pages/RxSwift系列/49_一个用户注册样例1_基本功能实现.html","title":"49.一个用户注册样例1：基本功能实现","keywords":"","body":"49.一个用户注册样例1：基本功能实现 "},"pages/RxSwift系列/50_结合MJRefresh使用1_下拉刷新.html":{"url":"pages/RxSwift系列/50_结合MJRefresh使用1_下拉刷新.html","title":"50.结合MJRefresh使用1：下拉刷新","keywords":"","body":"50.结合MJRefresh使用1：下拉刷新 "},"pages/RxSwift系列/51_DelegateProxy样例1_获取地理定位信息.html":{"url":"pages/RxSwift系列/51_DelegateProxy样例1_获取地理定位信息.html","title":"51.DelegateProxy样例1：获取地理定位信息","keywords":"","body":"51.DelegateProxy样例1：获取地理定位信息 "},"pages/RxSwift系列/52_DelegateProxy样例2_图片选择功能.html":{"url":"pages/RxSwift系列/52_DelegateProxy样例2_图片选择功能.html","title":"52.DelegateProxy样例2：图片选择功能","keywords":"","body":"52.DelegateProxy样例2：图片选择功能 "},"pages/RxSwift系列/53_DelegateProxy样例3_应用生命周期的状态变化.html":{"url":"pages/RxSwift系列/53_DelegateProxy样例3_应用生命周期的状态变化.html","title":"53.DelegateProxy样例3：应用生命周期的状态变化","keywords":"","body":"53.DelegateProxy样例3：应用生命周期的状态变化 "},"pages/RxSwift系列/54_sendMessage和methodInvoked的区别.html":{"url":"pages/RxSwift系列/54_sendMessage和methodInvoked的区别.html","title":"54.sendMessage和methodInvoked的区别","keywords":"","body":"54.sendMessage和methodInvoked的区别 "},"pages/RxSwift系列/55_订阅UITableViewCell里的按钮点击事件.html":{"url":"pages/RxSwift系列/55_订阅UITableViewCell里的按钮点击事件.html","title":"55.订阅UITableViewCell里的按钮点击事件","keywords":"","body":"55.订阅UITableViewCell里的按钮点击事件 "},"pages/RxSwift系列/56_通知NotificationCenter的使用.html":{"url":"pages/RxSwift系列/56_通知NotificationCenter的使用.html","title":"56.通知NotificationCenter的使用","keywords":"","body":"56.通知NotificationCenter的使用 "},"pages/RxSwift系列/57_键值观察KVO的使用.html":{"url":"pages/RxSwift系列/57_键值观察KVO的使用.html","title":"57.键值观察KVO的使用","keywords":"","body":"57.键值观察KVO的使用 "},"pages/RxSwift系列/58_表格图片加载优化.html":{"url":"pages/RxSwift系列/58_表格图片加载优化.html","title":"58.表格图片加载优化","keywords":"","body":"58.表格图片加载优化 "},"pages/RxSwift系列/59_检测当前值与初始值是否相同_isEqualOriginValue.html":{"url":"pages/RxSwift系列/59_检测当前值与初始值是否相同_isEqualOriginValue.html","title":"59.检测当前值与初始值是否相同：isEqualOriginValue","keywords":"","body":"59.检测当前值与初始值是否相同：isEqualOriginValue "},"pages/RxSwift系列/60_重复执行某个操作序列_repeatWhen.html":{"url":"pages/RxSwift系列/60_重复执行某个操作序列_repeatWhen.html","title":"60.重复执行某个操作序列：repeatWhen","keywords":"","body":"60.重复执行某个操作序列：repeatWhen "},"pages/RxSwift系列/61_监听滚动条滚动到底部的行为_reachedBottom.html":{"url":"pages/RxSwift系列/61_监听滚动条滚动到底部的行为_reachedBottom.html","title":"61.监听滚动条滚动到底部的行为：reachedBottom","keywords":"","body":"61.监听滚动条滚动到底部的行为：reachedBottom "},"pages/RxSwift系列/62_RxFeedback架构1_安装配置、基本用法.html":{"url":"pages/RxSwift系列/62_RxFeedback架构1_安装配置、基本用法.html","title":"62.RxFeedback架构1：安装配置、基本用法","keywords":"","body":"62.RxFeedback架构1：安装配置、基本用法 "},"pages/RxSwift系列/63_RxFeedback架构2：一个用户注册样例.html":{"url":"pages/RxSwift系列/63_RxFeedback架构2：一个用户注册样例.html","title":"63.RxFeedback架构2：一个用户注册样例","keywords":"","body":"63.RxFeedback架构2：一个用户注册样例 "},"pages/RxSwift系列/64_RxFeedback架构3_GitHub资源搜索样例.html":{"url":"pages/RxSwift系列/64_RxFeedback架构3_GitHub资源搜索样例.html","title":"64.RxFeedback架构3：GitHub资源搜索样例","keywords":"","body":"64.RxFeedback架构3：GitHub资源搜索样例 "},"pages/RxSwift系列/65_ReactorKit架构1_安装配置、基本用法.html":{"url":"pages/RxSwift系列/65_ReactorKit架构1_安装配置、基本用法.html","title":"65.ReactorKit架构1：安装配置、基本用法","keywords":"","body":"65.ReactorKit架构1：安装配置、基本用法 "},"pages/RxSwift系列/66_ReactorKit架构2_一个用户注册样例.html":{"url":"pages/RxSwift系列/66_ReactorKit架构2_一个用户注册样例.html","title":"66.ReactorKit架构2：一个用户注册样例","keywords":"","body":"66.ReactorKit架构2：一个用户注册样例 "},"pages/RxSwift系列/67_ReactorKit架构3_GitHub资源搜索样例.html":{"url":"pages/RxSwift系列/67_ReactorKit架构3_GitHub资源搜索样例.html","title":"67.ReactorKit架构3：GitHub资源搜索样例","keywords":"","body":"67.ReactorKit架构3：GitHub资源搜索样例 "},"pages/WebView系列/01_WebView与iOS原生的交互.html":{"url":"pages/WebView系列/01_WebView与iOS原生的交互.html","title":"1.WebView与iOS原生的交互","keywords":"","body":"1.WebView与iOS原生的交互 一、WKWebView的代理方法 1.1 WKNavigationDelegate 该代理提供的方法，可以用来追踪加载过程（页面开始加载、加载完成、加载失败）、决定是否执行跳转。 // 页面开始加载时调用 optional func webView(_ webView: WKWebView, didStartProvisionalNavigation navigation: WKNavigation!) // 当内容开始返回时调用 optional func webView(_ webView: WKWebView, didCommit navigation: WKNavigation!) // 页面加载完成之后调用 optional func webView(_ webView: WKWebView, didFinish navigation: WKNavigation!) // 页面加载失败时调用 optional func webView(_ webView: WKWebView, didFailProvisionalNavigation navigation: WKNavigation!, withError error: Error) 页面跳转的代理方法有三种，分为（收到跳转与决定是否跳转两种）： // 接收到服务器跳转请求之后调用 optional func webView(_ webView: WKWebView, didReceiveServerRedirectForProvisionalNavigation navigation: WKNavigation!) // 在收到响应后，决定是否跳转 optional func webView(_ webView: WKWebView, decidePolicyFor navigationResponse: WKNavigationResponse, decisionHandler: @escaping (WKNavigationResponsePolicy) -> Void) // 在发送请求之前，决定是否跳转 optional func webView(_ webView: WKWebView, decidePolicyFor navigationAction: WKNavigationAction, decisionHandler: @escaping (WKNavigationActionPolicy) -> Void) 1.2 WKUIDelegate optional func webView(_ webView: WKWebView, createWebViewWith configuration: WKWebViewConfiguration, for navigationAction: WKNavigationAction, windowFeatures: WKWindowFeatures) -> WKWebView? 下面代理方法全都是与界面弹出提示框相关的，针对于web界面的三种提示框（警告框、确认框、输入框）分别对应三种代理方法。下面只列举了警告框的方法。 optional func webView(_ webView: WKWebView, runJavaScriptAlertPanelWithMessage message: String, initiatedByFrame frame: WKFrameInfo, completionHandler: @escaping () -> Void) 1.3 WKScriptMessageHandler WKScriptMessageHandler其实就是一个遵循的协议，它能让网页通过JS把消息发送给OC。其中协议方法。 // 从web界面中接收到一个脚本时调用 func userContentController(_ userContentController: WKUserContentController, didReceive message: WKScriptMessage) 从协议中我们可以看出这里使用了两个类WKUserContentController和WKScriptMessage。WKUserContentController可以理解为调度器，WKScriptMessage则是携带的数据。 1.4 WKUserContentController WKUserContentController有两个核心方法，也是它的核心功能。 // js注入，即向网页中注入我们的js方法，这是一个非常强大的功能，开发中要慎用。 open func addUserScript(_ userScript: WKUserScript) // 添加供js调用oc的桥梁。这里的name对应WKScriptMessage中的name，多数情况下我们认为它就是方法名。 open func add(_ scriptMessageHandler: WKScriptMessageHandler, name: String) 1.5 WKScriptMessage WKScriptMessage就是js通知oc的数据。其中有两个核心属性用的很多。 open var name: String { get } 对应func add(_ scriptMessageHandler: WKScriptMessageHandler, name: String)添加的name。 open var body: Any { get } 携带的核心数据。js调用时只需window.webkit.messageHandlers.#name#.postMessage() 这里的name就是我们添加的name，是不是感觉很爽，就是这么简单，下面我们就来具体实现。 二、自定义CQWebView class CQWebView: UIView { // 增加webView属性 private lazy var webView: WKWebView = { let conf = WKWebViewConfiguration() conf.preferences.javaScriptEnabled = true conf.selectionGranularity = WKSelectionGranularity.character conf.allowsInlineMediaPlayback = true let webView = WKWebView(frame: .zero, configuration: conf) ... return webView }() } 2.1 增加js/oc交互方法 class CQWebView: UIView { ... // 增加js消息监听 func adddScriptMessageHandler(forName name: String) { webView.configuration.userContentController.add(self, name: name) } // 移除js消息监听 func removeScriptMessageHandler(forName name: String) { webView.configuration.userContentController.removeScriptMessageHandler(forName: name) } // oc执行js func evaluateJavaScript(_ javaScriptString: String, completionHandler: ((Any?, Error?) -> Void)? = nil) { webView.evaluateJavaScript(javaScriptString, completionHandler: completionHandler) } } 2.2 增加代理属性 // WebView代理 @objc protocol CQWebViewDelegate { // 接收 js 发来的消息 @objc optional func webView(_ webView: CQWebView, didReceiveMessage name: String, body: Any) } class CQWebView: UIView { ... weak var delegate: CQWebViewDelegate? } 2.3 实现WKScriptMessageHandler // js 和 swift 的交互 extension CQWebView: WKScriptMessageHandler { // 接收 js 发来的消息 func userContentController(_ userContentController: WKUserContentController, didReceive message: WKScriptMessage) { delegate?.webView?(self, didReceiveMessage: message.name, body: message.body) } } 三、JS调用Swift 2.1 完整html Untitled Document js调用swift // js调用swift function jsCallSwift(obj) { // 向 swift 发送数据，这里的‘msgBridge’就是 swift 中添加的消息通道的 name window.webkit.messageHandlers.msgBridge.postMessage(obj); } // swift调用js function swiftCallJs(msg){ document.getElementById('h').innerText+=msg; } 3.2 增加对js消息的监听 只需要调用CQWebView的adddScriptMessageHandler方法。 class ViewController: UIViewController { // MARK: - Properties private lazy var webView: CQWebView = { let webView = CQWebView(frame: .zero) webView.delegate = self webView.adddScriptMessageHandler(forName: \"msgBridge\") ... return webView }() } 3.2 实现对js消息的处理 extension ViewController: CQWebViewDelegate { func webView(_ webView: CQWebView, didReceiveMessage name: String, body: Any) { switch name { case \"msgBridge\": ... break default: break } } } 四、Swift调用JS 4.1 evaluateJavaScript方法使用 在html的js中已经定义了swiftCallJs方法等待调用，只需要调用CQWebView的evaluateJavaScript方法即可。 //swift 调 js函数 webView.evaluateJavaScript(\"swiftCallJs('\\( dic[\"msg\"] as! String)')\", completionHandler: { (any, error) in if (error != nil) { print(error ?? \"err\") } }) 4.2 WKWebView加载JS NSString *js = @\"\"; // 根据JS字符串初始化WKUserScript对象 WKUserScript *script = [[WKUserScript alloc] initWithSource:js injectionTime:WKUserScriptInjectionTimeAtDocumentEnd forMainFrameOnly:YES]; // 根据生成的WKUserScript对象，初始化WKWebViewConfiguration WKWebViewConfiguration *config = [[WKWebViewConfiguration alloc] init]; [config.userContentController addUserScript:script]; 参考文章 源码 Safari调试iOS中的JS iOS WKWebView 加载本地html文件（swift） 学习-WebKit(WKScriptMessageHandler) iOS下OC与JS的交互(WKWebview-MessageHandler实现) 自己动手打造基于 WKWebView 的混合开发框架（二）——js 向 Native 一句话传值并反射出 Swift 对象执行指定函数 WkWebKit - javascript on loaded page finds window.webkit is undefined WKWebview使用二三事 "},"pages/WebView系列/02_DSBridge的使用.html":{"url":"pages/WebView系列/02_DSBridge的使用.html","title":"2.DSBridge的使用","keywords":"","body":"2.DSBridge的使用 参考文章 DSBridge框架使用说明 使用DSBridge同H5交互 "},"pages/计算机网络原理/精讲1.html":{"url":"pages/计算机网络原理/精讲1.html","title":"1.1.精讲1","keywords":"","body":"1.1.精讲1 计算机网络的基本概念 计算机网络的起源：计算机网络是计算机技术与通信技术相互融合的产物。 计算机网络的定义：互联、自治的计算机的集合。 互联：todo 自制：todo 互联网与因特网：泛指由多个计算机网络互联而成的网络。 因特网：它指当前全球最大的、应用最广泛的计算机网络。 网络协议：网络通信实体之间在数据交换过程中需要遵循的规则或约定。 网络协议三要素：语法、语义、时序。 语法：定义实体之间交换信息的格式和结构。 语义：定义实体之间交换信息中的控制信息。 时序：定义实体之间交换信息的顺序以及如何匹配或者适应彼此的速度。 计算机网络的功能 硬件资源共享：计算资源（CPU）、存储资源、打印机与扫描仪I/O等，例：云存储、云计算。 软件资源共享：SaaS，例：大型办公软件、大型数据库系统等。 信息资源共享：信息检索，新闻浏览等。 计算机网络的分类 按覆盖范围分类 个域网：PAN，随身穿戴设备、便携设备通过无线技术构成的小范围网络，例如：蓝牙耳机和手机。 局域网：LAN，通常部署在办公室、办公楼、厂区、校区等。 城域网：MAN，覆盖一个城市范围的网络。 广域网：WAN，覆盖范围在几十到几千千米，可以实现异地城域网的互联。 按拓扑结构分类：星型、总线型、环型、网状、树形、混合型。 按交换方式分类 按网络用户属性 拓扑 topology，是研究几何图形或空间在连续改变形状后还能保持不变的一些特质的一个学科。它只考虑物体间的位置关系而不考虑他们的形状和大小。 星型 包括一个中央结点，网络中的主机通过点对点通信链路与中央结点链接。 优点：易于监控管理、故障诊断、隔离。 缺点：中央结点一旦故障，全网瘫痪。 总线型 网络采用一条广播信道作为公共传输介质。所有结点均与总线连接，结点间的通信均通过共享的总线进行。 优点：结构简单，电缆数量少，易于扩展。 缺点：中央结点一旦故障，全网瘫痪。 环形 利用通信链路将所有结点连成一个闭合的环。 优点：电缆长度比较短，可以使用光纤，易于避免冲突。 缺点：某结点故障引起全网瘫痪，加新（撤出）结点比较麻烦，等待时间较长。 网状 网络中的结点通过多条链路与不同的结点直接相连接。 优点：网络可靠性高，一条或多条链路故障时，网络仍然可以联通。 缺点：网络结构复杂，成本高。 树形 可以看作是总线型拓扑或星型拓扑结构网络的扩展。 优点：易于扩展，故障易隔离。 缺点：根结点要求高。 "},"pages/计算机网络原理/精讲2.html":{"url":"pages/计算机网络原理/精讲2.html","title":"1.2.精讲2","keywords":"","body":"1.2.精讲2 数据交换技术 todo 计算机网络性能 todo 计算机网络体系结构 todo "},"pages/计算机网络原理/精讲3.html":{"url":"pages/计算机网络原理/精讲3.html","title":"1.3.精讲3","keywords":"","body":"1.3.精讲3 域名系统（DNS） 根域名服务器 顶级域名服务器 权威域名服务器 中间域名服务器 域名解析过程 "},"pages/计算机网络原理/精讲5.html":{"url":"pages/计算机网络原理/精讲5.html","title":"1.5.精讲5","keywords":"","body":"1.5.精讲5 传输层 传输层的基本服务 传输层的复用和分解 等-停协议与滑动窗口协议 用户数据报协议（UDP） 传输控制协议（TCP） 一、传输层的基本服务 传输层的核心任务：应用进程之间提供端到端的逻辑通信服务。 1.1 传输层的功能 对应用层报文进行分段和重组 面向应用层实现复用和分解 实现端对端的流量控制 拥塞控制 传输层寻址 对报文进行差错检测 实现进程间的端到端可靠数据传输控制 吩咐刘拥寻差错-可靠 1.2 传输层寻址与端口 TCP/IP体系结构网络的解决方法：在传输层使用协议端口号，通常简称端口（port），在全网范围内利用“IP地址+端口号”唯一标识一个通信端点。 应用层与传输层间抽象的协议端口是软件端口。 传输层端口号为16位整数，可以编号65536个（2的16次方）。 小于26的端口号为常用端口号。 DNS默认的端口是53。 端口号 描述 类型 0-1023 熟知端口号 服务器端口号 1024-49151 登记端口号 服务器端口号 49152-65535 客户端号，或短暂端口号 客户端口号 1.3 无连接服务与面向连接服务 无连接服务 面向连接服务 数据传输之前：无需与对端进行任何信息交换，直接构造传输层报文段并向接受端发送。 数据传输之前：需要双方交换一些控制信息，建立逻辑连接，然后再传输数据，传输结束后还需要拆除连接。 类似于信件通信 类似于电话通信 二、传输层的复用和分解 2.1 复用和分解 多路分解：在目的主机，传输层协议读取报文段中的字段，标识出接收套接字，进而通过该套接字，再传输层的报文段中的数据交付给正确的套接字。 多路复用：再源主机，传输层协议从不同的套接字收集应用进程发送的数据块，并为每个数据块封装上首部信息（包括用于分解的信息）构成报文段，然后将报文段传递给网络层。 总结：支持众多应用进程共用同一个传输层协议，并能够将接收到的数据准确交付给不同的应用进程，成为传输层的多路复用与多路分解（简称复用与分解，也称为复用与分用）。 2.1 无连接的多路复用和分解 用户数据报协议（User Datagram Protocol,UDP）：Internet提供无连接服务的传输层协议。 UDP将应用层的数据封装成一个UDP报文段。 UDP套接字二元组：。 2.2 面向连接的多路复用和分解 传输控制协议（Transmission Control Protocol,TCP）：Internet提供面向连接服务的传输层协议。 TCP套接字四元组：。 三、等-停协议与滑动窗口协议 3.1 可靠数据传输基本原理 不可靠传输信道再数据传输中可能发生： 比特差错：1001-1000 乱序：数据块1、2、5、6、3、4 数据流失：数据块1、2、5 很多网络应用希望传输层提供可靠的数据传输服务。传输层主要有两个协议：TCP和UDP。TCP提供可靠数据传输服务。TCP要采取适当的措施，保证可靠传输数据。 基于不可靠信道实现可靠数据传输采取的措施： 差错检测：利用差错编码实现数据包传输过程中的比特差错检测。 确认：接收方向发送方反馈接收状态。ACK（肯定确认）；NAK（否定确认）。 重传：发送方重新发送接收方没有正确接收的数据。 序号：确保数据按序提交。 计时器：解决数据丢失问题。 3.2 停等协议 自动重传请求协议ARQ。 发送方，发送数据，等待确认。 接收方，肯定确认/否定确认。 发送方，后续数据/重传数据。 性能差，信道利用率低。 3.3 滑动窗口协议 流水线协议：管道协议。允许发送方再没有收到确认前连续发送多个分组。最典型的流水线协议：滑动窗口协议。 增加分组序号。 发送方和接收方可以换成多个分组。 确认只返回ACK。例如：正确接收分组01，则返回ACK01。 发送方的发送窗口（Ws）：发送方可以发送未被确认分组的最大数量。 接收方的接收窗口（Wr）：接收方可以缓存的正确到达的分组的最大数量。 滑动窗口协议，根据接收窗口的大小，可以具体分为： 回退N步协议：GBN协议（Go-Back-N） 选择重传协议：SR协议（Selective Repeat) GBN协议：发送窗口Ws>=1，接收窗口Wr=1，未按序到达的分组需要重传。 SR协议：发送窗口Ws>1，接收窗口Wr>1。发送方重传未被接收方确认的分组。 "},"pages/计算机网络原理/精讲6.html":{"url":"pages/计算机网络原理/精讲6.html","title":"1.6.精讲6","keywords":"","body":"1.6.精讲6 一、用户数据报协议（UDP） 用户数据报协议（User Datagram Protocol UDP）：Internet传输层协议，提供无连接、不可靠、数据报尽力传输服务。 1.1 UDP特点 应用进程容易控制发送什么数据以及何时发送，会出现分组丢失和重复。 无需建立连接。 无连接状态。 首部开销小，只有8个字节。 1.2 UDP数据报结构 UDP数据报首部：源端口号（16位）、目的端口号（16位）、长度（16位）、校验和（16位），一共8字节。 UDP数据报数据字段：应用数据 1.3 UDP校验和 提供差错检测功能，UDP的校验和用于检测UDP报文段从源到目的地传送过程中，其中数据是否发生了改变。 UDP校验和计算的内容包括3部分：UDP伪首部、UDP首部、应用数据。 1.4 UDP伪首部结构 源IP地址、目的IP地址、协议号：对应封装UDP数据报的IP分组的字段。 UDP长度字段：是该UDP数据报的字段，该字段参与计算两次。 UDP协议号：17。 1.5 UDP校验和计算规则 所有参与运算内容按16位对齐求和。 求和过程中遇到溢出（即进位）都被回卷（即进位与最低位再相加），最后得到的和取反码，就是UDP的校验和，填入UDP数据报的校验和字段。 UDP再生成校验和的时候，校验和字段全取0。 二、传输控制协议（TCP） 2.1 TCP报文段结构 传输控制协议(Transmission Control Protocol ,TCP)：Internet传输层协议。提供面向连接、可靠、有序、字节流 传输服务。 第一、应用进程先建立连接。 第二、每一条TCP连接只有两个端点。 第三、可靠交付：无差错，不丢失，不重复，按序到达 第四、全双工通信。 第五、面向字节流。 流：字节序列。应用程序和TCP的交互是一个个数据块，TCP把他们看做是无结构字节。 graph LR 应用层报文-->传输层TCP报文段 最大报文段长度（Maximum Segment Size,MSS）：报文段中封装的应用层数据的最大长度。 源端口号字段，目的端口号字段分别占16位。复用/分解来自/送到上层应用的数据。 序号字段、确认序号字段分别占32位。 序号字段：TCP的序号是对每个应用层数据的每个字节进行编号； 确认序号字段：是期望从对方接收数据的字节序号，即该序号对应的字节尚未收到； 首部长度字段占4位。指出TCP段的首部长度，以4字节为计算单位。最短是20字节；最长是60字节。 保留字段占6位。保留为今后使用，目前值为0。 URG、ACK、PSH、RST、SYN、FIN各占1位。为标志位字段；各占1位，取值为0或1； 紧急URG=1，紧急指针字段有效，优先传送。 确认ACK=1，确认序号字段有效；ACK=0时，确认序号字段无效。 推送PSH=1，尽快将报文段中的数据交付接收应用进程，不要等缓存满了再交付。 复位RST=1，TCP连接出现严重差错，释放连接，再重新建立TCP连接。 同步SYN=1，该TCP报文段是一个建立新连接请求控制段或者同意建立新连接的确认段。 终止FIN=1，TCP报文段的发送端数据已经发送完毕，请求释放连接。 接收窗口字段占16位。向对方通告我方接收窗口的大小。 校验和字段占16位。校验和字段检验的范围类似于UDP，计算方法与UDP校验和的计算方法相同。TCP协议号是6。 紧急指针字段占16位。URG=1时，才有效。指出在本TCP报文段中紧急数据共有多少个字节 选项字段长度可变，基本不用。最短为0字节，最长为40字节。 填充字段，取值全为0，目的是为了整个首部长度是4字节的整倍数。 要点： 序号字段：TCP的序号是对每个应用层数据的每个字节进行编号， 确认序号字段：是期望从对方接收数据的字节序号，即该序号对应的字节尚未收到。用ack_seq表示。 TCP段的首部长度最短是20字节。 2.2 TCP连接管理 TCP连接管理：连接建立与连接拆除。 TCP连接建立： 第一次握手： 客户向服务器发送连接请求段：SYN=1,seq=x SYN=1：建立连接请求控制段 seq=x：表示传输的报文段的第1个数据字节的序列号是x，并以此序列号代表整个报文段的序号 （补充：sequence number，序号的意思。） 客户端进入SYN_SEND（同步发送） 第二次握手：服务器收到TCP连接请求段后，如同意，则发回确认报文段： （SYN=1,ACK=1,seq=y, ack_seq=x+1） SYN=1：同意建立新连接的确认段 ack_seq=x+1：表示已经收到了序列号为x的报文段，准备接收序列号为x+1的报文段。 seq=y：服务器告诉客户确认报文段的序列号是y。 服务器由LISTEN进入SYN_RCVD（同步收到） 第三次握手：客户对服务器的 同意连接报文段 进行确认： （ACK=1,seq=x+1,ack_seq=y+1） seq=x+1：客户此次的报文段的序列号是x+1。 ack_seq=y+1：客户期望接收服务器序列号为y+1的报文段。 当客户发送ACK时，客户端进入ESTABLISHED状态； 当服务收到ACK后，也进入ESTABLISHED状态； 只有第三次握手可携带数据。 TCP连接拆除： 第一次挥手：客户向服务器发送释放连接报文段：（FIN=1,seq=u） FIN=1：发送端数据发送完毕，请求释放连接。 seq=u：传输的第一个数据字节的序号是u 客户端状态由ESTABLISHED进入FIN_WAIT_1（终止等待1状态） 第二次挥手：服务器向客户发送确认段：（ACK=1,seq=v,ack_seq=u+1） ACK=1：确认字号段有效。 ack_seq=u+1：服务器期望接收客户数据序号为u+1。 seq=v：服务器传输的数据序号是v。 服务器状态由ESTABLISHED进入CLOSE_WAIT（关闭等待） 客户端收到ACK段后，由FIN_WAIT_1进入FIN_WAIT_2 第三次挥手：服务器向客户发送释放连接报文段：（FIN=1,ACK=1,seq=w,ack_seq=u+1） FIN=1：请求释放连接 ACK=1：确认字号段有效。 ack_seq=u+1：表示服务器期望接收客户数据序号为u+1。 seq=w：表示自己传输的第一个数据字节的序号是w。 服务器状态由CLOSE_WAIT进入LAST_ACK（最后确认状态） 第四次挥手：客户向服务器发送确认段：（ACK=1,seq=u+1,ack_seq=w+1） ACK=1：确认字号段有效。 ack_seq=w+1：表示客户期望接收服务器数据序号为w+1。 seq=u+1：表示客户传输的数据的序号是u+1。 客户端状态由FIN_WAIT_2进入TIME_WAIT，等待2MSL时间，进入CLOSED状态。 服务器在收到最后一次ACK后，由LAST_ACK进入CLOSED。 2.3 TCP可靠数据传输 TCP实现可靠数据传输服务的工作机制： 可靠：保证接收方应用进程从缓冲区读出的字节流与发送方发出的字节流是完全一样的。 一、TCP实现可靠数据传输服务的工作机制： 1、应用层数据被分割成TCP认为最适合发送的数据块。 2、TCP发出一个段后，启动一个计时器，等待目的端确认收到这个报文段。 3、TCP首部中设有校验和字段，用于检测数据在传输过程中是否发生差错。 4、TCP报文段到目的端可能会失序，TCP会重新排序。 5、接收端收到重复的报文段，接收端根据序号把重复的报文段丢弃。 6、TCP提供流量控制。 保证接收方进程从缓冲区读出的字节流与发送方发出的字节流是完全一样的。 1、校验：与UDP一样 2、序号 3、确认 4、重传 5、计时器 序号：应用层数据被分割成TCP认为最适合发送的数据块。一个字节占用一个序号。序号字段指的就是一个报文段第一个字节的序号。 确认：TCP采用累积确认。此时返回确认序号为4。 TCP生成ACK的策略： 1、具有所期望序号的报文段按序到达，所有在期望序号及以前的报文段都已被确认。TCP延迟500ms发送ACK。 2、具有所期望序号的报文段按序到达、且另一个按序报文段在等待ACK传输，TCP接收方立即发送单个累计ACK，确认以上两个按序到达报文段。 3、拥有序号大于期望序号的失序报文段到达，TCP接收方立即发送重复ACK，指示下一个期望接收字节的序号。 4、收到一个报文段，部分或完全填充接收数据间隔。 重传：快速重传: TCP发送方接收到对相同序号的3次重复ACK，就说明被重复确认的报文段已丢失，这时候即便没有超时，也会重发该报文段。 计时器：计时器超时时间的调整。 计时器超时时间设置： TimeoutInerval=EstimatedRTT+4×DevRTT EstimatedRTT：抽样RTT的加权移动平均值。 DevRTT：偏差RTT 2.4 TCP流量控制 2.5 TCP拥塞控制 "},"pages/信息系统开发与管理/第一章_管理信息系统导论/第一节_管理信息系统概念及其发展.html":{"url":"pages/信息系统开发与管理/第一章_管理信息系统导论/第一节_管理信息系统概念及其发展.html","title":"第一节：管理信息系统概念及其发展","keywords":"","body":"第一节：管理信息系统概念及其发展 管理信息系统概念的起源 用“三化”来概括人类社会发展的历程，“三化”分别是指农业化、工业化和信息化。 高登·戴维斯定义管理信息系统说明其目标在三个层次上支持管理活动，高、中、低三个层次分别对应决策层、管理层和执行层。 人类发展历程的“三化”分别指农业化、工业化和信息化。 管理信息系统是现代化和信息技术的应用不断融合的产物。 现代化和信息技术不断融合的产物是管理信息系统。 1985年，管理信息系统的创始人高登·戴维斯给管理信息系统做了一个完整的定义：“它是一个利用计算机硬件和软件，手工作业，分析、计划、控制和决策模型，以及数据库的用户-机器系统。它能提供信息，支持企业或组织的运行、管理和决策功能”。 管理信息系统概念的演进 在管理领域，我们认为管理信息系统就是信息系统。 管理信息系统的定义 系统设计者不懂现代管理知识，是不可能设计出好的管理信息系统的。正确 计算机是管理信息系统的必要条件。错误 管理信息系统是一个一体化集成系统，包括数据的一体化和系统开发的一体化两个含义。 MIS是一个“一体化系统”，又称集成系统。 管理信息系统是一个一体化系统,一体化具有两个含义：数据的一体化和系统开发的一体化。 管理信息系统是以计算机技术、通信技术和软件技术为技术基础，最终服务与组织整体的管理和决策。 管理信息系统是一个“集成系统”，又称一体化系统。 只要有管理，就要有信息，就必然有管理信息系统。 管理信息系统=人+计算机，即管理信息系统是一个合成系统。 管理信息系统是利用现代管理的先进技术、方法和工具，向各级管理者提供经营管理的决策支持。 管理信息系统是一个“一体化”系统，其中“一体化”包含两个含义：数据的一体化和系统开发的一体化。 简述管理信息系统一体化的含义 管理信息系统是一个“一体化系统”或称“集成系统”。“一体化”具有两个含义： 数据的一体化：数据在屋里存储上可以分布存放，但在逻辑上却需要由统一的部门，统一的人员集中管理； 系统开发的一体化：要按总体规划，分布实施的原则进行管理信息系统的建设。 名词解释：管理信息系统。 管理信息系统是一个由人、机（计算机）组成的能进行管理信息的收集、传递、存储、加工、维护和使用的系统。 简述管理信息系统的基本功能。 管理信息系统是一个人机系统，同时它又是一个一体化集成系统。它以计算机技术、通信技术和软件技术为技术基础，将现代管理理论、现代管理方法及各级管理人员融为一体，最终为某个组织整体的管理与决策服务。 管理信息系统以计算机技术，通信技术和软件技术为技术基础，将现代管理理论、现代管理方法及各级管理人员融为一体，最终服务于组织整体的管理与决策。 "},"pages/信息系统开发与管理/第一章_管理信息系统导论/第二节_管理信息系统的分类.html":{"url":"pages/信息系统开发与管理/第一章_管理信息系统导论/第二节_管理信息系统的分类.html","title":"第二节：管理信息系统的分类","keywords":"","body":"第二节：管理信息系统的分类 管理信息系统从4个角度进行分类，不包括（A） A 功能结构 B 管理应用层次 C 数据处理方式 D 核心业务 关于管理信息系统的分类，电子商务系统隶属于（C）分类方式。 A 行业和业务职能 B 管理应用层次 C 核心业务 D 数据处理方式 在管理信息系统的分类中，电子业务系统的分类方式是依据（B） A 数据处理方式 B 核心业务活动 C 管理应用层次 D 行业和业务职能 管理信息系统可以按行业和业务职能进行分类（A）。 A 正确 B 错误 管理信息系统的分类方式包括核心业务、数据处理方式、管理应用层次、行业和业务职能4个方面。 按核心业务活动分类 天猫商城（www.tmall.com）是一种典型的（B） A B2B系统 B B2C系统 C C2C系统 D B2G系统 以下信息系统属于电子商务系统的是（B） A 科技管理系统 B 网上购物系统 C 会计信息系统 D 市场监管系统 以下系统属于电子业务系统的是（D） A 经济管理系统 B 市场监管系统 C 社会管理系统 D 会计信息系统 信息系统按核心业务可分成若干类型，其中市场监管系统属于（D） A 电子业务系统 B 电子财务系统 C 电子商务系统 D 电子政务系统 以下信息系统属于电子政务系统的是（D） A 学籍管理系统 B 制造管理系统 C 教务管理系统 D 经济管理系统 下列信息系统不属于电子政务系统的是（D） A 宏观经济管理系统 B 社会治安管理系统 C 市场监督管理系统 D 企业财务管理系统 下列信息系统属于电子政务系统的是（D） A 学校教务管理系统 B 企业生产管理系统 C 高校科技管理系统 D 公共安全管理系统 以下信息系统属于电子政务系统的是（A） A 户籍管理系统 B 学籍管理系统 C 生产管理系统 D 医院管理系统 以下属于电子商务系统的是（B） A 科技管理系统 B 网上购物系统 C 会计信息系统 D 市场监管系统 顾客在网上书店买书，这种模式属于电子商务系统的（B） A B2B B B2C C C2C D B2G 按核心业务活动分类，下列不属于电子业务系统的是（B） A 人力资源管理系统 B 经济管理系统 C 学籍管理系统 D 教务管理系统 按核心业务活动分类，主要针对企事业单位的具体业务过程建立的系统是指（A） A 电子业务 B 电子政务 C 电子商务 D 电子贸易 下列选项中，属于电子政务系统的是（A） A 社会管理系统 B 铁路客票系统 C 学籍管理系统 D 科技管理系统 以下信息系统属于电子政务系统的是（C） A 铁路客票系统 B 高校教务系统 C 市场监管系统 D 学籍管理系统 以下信息系统属于电子政务系统的是（C） A 铁路客票系统 B 高校教务系统 C 市场监管系统 D 企业计划系统 以下属于电子政务系统的是（A） A 社会管理系统 B 科技管理系统 C 教务管理系统 D 学生管理系统 铁路客票系统属于电子商务系统（B） A 正确 B 错误 淘宝网是一种电子业务系统（B） A 正确 B 错误 按核心业务活动分类，一般所说的管理信心系统都是指电子业务系统。 按核型业务活动分类，科技管理系统属于电子业务系统。 通常以网站形式出现的系统是电子商务系统。 对用户而言，电子商务系统通常以网站的形式出现的。 电子商务系统主要依托于Internet，可实现网上购物等活动。 按商务活动参与主体、电子商务系统划分的模式包括：B2B、B2C、C2C、B2G。 按核心业务活动分类，管理信息系统包括电子业务系统、电子政务系统和电子商务系统。 关于电子政务系统，简述政府的主要职能是什么。 政府的主要职能包括经济管理、市场监控、社会管理的公共服务。 管理信息系统按核心业务活动有哪些分类。 电子业务系统、电子政务系统、电子商务系统。 简述电子政务的突出特点。 使政府工作更有效、更精简； 使政府工作更公开、更透明； 为企业和居民提供更好的服务； 重新构造政府、企业、居民之间的关系，使之比以前更加协调，使企业和居民能够更好的参与政府的管理。 简述按商务活动参与主体，电子商务系统的划分模式。 按商务活动参与主体，电子商务系统的划分为企业对企业、企业对消费者、消费者对消费者、企业对政府等若干模式，分别简称为B2B、B2C、C2C、B2G。 按数据处理方式分类 下列信息系统属于分析型管理信息系统的是（B） A 工资发放系统 B 人力资源管理系统 C 会计记账系统 D 人事档案管理系统 下列属于分析型管理信息系统的是（A） A 决策支持系统 B 人事档案管理系统 C 工资发放系统 D 数据采集录入系统 目前，单独的操作型管理信息系统已比较少见（A） A 正确 B 错误 按数据处理方式分类，可分为操作型管理信息系统和分析型管理信息系统。 在信息化的初始阶段，企业所建立的管理信息系统多数属于操作性。 面向那些需要进行趋势分析、预测等管理决策需求而建立的是分析型管理信息系统。 按数据处理方式分类，操作型管理信息系统一般是面向具体的管理业务而建立的，功能比较简单。 按数据处理方式分类，管理信息系统包括操作型管理信息系统和分析型管理信息系统。 人们往往将操作型管理信息系统与分析型管理信息系统合并在一起，共同组织一个完整的管理信息系统。 完整的管理信息系统一般包括操作型和分析型两类子系统。 一些专门的分析型管理信息系统有时会借助数据仓库（DW）或决策支持系统（DSS）等技术。 管理信息系统按数据处理方式分类，可分为操作型管理信息系统和分析型管理信息系统。 名词解释：操作型管理信息系统。 主要是面向具体的业务而建立，功能简单，主要包括数据录入、修改、删除、打印、查找和简单的数据汇总计算等。 名词解释：分析型管理信息系统。 主要面向那些需要趋势分析、预测等管理决策需求而建立的，需要基于操作型管理信息系统的开发，一些分析型管理信息系统有时需要借助数据仓库（DW）或决策支持系统（DSS）等。 按管理应用层次 使组织的中层管理人员通过固定格式的报表、报告和综合查询、统计分析，了解和监视管理领域的运行状况，指的是（A） A 管理型管理信息系统 B 事务型管理信息系统 C 战略型管理信息系统 D 决策型管理信息系统 以下信息属于战略型管理信息系统的是（D） A 综合查询系统 B 统计分析系统 C 报表报告系统 D 制定计划系统 按管理应用层次分类，服务于组织的基层管理者，以具体的业务过程的自动化为目的指的是（A） A 实物型管理信息系统 B 管理型管理信息系统 C 战略型管理信息系统 D 组织型管理信息系统 按管理应用层次分类，为战略计划的制定和调整提供辅助决策功能的是（B） A 实物型管理信息系统 B 战略型管理信息系统 C 管理型管理信息系统 D 操作型管理信息系统 管理型管理信息系统主要服务于组织的基层管理者（B） A 正确 B 错误 战略型管理信息系统所需要的数据一般来源实物型管理信息系统和管理型管理信息系统，还有一些来自企业外部，其中外部数据所占比例较大。（A） A 正确 B 错误 管理型管理信息系统的主要目的是使组织的中层管理人员通过固定格式的报表、报告和综合查询、统计查询，了解和监视管理领域的运行情况。（A） A 正确 B 错误 战略型管理信息系统主要服务于组织的高层管理者。 在管理应用层次的分类下，主要服务于组织的基层管理者的是事物型管理信息系统。 在管理应用层次的分类下，服务于中层管理者的是管理型管理信息系统。 战略型管理信息系统主要服务于组织的高层管理者。 战略型管理信息系统的主要目的是为战略的制定和调整提供辅助的决策功能。 战略型管理信息系统的数据一般来源于事务型管理信息系统和管理型管理型信息系统，外部数据所在的比例较大。 管理型管理信息系统的分类方式是按管理应用层次进行划分的。 事务型管理信息系统的主要目的是具体业务过程的自动化。 按管理应用层次进行分类包括：事务型管理信息系统、管理型管理信息系统和战略型管理信息系统。 简述管理控制型管理信息系统。 管理型管理信息系统主要服务于组织的中层管理者，提供综合查询、统计分析和报表、报告等功能，设计多项业务的综合管理。主要目的是使组织的中层管理人员通过固定格式的报表、报告和综合查询、统计分析，了解监视管理领域的运行情况。 管理型管理信息系统的主要目的。 管理型管理信息系统的主要目的是使组织的中层管理人员通过固定格式的报表、报告和综合查询、统计分析，了解和监视管理领域的运行情况。 名词解释：事务型管理信息系统。 事务型管理信息系统是按管理应用层次分类的一种，主要服务于组织的基层管理者，主要目的是具体业务过程的自动化。 简述事务型管理信息系统的概念。 事务型管理信息系统主要服务于组织的基层管理者，主要目的是具体业务过程的自动化，一般操作型管理信息系统大多数属于事务型管理信息系统。 名词解释：战略型管理信息系统。 战略型管理信息系统主要服务于组织的高层管理者，主要目的是为战略的制定和调整提供辅助的决策功能。 管理信息系统按应用层次的分类。 管理信息系统按应用层次分类包括：事务型管理信息系统、管理型管理信息系统和战略型管理信息系统。 按行业和业务职能分类 将管理信息系统划分为铁路管理信息系统、林业管理信息系统、电力管理信息系统的划分方式是（A） A 按行业或部门 B 按业务职能 C 按管理应用层次 D 按数据的处理方式 管理信息系统的分类方法有很多，比如按行业或部门可以划分为铁路管理信息系统、林业管理信息系统、电力管理信息系统。对于某个具体的行业或部门，又可以按业务职能划分为若干管理信息系统。 "},"pages/信息系统开发与管理/第一章_管理信息系统导论/第三节_管理信息系统的结构.html":{"url":"pages/信息系统开发与管理/第一章_管理信息系统导论/第三节_管理信息系统的结构.html","title":"第三节：管理信息系统的结构","keywords":"","body":"第三节：管理信息系统的结构 功能结构 要设计出一个高质量的管理信息系统，就必须先了解它的结构，从使用者的角度上看，任何一个管理信息系统具有明确的目标，，并由若干具体功能组成。为完成这个目标，各功能相互联系，构成了一个有机组合的整体，表现出系统的特征，指的是管理信息系统的（A） A 功能结构 B 概念结构 C 职能结构 D 软硬件结构 从使用者角度看，任何一个管理信息系统均有明确的目标，并由若干具体功能组成。（A） A 正确 B 错误 要设计出一个高质量的管理信息系统，就必须先了解它的结构。 为完成管理信息系统的目标，各功能相互联系，构成了一个有机组合的整体，表现出系统的特征，指的是管理信息系统的功能结构。 名词解释：管理信息系统的功能结构。 要设计出一个高质量的管理信息系统，就必须先了解它的机构，从使用者的角度看，任何一个管理信息系统均有明确的目标，并由若干具体功能组成，为完成这个目标，各功能相互联系，构成了一个有机组合的整体，表现出的系统特征，指的是管理信息系统的功能结构。 名词解释：系统的结构。 “结构”指管理信息系统各部件的构成框架，由于对这些部件的不同理解，就构成了不同的结构方式。 简述管理信息系统的功能结构。 要设计出一个高质量的管理信息系统，就必须先了解它的结构，从使用者的角度看，任何一个管理信息系统均有明确的目标，并由若干具体功能组成，为完成这个目标，各功能互相联系，构成了一个有机组合的整体，表现出的系统特征，这就是管理信息系统的功能机构。 概念结构 不包含在管理信息系统概念结构中的要素是（B） A 信息管理者 B 系统开发者 C 信息处理器 D 系统使用者 管理信息系统的概念结构由信息源、信息处理器、信息用户和信息管理者组成。 管理信息系统的四个部件是信息源、信息处理器、信息用户和信息管理者。 管理信息系统四大部件中，信息源是信息产生地。 管理信息系统四大部件中，信息处理器担负着信息的传输、加工和存储等任务。 从概念结构上，管理信息系统由信息源、信息处理器、信息用户和信息管理者等四个部件组成。 简述管理信息系统的概念结构。 管理信息系统的功能结构反映了管理的业务职能，比较直观，易于理解。如果各个管理信息系统的功能结构进行抽象，会发现所有的管理信息系统均是由信息源、信息处理器、信息用户和信息管理者四个部件组成，这就是管理信息系统的概念结构。 组成管理信息系统的四大部件 所有的管理信息系统均由信息源、信息处理器、信息用户和信息管理者四大部件组成。 管理信息系统的概念结构包括哪几个部分？各自的作用是什么？ 管理信息系统的概念结构由四个部件组成：信息源、信息处理器、信息用户和信息管理者。 信息源是信息的产生地。 信息处理器担负着信息的传输、加工和存储等任务。 信息用户是信息的最终使用者，他们应用信息进行管理决策。 信息管理者负责信息系统的设计、实施和维护等工作。 简述管理信息系统四大部件的主要作用。 信息源是信息的产生地。 信息处理器担负着信息的传输、加工和存储等任务。 信息用户是信息的最终使用者，他们应用信息进行管理决策。 信息管理者负责信息系统的设计，实施和维护等工作。 管理职能结构 企业中的管理活动实际情况是纵、横交叉形成完整的管理活动。（A） A 正确 B 错误 横向看一个组织，管理活动是按职能排列的。 从纵向视野分析，管理活动的高、中、低三个层次分别对应战略计划层、管理控制层、执行控制层。 支持执行控制的处理分别是：事务处理、报表处理和查询处理。 实际企业中的管理活动不可能只是单纯的按层次划分，或者单纯的按职能划分。 管理控制子系统的任务是为企业各职能部门管理人员提供用于衡量企业效益、控制企业生产经营活动、制定企业资源分配方案等活动所需的信息。 从纵向视角分析，执行控制子系统的任务是确保基层生产经营活动的正常、有效的进行。 从横向视角将管理信息系统进行分类，列举4个例子。 生产管理信息系统 销售管理信息系统 物资管理信息系统 财务会计管理信息系统 执行控制子系统的任务是什么？ 执行控制子系统的任务是确保基层的生产经营活动正常、有效的进行。 简述战略计划子系统的主要任务。 战略计划子系统的主要任务是，为企业战略计划的制定和调整提供辅助的决策功能。 简述管理信息系统的管理职能结构。 管理信息系统的管理职能结构，主要从三方面分析：1.纵向视野:一般将管理活动分为高、中、低三个层次，即战略计划层、管理控制层和执行控制层。针对这三个层次建立的系统称为战略计划子系统、管理控制子系统、执行控制子系统。分别属于战略型、管理型、事务型管理信息系统。2.横向视野：横向来看一个组织，其管理活动是按职能排列的，可分为若干子系统：生产管理子系统、销售管理子系统、物资管理子系统、财务会计管理子系统、人力资源管理子系统。3.综合视野：实际企业中的管理活动不单纯按层次或职能划分，是横、纵交叉形成的完整的管理活动。 简述管理控制子系统的任务。 管理控制子系统的任务是为企业各职能部门的管理人员提供用于衡量企业效益、控制企业生产经营活动、制定企业资源分配方案等活动的所需要的信息。 软硬件结构 管理信息系统的硬件机构一般以硬件设备的物理位置安排和拓扑结构等方式展现。 软件大体可分为系统软件和应用软件两大类。 计算机的灵魂和思想是软件。 企业级信息化建设常用的是两种服务器，分别是：小型机和PC服务器。 在计算机的硬件中，现代的计算机主要采用冯诺伊曼体系结构。 按计算机在管理信息系统中发挥的作用，通常分为客户机和服务器两大类。 软件大体可分类系统软件和应用软件两大类。 按操作系统划分，目前常用的服务器由Windows服务器，Unix服务器和Linux服务器。 简述管理信息系统的硬件结构及其组成。 计算机和网络等硬件是系统的实体部分；包括主机、外设和网络硬件设备组成；计算机主要有客户机、各种服务器及连接设备等。 管理信息系统的软件结构包括哪两方面内容。 管理信息系统的软件结构包括两方面内容：一是描述管理信息系统软件的功能模块，是对管理信息系统功能结构的进一步补充和细化，一般可以用系统模块结构图的形式展示；二是依附于硬件结构的管理信息系统的软件结构，主要包括操作系统、数据库管理系统、应用开发工具和各种服务器软件等。 简述管理信息系统的硬件结构。 管理信息系统的硬件结构一般以硬件设备的物理位置和拓扑结构等方式展现。从结构原理上讲，现代的计算机主要采用冯诺伊曼体系结构，主要由运算器、控制器存储器和输入、输入设备组成。按计算机在管理信息系统中发挥的作用，通常可分为客户机和服务器两大类。 管理信息系统的软件结构主要包括哪些方面。 一是描述管理信息系统应用软件的功能模块，是对管理信息系统的功能结构的进一步补充和细化，一般可以用系统结构模块图的形式展现。 二是依附于硬件结构的管理信息系统的软件结构。主要包括操作系统、数据库管理系统、应用开发工具和各种服务器软件等。 以规模和操作系统为分类标准，企业级信息化建设有哪些常见的服务器？ 服务器有多种分类标准： 按规模划分。分为超级计算机、大型机、小型机和PC服务器。企业级信息化建设中常见的小型机和PC服务器。 按操作系统分。分类Windows服务器、Unix服务器和Linux服务器系统。企业级信息化建设中，小型机通常运行的是Unix操作系统，PC服务器上运行的是Windwos操作系统。近年来，PC服务器采用Linux操作系统的也日渐普遍。 服务器按操作系统划分的分类。 服务器按操作系统划分可分为Windows服务器、Unix服务器和Linux服务器。 服务器按规模划分的分类。 按规模划分，服务器可划分为超级计算机、大型机、小型机和PC服务器。 计算机按冯诺伊曼体系结构，有哪些组成部件？ 从结构原理上讲，现代的计算机主要采用冯诺伊曼体系结构，主要由运算器、控制器、存储器和输入、输出设备组成。 依附于硬件结构的管理信息系统的软件结构包括哪些内容？ 主要包括操作系统、数据库管理系统、应用开发工具和各种服务器软件等。 简述管理信息系统硬件结构描述的内容。 给信息系统硬件结构描述的是管理信息系统所依托的计算机及其网络系统的硬件设备组成及其连接方式。各硬件设备的功能和技术参数。 简述管理信息系统的软件结构。 管理信息系统的软件结构一般包括应用软件功能模块以及依附于硬件结构的系统软件，包括数据库管理系统、应用开发工具和各种服务器软件等。 网络计算结构 人们在笔记本电脑上浏览门户网站上的网页新闻时使用的计算模式最有可能是（D） A 中央主机分时处理模式 B 文件服务器模式 C 客户机/服务器模式 D 浏览器/服务器模式 在网络计算结构中，使用Web服务器结构的是（B） A 中央主机集中分时结构 B 浏览器/服务器结构 C 文件服务器结构 D 客户机/服务器结构 传统的C/S模式是两层结构的系统。 C/S模式的最大的优点是交互性强。 B/S产生和发展的主要得益于Internet技术的发展。 从逻辑上看，C/S模式是指进程间请求和服务的关系。 关于浏览器/服务器模式，客户端运行的浏览器软件，经过Web服务器的转化，变成HTML文档形式，转发给客户端浏览器。 B/S产生和发展得益于Internet技术的发展。 三层C/S结构分为表现层、业务层和数据层。 在客户服务器模式的三层结构中，表现层称为“瘦”客户。 传统的C/S模式是一种两层结构的系统，第二层是网络结合了数据库服务器。 B/S是浏览器/服务器模式的简称。 N层结构中比较常见的是三层，其中表现层仅负责与用户交互。 简述下浏览器/服务器（B/S）模式。 在这种模式下，客户端运行浏览器软件，浏览器以超文本方式向Web服务器提出访问数据库的要求，Web服务器接受客户端请求后，将这个请求转化为SQL语法，并交给数据库服务器，数据库服务器接受到请求后，验证其合法性，并进行数据处理，然后将处理后的结果返回Web服务器，Web服务器再一次将得到的所有结果进行转化，变成HTML文档形式，转发给客户端浏览器，以友好的Web页面形式显示出来。 简述C/S模式。 传统的C/S模式是一种两层结构的系统，第一层在客户机系统上结合了表现层与业务逻辑，第二层是通过网络结合了数据库服务器。 "},"pages/信息系统开发与管理/第一章_管理信息系统导论/第四节_管理信息系统的几种典型应用.html":{"url":"pages/信息系统开发与管理/第一章_管理信息系统导论/第四节_管理信息系统的几种典型应用.html","title":"第四节：管理信息系统的几种典型应用","keywords":"","body":"第四节：管理信息系统的几种典型应用 MRP系统 下列关于MRP系统所依据的管理理念，错误的是（D） A 供应与需求平衡 B 优先级计划原则 C 生产与供应计划必须根据需用时间和数量来确定优先顺序 D 供应必须大于需求 在MRP的三项数据中，主生产计划决定MRP的必要性和可行性。 MRP的核心是考虑物料与时间和数量之间的关系。 MRP系统所依据的管理理念中，供应必须与需求平衡。 MRP系统所依据的管理理念中，采用了优先级计划原则。 输入MRP的基本数据是主生产计划、物料清单和库存信息。 MRP的三项基本输入数据中，计算需求数量和时间基本数据是指物料清单和库存信息。 物料需求计划的基本内容是编制零件的生产计划和采购计划。 MRP的三项基本输入数据包括：主生产计划、物料清单和库存信息。 简述MRP的逻辑流程。 MRP的基本内容是编制零件的生产计划和采购计划。主生产计划是将生产计划大纲规定的产品系列或大类转换成特定产品或特定部件的计划，据此可以制定物料需求计划、生产进度计划与能力需求计划。 简述MRP的基本任务。 MRP的基本任务：从所需求产品的生产计划导出相关物料的需求量和需求时间；根据物料的需求时间和生产周期来确定其开始生产的时间。 简述MRP系统的管理理念。 MRP系统所依据的管理理念主要是：1.供应必须与需求平衡；2.优先级计划原则，即生产与供应计划必须根据需用时间和数量来确定优先顺序。 简述MRP的三项基本输入数据。 MRP的三项基本输入数据包括：1.主生产计划、物料清单和库存信息。其中，主生产计划决定MRP的必要性和可行性，另外两项是计算需求数量和时间的基本数据，他们的准确定直接影响MRP的运算结果。 名词解释：MRP系统。 准确的讲，MRP（物料需求计划的简称）是一种以物料需求计划与控制为主线的管理理念，基于此种管理方法形成的管理信息系统被称为MRP系统。 MRPII系统 一般企业使用MRPII以后，制造成本平均降低（B） A 5%～10% B 10%～15% C 15%～20% D 20%～25% MRPII已能动态监控产、供、销全部生产过程。（A） A 正确 B 错误 制造资源计划的简称是MRPII。（A） A 正确 B 错误 MRPII的基本思想就是把企业作为一个有机整体，基于企业经营目标制定生产计划，围绕物料集成组织内的各种信息，实现按需、按时进行生产。 MRPII是一种计划主导型管理模式。 MRPII与MRP的主要区别之一就是运用了管理会计的概念。 制造资源计划简称MRPII。 MRPII系统涉及只是物流，这只是企业生产管理的一个方面，与此相关的资金流还没有纳入其中。 一般企业运用MRPII以后，制造成本平均降低**10%～15%。 人们把制造、财务、销售、采购、工程技术等各子系统集成为一个一体化系统，被称为制造资源计划。 简述MRPII的基本思想是什么？ MMRPII的基本思想就是把企业作为一个有机整体。基于企业经营目标制定生产计划，围绕物料集成组织内的各种信息，实现按需、按时生产。 名词解释：MRPII。 人们把制造、财务、销售、采购、工程技术等各子系统集成为一个一体化系统，被称为制造资源计划，为了区别物料区别计划，简称MRPII。 ERP系统 1.ERP面向整个供应链的四大元素不包括（D） A 资金 B 货物 C 人员 D 物流 2.在ERP中，财务管理系统可以分为会计核算与财务管理。 3.在物流管理中，库存控制的主要作用是控制存储物料的数量。 4.ERP的核心功能是生产计划与控制管理。 5.一般，ERP软件的财务分为会计核算和财务管理两方面。 6.物流管理中，有分销管理、库存管理和采购管理。 7.可以用于覆盖一个组织的、进行物质资源、资金资源和信息资源集成一体化管理的集成信息系统的是ERP。 8.在ERP中，物流管理可有分销管理、库存管理和采购管理组成。 9.由MRP经过闭环MRP直到MRP II，其发展基本上是沿着两个方面延伸，其中一个是资源概念内涵的不断扩大。 10.由MRP经过闭环MRP直到MRP II，其中一个发展方向是计划闭环的形成。 11.ERP系统的产生与发展，时间被作为最关键的资源。 12.ERP是面向供应链管理的，它由四大元素，即资金、货物、人员和信息。 13.典型的ERP系统的功能主要包括财务管理、物流（供销）管理、生产计划与控制、人力资源等方面。 14.ERP的四大元素分别是资金、货物、人员和信息。 15.ERP系统的功能主要包括财务管理、物流管理、生产计划与控制管理、人力资源管理等方面。 16.ERP面向供应链的四大元素包括：资金、货物、人员和信息。 17.简述现在对ERP系统的定义。 现在，ERP这个术语包括所有可以用于覆盖一个组织的、进行物质资源、资金资源和信息资源集成一体化管理的集成信息系统。 18.物流管理中，分销管理有拿三个方面的功能？ 1. 对于客户信息的管理和服务。 2. 对于销售订单的管理。 3. 对于销售的统计与分析。 19.简述生产计划与控制管理的功能。 主要功能有： 1. 主生产计划 2. 物流需求计划 3. 能力需求计划 4. 车间控制 5. 制造标准 20.简述ERP的四项功能。 1. 财务管理：财务部分分为会计核算和财务管理。 2. 物流管理：包括分销管理、库存控制和采购管理。 3. 生产计划与控制管理：是ERP系统的核心功能。 4. 人力资源管理：是企业重要的资源。 "},"pages/信息系统开发与管理/第二章_管理信息系统的基本知识/第一节_管理的基本知识.html":{"url":"pages/信息系统开发与管理/第二章_管理信息系统的基本知识/第一节_管理的基本知识.html","title":"第一节：管理的基本知识","keywords":"","body":"第一节：管理的基本知识 管理的含义 1.管理者对未来事件的预测、制定行动方案的管理职能属于（A） A 计划 B 组织 C 协调 D 控制 2.管理包括计划、组织、指挥、协调和控制5个职能，其中是企业内部每一部分或每一个成员都服从整体目标的职能属于（C） A 计划 B 组织 C 协调 D 指挥 3.管理者对下属人员的工作进行检查、纠偏，使其达到要求，这种管理职能属于（D） A 计划 B 组织 C 协调 D 控制 4.关于管理的过程，其步骤不包括（D） A 发现问题 B 拟定方案 C 作出决策 D 职能划分 5.管理的首要职能是（B） A 协调 B 计划 C 组织 D 控制 6.在管理现代化中，“重视经营和决策的思想”体现的是（A） A 管理思想 B 管理组织 C 管理方法 D 管理手段 7.管理现代化主要包括管理思想、管理组织、管理方法和管理手段的现代化。（A） A 正确 B 错误 8.组织是管理的首要职能。（B） A 正确 B 错误 9.管理就是去营造一种激励环境，使处于其中的所有工作人员努力工作，发挥群体的协同效应，以达到企业或组织的目标。（A） A 正确 B 错误 10.管理工作是通过协调其他人的活动来进行的，追求群体的协同效应。（A） A 正确 B 错误 11.管理现代化主要包括管理思想、管理组织、管理方法和管理手段的现代化。 12.管理职能是一个整体，通过指挥协调职能，将个人工作与集体目标协调一致。 13.管理职能相互依存，通过组织职能，建立实现目标的手段。 14.管理职能不可分割，通过控制职能，可以保证计划的实现。 15.管理职能相互关联，通过计划职能明确组织目标和方向。 16.管理职能中，计划的关键是目标的确定问题。 17.关于管理职能，计划的准则是进程的时序。 18.对未来事件通过预测制定方案的是管理职能中的计划。 19.在管理职能中，对人财物进行配备的是组织。 20.管理职能中，管理过程中带有综合性、整体性的一种职能是协调。 21.对所属对象的行为进行发令、调度的管理职能是指挥。 22.控制职能对下属人员的行为进行检测纠正，使其按要求工作。 23.重视经营、重视决策的思想体现了管理思想的现代化。 24.经营预测和决策方法体现了管理方法的现代化。 25.表现在计算机和通信技术在管理领域中的应用，对大企业有重要意义的事管理手段的现代化。 26.管理手段的现代化能直接促进管理体制、管理组织、管理方法现代化进程。 27.管理的过程是基于信息的决策过程。 28.管理的过程包括：发现问题、拟定方案和作出决策。 29.管理的过程一般包括以下步骤：发现问题、拟定方案和作出决策。 30.管理工作是通过协调其他人的活动来进行的，追求群体的协同效应。 31.管理工作的中心是管理其他人的工作。 32.管理现代化主要包括管理思想、管理方法、管理组织和管理手段的现代化。 33.管理过程通常包括以下步骤，即发现问题、拟定方案和作出决策。 34.管理的基本职能包括计划、组织、指挥、协调和控制。 35.管理在于营造一种激励环境，使处于其中的所有工作人员努力工作，发挥群体的协同效应，以达到企业或组织的目标。 36.管理过程包括发现问题、拟定方案和作出决策的过程。 37.管理的五大基本职能是计划、组织、指挥、控制和协调。 38.管理现代化主要包括管理思想、管理组织、管理方法和管理手段的现代化。 39.管理现代化主要包括哪些方面的内容？ 管理现代化主要包括管理思想、管理方法、管理组织和管理手段的现代化。 40.管理职能中组织的基本要求是什么？ 它由两个基本要求：一是按目标要求设置机构、明确岗位、配备人员、规定权限、赋予职责，并建立一个统一的组织系统；二是按实现目标的计划和进程，合理地组织人力、物力和财力，并保证它们在数量和质量上相互匹配，以取得最佳的经济和社会效益。 41.在管理职能中，计划重点解决的问题是什么？ 计划重点解决的两个基本问题：一是目标的确定问题，这是计划的关键；二是进程的时序，即先做什么，后做什么，可以同时做什么，这是计划的准则。 42.简述管理组织的现代化包括哪些方面的内容？ 管理组织的现代化包括管理体制、机构设置、生产组织和劳动组织等几个方面的现代化。 43.关于管理的概念由哪些含义？ 管理就是由一个或更多的人来协调他人的活动，以便收到个人单独活动所不能收到的效果而进行的各种活动。这一概念由三方面的含义： 1. 管理工作的中心是管理其他人的工作。 2. 管理工作是通过协调其他人的活动来进行的，追求群体的协同效应。 3. 管理就是去营造一种激励环境，使其所有工作人员努力工作，发挥群体的协同效应，达到企业或组织的目标。 44.名词解释：管理。 管理就是由一个或更多的人来协调他人的活动，以便收到个人单独活动所不能收到的效果而进行的各种活动。这一概念由三方面的含义： 1. 管理工作的中时管理其他人的工作。 2. 管理工作时通过协调其他人的活动来进行的，追求群体的协同效应。 3. 管理就是去营造一种激励环境，使其所有工作人员努力工作，发挥群体的协同效应，达到企业或组织的目标。 45.简述管理由哪些基本职能。 管理的基本职能： 1. 计划。这是管理的首要职能，它对未来事件作出预测，以制定出行动方案。 2. 组织。它是指完成计划所需的组织结构、规章制度、人财物的配备等。 3. 指挥。它是指对所属对象的行为进行发令、调度、检查。 4. 协调。它是指使组织内部的每一部分或每一成员的个别行动都能服从整个集体目标，是管理过程中带有综合性、整体性的一种职能。 5. 控制。它是指对下属人员的行为进行检测，纠正偏差，使其按规定的要求工作。 46.控制职能必须具备三个基本条件？ 控制职能必须具备三个基本条件：一是有明确的执行标准；二是及时获得发生偏差的信息；三是有纠正偏差的有效措施。缺少任何一个条件，管理活动便会失去控制。 组织结构 1.一个企业的组织内下级主要负责人只接受上一级主要负责人的指挥与控制，这个企业的组织机构属于（B） A 职能制组织结构 B 直线制组织结构 C 矩阵式组织结构 D 事业部组织结构 2.组织机构中，最早最简单的组织形式是（A） A 直线制组织结构 B 矩阵式组织结构 C 职能制组织结构 D 事业部制组织结构 3.下列组织机构中，由两维组成，一维是直线组织，另一维是任务的组织结构是（B） A 直线制组织结构 B 矩阵式组织结构 C 职能制组织结构 D 事业部制组织结构 4.各级行政单位除主管负责人外，还相应设立一些职能机构，指的是（A） A 事业部制组织结构 B 直线制组织结构 C 职能制组织结构 D 矩阵式组织结构 5.直线制结构是一种树状组织。（A） A 正确 B 错误 6.若一种组织结构由两维组成，则这样的组织结构称为矩阵式组织结构。 7.直线制结构比较适用于任务明确且控制严格的情况，是一种树状组织。 8.为加强任务过程的负责制，大规模企业采取矩阵式结构。 9.直线制结构比较适用于任务明确，而又要求领导集中、控制严格的情况，是一种树状组织。 10.直线制是一种最早也是最简单的组织形式。 11.组织是保证管理目标实现的重要手段。 12.了解管理的组织结构将有利于我们分析和设计管理信息系统。 13.职能制组织结构的优点是减少了最高领导者的负担。 14.如一个组织各级行政单位为从上到下实行的体制是下属部门只服从直接上级的命令，则这样的组织结构称为直线制组织机构。 15.如果在厂长下面设立职能部门和人员，协助厂长从事职能管理工作，指的是职能制组织结构。 16.矩阵式组织结构一维是直线组织，另一维是任务。 17.企业管理的组织结构一般分为三种，分别是直线制组织结构、职能制组织结构和矩阵式组织结构。 18.典型的组织结构包括直线制组织结构、职能制组织结构和矩阵式组织结构。 19.职能制组织结构的优缺点。 职能制组织结构的优点是减少了最高领导者的负担，但缺点是容易造成办事效率低下等现象。 20.简述矩阵式组织结构的优缺点。 其优点是加强了横向联系、具有较大的机动性。其主要缺点是人员受双重领导，有时不易分清责任。 21.名词解释：职能制组织结构。 职能制组织结构，是各级行政单位除主管负责人外，还相应地设立一些职能机构。如在厂长下面设立职能部门和人愿，协助厂长从事职能管理工作。 22.名词解释：直线制组织结构。 直线制是一种最早也是最简单的组织形式。它的特点是企业各级行政单位从上倒下实行垂直领导，下属部门只接受一个上机的指令，各级主管负责人对所属单位的一切问题负责。直线制结构比较适用于任务明确，而又要求领导集中、控制严格的情况，是一种树状组织。 23.简述典型的组织结构有哪几种？ 典型的组织结构主要包括以下三种：直线制组织结构；职能制组织结构；矩阵式组织结构。 24.名词解释：矩阵式组织结构。 矩阵式组织结构有两维组成，一维是直线组织，另一维是任务。其优点是加强了横向联系、具有较大的机动性。其主要缺点是人员受双重领导，有时不易分清责任。 管理部门的划分方法 1.在管理部门的划分方法中，最广泛采用的是一种基本方法的划分依据是（C） A 地区 B 产品 C 职能 D 市场 2.一个大型全国连锁零售企业划分管理部门时，合适的划分原则是（C） A 按工艺划分 B 按产品部件划分 C 按地区划分 D 按市场前景划分 3.在多种经营的大规模组织中，划分部门比较流行的方式是（B） A 按地区划分 B 按产品划分 C 按职能划分 D 按顾客划分 4.银行总行及其分行的管理模式是（A） A 按地区管理 B 按职能管理 C 按产品管理 D 按顾客管理 5.一个大型启辰制造企业分成发动机分厂、车身分厂、轴承分厂。这种管理部门的划分方式属于（B） A 按工艺划分 B 按产品部件划分 C 按地区划分 D 按市场前景划分 6.对一个一般制造企业而言，划分管理部门的原则是（C） A 按市场划分 B 按地区划分 C 按职能划分 D 按工艺划分 7.在不同地区设置政府部门的管理模式是（C） A 按职能管理 B 按产品管理 C 按地区管理 D 按顾客管理 8.职能制和矩阵制是目前多数企业采用的组织结构。 9.简述管理部门的划分方法。 管理部门的划分方法：1.按职能划分部门。根据专业原则，以工作或任务的性质维基础来划分部门的，如制造使用的是生产、销售、财务等；2.按地区划分部门。例如，处于不同地区的政府机关、银行、法院、工商等；3.按产品划分部门。例如：汽车制造企业分成发动机分厂、车身分厂、轴承分厂等。 管理幅度与层次 1.负责拟定公司中长期发展规划、经营计划、资本经营规划和方案的管理层是（A） A 战略决策层 B 业务管理层 C 业务执行层 D 战术管理员 2.以下关于“管理幅度”的说法正确的是（C） A 管理幅度越大，管理效率越高 B 管理幅度越大，管理效率越低 C 有效管理幅度越大，管理层次越少 D 有效管理幅度越大，管理层次越多 3.在管理层次中，高层管理对应的是（A） A 战略级管理 B 战术级 C 执行层 D 作业层 4.基层管理又称为（A） A 作业层管理 B 战略级管理 C 战术级管理 D。技术级管理 5.战术级管理对应的是（C） A 高层管理 B 基层管理 C 中层管理 D 执行层管理 6.关于管理幅度，当超过某个限度时，管理的效率就会随之下降。（A） A 正确 B 错误 7.较大的管理幅度意味着较多的层次。（B） A 正确 B 错误 8.扁平结构是一种管理层次少而管理宽度大的结构。 9.按照管理幅度大小和管理层次多少划分，直式结构管理层次较多。 10.为了有效管理，我们尽量减少管理层次，称为管理扁平化。 11.按照管理幅度大小和管理层次多少，分成两种结构：扁平结构和直式结构。 12.企业一名管理中有效地监督、管理其直接下属的人数称为管理幅度。 13.管理幅度又称为“管理跨度”或“管理宽度”。 14.管理幅度又称为“管理跨度”或“管理宽度”。 15.管理幅度又称为“管理跨度”或“管理宽度”。 16.高层管理属战略级管理，它是一个组织最高领导层。 17.按照管理幅度的大小及管理层次的多少，分成两种结构：扁平结构和直式结构。 18.按照管理中幅度的大小及管理层次的多少，组织可以分成两种结构，其中管理层次少而管理宽度大的结构，称为扁平结构。 19.将管理划分为三个层次：高层管理、中层管理、基层管理。 20.通常，当超过了管理幅度时，就必须增加一个管理层次。 21.根据企业的内外情况，分析和制定该组织长远目标及政策的管理层属于高层管理层。 22.在管理层次中，战略级管理对应的是高层管理。 23.名词解释：管理层次。 管理层次就是指管理组织划分为多少个等级。管理者的能力有限，当下属人员太多超出了自己的管理幅度时，需要划分层次，不同的管理层次标志着不同的职责和权限。我们将管理划分为三个层次：高层管理、中层管理、基层管理。 24.影响管理幅度的因素有哪些？ 影响管理幅度的因素很多，比如管理者的个人能力、工作的难易程度，以及信息的沟通和处理效率。 25.名词及时：管理幅度。 管理幅度：又称“管理跨度”或“管理宽度”，是指一名管理者有效地监督、管理其直接下属的人数是有限的，当超过某个限度时，管理的效率就会随之下降。 26.名词解释：管理扁平化。 一般来说，为了达到有效管理，应尽可能地减少管理层次，我们将这一过程称为管理扁平化。 管理层次与决策类型 1.关于结构化决策的识别程度，下列描述正确的是（A） A 问题确定 B 问题较难确定 C 问题不确定 D 参数难以量化 2.关于半结构化决策的特点，描述正确的是（D） A 问题不确定 B 不复杂 C 无法建模 D 信息主要来自内部 3.非结构化决策的特点是（A） A 参数难以量化 B 不复杂 C 信息来源于内部 D 自动化的决策方式 4.结构化决策的特点是（B） A 问题比较复杂 B 所需信息来自企业内部 C 问题较难识别 D 借助人工智能决策 5.关于管理活动的决策过程，“账务处理”体现的决策方式是（A） A 结构化决策 B 半结构化决策 C 非结构化决策 D 组织结构化决策 6.很难用确定的决策模型来描述，强调决策者的主观意识，指的是（B） A 半结构化决策 B 非结构化决策 C 结构化决策 D 组织结构化决策 7.关于管理活动的决策过程，“物资出入库管理”的活动体现的决策方式是（B） A 非结构化决策 B 结构化决策 C 组织结构化决策 D 半结构化决策 8.“市场预测”的活动体现的决策方式是（A） A 半结构化决策 B 组织结构化决策 C 结构化决策 D 非结构化决策 9.“物资配送”的活动体现的决策方式是（A） A 半结构化 B 结构化 C 组织结构化 D 非结构化 10.非结构化决策的特点是（C） A 问题不易确定 B 信息来自企业内部 C 模型容易描述 D 用于短期局部决策 11.管理活动的高、中、低三个层次分别对应是（D） A 非结构化决策层次、半结构化决策、结构化决策 B 结构化决策、非结构化决策、半结构化决策次 C 非结构化决策、结构化决策、半结构化决策 D 结构化决策、半结构化决策、非结构化决策 12.非结构化决策强调决策者的主观意识。（A） A 正确 B 错误 13.非结构化决策的信息来源是外部和内部综合信息。（B） A 正确 B 错误 14.半结构化决策的复杂程度是很复杂。（A） A 正确 B 错误 15.非结构化决策的问题不确定，参数难以量化。（A） A 正确 B 错误 16.结构化决策的决策方式是自动化。（A） A 正确 B 错误 17.非结构化决策的问题一般都带有全局性、战略性和复杂性。 18.非机构化决策强调决策者的主观意识，问题一般都带有全局性、战略性和复杂性。 19.在决策类型中，强调决策意志的是非结构化决策。 20.依据一定的规则实现决策过程的自动化的是结构化决策。 21.半结构化决策通常是指企业职能部门主管业务人员的计划控制等管理决策活动。 22.结构化决策通常指确定型的管理问题。 23.企业主观计划控制的管理决策活动指的是半结构化决策。 24.管理活动的高、中、低三个层次分别对应的决策过程是：非结构化决策、半结构化决策和结构化决策。 25.非结构化的特点是什么？ 非结构化的特点是很难用确定的决策模型来描述，它强调决策者的主观意识。这类问题一般都带有全局性、战略性和复杂性。 26.名词解释：非结构化决策。 非结构化决策时指很难用确定的模型来描述的一类管理决策活动，它强调决策者的主观意识，这类问题一般都带有全局性、战略性和复杂性。 27.名词解释：结构化决策。 结构化决策通常指确定的管理问题，它依据一定的决策规则或通用的模型来实现其决策过程的自动化。解决这类问题通常采用数据管理方式，它着眼于提高处理的效率和质量。 28.名词解释：半结构化决策。 半结构化决策是指企业职能部门住管业务人员的计划控制等管理决策活动。它多属于短期的、局部的决策。决策的过程中，在结构化决策过程所提供的信息的基础上，一般应有专用模型来帮助。 "},"pages/信息系统开发与管理/第二章_管理信息系统的基本知识/第二节_信息的基本知识.html":{"url":"pages/信息系统开发与管理/第二章_管理信息系统的基本知识/第二节_信息的基本知识.html","title":"第二节：信息的基本知识","keywords":"","body":"第二节：信息的基本知识 信息与数据 1.名词解释：信息。 信息是经过加工的数据，是有一定的含义、能减少不确定性、对决策或行为有现实或潜在价值的数据。 信息的基本属性 1.下列关于信息的基本属性，正确的是（A） A 信息是普遍存在的 B 信息具有消耗性 C 信息是不可压缩 D 信息具有虚拟性 2.信息被利用时可以产生能量或时间的特性属于信息的（B） A 可交互性 B 可转化性 C 可扩散性 D 可共享性 3.下列关于信息的基本属性，正确的是（A） A 信息具有事实性 B 信息具有消耗性 C 信息具有不可压缩性 D 信息具有不可变换性 4.信息是事务运动和状态改变的方式，反映了信息具有（A） A 普遍性 B 事实性 C 共享性 D 变换性 5.关于信息的基本属性，下列描述正确的是（A） A 信息是普遍存在的 B 信息描述了事物的恒定状态 C 信息不可压缩 D 信息是可消耗的 6.俗话说，“没有不透风的墙”，体现了信息具有（A） A 扩散性 B 变换性 C 可转化性 D 层次性 7.信息处理应该抓住事物的主要矛盾，从数据中去粗取精，去伪存真，指的是信息的（C） A 共享性 B 变换性 C 可压缩性 D 层次性 8.信息与其他物质资源不同，在使用过程中不但不会被消耗，而且还可能出现再生或增殖，指的是信息的（C） A 层次性 B 普遍性 C 非消耗性 D 共享性 9.信息可以负载在其他一切可能的物质载体和能量形式上，体现了信息的（C） A 普遍性 B 事实性 C 变换性 D 层次性 10.以下关于信息的基本属性，正确的是（C） A 信息不可共享 B 信息不可变换 C 信息不可消耗 D 信息不可压缩 11.以下关于信息属性的描述，正确的是（D） A 信息不具层次性 B 信息具有可消耗性 C 信息不可压缩性 D 信息具有可转化性 12.信息的基本属性很多，其中信息力图冲破非自然约束传播到四面八方的属性是（B） A 共享性 B 扩散性 C 变换性 D 事实性 13.下列关于信息的基本属性，正确的是（A） A 信息具有变换性 B 信息不可转化 C 信息没有共享性 D 信息不可扩散 14.信息力图冲破非自然约束传播到四面八方的特性是（B） A 共享性 B 扩散性 C 变换性 D 事实性 15.以下关于信息的基本属性，正确的是（D） A 信息没有转化性 B 信息没有扩散性 C 信息不具普遍性 D 信息不具消耗性 16.信息的基本属性不包括（D） A 事实性 B 扩散性 C 变换性 D 消耗性 17.以下关于信息的基本属性，正确的是（A） A 信息具有事实性 B 信息不可压缩 C 信息具有消耗性 D 信息不可扩散 18.以下关于信息的基本属性，正确的是（D） A 信息不可转化 B 信息不可压缩 C 信息不被扩散 D 信息不回被消耗 19.以下关于“信息”的说法正确的是（B） A 信息可以增加不确定性 B 信息可以被压缩 C 信息可以被消耗 D 信息价值总为正 20.将企业的全部信息集中管理，充分共享，信息才可能成为企业可利用的资源，指的是信息的（C） A 变换性 B 层次性 C 共享性 D 事实性 21.信息经过集中、综合和概括处理后，不回丢失信息的本质，反映了信息的（A） A 可压缩性 B 共享性 C 事实性 D 非消耗性 22.描述事物运动和状态的变化，是指信息具有（B） A 层次性 B 事实性 C 变换性 D 可压缩性 23.信息可以变换成物质、能量和时间反映了它的（A） A 变换性 B 共享性 C 可转化性 D 非消耗性 24.信息被利用时可以产生能量或时间的特性，这属于信息的（B） A 可交换性 B 可转化性 C 可扩散性 D 可压缩性 25.将管理划分为战略级、策略级和执行层，反映了信息的（A） A 层次性 B 事实性 C 非消耗性 D 共享性 26.信息时不可变换的。（B） A 正确 B 错误 27.信息是可以共享的。（A） A 正确 B 错误 28.从潜在的意义上讲，信息是可以转化的。（A） A 正确 B 错误 29.信息是一种特殊的资源，职能共享不能交换。（A） A 正确 B 错误 30.信息和管理不一样，没有层次性。（B） A 正确 B 错误 31.信息是不可压缩的。（B） A 正确 B 错误 32.信息的非消耗性是指在使用过程中不但不会被消耗，而且还可能出现再生或增殖。（A） A 正确 B 错误 33.信息力图冲破保密的、非自然的约束想四面八方传播，体现了信息具可压缩性。（B） A 正确 B 错误 34.信息具有事实性。（A） A 正确 B 错误 35.只要有事物存在，只要有事物运动，就会有它们运动的状态和方式，就存在信息，体现了信息的普遍性。（A） A 正确 B 错误 36.只要有事物存在级其运动，就会有它们运动的状态和方式，就存在信息，这就是信息的普遍性。 37.信息的特性之一是描述事物运动和状态的改变，这种特性称为信息的事实性（真实性）。 38.信息可以负载在其他一切尽可能的物质载体和能量形式上，指的是信息的变换性。 39.信息在一定的条件下，可以转化为物质、能量、时间及其他。 40.信息可以负载在其他一切可能的物质载体和能量形式上，指的是信息的变换性。 41.信息是一种特殊的资源，职能共享不能交换。 42.信息是普遍存在的，它是事物运动和状态改变的方式。 43.信息的非消耗性是指信息与其他物质资源不同，它在使用过程中不但不会被消耗，而且还可能出现再生或增殖。 44.信息经过浓缩、集中、综合和概括处理等后，不至于影响其中的含义，体现了信息的可压缩性。 45.对基层游泳的信息，对高层来说可能是数据。因此，信息与管理一样，也具有层次性。 46.将管理分成三个层次，高层管理对应战略级。 47.将管理分成三个层级，中层管理对应策略级。 48.将管理分成三个层级，基层管理对应执行层。 49.关于信息的基本属性，事实使信息具有价值，不符合事实的信息反而会将决策引入歧途。 50.信息的可压缩性是指信息经过浓缩、集中、综合和概括等处理后，而不至于丢失信息的本质。 51.名词解释：信息的普遍性。 信息的普遍性：信息是普遍存在的，它是事物运动和状态改变的方式。因此，只要有事物存在，只要有事物运动，就会有它们运动的状态和方式，就存在信息。 52.名词解释：信息的共享性。 信息是一种特殊的资源，只能共享不能交换，只有企业的全部信息集中管理，充分共享，信息才可能成为企业可利用的资源。 53.名词解释：信息的非消耗性。 信息的非消耗性是指信息与其他物质资源不同，它在使用过程中不但不会被消耗，而且还可能出现再生或增殖。 54.简述信息的层次性的含义。 通常将管理分成三个层次，即高层管理（战略级）、中层管理（策略级）和基层管理（执行层）。这是信息的层次性。对于同一个问题，处于不同的管理层次，要求不同的信息，对基层有用的信息，对高层来说可能是数据。因此，信息与管理一样，也具有层次性。 55.简述信息具有普遍性的含义。 信息具有普遍性的含义是：信息是不变存在的，它是事物运动和状态改变的方式。因此，只要有事物存在，只要有事物运动，就会有它们运动的状态和方式，就存在信息。 56.名词解释：信息的扩散性。 信息的扩散性是信息的本性，信息力图冲破保密的、非自然的约束，通过各种渠道和手段向四面八方传播。 57.名词解释：信息的变换性。 信息的变换性是指：信息是事物运动的状态和方式，不是事物本身，因此信息可以负载在其他一切尽可能的物质载体和能量形式上。 58.名词解释：信息的事实性。 信息描述了事物运动和状态的改变，因此，它具有事实性，这是信息的重要基本性质之一，事实是信息具有价值，不符合事实的信息其价值可能为负，不但不会辅助决策，反而会将决策引入歧途。 59.名词解释：信息的可压缩性。 信息进故宫浓缩、集中、综合和概括处理后，不至于丢失信息的本质。在进行信息处理的时候应该抓住事物的主要矛盾，从数据中去粗取精，去伪存真，对原始数据进行集中、综合和概括，抽取出最能说清问题的信息，从这种意义上讲，信息具有可压缩性。 60.信息的基本属性有哪些？ 信息的基本属性主要有：普遍性、事实性、层次性、可压缩性、扩散性、非消耗性、共享性、变换性和可转化性。 信息处理的生命周期 1.在信息的生命周期中，信息维护的上一个阶段是信息（B） A 收集 B 存储 C 传输 D 使用 2.信息的生命周期分为多个过程，（C）不是其中的过程。 A 需求 B 收集 C 修改 D 处理 3.在信息的生命周期阶段中，信息处理的下一个阶段是信息（C） A 手机 B 传输 C 存储 D 维护 4.信息的生命周期包括多个阶段，下列阶段中，相互衔接的是（D） A 收集和处理 B 处理和使用 C 存储和传输 D 传输和处理 5.为了确保信息传输的效率，下列选项正确的是（B） A 建立小容量的信息通道 B 规定合理的信息流程 C 增加信息传递的环节 D 信息传递的环节越多，传递的速度就越快 6.信息维护的主要目的不包括（D） A 准确性 B 及时性 C 安全性 D 开放性 7.不断更新数据、维护数据的安全性和完整性指的是信息（B） A 处理 B 维护 C 使用 D 存储 8.信息的（A）是管理人员对信息的需求而进行的原始数据的获取过程。 A 收集 B 处理 C 维护 D 使用 9.信息维护的主要目的不包括（B） A 保证信息的准确性 B 保证信息的共享型 C 保证信息的及时性 D 保证信息的安全性 10.在信息的存储问题上，主要由信息使用的目的决定的是（A） A 保存何种信息 B 信息存储介质 C 信息的保存时间 D 信息的存储方式 11.对收集的信息进行去粗取精、由表及里的处理过程是信息（B） A 维护 B 加工 C 传输 D 使用 12.信息的存储涉及的问题不包括（B） A 保存什么信息 B 保存位置 C 存储介质 D 存储方式 13.关于信息收集阶段面临的问题，最后的问题是用何种形式将收集结果表现出来。（A） A 正确 B 错误 14.二次信息是指按照某种规则经过变换、综合分析和统计推断所产生的信息的总称。（A） A 正确 B 错误 15.信息存储活动不需要考虑存储方式的问题。（B） A 正确 B 错误 16.保存社么信息是信息存储需要考虑的问题。（A） A 正确 B 错误 17.信息维护的主要目的是保证信息的（ABCD） A 保密性 B 准确性 C 及时性 D 安全性 E 无误性 18.为了确保信息传输的效率，需要做的工作包括（BCD） A 增加传递方式 B 建立大容量的信息通道 C 规定合理的信息流程 D 减少信息传递的环节 19.关于信息的存储问题，包括（ABCD） A 保存什么信息 B 保存时间 C 存储方式 D 存储介质 20.信息维护的主要目的是保证信息的准确性、及时性、安全性和保密性。 21.信息维护的主要目的是保证信息的准确性、及时性、安全性和保密性。 22.信息加工产生延迟，体现了信息的特征是滞后性。 23.为了保证信息的传输效率，注意两点：一是技术问题；二是语义问题。 24.信息按时间分，可分为一次信息和二次信息。 25.信息维护的主要目的是保证信息的准确性、及时性、安全性和保密性。 26.信息的收集是根据管理人员对信息的需求而进行的原始数据的获取过程。 27.信息的生命周期阶段包括需求、收集、传输、处理、存储、维护、使用和退出等过程。 28.信息收集的首要问题是如何将需要的信息识别出来，其次是收集的方法问题。 29.信息存储活动涉及的问题包括保存什么信息、存储介质、保存时间、存储方式。 30.信息收集的首要问题是如何将需要的信息识别（或检测）出来。 31.从信息的产生到最终被使用而退出的过程，称为信息处理的生命周期。 32.信息的产生、使用至退出的整个过程称为信息的生命周期。 33.信息存储活动主要涉及保存什么信息、存储介质、保存时间、存储方式四个问题。 34.信息的存储活动主要涉及哪些问题？ 信息的存储活动主要涉及保存什么信息、存储介质、保存时间、存储方式四个问题。 35.名词解释：信息的维护。 信息维护，狭义上是指不断更新数据、维护数据的安全性和完整性，使信息保持可用状态。广义上是指信息系统的开发和运行中的一切数据管理工作。 36.信息维护的目的 信息维护的目的是保证信息的准确性、及时性、安全性和保密性。 37.保证信息的传输效率，我们需要做哪些工作？ 1.建立大容量的信息通道；2.规定合理的信息流程；3.减少信息传递的环节。 38.名词解释：信息的收集。 信息的收集是根据管理人员对信息的需求而进行的原始数据的获取过程。 39.信息传输效率需要注意的问题。 为了确保信息传输的效率，必须注意两个问题，一是技术问题，即如何快速、准确地传输信息；二是语义问题，即在传输过程中如何确切地表达信息的意义。 40.确保信息的传输效率，要做到哪几点？ 为了确保信息传输的效率，需要做到以下几点：1.建立大容量的信息通道；2.规定合理的信息流程；3.减少信息传递的环节。 41.名词解释：信息处理的生命周期。 信息与其他资源一样具有生命周期，从信息的产生到最终被使用而发挥作用，可将信息的生命周期分为需求、收集、传输、处理、存储、维护、使用和退出等过程。 42.名词解释：信息加工。 信息加工就是对收集的信息进行去真伪、去粗取精、由表及里、由此及彼的加工过程。 管理信息与决策 1.高层管理者对信息的要求是（B） A 信息来源主要是企业内部 B 信息的精度要求比较高 C 信息的范围比较宽 D 信息发生时间是过去的 2.以下关于管理信息的特性，正确的是（） A 信息来源的分散性 B 信息处理方式的单一性 C 信息量大来源单纯 D 信息加工和使用一致性 3.以下关于信息与决策的关系，正确的是（A） A 决策过程就是信息处理过程 B 决策信息的数量越多越好 C 不同层次决策需要相同信息 D 高层决策只需结构化信息 4.基层管理者对信息要求的特点是（D） A 信息来源以外部为主 B 信息范围较宽 C 信息发生时间不确定 D 信息精度很高 5.以下关于管理信息的特性，正确的是（A） A 信息量大，来源多样 B 信息加工和使用的一致性 C 信息来源的集中性 D 信息处理方式的单一性 6.中层管理者对信息要求的特点是（D） A 信息来自外部 B 信息发生的时间是当前的 C 信息内容具体 D 信息的精度要求是较高的 7.决策过程中的每一步都离不开信息。（A） A 正确 B 错误 8.关于管理信息与决策的关系，低层管理的发生频率是一般使用。（B） A 正确 B 错误 9.中层管理的信息范围较窄。（A） A 正确 B 错误 10.管理信息的发生、加工和使用时间、空间上是一致的。（A） A 正确 B 错误 11.管理信息的特点包括（ABCD） A 信息来源的分散性 B 信息量大且多样性 C 信息处理方式的多样性 D 信息的发生、加工和使用时间、空间上的不一致性 12.管理信息的特点中有：信息量大且多样性。 13.不同的管理层次需要不同层次的信息，高层信息大多数来自组织的外部。 14.简述管理信息除了具有信息的一般属性外，还有哪些特点？ 管理信息除了具有信息的一般属性外，还有的特点：1.信息来源的分散性。2.信息大且多样性。3.信息处理方法的多样性。4.信息的发生、加工和使用时间、空间上的不一致性。 15.简述信息与决策的关系。 1.决策需要信息的支持。决策的目的是为了消除不确定性，需要大量、准确、全面、及时的信息作为依据。可以说决策过程中的每一步都离不开信息，从某种程度上讲，决策过程可以视为一个信息处理过程。2.不同的管理层次需要不同的信息。管理决策需要信息，但并不是说信息越多越好，实际上，不同程度的决策需要不同的信息，所有，如何把适当的信息提供给不同的管理决策者是非常重要的。 "},"pages/信息系统开发与管理/第二章_管理信息系统的基本知识/第三节_系统的基本知识.html":{"url":"pages/信息系统开发与管理/第二章_管理信息系统的基本知识/第三节_系统的基本知识.html","title":"第三节：系统的基本知识","keywords":"","body":"第三节：系统的基本知识 系统的概念 1.系统可以分解的基本要素不包括（C） A 输入 B 处理 C 更新 D 反馈 2.以下关于系统的概念，错误的是（D） A 系统都由要素组成 B 系统都有一定的结构 C 系统都有一定的目的 D 系统都是闭环系统 3.下列关于系统的概念，错误的是（C） A 系统都有一定的功能 B 系统都有要素组成 C 系统整体的性能等于各要素的性能只和 D 系统都有一定的结构 4.系统存在的基本条件包括（ABC） A 系统是由若干要素组成的 B 系统有一定的结构 C 系统有一定的功能 D 系统的功能越多越好 5.系统可以分解为输入、处理、输出、反馈和控制五个基本要素。 6.无论何种系统都可以分解为输入、处理、输出、反馈和控制五个基本要素。 7.系统可以分解的五个要素分别是输入、处理、输出、反馈和控制。 8.系统存在的三个基本条件包括：由若干要素（部分）组成、有一定的结构、有一定的功能。 9.系统存在的三个基本条件包括：由若干要素（部分）组成、有一定的结构、有一定的功能。 10.从哪几方面对系统进行分析？ 我们一般从系统的要素、结构和功能三个方面进行分析：1.系统是由若干要素组成的。2.系统有一定的结构。3.系统有一定的功能。系统的功能是系统与外部环境相互联系和相互作用表现出来的性质、能力和功能。 11.名词解释：系统。 系统是由一些相互联系、相互制约的若干组成部分结合而成的、具有特定功能的一个有机整体（集合）。 12.简述系统存在的基本条件。 系统存在的三个基本条件：1.系统是由若干要素（部分）组成的。2.系统有一定的结构。3.系统有一定的功能。 系统的分类 1.按系统与环境的关系分类，生物系统属于（B） A 封闭系统 B 开放系统 C 概念系统 D 物理系统 2.系统按复杂程度分类，宗教属于（B） A 生物系统 B 社会系统 C 物理系统 D 自然系统 3.系统按复杂程度分类，最复杂的是（B） A 自然系统 B 宇宙系统 C 物理系统 D 生物系统 4.系统按复杂程度分类，艺术属于（D） A 自然系统 B 生物系统 C 物理系统 D 社会系统 5.系统按照是否有反馈机制分为（B） A 封闭系统和开放系统 B 开环系统和闭环系统 C 概念系统和物理系统 D 社会系统和经济系统 6.按复杂程度分类，最复杂的系统是（A） A 宇宙系统 B 物理结构系统 C 人类系统 D 社会系统 7.闭环系统是系统的输出量不对系统的控制产生任何影响。（B） A 正确 B 错误 8.系统按复杂程度分类艺术属于自然系统。（B） A 正确 B 错误 9.按复杂程度分类，计算机、汽车、桥梁等属于物理结构系统。 10.按系统的抽象程度分类，最抽象的系统是概念系统。 11.如一个系统的控制器能够对输出行为和期望行为之间的偏差进行调节，消除偏差，以获得预期的系统性能，则这个系统属于闭环（或反馈）系统。 12.涉及经济和政治，按复杂程度分类的系统是社会系统。 13.输入输出之间存在反馈机制的是闭环（或反馈）系统。 14.按抽象程度分类，客观存在可以实际运作的系统是物理系统。 15.按系统与环境的关系分类，可分为封闭系统和开放系统*。 16.按是否有反馈机制分类，可分为开环系统和闭环系统。 17.从系统分类说，政治、经济、法律和宗教属于社会系统。 18.如果一个系统的输出量不对系统的控制产生任何影响，则这个系统属于开环系统。 19.如果一个系统的输出端与输入端之间不存在反馈，这样的系统称为开环系统。 20.按系统的抽象程度分类可分为三类，分别是：概念系统、逻辑系统、物理系统。 21.如一个系统可以与环境进行信息和能量交换，则这个系统属于开放系统。 22.如一个系统不与环境进行信息和能量交换，不为系统以外的因素所干扰，则这个系统属于封闭系统。 23.按系统与环境的关系分类，社会系统属于开放系统。 24.人类系统按系统与环境分类，属于开放系统。 25.按系统的抽象程度分类可分为：概念系统、逻辑系统、物理系统。 26.名词解释：闭环系统。 如一个系统的控制器能够对输出行为和期望行为之间的偏差进行调整，消除偏差，以获得预期的系统性能，则这个系统属于闭环系统（或反馈系统）。 27.名词解释：逻辑系统。 逻辑系统是在概念的基础上，构造出来的原理上行得通的系统。 28.名词解释：开环系统。 按是否有反馈机制分类，可分为开环系统和闭环系统。如果一个系统的输出端和输入端之间不存在反馈，系统的输出量不对系统的控制产生任何影响，这样的系统称开环系统。 29.系统按复杂程度的分类有哪些？ 系统按复杂程度分类分为：物理结构系统、生物系统、人类系统、社会系统、宇宙系统。 系统的属性 1.不同层次上的系统运动都存在组织化的倾向，而不同系统之间存在着系统同构，体现了系统的（A） A 统一性 B 整体性 C 层次性 D 关联性 2.如果一个系统由若干子系统组成，该系统本身又是更大系统的子系统，这种特性称为系统的（B） A 整体性 B 层次性 C 扩展性 D 统一性 3.系统的属性不包括（D） A 整体性 B 关联性 C 层次性 D 共享性 4.一个系统总是由若干子系统组成的，该系统本身又可看做更大系统的一个子系统，这就构成了系统的（C） A 关联性 B 统一性 C 层次性 D 整体性 5.系统的总体性能大于各要素性能只和的特性称为系统的（A） A 整体性 B 统一性 C 关联性 D 层次性 6.系统性能大于各子系统性能之和的特性，称为系统的（C） A 统一性 B 关联性 C 整体性 D 层次性 7.系统与其子系统或要素之间相互依存和作用的特性，称为系统的（C） A 整体性 B 层次性 C 关联性 D 统一性 8.客观物质运动的层次性和各不同层次上系统运动的特殊性，体现了系统的（C） A 整体性 B 关联性 C 统一性 D 层次性 9.离开关联性就不能揭示复杂系统的本质。（A） A 正确 B 错误 10.一个系统由若干子系统组成，该系统就是最大的系统。（B） A 正确 B 错误 11.系统的属性包括（ABCD） A 整体性 B 关联性 C 层次性 D 统一性 E 一致性 12.客观物质运动的层次和各不同层次上系统运动的特殊性，这主要表现在不同层次上系统运动规律的统一性。 13.不同层次上的系统运动都存在组织化的倾向，而不同系统之间存在着系统同构。 14.系统本身也以可看做是更大的系统的一个子系统，体现了系统的层次性。 15.在处理系统问题时，要注意研究系统的结构与功能的关系，重视提高系统的整体功能。 16.系统与环境之间相互作用、相互依存和相互关系体现了系统的关联性。 17.虽然系统是由要素或子系统组成的，但系统的整体性能可以大于各要素的性能之和。 18.系统的属性中，关联性是指系统与其子系统之间、系统内部各子系统之间以及系统与环境之间相互作用，相互依存和相互关系。 19.系统的属性有整体性、关联性、层次性和统一性。 20.系统的属性包括整体性、关联性、层次性和统一性。 21.名词解释：系统的属性 统一性。 一般系统论承认客观物质运动的层次性和各不同层次上系统运动的特殊性，这主要表现在不同层次上系统运动规律的统一性，不同层次上的系统运动都存在组织化的倾向，而不同系统之间存在着系统同构。 22.名词解释：系统的关联性。 关联性是系统的一个属性，是指系统与其子系统之间、系统内部各子系统之间和系统与环境之间的相互作用、相互依存和相互关系。离开关联性就不能揭示复杂系统的本质。 23.名词解释：系统的整体性。 整体性是系统的一个属性。虽然系统是由要素或子系统组成的，但系统的整体性能可以大于各要素的性能之和。因此在处理系统问题时，要注意研究系统的结构与功能的关系，重视提高系统的整体功能。 24.名词解释：系统的层次性。 一个系统总是由若干子系统组成的，该系统本身又可看做是一个更大的系统的一个子系统，这就构成了系统的层次性。 系统的分解 1.下列系统原则中，不正确的是（B） A 可控制原则 B 子系统规模相同原则 C 功能聚合性原则 D 子系统接口标准化原则 2.系统的分解减少了我们分析问题的难度。（A） A 正确 B 错误 3.将系统按一定的原则分解为若干子系统后，功能和结构会变复杂（B） A 正确 B 错误 4.系统分解为了保证准确，考虑的原则包括（ABD） A 可控制性原则 B 功能聚合性原则 C 可分解原则 D 接口标准化原则 E 流程化原则 5.经过系统的分解后，大大降低了功能和结构的复杂程度，减少了我们分析问题的难度。 6.面对一个复杂的系统，人们将其划分解为若干子系统，从而降低分析的难度，这种系统方法称为系统的分解，或系统分析。 7.系统分解的过程就是确定子系统边界的过程。 8.系统分解的过程中，子系统之间的连接点被定义为接口。 9.为了保证系统分解的准确性和合理性，考虑的原则包括可控制原则、功能聚合性原则和接口标准化原则。 10.系统分解需要考虑三个原则，分别是可控制原则、功能聚合性原则和接口标准化原则。 11.确保系统分解的准确性，需要考虑哪些原则？ 为了保证系统分解的准确性和合理性，主要考虑三个原则：1.可控制原则；2.功能聚合性原则；3.接口标准化原则。 12.名词解释：系统分解。 面对一个庞大复杂的系统，无法把系统所有元素之间关系表达清楚，这时要将系统按一定的原则分解为若干子系统，经过系统的分解后，其功能和结构的复杂程度大大降低，减少了我们分析问题的难度。这种系统的方法就是系统的分解。 "},"pages/信息系统开发与管理/第二章_管理信息系统的基本知识/第四节_信息技术的基本知识.html":{"url":"pages/信息系统开发与管理/第二章_管理信息系统的基本知识/第四节_信息技术的基本知识.html","title":"第四节：信息技术的基本知识","keywords":"","body":"第四节：信息技术的基本知识 信息技术的基本知识 1.从管理信息系统开发角度看，三大核心技术指的是网络技术、数据库技术和开发语言。 2.从管理信息系统开发角度看，网络技术、数据库技术和开发语言是管理信息系统的三大核心技术。 3.关于信息的收集、识别、提取、变换、存储、处理、检索、检测、分析和利用等各种技术的总称，是指信息技术。 4.站在管理信息系统开发角度，网络技术、数据库技术和开发语言是管理信息系统的三大核心技术。 5.管理信息系统的三大核心技术是网络技术、开发语言和数据库技术。 6.名词解释：信息技术。 信息技术是有关信息的收集、识别、提取、变换、存储、处理、检索、检测、分析和利用等各种技术的总称，是管理信息系统的重要基础。 网络技术 1.管理信息系统以计算机网络为基础的主要原因是，不包括（A） A 增加投资 B 有利于信息的安全存储 C 节省投资 D 上下级信息的交流 2.安全的网络不具有的特征是（A） A 不可审查性 B 保密性 C 完整性 D 可用性 3.最基本的拓扑结构不包括（B） A 总线型 B 曲线型 C 星型 D 环型 4.访问互联网时，用户使用的网络协议是（A） A TCP/IP B IPX/SPX C NetBEUI D X.25 5.用户访问互联网，必须使用IPX/SPX协议。（B） A 正确 B 错误 6.最基本的拓扑结构分为总线型、星型、环形。（A） A 正确 B 错误 7.网络安全指网络系统的硬件、软件及其系统中的数据受到保护，系统可以连续可靠正常地运行，网络服务不中断。 A 正确 B 错误 8.安全的网络是不可审查的。（B） A 正确 B 错误 9.计算机网络的功能表现在硬件资源的共享、软件资源共享和用户信息交换上。（A） A 正确 B 错误 10.计算机网络的功能主要表包括（ABE） A 硬件资源共享 B 用户信息交换 C 较小的冗余度 D 容易扩展 E 软件资源共享 11.管理信息系统以计算机网络为基础的主要原因包括（ABCE） A 横向部门间的信息交流 B 上下级间的信息交流 C 节省资源 D 不利于信息的共享 E 有利于信息的安全存储 12.安全网络的特征包括（ABCDE） A 保密性 B 完整性 C 可用性 D 可控性 E 可审查性 13.能连接多个城市、地区或国家的计算机网络称为广域网。 14.Internet是一种范围覆盖全球的广域网络。 15.网络按地理范围分类，可以分为局域网和广域网。 16.网络按地理范围分类，可以分为局域网和广域网。 17.不同计算机之间必须使用相同的网络协议才可以进行通信。 18.最基本的拓扑结构分为：总线型、星型和环型。 19.网络按通信介质分类，分为有线网和无线网。 20.用户如果访问Internet，必须使用TCP/IP协议。 21.网络拓扑结构是指计算机及网络设备在空间上的排列形式。 22.计算机网络是将计算机互连起来，以功能完整的网络软件实现资源共享和信息传递的系统。 23.计算机网络就是利用通信设备和线路将地理位置不同、功能独立的两台或两台以上计算机互连起来。 24.网络安全指的是网络系统的硬件、软件及其系统中的数据收到保护。 25.计算机及网络设备在空间上的排列形式指的是网络拓扑结构。 26.用户如果访问Internet，必须使用TCP/IP协议。 27.不同的计算机之间必须使用相同的网络协议才可以进行通信。 28.计算机网络的功能主要表现在硬件资源共享和用户之间的信息交换三个方面。 29.安全的网络一般具有保密性、完整性、可用性、可控性、和可审查性。 30.网络安全从本质上讲是网络的信息安全。 31.在网络上，各台计算机相互“对话”的语言称为网络协议。 32.安全的网络一般具有保密性、完整性、可用性、可控性、可审查性。 33.名词解释：广域网。 广域网跨接更大的物理范围，能连接多个城市、地区或国家，Internet就是一种范围覆盖全球的广域网络。 34.列举常见的三种网络协议。 TCP/IP协议、IPX/SPX协议、NetBEUI协议。 35.计算机网络的功能表现在哪些方面？ 硬件资源共享、软件资源共享和用户间信息交换。 36.名词解释：计算机网络。 计算机网络就是利用通信设备和线路讲地理位置不同、功能独立的两台或两台以上计算机互连起来，以功能完善的网络软件实现资源共享和信息传递的系统。 37.管理信息系统以计算机网络为基础的原因。 1.上下级间信息的交流；2.横向部门间信息的交流；3.节省投资；4.有利于信息的安全存储。 38.网络安全指的是哪些内容？ 网络安全指的是网络系统的硬件、软件及其系统中的数据受到保护，不因偶然的或者恶意的原因而遭受破坏、更改、泄漏，系统连续可靠正常的运行，网络服务不中断。网络的安全一般具有保密性、完整性、可用性、可控性和可审查性五个特征。 39.简述安全的网络一般具有的五个特征。 安全的网络一般具有保密性、完整性、可用性、可控性和可审查性五个特征。 数据库技术 1.数据表是一个由行和列组成的二维结构，行称为（C） A 字块 B 字段 C 记录 D 字串 2.数据表是一个由行和列组成的二维结构，列称为（A） A 字段 B 字节 C 记录 D 字块 3.管理信息系统常见的“CRUD”，“C”对应数据库SQL语言中的（A） A 插入 B 修改 C 删除 D 查询 4.关系数据库的标准语言是（A） A SQL语言 B C语言 C GO语言 D 汇编语言 5.数据库所保存的数据是指长期储存在计算机内、有组织的、可共享的数据集合。（A） A 正确 B 错误 6.SQL语言中没有修改语句。（B） A 正确 B 错误 7.在管理信息系统中，Access属于大型数据库。（B） A 正确 B 错误 8.数据表是一个由行和列组成的二维结构。（A） A 正确 B 错误 9.数据库是各种数据的集合和容器。 10.表是存放数据的基本数据结构。 11.对数据库进行管理，位于用户和操作系统之间的工具是数据库管理系统（DBMS）。 12.数据库所保存的数据是指长期存储在计算机内、有组织的、可共享的数据集合。 13.数据库是各种相关数据的集合和容器。 14.结构化查询语言简称SQL语言。 15.关系数据库的标准语言是结构化查询语言。 16.数据库管理系统用户和操作系统之间。 17.数据库管理系统缩写为DBMS。 18.在管理信息系统中，通常会把数据库管理系统分成两大类，一是网络数据库和大型数据库，二是桌面或小型数据库。 19.数据库管理系统是对数据库进行管理的系统软件。 20.在管理信息系统中，一般会将数据库管理系统分成两大类：分别是网络数据库或大型数据库以及桌面及小型数据库。 21.在数据表中，行称为记录。 22.数据表中，行称为记录，列称为字段。 23.数据表是数据库的重要组成部分，通常称为表。 24.常用的SQL语句有哪些？ 常用的SQL语句主要有：插入（Insert）、修改（Update）、删除（Delete）、查询（Select）等。 25.简述管理信息系统中，数据库关系系统的两大分类。 在管理信息系统中，通常将数据库管理系统分成两大类，一是网络数据库或大型数据库，它主要应用于企业级管理信息系统的开发，比如Oracle、SQL Sever和DB2等，二是桌面或小型数据库，它主要用于单机版管理信息系统开发，比如Access、Foxpro等。 26.名词解释：数据库管理系统。 数据库管理系统是对数据库进行管理的系统软件，位于用户和操作系统之间，为用户或应用程序提供访问数据的方法和工具。 27.名词解释：DBMS。 数据库管理系统是对数据库进行管理的系统软件，位于用户和操作系统之间，为用户或应用程序提供访问数据的方法和工具。 计算机语言 1.高级语言是第（C）代计算机语言。 A 一 B 二 C 三 D 四 2.关于计算机语言，下列说法错误的是（D） A 汇编语言改变了机器语言的不直观性 B 高级语言具有较好的可移植性 C 机器语言可以直接和机器打交道 D 汇编语言可以被机器直接识别 3.Java属于程序设计语言汇中的（C） A 机器语言 B 汇编语言 C 高级语言 D 结构化查询语言 4.能被计算机直接理解和执行的是（A） A 机器语言 B 汇编语言 C 高级语言 D 结构化查询语言 5.程序设计语言发展经历的三个阶段不包括（C） A 机器语言 B 汇编语言 C 汉语 D 高级语言 6.汇编语言的可移植性比较好。（B） A 正确 B 错误 7.计算机语言，指人与计算机之间通信的语言。（A） A 正确 B 错误 8.第一代计算机语言是机器语言。 9.第二代计算机语言是汇编语言。 10.高级语言是第三代高级语言。 11.计算机程序设计语言，经历了机器语言、汇编语言到高级语言的历程。 12.程序设计语言也称为计算机语言。 13.程序设计语言中，可以直接和计算机打交道的是机器语言。 14.程序设计语言的发展通常分为机器语言、汇编语言和高级语言三个阶段。 15.汇编语言的缺点是什么？ 涉及机器的硬件细节，难学难用，容易出错，且无法移植，不易维护。 16.名词解释：计算机语言。 计算机语言又称程序设计语言，指人与计算机之间通信的语言，是人与计算机之间传递信息的媒介，是开发管理信息系统软件必备的工具。 17.名词解释：汇编语言。 是基于助记符的计算机语言。汇编语言与机器指令之间基本上是一一对应的关系。汇编语言不能被机器直接识别。 "},"pages/信息系统开发与管理/第三章_系统开发方法概述/第一节_管理信息系统开发的基本问题.html":{"url":"pages/信息系统开发与管理/第三章_系统开发方法概述/第一节_管理信息系统开发的基本问题.html","title":"第一节：管理信息系统开发的基本问题","keywords":"","body":"第一节：管理信息系统开发的基本问题 系统开发具备的条件 1.对企业管理信息系统开发和使用成败起决定作用的首要条件是（D） A 较强的开发队伍 B 较好的硬件条件 C 正确的开发策略 D 企业高层领导的重视与介入 2.关于系统开发具备的条件，下列说法错误的是（A） A 企业高层领导不需要介入 B 企业业务人员要有积极性 C 企业要有一定的科学管理基础 D 要有一定的投资保证 3.对于企业的管理基础工作，管理信息系统开发必备条件之一是（C） A 先进、实用的信息基础设施 B 规范、完善的信息管理制度 C 科学、有效的信息管理方法 D 准确、完整的企业基础数据 4.对企业管理信息系统开发和使用成败起决定作用的是（C） A 投资保证 B 科学管理基础 C 高层领导重视与介入 D 业务人员的积极性 5.计算机能将大批量数据高速、准确地进行各种加工处理，产生对企业管理有用的信息，体现了（C） A 企业高层领导应重视和介入 B 企业业务人员要有积极性 C 企业要有一定的科学挂历基础 D 要有一定的投资保证 6.在一般制造企业中，管理信息系统的直接操作者与使用者主要是（C） A 企业各级领导 B 企业信息中心工作人员 C 企业各类业务人员 D 企业基层职工 7.企业进行管理信息系统开发必须具备的条件中，最重要的一个是（B） A 有一定的计算机应用基础 B 有一定的科学管理基础 C 有较好的系统开发方案 D 有较强的使用、维护力量 8.对企业管理信息系统开发和使用成败起决定作用的首要条件是（D） A 较强的开发队伍 B 较好的硬件条件 C 较充裕的开发时间 D 企业高层领导的重视和介入 9.企业开发管理信息系统应具备的条件中，没有（B） A 企业要有一定的科学管理基础 B 企业要减少投资 C 企业高层领导应重视和介入 D 企业业务人员要有积极性 10.管理信息系统在进行开发时，系统的规模要越大越好。（B） A 正确 B 错误 11.系统建成投入使用后，企业各业务人员是系统的直接操作者。（A） A 正确 B 错误 12.只有企业最高层领导才有权利和权威在企业宣布建立管理信息系统的觉得以及落实组织机构，动员全企业支持系统开发。（A） A 正确 B 错误 13.开发管理信息系统之前，企业应该要有一定的科学管理基础。（A） A 正确 B 错误 14.企业开发管理信息系统必须具备的条件包括（ABDE） A 企业业务人员要有积极性 B 要有一定的投资保证 C 要有一定的经济效益 D 企业要有一定的科学管理基础 E 企业高层领导应重视和介入 15.管理信息系统投入使用后，维护费用占总投资的主要部分。 16.管理信息系统的规模较大程度地取决于企业的投资额。 17.在管理信息系统投入使用以后，长期而重要的任务指的是系统的维护工作。 18.企业开发管理信息系统必须具备条件中，企业业务人员要有积极性。 19.企业要有一定的投资保证是企业开发管理信息系统应具备的一个条件。 20.企业各类业务人员是管理信息系统主要的直接操作者与使用者。 21.企业高层领导人员的重视与介入对信息系统开发与使用的成败起决定性作用。 22.系统建成投入使用后，企业业务人员是系统的直接操作者与使用者。 23.企业具有一定的科学管理基础是进行管理信息系统开发的一个必要条件。 24.没有科学管理的基础，就无法建成有效的计算机管理信息系统。 25.企业高层领导重视与介入的主要意义是什么？ 企业高层领导重视和介入的主要意义在于，只有企业最高层领导才有权利和权威在企业宣布建立管理信息系统的决定以及落实组织机构，动员全企业支持系统开发。 26.简述企业开发管理信息系统必须具备的条件。 一般来说，企业开发管理信息系统必须具备以下条件：1.企业高层领导应重视和介入；2.企业业务人员要有积极性；3.企业要有一定的科学管理基础；4.要有一定的投资保证。 27.开发管理信息系统之前企业应具备哪些基本条件？ 1.企业高层领导重视和介入；2.企业业务人员要有积极性；3.企业要有一定的科学管理基础；4.要有一定的投资保证。 28.简述企业要科学管理的内容。 在企业中，没有科学管理的基础，就无法建成有效的计算机管理信息系统，计算机能将大批量数据高速、准确地进行各种加工处理，产生对企业管理有用的信息，但是它的前提是输入的数据准确、完整，否则便成了“假数真算”，根本不可能得到具有指导意义的信息。 系统开发前的准备工作 1.管理信息系统的开发策略很大程度上决定了系统的成败。下列不属于正确开发策略的是（D） A 根据企业的实际情况选择合适的方法 B 使系统具有恰当的目标 C 动员企业各方面的力量 D 从系统设计入手，有步骤地实施系统 2.管理信息系统能否真正取得效益，与企业基础数据的完整程度以及（B）有很大关系。 A 规范程度 B 真实程度 C 困难程度 D 实用程度 3.关于系统开发前的准备工作，如果没有（D），系统就像一座没有原料的加工厂。 A 系统目标 B 开发策略 C 投资金额 D 基础数据 4.管理信息系统开发的出发点是（A） A 系统开发部标 B 系统开发策略 C 系统开发方法 D 系统开发绩效 5.管理信息系统开发中一项最重要的基础工作是收集、整理、规范（B） A 企业运营数据 B 企业基础数据 C 企业环境数据 D 企业决策数据 6.系统开发的前期准备工作不需要确定（D） A 投资金额 B 开发策略 C 基础数据 D 总体结构 7.系统开发前的准备工作，不包括（D） A 借鉴同类系统的开发经验 B 确定系统目标、开发策略和投资金额 C 手机和整理基础数据 D 企业开发必须要自主创新 8.借鉴同类系统的开发经验是指吸取其他企业或组织中类似的管理信息系统的开发经验和失败教训，就能在本企业系统开发中少走弯路。 A 正确 B 错误 9.管理信息系统开发前的准备工作之一是确定系统目标、开发策略和投资金额。 10.系统开发目标是管理信息系统开发的出发点。 11.正确开发策略的内容中，组织由管理人员和技术人员参加的开发队伍。 12.正确的开发策略是从总体规划入手，有步骤地实施系统；注重系统的运行、维护和更新等。 13.系统开发前的准备工作中有一项内容是：确定系统目标、开发策略和投资金额。 14.系统的目标是管理信息系统设计的出发点。 15.确定系统目标、开发策略和投资金额是系统开发前的一项准备工作。 16.系统开发前的一项准备工作是借鉴同类系统的开发经验。 17.管理信息系统能否取得效益，与存储在系统中的企业基础数据的完整程度以及真实程度有很大关系。 18.收集、整理、规范企业的基础数据是管理信息系统开发中一项最重要的基础工作。 19.吸取其他企业同类管理信息系统开发的经验教训是企业管理信息系统开发成功的重要措施。 20.企业基础数据的收集、规范和整理，是管理信息系统开发前前的基础性工作。 21.名词解释：正确的开发策略。 所谓正确的开发策略，就是能根据企业的实际情况选择合适的方法，采用正确的方式和手段来建立系统，使系统具有恰当的目标；能动员企业各方面的力量；组织由管理人员和技术人员参加的开发队伍；从总体规划入手，有步骤地实施系统；注重系统的运行、维护和更新等。 22.简述正确的开发策略的内容。 所谓正确的开发策略，就是能根据企业的实际情况选择合适的方法，采用正确的方式和手段来建立系统，使系统具有恰当的目标；能动员企业各方面的力量；组织由管理人员和技术人员参加的开发队伍；从总体规划入手，有步骤地实施系统；注重系统的运行、维护和更新等。 23.系统开发前的准备工作有哪些内容？ 系统开发前的准备工作主要有以下几个方面：1.借鉴同类系统的开发经验；2.确定系统目标、开发策略和投资金额；3.收集和整理基础数据。 系统开发的困难因素 1.管理信息系统开发失败的重要原因之一是（B） A 重视规划，轻视编程 B 重视编程，轻视规划 C 重视设计，轻视实施 D 重视实施，轻视设计 2.管理信息系统的开发过程中，大部分的工作是（C）脑力劳动的结果。 A 组织者 B 管理人员 C 开发人员 D 业务人员 3.下列选项中，不属于系统开发困难因素的是（D） A 新系统对当前管理模式影响大 B 管理信息系统的效益不易用货币形式直接反映 C 基础数据的准确性与完整性差 D 重视规划，轻视编程 4.关于造成系统开发困难的各项说法中，错误的是（D） A 新系统对当前管理模式影响较大 B 系统的效益不易用货币形式直接反映 C 基础数据的准确性与完整性差 D 重视规划，轻视编程 5.在系统建设过程中，各阶段所引入的错误具有潜伏期，越早潜入的错误越晚才会发现，指的是（A） A 堆栈现象 B 轻视规划 C 新系统影响较大 D 基础数据的完整性差 6.关于系统开发的困难因素，描述不正确的是（A） A 旧的系统对当前管理模式有影响 B 基础数据完整性差 C 重视编程，轻视规划 D 堆栈现象 7.企业的管理基础工作混乱，难以获得准确、完整的原始数据导致管理信息系统不能较好地发挥作用。（A） A 正确 B 错误 8.管理信息系统的建设有其自身的发展规律，各阶段所引入的错误具有潜伏期，越早潜入的错误越晚才能发现。（A） A 正确 B 错误 9.重视程序设计，轻视总体规划的思想是导致系统开发失败的重要原因。（A） A 正确 B 错误 10.对于大多数企业来说，管理信息系统带来的经济效益不易用货币形式直接反映，属于间接效益。 11.现在，管理信息系统开发往往要和企业变革同时进行。 12.管理信息系统的开发过程有别于其他类型的过程，属于知识密集型的工作。 13.管理信息系统的效益不易用货币形式直接反映，是管理信息系统开发的一个困难因素。 14.基础数据的准确性与完整性差，是管理信息系统开发的一个困难因素。 15.管理信息系统的建设有其自身的发展规律，系统开发过程一般分阶段进行的，各阶段所引入的错误具有潜伏期。 16.管理信息系统开发的一个困难因素采用增加开发人员的方式来加快进度。 17.管理信息系统开发的一个困难因素是：重视编程，轻视规划。 18.管理信息系统开发的一个困难因素是新系统对当前管理模式影响较大。 19.管理信息系统对企业的各种数据进行加工处理，提炼信息，用来辅助管理决策。 20.系统开发过程一般是分阶段进行的，每一个阶段都可能由于开发人员对用户需求的理解出现偏差等原因引入错误。 21.管理信息系统开发失败的最重要原因是重视编程，轻视规划。 22.管理信息系统投入使用后，企业必须适应新系统来的管理思想、管理方法和管理组织的变革。 23.系统开发的困难因素主要有哪些？ 1.新系统对当前管理模式影响较大；2.管理信息系统的效益不易用货币形式直接反映；3.基础数据的准确性和完成性差；4.重视编程，轻视规划；5.无法采用增加开发人员的方式来加快进度；6.存在堆栈现象。 24.简述信息系统开发的困难因素。 管理信息系统开发的困难因素主要有：1.新系统对当前管理模式影响较大；2.管理信息系统的效益不易用货币形式直接反映；3.基础数据的准确性和完整性差；4.重视编程，轻视规划；5.无法采用增加开发人员的方式来加快进度；6.存在堆栈现象。 25.名词解释：堆栈现象。 管理信息系统的建设有其自身的发展规律，系统开发过程一般是分阶段进行的，每一个阶段都可能由于开发人员对用户需求的理解出现偏差等原因引入错误，并且各阶段所引入的错误具有潜伏期，越早潜入的错误越晚才能发现，我们将此现象称为堆栈现象。 "},"pages/信息系统开发与管理/第三章_系统开发方法概述/第二节_管理信息系统的开发方法.html":{"url":"pages/信息系统开发与管理/第三章_系统开发方法概述/第二节_管理信息系统的开发方法.html","title":"第二节：管理信息系统的开发方法","keywords":"","body":"第二节：管理信息系统的开发方法 系统开发方法的产生背景 1.关于软件危机的爆发，说法错误的是（D） A 软件的可靠性差 B 软件开发费用和进度失控 C 生产出来的软件难以维护 D 软件危机的出现和管理无关 2.常用的开发方法不包括（C） A 结构化方法 B 原型法 C 非结构化 D 面向对象的方法 3.为摆脱软件危机的影响，推进开发的成功率，管理信息系统的研制必经之路是（B） A 科学化 B 工程化 C 自动化 D 智能化 4.软件危机的主要表现之一是软件难以（C） A 生产 B 使用 C 维护 D 评价 5.软件危机爆发主要表现不包括（A） A 软件不可以修改 B 软件开发费用和进度失控 C 软件的可靠性差 D 生产出来的软件难以维护 6.软件危机的主要表现之一是软件难以维护。 7.为摆脱软件危机的影响，管理信息系统的研制必须走工程化的道路。 8.软件危机爆发主要表现在三个方面，其中一个方面是软件的可靠性差。 9.造成“软件危机”的原因是多方面的，没有或未能认真遵循开发规范是其中的一个方面。 10.未能充分理解和正确表达用户的需求是造成“软件危机”的一个原因。 11.造成“软件危机”的原因是多方面的，有技术上的，但更主要的是“管理”上的。 12.管理信息系统开发方法主要有三大类：结构化方法、原型法以及面向对象的方法。 13.目前常用的系统开发方法主要有结构化方法、原型法和面向对象方法等三大类。 14.目前常用的三种开发方法。 目前常用的一些开发方法主要有三大类，即结构化方法，原型法，以及面向对象的方法。 15.简述软件危机爆发的主要表现。 软件危机爆发主要表现在三个防霾呢：1.软件开发费用和进度失控；2.软件的可靠性差；3.生产出来的软件难以维护。 16.简述造成“软件危机”的原因。 造成“软件危机”的原因是多方面的，有技术上的，但更主要的是管理上的，比如采用了不适当的开发方法；未能充分理解和正确表达用户的需求；没有或未能认真遵循开发规范；项目管理不力；不重视资料、文档工作等。 结构化开发方法 1.结构化分析方法的系统说明书的构成中，没有（D） A 一套分层的数据流图 B 一本数据字典 C 一组加工说明 D 分析报告 2.结构化设计的主要特点不包括（A） A 互相联系、功能较多的模块结构 B 块内联系大 C 块间联系小 D 采用模块结构图的描述方式 3.（A）是“结构化分析”和“结构化设计”的统称。 A 结构化方法 B 原型法 C 面向对象的方法 D 面向用户的方法 4.采用结构化设计方法所设计的系统，其模块的主要特点之一是（A） A 相对独立、功能单一 B 相互独立、功能多样 C 相对独立、功能多样 D 相互联系、功能多样 5.结构化分析的主要内容包括一组分层的（B） A 系统流程图 B 数据流图 C 模块结构图 D 程序流程图 6.结构化设计中，各模块之间的关系应该是（D） A 模块内部联系和模块之间的联系都要大 B 模块内容和模块之间的联系都要小 C 模块内部联系要小，模块之间联系要大 D 模块内部联系要大，模块之间联系要小 7.结构化分析方法的基本思想可以概括为（C） A 自底向上，逐步实现 B 自顶向下，逐步实现 C 自顶向下，逐步实现 D 自底向上，逐层分解 8.一个好的结构化设计的主要特点包括（A） A 相对独立的模块结构 B 模块内联系小 C 由粗到细，逐步求精 D 模块间联系大 9.结构化分析方法解决复杂问题的两个基本手段是（C） A 分解、具体化 B 集成、具体化 C 分解、抽象 D 集成、抽象 10.结构化分析方法的一个重要特点是分析结果的描述方式尽可能采用（B） A 文字 B 图形 C 图像 D 多媒体 11.用结构化分析方法获得的系统说明书包括（D） A 系统规划、系统分析、系统设计、系统实施 B 系统总体设计、系统详细设计、数据库设计、人机界面设计 C 系统流程图、数据流图、模块结构图、程序流程图 D 数据流图、数据字典、加工说明、补充材料 12.用文字等形式详细描述系统每一个基本处理过程的工具是（C） A 数据流图 B 数据字典 C 加工说明 D E-R图 13.结构化分析方法的基本思想是（A） A 自顶向下，逐步求精 B 自底向上，逐层集成 C 自外向内，逐步深入 D 自内向外，逐步扩展 14.在结构化分析方法中，用来辅助进行系统分析的材料是（D） A 数据流图 B 数据词典 C 加工说明 D 补充材料 15.结构化方法是目前最成熟、应用最为广泛的管理信息系统开发方法之一。（A） A 正确 B 错误 16.用结构化分析方法获得的系统说明书由以下四部分组成：数据流图、数据字典、加工说明、补充材料。 17.结构化设计的基本思想是将系统设计成由相对独立、单一功能的模块组成的结构。 18.结构化设计的一个特点是采用模块结构图的描述方式。 19.在结构化设计中，模块内部联系要大。 20.结构化设计的基本思想是设计成相对独立、功能单一的模块结构。 21.结构化方法是“结构化分析”和“结构化设计”的统称。 22.结构化方法是基于瀑布模型提出的。 23.结构化分析方法的基本思想是自顶向下、逐步求精。 24.结构化分析方法的一个重要特点是，分析结果的描述尽可能采用图形方式。 25.结构化分析方法解决发砸问题的两个基本手段是分解、抽象。 26.结构化设计衡量模块“相对独立”的准则之一是块内联系要大。 27.结构化方法解决复杂问题的两个基本手段是分解和抽象。 28.简述结构化分析方法的基本手段、基本策略和主要内容。 结构化分析的基本手段是分解与抽象，基本策略是自顶向下、由粗到细、逐步求精，主要内容包括一套分层的数据流图，一本数据词典，一组加工说明和补充材料。 29.结构化分析方法的基本思想是什么？ 结构化分析方法是一个简单明了、使用很广的系统分析的方法，其基本思想可以概括为一句话：“自顶向下，由粗到细，逐步求精”。 30.结构化分析的主要内容有哪些？ 结构化分析的主要内容包括四个部分：1.一套分层的数据流图；2.一本数据字典；3.一组加工说明；4.补充材料。 31.名词解释：结构化分析。 结构化分析方法是一个简单明了、使用很广的系统分析的方法，其基本思想可以概括为一句话：“自顶向下，由粗到细，逐步求精”，或称为“自顶向下，逐层分解”。 32.简述结构化设计的主要特点。 1.相对独立、功能单一的模块结构；2.块内联系大，块间联系小；3.采用模块结构图的描述方法。 33.简述结构化设计方法中，系统设计方法的主要特点。 结构化设计方法是使用最广的一种系统设计方法，其主要特点是：1.模块结构相对独立、功能单一；2.块内联系大、块间联系小；3.采用模块结构图的描述方法。 原型化开发方法 1.作为一种有效的方法，原型法一般可用来确定（C） A 系统目标 B 系统功能 C 用户需求 D 用户偏好 2.系统开发人员在获取用户需求后，不抛弃原型系统，而是根据用户意见，反复修改和扩充原型系统，直至用户满意，此种原型法数据（C） A 探索型 B 实验型 C 演化型 D 混合型 3.信息系统开发的原型法中的“原型”是指（C） A 原油系统的类型 B 原有系统部分功能的模块组合 C 新系统部分功能的早期版本 D 新系统部分功能的系统设计 4.探索型原型法适用的系统开发场合是（D） A 系统规模较大 B 需求变动频繁 C 目标明确，但系统方案不确定 D 目标含糊，系统特性不确定 5.应用原型法时，快速开发出原型系统后，要求用户旨先对此系统进行（C） A 分析 B 设计 C 试用 D 审核 6.实现型原型法的适用场合是（A） A 系统规模大，开发方案不确定 B 需求变动较大 C 系统开发目标含糊 D 系统特性不明确 7.原型法的第一步是获取一组（B） A 用户的基本目标 B 用户的基本需求 C 系统的基本功能 D 系统的基本结构 8.下列选项中，不属于原型化方法的分类方式是（A） A 研究型 B 探索型 C 演化型 D 实验型 9.在管理信息系统的开发方法中，采用启发式方法，最终提出明确的需求的方式（A） A 原型法 B 面向对象 C 结构化 D 非结构化 10.原型法采用启发式方法，引导用户逐渐加深对系统的理解，最终提出明确的需求。 11.主要应用于系统规模大、开发方案不确定情况下的系统开发的原型法属于实验型原型法。 12.系统开发中，用原型来表示系统早期运行版本。 13.原型化开发方法又称为快速原型法。 14.在原型化方法中，针对开发目标模糊、人员缺乏经验的情况探索型方式。 15.原型化方法的运用方式中，实验型用于验证方案是否合适，规格说明是否可靠。 16.原型化方法中，演化型方式认为信息系统本质上就是不断演化的。 17.原型化方法根据系统开发的目的和策略可以划分为探索型、实验型和演化型等三类。 18.系统开发人员在获取用户需求后，不抛弃原型系统，而是根据用户意见，反复修改和扩充原型系统，直至用户满意。此种原型法属于演化型。 19.在管理信息系统开发中，人们一般将能反映新系统的部分重要功能和特征的早期可运行版本称为原型。 20.原型化方法的分类包括：探索型、实验型和演化型。 21.简述原型法的主要步骤。 原型法要求在获得一组基本的用户需求后，快速地开发出新系统的一个原型，用户、开发者及其他有关人员在试用原型的过程中，加强通信和反馈，通过反复评价和反复修改原型系统，逐步确定各种需求的细节，适应需求的变化，从而最终提高系统的质量。因此，原型化方法是一种确定用户需求的有效方法，它采用“启发式”方法，引导用户逐渐地加深对系统的理解，最终提出明确的需求。 22.简述原型化方法的3种类型。 原型化方法可表现为不同的运用方式，一般可分为三类：1.探索型：主要是针对开发目标模糊、用户和开发人员对项目都缺饭经验的情况；2.实验型：用于大规模开发和实现之前考核、验证方案是否合适，规格说明是否可靠；3.演化型：该方法认为信息系统本质上就是不断演化的，重点关注问题是如何才能使信息系统适应不可避免的变化。 23.名词解释：原型化开发方法。 原型化开发方法要求在获得一组基本用户需求后，快速地开发出新系统的一个原型，用户、开发者及其他有关人员在试用原型的过程中，加强通信和反馈，通过反复评价和反复修改原型系统，逐步确定各种需求的细节，适应需求的变化，从而最终提高系统的质量。因此，原型化方法是一种确定用户需求的有效方法。 面向对象开发方法 1.面向对象方法中，继承是下列何种要素间数据与方法的机制（D） A 对象与对象 B 类与类 C 对象与类 D 父类与子类 2.在面向对象方法中，下列说法正确的是（D） A 对象是类的抽象，子类是对象的实例 B 类是对象的抽象，超类是子类的实例 C 对象是类的实例，子类是超类的实例 D 类是对象的抽象，超类是子类的抽象 3.面向对象中，父类与子类共享数据与方法的机制是（C） A 抽象 B 实例化 C 继承 D 多态性 4.关于面向对象开发方法，（A） A 方法 B 消息 C 对象 D 封装 5.（A）与多态性 增加了管理信息系统的灵活性、可读性，提高了系统效率。 A 重载 B 对象 C 封装 D 继承 6.在面向对象方法中，对象是指（D） A 相互关联的数据集 B 相互关联的程序模块 C 相互关联的操作 D 相互关联的实体 7.面向对象方法中对象的四个组成部分是（A） A 对象标识、操作集合、数据结构、消息接口 B 对象标识、对象属性、对象功能、对象数据 C 对象标识、对象数据、对象流程、对象功能 D 对象标示、对象数据流、信息处理、消息接口 8.下面不属于面向对象理论的主要概念的是（D） A 对象 B 类和实例 C 消息和方法 D 数据字典 9.对象的封装机制是指对象是若干要素的集成体，这些要素包括（A） A 数据与方法 B 消息与方法 C 多种操作 D 多种数据 10.以下关于面向对象方法中对象和类的说法正确的是（B） A “北京大学”是一个类 B “大学” 是一个类 C “大学”和“北京大学”之间是继承的关系 D “大学”和“北京大学”之间没有关系 11.提供与人类在解决问题的思维方式相容能力，体现了面向对象开发方法中的（B） A 封装 B 多态性 C 实例 D 重载 12.在面向对象开发方法中，信息隐蔽技术指的是（C） A 对象 B 类 C 封装 D 继承 13.面向对象方法中，对象的四个组成部分是对象标识、操作集合、数据结构、消息接口。 14.面向对象方法中，对象是数据结构以及作用于此结构上的数据操作的封装体。 15.在面向对象方法中，对象是面向对象理论的基石。 16.面向对象方法中，继承是子类自动共享父类中的方法和数据的机制。 17.面向对象方法主要包括面向对象的分析方法、面向对象的设计方法和面向对象的编程方法。 18.对象由对象的标识、对象中的操作集合、对象的数据结构*，及对象对外消息接口（MS）四个部分。 19.面向对象方法中，对象具有封装和能动两种特性。 20.面向对象开发方法中，消息建立了通信间的桥梁。 21.继承是父类与子类共享数据与方法的机制。 22.面向对象方法中，使对象称为数据与方法集成体的机制是封装机制。 23.在面向对象方法中，对象是指相互关联的实体。 24.在面向对象方法中，类是对象的抽象。 25.面向对象方法中，对象具有“封装”与“能动”两种特性。 26.类是对象的抽象描述。 27.在面向对象方法中，类是对象的抽象描述。 28.简述面向对象方法中的定义及其组成部分。 对象是指一些相互关联的实体，它是面向对象理论的基石，由对象的标示、对象中的操作集合、对象的数据结构、及对象对外消息接口四部分组成，具有封装和能动的两种特性。 29.简述面向对象方法中的封装和继承。 1.在面向对象方法中，封装是一种信息的隐蔽技术。封装机制使对象成为数据与方法的集合体，外部视图显示的只是对象封装界面上的信息。封装使对象内部模块的改变不会对其他对象产生致命的后果，这在模型体系集成时非常有用；2.继承是子类自动共享父类中的方法和数据的机制，描述了人类由一般到特殊、自顶向下的演绎能力。它是管理信息系统的体系结构具有开放性。对于父类，继承意味着“遗传”，对于子类继承又意味着“变异”。 30.面向对象方法中，对象的组成部分。 在面向对象开发方法中，对象是一些相互关联的实体，它是面向对象理论的基石，由对象的标识、对象中的操作集合、对象的数据结构，及对象对外消息接口（MS）四部分组成。 31.名词解释：实例化过程。 类是创建对象的蓝图，从某种意义上讲，对象是类的实例，由类到对象的过程称为实例化过程。 32.名词解释：面向对象方法。 面向对象方法是一种基于面向对象理念的系统开发方法。它将面向对象的思想应用于软件开发过程中，指导开发活动。 33.简述面向对象方法中对象的组成部分。 在面向对象方法中，对象由以下4部分组成：对象的标识、对象中的操作集合、对象的数据结构、对象对外的消息接口。 "},"pages/信息系统开发与管理/第三章_系统开发方法概述/第三节_结构化开发方法的开发过程.html":{"url":"pages/信息系统开发与管理/第三章_系统开发方法概述/第三节_结构化开发方法的开发过程.html","title":"第三节：结构化开发方法的开发过程","keywords":"","body":"第三节：结构化开发方法的开发过程 系统开发阶段划分 1.从系统开发的阶段划分来看，系统分析的下一阶段是（B） A 系统规划 B 系统设计 C 系统实施 D 运行维护 2.按结构化方法，MIS的开发分阶段进行，其中相互衔接的两个阶段是（A） A 总体规划、系统设计 B 系统分析、系统评价 C 运行维护、系统评价 D 系统实施、系统评价 3.按结构化方法，系统实施的下一阶段是（A） A 运行维护 B 系统评价 C 系统设计 D 系统分析 4.系统分析的下一阶段是（B） A 系统规划 B 系统设计 C 可行性分析 D 初步调查 5.按结构化方法，MIS的方法分阶段进行，其中相互衔接的两个阶段是（C） A 系统规划、系统设计 B 系统分析、系统评价 C 系统实施、系统维护 D 系统实施、系统评价 6.从系统开发的阶段划分来看，系统分析的前一阶段是（A） A 系统规划 B 系统设计 C 系统实施 D 运行维护 7.按结构化方法，运行维护的下一阶段是（D） A 系统分析 B 系统设计 C 系统实施 D 系统评价 8.管理信息系统生命周期中的第一个阶段是总体规划*，也是系统开发过程的第一步。 9.从系统开发的阶段划分来看，系统分析是系统总体规划工作的继续，是系统设计工作的前导。 10.按结构化方法，管理信息系统开发通常包括总体规划、系统分析、系统设计、系统实施、运行维护和系统评价。 11.按结构化方法，管理信息系统开发通常包括总体规划、系统分析、系统设计、系统实施、系统运行维护和系统评价6个主要阶段。 12.任何一个系统都有其产生、发展和灭亡的生命历程，被称作系统的生命周期。 13.按结构化方法，管理信息系统的开发阶段进行，其系统实施、运行维护两个阶段依先后顺序相互衔接。 14.总体规划是管理信息系统生命周期的第一个阶段，是MIS的概念形成时期。 15.简述管理信息系统的生命周期。 系统的生命周期：出现需求，提出新系统方案，系统产生、成长，系统成熟，系统衰退，系统废弃。 16，名词解释：“瀑布”模型。 按结构化方法，管理信息系统通常包括总体规划、系统分析、系统设计、系统实施、运行维护、系统评价6个主要阶段，各阶段像瀑布流水一样联为一体，称为“瀑布”模型。 17.按结构化方法，系统开发的主要阶段由哪些？ 按结构化方法，管理信息系统开发通常包括总体规划、系统分析、系统设计、系统实施、运行维护和系统评价6个主要阶段。 18.管理信息系统开发的六个阶段分别是什么？ 按结构化方法，管理信息系统开发通常包括总体规划、系统分析、系统设计、系统实施、运行维护、系统评价6个主要阶段。 19.名词解释：系统的生命周期。 任何一个系统都有产生、发展和灭亡的生命历程，被称作系统的生命周期。 各阶段主要内容 1.详细需求分析习惯上称为系统分析，主要明确未来系统（B） A 是什么 B 干什么 C 如何做 D 何时做 2.在系统实施中，（A）是编写管理信息系统应用软件。 A 系统实现 B 系统测试 C 直接切换 D 平行切换 3.系统实施中，（B）需要验证管理信息系统的全部功能及性能要求。 A 系统实现 B 系统测试 C 系统切换 D 系统维护 4.系统维护工作量在整个生命周期的占比是（A）左右。 70% 60% 50% 40% 5.系统维护的工作内容，不包括（C） A 程序的维护 B 数据的维护 C 文档的维护 D 设备的维护 6.总体设计要划分子系统，确定好结构，画出（A） A 模块结构图 B 业务流程图 C 数据流图 D 实体联系图 7.回答“系统干什么”的阶段是（C） A 总体规划 B 系统设计 C 系统分析 D 运行维护 8.关于系统说明说的作用，错误的是（A） A 描述了新系统的数据模型 B 为用户和开发人员的交流和监督提供基础 C 作为新系统验收和评价的依据 D 作为开发人员进行系统设计和实施的基础 9.系统设计阶段要解决的问题是（D） A 为什么 B 是什么 C 干什么 D 怎么干 10.可以从多方面对系统进行评价，其中一方面是评价系统的（B） A 体验 B 经济 C 使用 D 整体 11.在管理信息系统的生命周期中，总体规划阶段主要明确所开发的系统（A） A 是什么 B 做什么 C 怎么做 D 做的效果 12.总体规划可以概括为（C） A 明确系统“做什么”的问题 B 明确系统“如何做的”的问题 C 明确系统“是什么”的问题 D 明确系统“谁来做”的问题 13.系统分析回答的是系统（B） A “是什么”的问题 B “干什么”的问题 C “怎么干”的问题 D “为什么干”的问题 14.总体规划阶段需要回答的问题是该信息系统（A） A 是什么 B 干什么 C 如何干 D 为什么干 15.回答“系统怎么干”的阶段是（B） A 系统分析 B 系统设计 C 系统实施 D 系统评价 16.系统切换方式不包括（D） A 直接切换 B 间接切换 C 平行切换 D 分段切换 17.体现“系统是神”的阶段是（C） A 系统分析 B 系统设计 C 总体规划 D 系统评价 18.系统实施阶段的主要文档是（D） A 系统说明说 B 系统设计说明说 C 系统分析报告 D 用户操作手册 19.对管理信息系统进行总体规划时，为了使领导对系统的开发与否作出决策，并筹集大量的费用，需要有一个概略的投资方案。（A） A 正确 B 错误 20.明确系统“是什么”是管理信息系统总体规划阶段的目的。 21.系统实施阶段的主要文档是用户操作手册。 22.系统维护工作主要包括：程序的、数据的、代码的和设备的维护。 23.将系统分析说明书转换成具体的计算机实施方案是系统设计说明书。 24.总体规划划分系统的子系统，确定结构，画出模块结构图。 25.回答“系统是什么”的阶段是总体规划。 26.系统分析阶段明确表达成的书面材料是系统说明书。 27.系统评价的一项内容是对系统的功能和性能进行评价。 28.系统维护的工作主要包括程序的、数据的、代码的和设备的维护。 29.系统转换成功，意味着整体开发过程的结束，即可投入正常运行，接着的任务是新系统的正常使用和维护。 30.在系统投入正式运行一段时间后，需要对系统运行后的实际效果进行评价。 31.对管理信息系统进行总体规划时，在实际进行系统分析之前，应拿出一个由说服力的系统可行性说明，对系统的效果作出论证。 32.系统设计可分为总体设计和详细设计两个阶段，最终需要交付系统设计说明书。 33.为了总结管理信息系统的运行效果，必须在适当的时候着手进行系统评价工作。 34.从系统开发的阶段划分来看，系统分析时总体规划工作的继续，是系统设计工作的前导。 35.在系统设计阶段，设计的结果以系统设计说明书的形式提交。 36.从结构化方法开发过程的角度看，管理信息系统建设的第一步是总体规划。 37.MIS总体规划中两个相互衔接的步骤是拟定系统实施方案和可行性分析。 38.系统设计可分为总体设计和详细设计两个阶段。 39.从哪些方面对系统进行评价？ 通常可以从几方面对系统进行评价：1.系统的功能和性能；2.系统的经济效果；3.其他方面的评价：文档是否齐全、清晰、程序量的大小、开发周期的长短。 40.简述系统实施阶段的主要工作。 该阶段的工作主要包括：系统实现、系统测试和系统切换等内容。 41.系统测试的任务。 系统测试的任务是发现系统存在的问题，验证和确认管理信息系统是否满足系统说明书文件的全部功能及性能要求。 42.系统运行维护阶段的主要任务。 系统转化成果，意味着整体开发过程的结束，即可投入正常运行，接着的任务是新系统的正常使用和维护，并撰写运行状况报告，这些就是胸运行维护阶段的主要任务。 43.系统分析阶段的主要任务。 系统分析阶段的主要任务是开发人员同用户一起，通过对当前系统的详细调查和分析充分理解新系统目标，即用户的需求，并将它明确的表达成书面资料-系统说明书。 44.名词解释：系统说明书。 系统分析阶段开发人员同用户一起，通过对当前系统的详细调查和分析充分理解新系统目标，即用户的需求，并将它明确地表达成书面资料-系统说明书。系统说明书在系统开发过程中非常重要，要求该文档完整、一致、精确且简明易懂和易于维护。 45.对管理信息系统进行总体规划，是处于哪些考虑？ 对管理信息系统进行总体规划，是出于以下的考虑：1.管理信息系统是由众多的子系统所组成的，为了他们的组成和关系由初步了解，以便进一步的分析工作，就必须先从总体上提出方案；2.为了使领导对系统的开发与否作出决策，并筹集大量的费用，需要有一个概括的投资方案；3.在实际进行系统分析之前，应拿出一个有说服力的系统可行性说明，对系统的效果作出论证；4.由于财力限制，用户往往需要分歧分批地实施子系统，因而需要实现作出分批开发计划。 46.名词解释：系统分析。 系统分析阶段主要是解决“干什么”的问题。系统分析阶段的主要任务是开发人员同用户一起，通过对当前系统的详细调查和分析充分理解新系统目标，即用户的需求，并将它明确地表达成书面资料-系统说明书。 47.名词解释：系统系统的总体规划。 总体规划时管理信息系统建设的第一步，意在通过初步的、总体的需求分析，回答“系统是什么”的问题，进行可行性论证。 48.系统说明说的三个作用。 1.描述了新系统的逻辑模型，作为开发人员进行系统设计和实施的基础；2.作为用户和开发人员之间的协议和合同，为双方的交流和监督提供基础；3.作为新系统验收和评价的依据。 49.系统说明书作用体现在哪些方面？ 1.描述了新系统的逻辑模型，作为开发人员进行系统设计和实施的基础；2.作为用户和开发人员之间的协议和合同，为双方交流和监督提供基础；3.为新系统验收和评价提供依据。 50.名词解释：系统切换。 系统切换是指以新系统代替当前系统的过程，通常包括人员培训、运行环境准备、数据准备、管理制度和流程准备等工作，一般有直接切换、平行切换和分段切换三种更策略。 51.名词解释：总体规划。 总体规划是管理信息系统生命周期的第一阶段，其目的是明确系统“是什么”的问题，描绘出信息系统的架构，对信息系统提出完整、清晰的要求。 52.名字解释：系统评价。 在系统投入正式运行一段时间后，为了了解新系统是否达到了预期的目标和要求，同时也为了总结开发经验，需要对系统运行后的实际效果进行评价。 53.总体规划阶段主要包括哪些工作？ 1.对当前系统进行初步调查；2.分析和确定系统目标；3.分析子系统的组成以及基本功能；4.拟定系统的实施方案；5.进行系统的可行性研究；6.编写可行性报告。 "},"pages/信息系统开发与管理/第三章_系统开发方法概述/第四节_开发过程组织与管理方法.html":{"url":"pages/信息系统开发与管理/第三章_系统开发方法概述/第四节_开发过程组织与管理方法.html","title":"第四节：开发过程组织与管理方法","keywords":"","body":"第四节：开发过程组织与管理方法 开发过程组织与管理方法 项目管理的主要内容 1.针对每一类开发人员制定其工作过程中的责任、义务、完成任务的质量标准是（C）阶段的内容。 A 计划安排 B 经费管理 C 审计控制 D 质量保证 2.编制出任务完成计划表，作为计划实施监控的基础，指的是项目管理的（B） A 任务划分 B 计划安排 C 经费管理 D 审计控制 3.对系统开发各个阶段进行技术评审属于项目管理的（C） A 审计控制工作 B 风险管理工作 C 质量保证工作 D 计划安排工作 4.项目经理可以运用经济杠杆来控制整个开发工作，指的是（B） A 任务划分 B 经费管理 C 审计控制 D 质量保证 5.管理信息系统开发期间的项目管理有6项主要内容，其中包括（D) A 任务划分、计划安排、总体规划、运行维护 B 经费管理、风险管理、系统实施、系统评价 C 计划安排、经费管理、系统设计、系统评价 D 任务划分、计划安排、风险管理、质量保证 6.风险管理是对已经发生的问题采取事先预防的处理方法（B） A 正确 B 错误 7.在项目管理中，任务负责人有任意支配经费的权利（B） A 正确 B 错误 8.任务划分有利于将任务责任落实到人，便于分清每个人的职责，有利于责、权、利相结合的监督和管理方式。（A） A 正确 B 错误 9.为了保证系统开发工作的顺利进行，需要掌握项目的进展情况，及时处理开发过程中出现的问题，及时修正开发工作中出现的偏差。（A） A 正确 B 错误 10.审计控制按照计划对每项任务进行审计，分析执行任务计划表和经费的变化情况，确定需要调整、变化的部分，并根据任务完成计划表和审计结果。（A） A 正确 B 错误 11.项目管理的重点和难点是质量保证。 12.广义上，项目管理包括系统开发期和系统维护其的管理。 13.项目管理的主要内容中，任务划分是把整个开发工作定义为一组任务的集合。 14.质量保证的一项内容是制定并执行系统测试策略和测试计划。 15.保证系统开发过程及相关技术标准的一致性是质量保证的一项内容。 16.对系统开发各阶段进行技术评审是质量保证的一项内容。 17.风险管理的主要任务是对潜在的问题采取事先预防的处理方法，以便尽可能地提高系统开发的成功和开发进程。 18.通过审计与控制，对于系统开发中出现的变化情况，项目经理要及时与用户和主管部门联系，针对变化情况采取相应的对策。 19.风险管理的主要任务是对潜在的问题采取事先预防的处理方法，以便尽可能地提高系统开发的成功率和开发进程。 20.质量保证是项目管理的重点和按点，它一般分为三个阶段，即事前准备、过程监控、事后评审等。 21.质量保证的一项内容是研究并选用系统开发方法和工具。 22.在总体规划阶段对系统质量提出要求时质量保证的一项内容。 23.在项目管理过程中，项目经理可以运用经济杠杆来控制整个开发工作。 24.关于进行任务划分的目的，其中一个方面是有利于资金的分配。 25.项目管理的计划安排阶段，任务完成计划表要表明任务的开始时间、结束时间，以及任务之间的相互依赖的程度，一次作为计划实施监控的基础。 26.计划安排中的计划制定出来后，可以编制出任务完成计划表。 27.任务完成计划表要表明任务的开始时间、结束时间，以及任务之间的相互依赖程度，以此作为计划实施监控的基础。 28.项目管理的主要内容有多个方面，其中一项内容是任务划分。 29.项目经理可以运用经济杠杆来控制圳个开发工作，经费的有效运作可以起到事半功倍的效果。 30.风险管理通常可以划分为风险识别、风险分析、风险缓和和风险跟踪四个阶段。 31.管理信息系统开发项目管理中质量保证一般分为三个阶段，即事前准备、过程监控、事后评审。 32.信息系统开发过程项目管理的主要内容一般包括任务划分、计划安排、经费管理、审计控制、风险管理、质量保证等方面。 33.管理信息系统开发期间的项目管理有6项主要内容，其中包括任务划分、计划安排、经费管理、审计控制、风险管理、质量保证。 34.风险管理分为风险识别、风险分析、风险缓和和风险跟踪四个阶段。 35.风险管理分为风险识别、风险分析、风险缓和和风险跟踪四个阶段。 36.任务划分的目的。 一方面将整个工作划分成若干个较具体的任务群，有利于任务责任落实到人，便于分清每个人的职责，有利于责、权、利相结合的监督和管理方式，另一方面这样做有利于资金的分配，保证资金的有效控制。 37.项目管理中，开发计划的内容划分为哪些方面？ 开发计划可以划分为计算机软硬件的配置计划、开发计划、测试和评估计划、验收保证计划、系统工程管理计划、培训计划、安装计划等。 38.风险管理的主要任务是什么？ 风险管理的主要任务是对潜在的问题采取事先预防的处理方法，以便尽可能地提供系统开发的成功率和开发进程。 39.项目管理中，质量保证分为哪几个阶段？ 在管理信息系统开发项目管理中，质量保证一般分为三个阶段，即事前准备、过程监控、事后评审等。 40.项目管理中，质量保证包括哪些内容？ 质量保证通常包括以下内容：1.在总体规划阶段对系统质量提出要求，并在系统分析阶段进行细化和完善；2.研究并选用系统开发方法和工具；3.对系统开发各阶段进行技术评审；4.制定并执行系统测试策略和测试计划；5.按要求形成系统文档并对文档的改变进行控制；6.保证系统开发过程及相关技术标准的一致性；7.建立系统质量的度量模型和相应的机制；8.对各种质量保证进行记录，并形成报告。 41.简述项目管理中经费管理的主要内容。 经费管理在整个开发项目管理中处于重要的地位。项目经理可以运用经济杠杆来控制整个开发工作。经费的有效运用可以起到事半功倍的效果，繁殖，也许化了很多钱，开发工作却毫无进展。在项目管理中，赋予任务责任人一定职责的同时，还要赋予他一定的经费支配权，同时要对其进行适当的控制。所以任务责任人不可以随意支配经费，是需要收到一定的约束的。 42.简述项目管理中审计控制的内容。 审计控制是按照所采用的开发方法，针对每一类开发人员制定出其工作过程的责任、义务、完成任务的质量标准等，按照计划对每项任务进行审计，分析执行任务计划表和经费的变化情况，确定需要调整、变化的部分，并根据任务完成计划表和审计结果，掌握项目进展情况，及时处理开发过程中出现的问题，及时修正开发工作中出现的偏差，保证系统开发工作的顺利进行。 43.项目管理中，任务完成计划表需要表明哪些内容？ 计划制定出来后，可以编制出任务完成计划表，表明任务的开始时间、结束时间，以及任务之间的依赖程度，以此作为计划实施监控的基础。 44.名词解释：任务划分。 任务划分是把整个开发工作定义为一组任务的集合，这组任务又可以进一步划分若干子任务，进而形成具有层次结构的任务群。 45.简述系统开发过程项目管理的主要内容。 管理信息系统开发过程项目管理的主要内容，一般包括任务划分、计划安排、经费管理、审计控制、风向管理和质量保证等几个方面的内容。 46.风向管理的四个阶段。 风险管理通常可以划分为风险识别、风险分析、风险缓和和风险跟踪四个阶段。 项目管理组的组成 1.作为系统开发的核心人物之一、起着用户和系统开发其他人员之间的桥梁与接口作用的系统开发项目组成员（B） A 程序员 B 系统分析员 C 系统设计院 D 用户代表 2.下列人员中，负责制定网络配置方案的（B) A 系统分析员 B 硬件网络设计员 C 数据库管理员 D 程序员 3.系统开发的核心人物是（C） A 项目组长 B 用户 C 系统分析员 D 程序员 4.项目小组中，（A）负责系统的总体设计、模块设计以及模块之间的接口设计工作。 A 系统设计员 B 数据库管理员 C 系统分析员 D 程序员 5.项目管理过程中，（A）负责依据系统逻辑方案进行系统的程序设计，实现方案中的各项功能。 A 程序员 B 数据库管理员 C 系统设计员 D 硬件网络设计员 6.在开发中，要做好（C）的培训工作，提供他们的参与意识及使用新系统的积极性。 A 项目组长 B 程序员 C 用户 D 系统分析员 7.在信息系统开发中，信息系统建设的参与者和最终使用者是（A） A 用户 B 项目组长 C 程序员 D 系统分析员 8.项目小组中，整个项目的领导是（D） A 系统分析员 B 硬件网络设计员 C 程序员 D 项目组长 9.项目小组的组成不包括（D） A 硬件网络设计员 B 数据库管理员 C 系统设计员 D 业务员 10.项目管理小组中，负责监督和控制数据库的运行以及相关改进工作的人是（B） A 硬件网络设计员 B 数据库管理员 C 系统设计员 D 程序员 11.在信息系统开发中，用户是信息系统建设的参与者和最终使用者，他们懂得具体的管理需求和信息需求。（A） A 正确 B 错误 12.一个成功的系统分析员必须真正把人的因素同计算机解决问题中用到的方法和规程结合在一起。（A） A 正确 B 错误 13.项目小组中，程序员需要负责进行用户的使用培训工作。（A） A 正确 B 错误 14.项目组长应是具备管理知识的专业人才（B） A 正确 B 错误 15.项目管理小组中，负责依据新系统逻辑方案制定硬件网络配置方案的是系统设计员。（B） A 正确 B 错误 16.下列选项中，数据项目小组成员的是（ABCDE） A 项目组长 B 用户 C 系统分析员 D 硬件网络设计员 E 数据库管理员 17.项目管理小组中，程序员负责系统的测试和试运行工作。 18.管理小组中，硬件网络设计员负责依据新系统逻辑方案制定硬件网络配置方案。 19.管理小组中，数据库管理员负责监控和控制数据库的运行以及相关改进工作。 20.管理小组中，系统设计员负责进行系统的总体设计以及模块之间的接口设计工作。 21.在项目管理小组中，信息系统建设的参与者和最终使用者是用户。 22.项目负责人也称为项目组长。 23.系统开发必须在项目小组中各类人员的共同努力下才能完成，因此要做好这些开发人员的组织协调工作。 24.在项目管理过程中，程序员负责进行用户的使用培训工作以及系统的测试和试运行。 25.系统开发中，组织与协调工作可以通过一个项目领导小组来实现。 26.在项目管理过程中，负责系统的程序设计、负责进行用户的使用培训的人员是程序员。 27.项目管理过程中，系统设计员负责系统的总体设计、模块设计以及模块之间接口的设计工作。 28.管理小组中，数据库管理员负责依据系统逻辑方案中提出的数据需求进行数据库设计、定义和存储工作。 29.硬件网络设计员负责制定硬件网络配置方案。 30.硬件网络设计员负责依据新系统逻辑方案中提出的对硬件网络的基本要求制定硬件网络配置方案。 31.管理小组中，数据库管理员负责依据系统逻辑方案中提出的数据需求进行数据库设计、定义和存储工作。 32.系统分析员在总体规划和系统分析阶段，需要大量的人际关系方面的经验。 33.系统开发的核心人物是系统分析员。 34.在信息系统开发的过程中，要做好广大用户的培训工作，提高他们的参与意识及使用新系统的积极性。 35.在信息系统开发中，用户既要负责地提出系统需求，又要及时纠正系统开发中的偏差。 36.在信息系统开发中，用户懂得具体的管理需求和信息需求。 37.项目组长在实施项目领导工作时，要时刻注意所开发的系统是否符合最初制定的目标。 38.项目组长在实施项目领导工作时，要时刻注意所开发的系统是否符合最初制定的目标；在开发工作中是否运用了预先选择的开发方法；哪些人适合做哪些工作等一系列总体性的管理工作。 39.项目小组中，整个项目的领导者时项目组长。 40.项目组长应是一个具有系统开发和管理知识的复合型人才。 41.项目小组的组成包括：项目组长、用户、系统分析员、硬件网络设计员、数据库管理员、系统设计员和程序员。 42.系统分析员在总体规划和系统分析阶段，需要大量的人际关系方面的经验，以便与用户一起工作，确定需求和把这些需求转变为逻辑方案。 43.项目组长在实施项目领导工作时，需考虑开发工作是否运用了预先选择的开发方法。 44.项目组长应是一个具有系统开发和管理知识的复合型人才。 45.项目小组的组成主要包括：项目组长、用户、系统分析员、硬件网络设计员、数据库管理员、系统设计员和程序员。 46.在信息系统开发过程中，项目管理组的组成成员和组织形式一般来说可以根据项目经费的多少和系统的大小来确定。 47.项目小组组成主要包括：项目组长、用户、系统分析员、硬件网络设计员、数据库管理员、系统设计员和程序员。 48.在系统开发中，系统分析员自身需要具备哪些条件？ 系统分析员必须具备相对广博的知识，既要懂得计算机知识，又要具备管理知识和管理才干。系统分析员在系统开发中担任着重要的角色，其工作的好坏直接影响系统开发的质量。 49.项目组长的任务是什么？ 项目组长的任务是保证整个开发项目的顺利进行，负责协调开发人员之间、用户之间、开发人员和用户之间的关系，同时拥有资金的支配权，可以把资金作为强有力的工具来进行项目管理。 50.项目组长在实施项目领导工作时要注意哪些方面？ 项目组长在实施项目领导工作时，要时刻注意所开发的系统是否符合最初制定的目标；在开发工作中是否运用了预先选择的开发方法；哪些人适合哪些工作等一系列总体性的管理工作。 51.简述MIS开发项目小组的组成。 项目小组的组成主要包括以下方面：项目组长、系统分析员、硬件网络设计员、数据库管理员、系统设计员、程序员。 文档的管理 1.系统开发建设过程一旦需要对某个文件进行修改时，要及时、准确地修改与之相关联的文档，指的是（B） A 文档要标准化和规范化 B 维护文档的一致性 C 维护文档的可追踪性 D 文档管理的制度化 2.(B)是系统开发建设的生命线 A 用户 B 文档 C 系统软件 D 网络 3.文档管理应遵循的一个重要原则是（A） A 文档标准化 B 文档实用化 C 文档简化 D 文档优化 4.文档的管理遵循的原则，不包括（D） A 文档要标准化 B 维护文档的一致性 C 维护文档的可追踪性 D 维护文档的共享性 5.在系统开发前必须首先选择或制定文档标准，体现的原则是（A） A 文档要标准化和规范化 B 维护文档的一致性 C 维护文档的可追踪性 D 文档管理制度化 6.系统开发建设过程是一个比较稳定的静态过程。（B） A 正确 B 错误 7.文档要标准化和规范化是文档管理应遵循的一个原则，指的是：在系统开发前必须首先选择或制定文档标准，在统一标准的制约下，开发人员负责建立所建立承担任务的文档资料。（A） A 正确 B 错误 8.没有良好的用户需求文档，系统分析和设计就失去了可靠的依据，开发过程也就不可能以有次序、可管理的方式进行。（A） A 正确 B 错误 9.系统开发建设过程一旦需要对某个文档进行修改时，要及时、准确地修改与之相关联的文档，而这一过程又必须有相应的制度来保证。 10.在系统开发前必须首先选择或制定文档标准，在统一标准的制约下，开发人员负责建立所承担任务的文档资料。 11.文档管理应遵循的一个原则是文档要标准化和规范化，指的是：在系统开发前必须首先选择或制定文档标准，在统一标准的制约下，开发人员负责建立所承担任务的文档资料。 12.维护文档的一致性是文档管理应遵循的一条原则。 13.系统开发建设过程是一个不断变化的动态过程，一旦需要对某个文档进行修改时，要及时、准确地修改与之相关联的文档。 14.系统开发建设的生命线时文档。 15.没有良好的用户需求文档，系统分析和设计就失去了可靠的依据。 16.没有良好的用户需求文档，系统分析和设计就失去了可靠的依据，开发过程也就不可能以有次序、可管理的方式进行，从而造成严重的低效率甚至失败，同时也会给系统的运行和维护工作带来困难，降低管理信息系统的使用生命周期。 17.文档要标准化和规范化时文档管理应遵循的一个原则。 18.简述在管理信息系统建设的各个阶段都需要有相应文档资料的意义。 文档是系统开发建设的生命线，它贯穿于管理信息系统开发的真个过程，系统开发的各个层次和阶段都要有相应的文档。没有良好的用户需求文档，系统分析和设计就失去了可靠的依据，开发过程也就不可能以有次序、可管理的方式进行，从而造成严重的低效率甚至失败，同时也会给系统的运行和维护工作带来困难，降低了管理信息系统的使用生命周期。因此在管理信息系统建设的各个阶段都需要有相应的文档资料。 "},"pages/信息系统开发与管理/第四章_总体规划/第一节_总体规划的目的和步骤.html":{"url":"pages/信息系统开发与管理/第四章_总体规划/第一节_总体规划的目的和步骤.html","title":"第一节：总体规划的目的和步骤","keywords":"","body":"第一节：总体规划的目的和步骤 总体规划的目的 1.管理信息系统总体规划的目的是要明确系统（A） A 是什么 B 做什么 C 怎么做 D 谁来做 2.总体规划的目的不包括（D） A 协调子系统间的工作 B 保证信息共享 C 使开发工作有序进行 D 明确系统实现所需软硬件 3.总体规划的目的不包括（A） A 保证信息的准确 B 保证信息共享 C 协调子系统间的工作 D 是开发工作有序进行 4.下述不属于管理信息系统总体规划目的是（A） A 确定具体需求 B 保证信息共享 C 协调子系统间的工作 D 使开发工作有序进行 5.明确未来系统“是什么“是管理信息系统总体规划的目的。 6.简述管理信息系统总体规划的目的。 管理信息系统总体规划的目的是明确系统“是什么”的问题，也就是进行顶层设计，描绘信息系统架构，并对目标系统提出完整、准确、清晰、具体的要求。保证信息共享、协调子系统间的工作，使开发有序进行。 总体规划的主要步骤 1.总体规划的内容中不应包括（C） A 新系统的目标 B 新系统的子系统划分 C 新系统的技术设计 D 新系统的投资方案 2.总体规划中的需求初步调查内容至少包括（D） A 新系统的技术方案 B 新系统的运行管理方案 C 新系统的人员培训计划 D 新系统的开发条件 3.总体规划中，新系统目标具备的重要特性不包括（D） A 目标的多重性 B 目标的依附性 C 目标的适应性 D 目标的短期性 4.估算新系统开发费用以及新系统的效益体现了可行性分析中的（A） A 经济可行性 B 管理可行性 C 开发环境可行性 D 技术可行性 5.可行性分析报告中，项目摘要属于（C） A 现行系统调查与分析 B 揭露 C 引言 D 可行性论证 6.概括地阐述可行还是不可行指的是可行性分析报告中的（A） A 结论 B 引言 C 新系统假设方案 D 可行性论证 7.技术可行性的重点之一是（C） A 领导人员的文化素质与技术水平 B 管理人员的文化素质与技术水平 C 业务人员的文化素质与技术水平 D 财务人员的文化素质与技术水平 8.企业能否抽调必要的骨干力量参加系统开发，在可行性分析中属于（D） A 技术可行性 B 经济可行性 C 管理可行性 D 开发环境可行性 9.当前管理体制下企业是否提供新系统开发必须的基础数据，在可行性分析中数据（C） A 技术可行性 B 经济可行性 C 管理可行性 D 开发环境可行性 10.经济可行性分析的重点是新系统的（D） A 直接效益与间接效益 B 管理效益与经济效益 C 开发费用与运行费用 D 开发运行费用与效益 11.总体规划中，需求初步调查的下一个步骤是（A） A 确定系统建设目标 B 编制可行性分析报告 C 可行性分析 D 确定子系统基本功能 12.总体规划中两个相互衔接的步骤是可行性分析和（D） A 需求初步调查 B 确定系统建设目标 C 确定子系统基本功能 D 编制可行性分析报告 13.企业最搞层领导考虑替代现有系统的需求是否迫切体现了（C） A 开发环境可行性 B 技术可行性 C 管理可行性 D 经济可行性 14.在总体规划中，下述不属于需求初步调查的是（D） A 现行系统的目标和任务 B 现行系统的环境和约束条件 C 现行系统的业务流程 D 业务和数据的具体化情况 15.可行性分析报告的内容不包括（A） A 旧的参考方案 B 可行性论证 C 新系统建设方案 D 现行系统调查与分析 16.企业能否抽调必要的骨干力量参加系统开发，在可行性分析中属于开发环境的可行性。 17.经济可行性分析的重点是新系统的开发运行费用与效益。 18.MIS总体规划的步骤一线后顺序包括需求初步调查、确定系统建设目标、初步确定子系统组成与基本功能等等。 19.系统开发过程的第一个正式文档是可行性分析报告。 20.当前管理体制下企业是否能提供新系统开发必须的基础数据，在可行性分析中属于管理的可行性。 21.可行性中的可能性是指系统开发的条件是否具备，必要性是指是否需要开发新系统。 22.可行性分析主要论证系统开发的技术可行性、经济可行性、管理可行性和开发环境可行性。 23.可行性分析中的可行性通常包括可能性与必要性两个方面。 24.管理信息系统开发的可行性研究通常包括技术上的可行性、经济上的可行性、管理上的可行性和开发环境的可行性。 25.可行性分析包括技术上的可行性、经济上的可行性、管理上的可行性和开发环境的可行性等4个方面。 26.信息系统开发的可行性分析一般包括技术可行性分析、经济可行性分析、开发环境可行性分析和管理可行性分析。 27.明确新系统的开发条件是总体规划中需求初步调查这一步骤的内容之一。 28.可行性分析中的可行性通常包括可能性与必要性两个方面。 29.简述信息系统总体规划中可行性分析的四个方面的内容。 可行性分析主要内容：1.技术上的可行性。根据新系统的目标来考虑系统的软硬件设备、计算机联网能力、网络及数据安全保护措施、输入输出设备、大容量存储设备；2.经济上的可能性。估算新系统开发和运行所需的费用，以及新系统的效益；3.管理上的可行性。主要考虑管理体制是否可以提供所必须数据，需求是否迫切，以及业务人员对新系统的适应能力；4.开发环境的可行性。考虑企业能否为新系统开发建设提供长期、良好的环境。 30.名词解释：经济可行性。 经济可行性是可行性分析的一个方面。新系统的经济可行性分析的含义是，估算新系统开发和运行所需的费用，以及新系统的效益，将投资和效益进行比较，说明在经济上是合算的。 31.简述系统规划中新系统目标应具备的重要特性。 在制订具体的新系统目标，应考虑使目标具备以下重要特性：1.目标的总体战略性；2.目标的多重性；3.目标的依附性；4.目标的适应性；5.目标的长期性。 32.名词解释：技术可行性 技术上的可行性是根据新系统的目标来考虑系统的软硬件涉别、计算机联网能力、网络及数据安全保护措施、输入输出设备、大容量存储设备等。技术力量不仅考虑技术人员的数量，更应考虑他们的经验和水平。 33.简述总体规划中可行性分析报告的主要内容。 可行性分析报告是系统开发过程中的第一个正式文档，目前尚没有统一的编写格式，报告的内容通常由几部分组成：1.引言。主要包括项目摘要、开发背景、参考资料和专门术语等内容；2.现行系统调查与分析；3.新系统建设方案；4.可行性论证；5.结论。 34.简述总体规划中需求初步调查的主要内容。 1.现行系统的目标和任务；2.现行系统概况；3.现行系统的环境和约束条件；4.现行系统的业务流程和子系统的划分；5.新系统的开发条件。 35.简述总体规划的主要步骤。 1.需求初步调查；2.确定系统建设目标；3.初步确定子系统组成与基本功能；4.拟定系统实现方案；5.可行性分析；6.编制可行性分析报告。 36.名词解释：管理可行性。 考虑当前系统的管理体制是否有条件提供新系统所必须的各种数据，企业最高层领导及各级管理人员对开发建设一个新系统来替代现有系统的需求是否迫切，即新系统的必要性。此外，对新系统运行后将对各方面产生的影响也应加以考虑。另外还应考虑当前系统的业务人员对新系统的适应能力。 37.信息系统总体规划的主要步骤有哪些？ 1.需求初步调查；2.确定系统建设目标；3.初步确定子系统组成与基本功能；4.拟定系统实施方案；5.可行性分析；6.编制可行性分析报告。 38.名词解释：开发环境的可行性。 开发性的可行性是可行性分析的一个方面。企业领导意见是否一致，有无资金，能否抽出骨干力量参加新系统开发等，简单地说就是企业能为新系统的开发建设提供一个长期的、良好的环境。 39.名词解释：管理可行性。 管理上的可行性是指考虑当前系统的管理体制是否有条件提供新系统所必须的各种数据，企业最高层领导及各级管理人员对开发建设一个新系统来替代现有系统的需求是否迫切，即新系统的必要性。 40.总体规划中，初步调查的主要内容。 1.现行系统的目标和任务；2.现行系统概况；3.现行系统的环境和约束条件；4.现行系统的业务流程和子系统的划分；5.新系统的开发条件。 "},"pages/信息系统开发与管理/第四章_总体规划/第二节_系统规划的主要步骤.html":{"url":"pages/信息系统开发与管理/第四章_总体规划/第二节_系统规划的主要步骤.html","title":"第二节：系统规划的主要步骤","keywords":"","body":"第二节：系统规划的主要步骤 企业系统规划法 1.BSP法可用于管理信息系统的（A） A 规划 B 分析 C 实施 D 维护 2.BSP法可用于管理信息系统的总体规划。 3.用于管理信息系统总体规划的常见方法有三种，分别是企业系统规划法、关键成功因素法和战略目标集转化法。 方法概述 1.BSP法遵循的一个原则是从方法论到摆脱对（D） A 管理制度的依赖性 B 管理方法的依从性 C 组织文化的依从性 D 组织机构的依从性 2.在总体思路上，BSP法遵循的原则是（D） A 自下而上的分析与自上而下的规划相结合 B 自上而下的分析与自下而上的规划相结合 C 自下而上的规划与自上而下的实施相结合 D 自上而下的规划与自下而上的实施相结合 3.BSP法必须遵循的首要原则是支持（A） A 企业的总目标 B 信息系统的总目标 C 企业过程 D 信息系统总体结构 4.以下属于BSP法工作流程中核心部分的是（C） A 研究项目的确定 B 研究组成员的选择 C 定义企业过程 D 制订开发建议书与开发计划 5.企业系统规划方法（BSP）的实质是（B） A 信息系统战略转化成企业战略 B 企业战略转化成信息系统战略 C 信息系统过程转化成企业过程 D 企业过程转化成信息系统过程 6.下面关于BSP方法的叙述，不正确的是（C) A BSP方法是一种结构化方法 B BSP方法的实质是企业战略转化成信息系统战略 C BSP方法推荐采用自下而上的方式识别信息系统目标和数据 D BSP方法可使信息系统在结构上有良好的整体性 7.BSP法必须遵循的主要原则不包括（D） A 支持企业的总目标 B 面向企业中管理各层次的要求 C 从方法上摆脱信息系统对旧组织结构的依从性 D 必须自上而下规划实施 8.在总体思路上，BSP法遵循的原则是自上而下的规划与自下而上的实施相结合。 9.BSP法工作流程中的核心部分之一是定义企业过程和定义数据类。 10.BSP（企业系统规划）方法工作流程中的核心部分之一是定义企业过程和定义数据类。 11.BSP法实质是企业战略转化信息系统战略的过程。 12.从方法论上摆脱对旧组织机构的依从性是BSP法遵循的一个原则。 13.BSP方法着眼于企业活动过程，从方法论上摆脱了信息系统对旧组织结构的依从性。 14.企业系统规划方法（BSP）工作流程中的核心部分包括了定义企业过程和定义数据类。 15.BSP法必须遵循的首要原则是支持企业的总目标。 16.简述企业系统规划（BSP）法必须遵循的主要原则。 企业系统规划（BSP）法必须遵循的主要原则：1.支持企业的总目标；2.面向企业中管理各层次的要求；3.从方法论上摆脱信息系统对旧组织机构的依从性；4.是信息系统在结构上有良好的整体性；5.自上而下规划和自下而上实施相结合。 17.简述BSP方法工作流核心部分的内容。 BSP方法工作流程的核心部分为1.定义企业过程和数据类；2.分析研究现行系统寻求企业的支持；3.研究管理部门对系统的要求；4.确定新信息系统的体系结构；5.确定新系统的实现优化顺序。 18.简述BSP方法工作流程的核心部分。 1.定义企业过程和数据类；2.分析研究现行系统对企业的支持；3.研究管理部门对系统的要求；4.确定新信息系统的体系结构；5.确定新信息系统的实现优化顺序。 19.名词解释：企业系统规划方法（BSP） 是由美国IBM公司与20世纪70年代创造并逐渐发展起来的一种对管理信息系统进行规划和设计的结构化方法，至今仍广为应用，并在许多领域取得了成功。 方法概述 1.BSP法遵循的一个原则是从方法论上摆脱对（D） A 管理制度的依从性 B 管理方法的依从性 C 组织文化的依从性 D 组织机构的依从性 2.在总体思路上，BSP法遵循的原则是（D） A 自下而上的分析与自上而下的规划相结合 B 自上而下的分析与自下而上的规划相结合 C 自下而上的规划与自上而下的实施相集合 D 自上而下的规划与自下而上的实施相结合 3.BSP法必须遵循的首要原则是支持（A） A 企业的总目标 B 信息系统的总目标 C 企业过程 D 信息系统总体结构 4.以下属于BSP法工作流程中核心部分的是（C) A 研究项目的确定 B 研究组元的选择 C 定义企业过程 D 制订开发建议书与开发计划 5.企业系统规划方法（BSP）的实质是（B） A 信息系统战略转化成企业战略 B 企业战略转化成信息系统战略 C 信息系统过程转化成企业过程 D 企业过程转化成信息系统过程 6.下面关于BSP方法的叙述，不正确的是（C） A BSP方法是一种结构化方法 B BSP方法的实质是企业战略转化成信息系统战略 C BSP方法推荐用自下而上的方式识别信息系统目标和数据 D BSP方法可使信息系统在结构上由良好的整体性 7.BSP法必须遵循的主要原则不包括（D） A 支持企业的总目标 B 面向企业中管理各层次的要求 C 从方法上摆脱信息系统对旧组织机构的依从性 D 必须自上而下规划实施 8.在总体思路上，BSP法遵循的原则是自上而下的规划与自下而上的实施相结合。 9.BSP法工作流程中的核心部分之一是定义企业过程和定义数据类。 10.BSP（企业系统规划）方法工作流程中的核心部分之一是定义企业过程和定义数据类。 11.BSP法的实质是企业战略转化成信息系统战略的过程。 12.从方法论上摆脱对旧组织机构的依从性是BSP法遵循的一个原则。 13.BSP方法着眼于企业活动过程，从方法上摆脱了信息系统对旧组织机构的依从性。 14.企业系统规划方法（BSP）工作流程中的核心部分包括定义企业过程和定义数据类。 15.BSP法必须遵循的首要原则是支持企业的总目标。 16.简述企业系统规划（BSP）法必须遵循的主要原则。 1.支持企业的总目标；2.面向企业中管理各层次的要求；3.从方法上摆脱信息系统对旧组织机构的依从性；4.使信息系统在结构上有良好的整体性；5.自上而下规划和自下而上实施相结合。 17.简述BSP方法工作流程核心部分的内容。 1.定义企业过程和数据类；2.分析研究现行系统寻求企业的支持；3.研究管理部门对系统的要求；4.确定新信息系统的体系结构；5.确定新信息系统的实现优化顺序。 18.简述BSP方法工作流程的核心部分。 1.定义企业过程和数据类；2.分析研究现行系统对企业的支持；3.研究管理部门对系统的要求；4.确定信息系统的体系结构；5.确定新信息系统的实现优化顺序。 19.名词解释：企业系统规划方法（BSP）。 是由美国IBM公司于20世纪70年代创造并逐渐发展起来的一种对管理信息系统进行规划和设计的结构化方法，至今仍广为应用，并在许多领域取得了成功。 工作流程 1.BSP工作中的最后一项流程是（A） A 研究成果报告 B 制定建议书和开发计划 C 确定子系统开发的优先顺序 D 提出判断和结论 2.在BSP工作流程中，分析当前系统支持阶段的分析工具（A） A 举证 B 数据流图 C 流程图 D 实体联系图 3.研究管理部门对系统的要求是（A） A 自上而下 B 自下而上 C 从前往后 D 从后往前 4.关于“研究管理部门对系统的要求”的达到目标，错误的是（D） A 对研究组已得出的结论进行核实 B 弄清企业将来的发展方向、信息需求、主要的障碍和机会 C 确定企业存在的问题 D 提出判断和结论 5.关于BSP的工作流程，说法错误的是（D） A BSP的研究必须在企业最高领导和最高管理部门参与的前提下才能开发 B 研究开始前需要收集好相关基础资料 C 研究的准备活动成果是制订研究计划 D 研究的准备活动下一阶段是定义企业过程 6.BSP法中，定义数据类时最重要的是关联数据类与（B） A 企业目标 B 企业过程 C 企业资源 D 企业组织 7.BSP方法中，企业过程关注的主要内容是（A） A 企业资源转化与资源管理 B 企业信息转化与利用 C 子系统件的信息交换 D 企业部门的信息交换 8.在BSP方法中，“建立过程和数据类矩阵，并为设计出信息系统结构提供基本数据”这一步应发生在（B） A 定义企业过程 B 定义数据类阶段 C 分析当前的系统支持阶段 D 研究管理部门对系统的要求阶段 9.BSP法中，定义数据类时最重要的是要把数据类与企业过程关联起来。 10.企业过程是企业资源转化与资源管理中的有逻辑关系的决策和活动。 11.企业过程是企业资源转化与资源管理中的有逻辑关系的决策和活动。 12.定义数据类时最重要的是要把数据类与企业过程关联起来。 13.名词解释：BSP法中的企业过程。 企业过程是指企业资源转化和资源管理中有逻辑关系的决策和活动。通过企业过程的研究，可以了解企业的功能、任务、信息需求和关联；从而作出关系矩阵，通过关系矩阵可进一步形成系统信息模型，所以说企业过程是企业环境与信息系统之间的界面。 14.名词解释：数据类 在BSP法中，数据类是指支持企业所必需的，在逻辑上互相联系的，并能组成相对独立的完整数据单位的那些数据部分。 15.简述BSP法的工作流程中“研究管理部门对系统的要求”这一工作达到的目的。 “研究管理部门对系统的要求”这一工作应达到的目的：1.对研究组已得出的结论进行核实。重点核实职责、目标、关键成功因子等方面；2.弄清企业将来的发展方向、信息需求、主要的障碍和机会；3.确定企业存在的问题，将他们与企业过程和数据类联系起来进行考虑；4.提出解决这些问题可能的方法、确定潜在的效益。 定义企业过程 1.BSP法中关键性的企业资源是（B） A 计划与控制 B 产品与服务 C 材料与设备 D 资金与资产 2.在BSP方法中，定义企业过程应该从三个方面入手，它们是（D） A 人员、设备、资金 B 产品、服务、管理 C 产品与服务、人员、物资 D 计划与控制、产品与服务、支持性资源 3.资源的生命周期阶段不包括（B） A 需求 B 计划 C 服务 D 退出 4.为实现企业目标必须使用、引用和消耗的资源属于（A） A 支持性资源 B 关键性资源 C 产品与服务 D 计划与控制 5.BSP法中，企业资源的生命周期分为以下四个阶段（C） A 规划、开发、利用、回收 B 需求、生产、消费、回收 C 需求、获取、服务、退出 D 分析、设计、实施、更新 6.BSP方法的核心是（A） A 定义企业过程 B 定义数据类 C 研究管理部门系统的要求 D 设计信息系统总体结构 7.BSP法中企业过程定义的一般步骤不包括（D） A 从计划/控制资源着手 B 识别产品/服务过程 C 支持性资源识别企业过程 D 识别企业过程 8.计划与控制、产品与服务、支持性资源是BSP法定义的企业三项资源。 9.BSP法中，企业资源的生命周期分为需求、获取、服务、退出4个阶段。 10.BSP法中关键性的企业资源是产品与服务。 11.企业过程定义的一个步骤是支持资源识别企业过程。 12.企业资源的生命周期阶段分为需求、获取、服务、退出。 13.在定义企业过程中，生命周期是指一项资源由取得到退出过程所经理的阶段。 14.企业资源中不以具体形式存在的“资源”是企业的计划与控制。 15.企业系统规划（BSP）定义的企业资源中有一类不以具体产品形式存在的资源，就是企业的计划与控制。 16.BSP法定义的企业资源中有一类不以具体产品形式存在的资源，就是计划与控制。 17.企业内的支持性资源。 支持性资源是指为实现企业目标必须使用、引用和消耗的那些资源，如原材料、资金、设备人员等。 18.简述BSP法中企业过程定义（识别）的一般步骤。 1.从计划/控制资源着手。可以识别出企业战略规划与管理控制两个层次的过程。2.识别产品/服务过程。根据其需求、获取、服务、退出4阶段的生命周期，对每一阶段，用一些过程进行管理。3.从支持性资源识别企业过程。其方法与产品/服务识别的过程类似。 19.名词解释：资源的生命周期 资源的生命受气可以用来作为研究企业过程的一种手段。所谓生命周期是指一项资源由取得到退出过程所经理的阶段。一般划分方法可分为四个阶段，各个阶段分别为：需求、获取、服务、退出。 20.名词解释：BSP方法中的企业资源。 BSP方法中的企业资源包括企业的计划与控制、产品与服务（关键性资源）和支持性资源。 21.简述企业的三种资源。 1.企业内的资源可分为关键性资源和支持性资源，还有一类不以具体产品形式存在的“资源”，那就是企业的计划与控制；2.关键性资源是指企业中产品与服务；3.支持性资源是指为实现企业目标必须使用、引用和消耗的那些资源。 定义数据类 1.BSP法将企业数据分为四种类型，它们是（D） A 事务型、管理控制型、决策型、综合型 B 事物型、统计型、报告型、计划型 C 库存文档型、分析型、统计型、计划型 D 计划型、事物型、统计型、库存文档型 2.企业税后利润属于（C） A 计划型数据 B 事物型数据 C 统计型数据 D 库存文档型数据 3.有一类企业数据能够反映资源生命周期各个阶段过渡过程的相关变化，它属于（B） A 库存文档型数据类 B 事物型数据类 C 计划型数据类 D 统计型数据类 4.反映企业状况并提供某些反馈信息的企业数据属于（D） A 库存文档型数据 B 事物型数据 C 计划型数据 D 统计型数据 5.企业数据的四种类型不包括（D） A 计划型 B 事物型 C 库存文档型 D 数据型 6.企业数据分成4中类型，即事物型、计划型、统计型和库存文档型数据。 7.企业数据的方法有企业实体法和企业过程法。 8.BSP法将企业数据分为计划型、事物型、综合统计型和库存文档型4种类型。 9.BSP法中企业数据分成4中类型，即事物型、计划型、统计型和库存文档型数据。 10.识别企业数据的方法有两种：企业实体法和企业过程法。 11.简述BSP法中按资源生命周期不同阶段需求定义的4中数据类型。 库存文档型、事物型、计划型和综合统计型。 12.名词解释：计划型数据类 计划型数据类是企业数据的一种类型。计划型数据类反映的内容：反映目标、资源、转化过程等计划值；计划型数据类的特点：1.可鞥与多个库存型数据有关；2.各种计划、预测、预算、调度表等。 13.名词解释：识别企业数据的企业过程法。 企业过程法是利用以前识别的企业过程，分析每一个过程利用什么数据，产生什么数据，或者说每个过程的输入和输出数据是什么。主要工具是过程/数据矩阵。 14.名词解释：统计型数据类 反映企业状况，提供某些反馈信息。特点：1.一般来自其他类型数据类的采样；2.历史性、对照行或评价行的参考数据；3.数据的综合性强。 15.名词解释：企业过程法 企业过程法是利用以前识别的企业过程，分析每一个过程利用了什么数据，产生什么数据，或者说每一过程的输入和输出数据是什么。 16.名词解释：企业实体法 企业的实体是指顾客、产品、材料及人员等企业中客观存在的东西，联系于每个实体的生命周期阶段就有各种数据，如事物型、计划型和统计型等。 设计系统总体结构与开发顺序 1.确定子系统开发优先顺序的一个基本原则是根据（C） A 子系统的规模大小 B 子系统的复杂程度 C 子系统需求程度于效益的评估 D 系统开发者的经验与偏好 2.U/C矩阵（图）中某单元格的U表示（B） A 单元格对应的企业过程使用对应的数据类 B 单元格对应的企业过程产生对应的数据类 C 单元格对应的数据类使用对应的企业过程 D 单元格对应的数据类生成对应的企业过程 3.总体规划阶段确定子系统划分的基本原则，描述错误的是（D） A 子系统在功能上应有相对的独立性 B 通常子系统可以跨越两个以上的企业过程 C 子系统在数据上应有自身的完整性 D 一般一个数据类只能由一个子系统产生 4.划分子系统采用（A）的方式 A U/C图 B 数据流图 C 业务流程图 D 实体联系图 5.子系统与数据类的正确关系是（B） A 一个子系统职能产生一个数据类 B 一个数据类职能由一个子系统产生 C 一个子系统职能接受一个数据类 D 一个数据类职能书香一个子系统 6.总体规划阶段确定子系统开发优先顺序描述不正确的是（D） A 子系统需求程度与潜在效益的评估 B 进行技术约束分析 C 按确定的评估准则对每个子系统进行评估 D 实际工作中，采用流程图方式进行划分 7.确定子系统开发优先顺序的一个重要原则是根据子系统的需求迫切性和潜在效益高低。 8.子系统与数据类的正确关系是一个数据类由一个子系统产生。 9.简述BSP法中子系统划分的方法 在BSP方法中，划分子系统的过程实际上就是定义信息结构的过程。具体的做法是用U/C图，U表示使用，C表示产生。U/C图的左列是企业过程，最上一行列出数据。如果某过程产生某数据，就在矩阵某行某列中写C。如果某过程使用数据，则在其对应的单元中写U。开始时数据类和过程时随机排列的；U、C在矩阵中排列也是分散的。我们以调换过程和数据类的顺序的方法，尽量使U、C集中在对角线上排列。然后把U、C比较集中的区域用粗线条框起来，这样形成的狂就是一个个子系统。在粗线框外的U表示一个系统用另一个系统的数据，图中用带箭头的线表示。这样就完整了子系统的划分，即确定了信息结构的主流。 10.简述在总体规划阶段确定子系统划分和开发优先顺序的基本原则 子系统划分的基本原则：1.子系统在功能上应有相对的独立性；2.子系统在数据上应有自身的完整性。开发优先顺序的基本原则：1.子系统需求程度与潜在效益的评估；2.技术约束分析。 11.简述采用U/C矩阵划分子系统的步骤 1.建立初步U/C矩阵；2.对初步U/C矩阵进行行列变换；3.用箭头描述子系统间的数据关系。 12.采用U/C矩阵图的方式进行子系统划分，一般包括哪些步骤？ 1.建立初步U/C矩阵，根据调查结果建立企业过程和数据类的对应关系；2.对初步U/C矩阵进行行列变换，形成初步的子系统；3.用箭头描述子系统间的数据关系。 13.名词解释：U/C矩阵 U/C矩阵是用来表达过程与数据两者之间的关系的矩阵。用U表示过程对数据类的使用，用C表示过程对数据类的产生。 "},"pages/信息系统开发与管理/第五章_系统分析/第一节_系统分析概论.html":{"url":"pages/信息系统开发与管理/第五章_系统分析/第一节_系统分析概论.html","title":"第一节：系统分析概论","keywords":"","body":"第一节：系统分析概论 系统分析的目的与难点 1.关于系统分析的对策不包括（D） A 做好用户事前培训工作 B 做好系统开发人员的培训工作 C 选择正确的开发方法和良好的表达工具 D 只需对用户的培训管理信息系统方面的知识 2.确定新系统初步的逻辑模型，属于系统开发的阶段是（B） A 系统规划 B 系统分析 C 系统设计 D 系统维护 3.从系统开发的阶段划分来看，系统设计的前一阶段是（B） A 系统规划 B 系统分析 C 系统运行 D 系统实施 4.以下属于结构方法开发过程的系统分析阶段的工作是（C) A 确定系统目标 B 评价系统经济效果 C 建立系统逻辑模型 D 确定系统模块结构 5.属于系统分析任务的有（A） A 详细调查收集和分析用户需求 B 建立过程数据矩阵 C 设计信息系统总体结构 D 确定系统建设目标 6.系统分析任务不包括（B） A 详细调查收集和分析用户需求 B 进行业务流程分析 C 确定新系统初步的逻辑模型 D 编制系统说明书 7.系统分析的工作成果是系统设计和系统实施的依据。 8.系统分析也称为系统逻辑设计。 9.开展系统分析工作的关键点或称工作要点是理解和表达用户的需求（或需要）。 10.系统分析旨在获得一个合理的新系统的逻辑模型，为后一阶段的系统设计提供详细的开发方案。 11.系统分析的目的是将用户的需求及其解决方案确定下来。 12.系统分析阶段的主要任务包括：详细调查、收集和分析用户需求，确定新系统初步的逻辑模型，编制系统说明说。 13.系统分析阶段的主要任务包括：详细调查收集和分析用户需求，确定新系统初步的逻辑模型，编制系统说明书。 14.系统分析阶段的主要任务：详细调查、收集和分析用户需求，确定新系统初步的逻辑模型，编制系统说明书。 15.名词解释：信息系统的逻辑模型。 逻辑模型是指仅在逻辑上确定的新系统模型，而不涉及具体的物理实现，也就是要解决系统“干什么”，而不是“如何干”。逻辑模型由一组图表工具进行描述。 16.关于系统分析的工作难点与对策，基本对策考虑哪些方面？ 1.做好用户事前培训工作；2.做好系统开发人员的培训工作；3.选择正确的开发方法和良好的表达工具。 17.系统分析阶段的主要任务。 1.详细调查收集和分析用户需求；2.确定新系统初步的逻辑模型；3.编制系统说明书。 系统分析的逻辑与步骤 1.（A）模型反映了系统的性质。 A 逻辑 B 物理 C 虚拟 D 标准 2.产生数据流图的阶段是（D） A 系统规划 B 系统设计 C 系统实施 D 系统分析 3.系统分析阶段的主要工作有（B） A 系统总体设计 B 编写系统设计说明书 C 业务流程分析 D 系统详细设计 4.在结构化方法中，系统分析阶段的主要工作步骤有：详细调查、业务流程分析、数据流程分析和编写系统分析说明书。 5.逻辑模型和物理模型的主要差别是做什么和如何做的差别。 6.系统分析阶段的主要工作步骤有：详细调查、业务流程分析、数据流程分析和编写系统分析说明书。 7.逻辑模型和物理模型的主要差别是做什么和如何做的差别。 8.系统分析阶段的主要工作步骤包括：详细调查、业务流程分析、数据流程分析和编写系统分析说明书。 9.系统分析阶段的主要工作步骤。 在结构化方法中，系统分析阶段的主要工作步骤有：详细调查、业务流程分析、数据流程分析和编写系统分析说明书。 "},"pages/信息系统开发与管理/第五章_系统分析/第二节_详细调查.html":{"url":"pages/信息系统开发与管理/第五章_系统分析/第二节_详细调查.html","title":"第二节：详细调查","keywords":"","body":"第二节：详细调查 详细调查的目的和难点 1.需要搞清楚用户的想法和要求是（A）的目的 A 详细调查 B 业务分析 C 数据分析 D 系统分析 2.系统分析详细调查中，属于难点问题的是（C） A 过多的用户参与 B 用户需求的多样性 C 用户与开发人员的交流困难 D 用户过早参与 3.详细调查阶段工作的主要难点不包括（D） A 没有足够的用户参与 B 用户的需求经常变更 C 用户与开发人员很难进行交流 D 成本太高 4.详细调查的主要难点有：没有足够的用户参与，用户的需求经常变更，用户与开发人员很难进行交流 5.详细调查的目的就是通过一系列的调研活动，尽可能准确、详细地了解用户的需求。 6.简述详细调查的目的。 详细调查的目的就是要搞清楚用户的这些想法和要求，换句话说，就是通过一系列的调研活动，尽可能准确、详细地了解用户需求。管理信息系统开发过程中遇到的需求问题，都是由于调研不清、不细造成的，为了保证系统开发的成果，必须对这一阶段给予充分的时间和高度的重视。 7.简述详细调查的主要难点。 对需求进行详细调查时一项重要的工作，也是最困难的工作。详细调查阶段工作的主要难点有：1.没有足够的用户参与；2.用户的需求经常变更；3.用户与开发人员很难进行交流。 详细调查的主要内容 1.负责公司的经营与日常工作的是（B） A 临高决策层 B 业务管理层 C 业务执行层 D 战略管理层 2.关于数据流程调查的任务，说法错误的是（D） A 收集可以收集的单据、凭证、报表材料 B 搞清材料的出处和相互关系 C 调查清楚数据间的前后关系 D 调查部分数据的出处 3.详细调查中用来描述当前系统结构的层次和隶属关系的图形是（A） A 组织结构图 B 数据流图 C E-R图 D 模块结构图 4.由下属分公司等机构组成，完成日常的生产、业务和调度等工作的是（C） A 战略管理层 B 业务管理层 C 业务执行层 D 战术管理层 5.负责拟定公司中长期发展规划、经营方针、资本经营规划和方案的管理层是（A） A 领导决策层 B 业务执行层 C 业务管理层 D 战术管理层 6.数据流程调查的工作分为的两个步骤是“分析”和（B） A 整理 B 手机 C 设计 D 实施 7.负责执行董事会决议的是业务管理层。 8.详细调查的主要内容包括：组织机构的调查、业务流程的调查、数据流程的调查、薄弱环节的调查和其他信息的调查。 9.详细调查的主要内容有：组织机构的调查、业务流程的调查、数据流程的调查、薄弱环节的调查以及其他信息的调查。 10.详细调查的主要内容有：组织机构的调查、业务流程的调查、数据流程的调查、薄弱环节的调查以及其他信息的调查。 11.数据流程的三个任务。 1.收集可以收集的单据、凭证、报表材料，搞清出处和相互关系；2.必须调查清楚每个数据的出处；3.必须调查清楚数据间的前后关系、运算公式和勾稽关系。 12.详细调查的主要内容。 1.组织机构的调查；2.业务流程的调查；3.数据流程的调查；4.薄弱环节的调查；5.其他信息的调查。 详细调查的方法与原则 1.详细调查的方法与原则有（D） A 自下而上的全面开展 B 只抢到分工而不强调协作 C 存在的一定是合理的 D 事先计划 2.了解现行系统的方法是（A） A 参与业务实战 B 书面调查 C 收集资料 D 开调查会 3.详细调查的基本方法中，最有效的是（A） A 开调查会 B 参与业务实战 C 手机资料 D 书面调查 4.详细调查的原则不包括（D） A 事先计划 B 采访持关键信息的人 C 自顶向下全面展开 D 存在即合理 5.需求调查人员应注意的事项描述有误的是（A） A 选择专业的语言 B 倾听比表达重要 C 及时反映避免误解 D 以学习的态度展开工作 6.用调查表向有关部门和个人征求意见和手机数据的调查方法是（C） A 收集资料 B 开调查会或个别访问 C 书面调查 D 参加业务实践 7.下述关于系统详细调查原则的说法中，错误的是（C） A 采访持关键信息的人 B 自顶向下全面展开 C 存在的就是合理的 D 分工协作相结合 8.了解现行系统最好的方法（D） A 收集资料 B 书面调查 C 个别调查 D 参加业务实践 9.关于详细调查的基本方法，不包括的是（A） A 写分析报告 B 书面调查 C 开调查会或个别访问 D 收集资料 10.详细调查的原则中，是自顶向下全面展开。 11.详细调查的基本方法有：收集资料、开调查会或个别访问、书面调查和参加业务实战。 12.详细调查的基本方法包括：收集资料、开调查会或个别访问、书面调查和参加业务实战。 13.详细调查的基本方法。 1.收集资料；2.开调查会或个别访问；3.书面调查；4.参加业务实战。 14.需求调查人员需要注意哪些原则？ 1.选择默契的语言；2.倾听比表达重要；3.及时翻译避免误解；4.以学习的态度展开工作。 15.详细调查的原则。 1.事先计划；2.采访持关键信息的人；3.自顶向下全面展开；4.存在的不一定是合理的；5.分工和协作相结合；6.主动沟通的工作方式； "},"pages/信息系统开发与管理/第五章_系统分析/第三节_业务流程分析.html":{"url":"pages/信息系统开发与管理/第五章_系统分析/第三节_业务流程分析.html","title":"第三节：业务流程分析","keywords":"","body":"第三节：业务流程分析 业务流程的概念 1.业务流程的特点是（A） A 目标性、逻辑性、完整性 B 目标性、逻辑性、层次性 C 严谨性、可视性、完整性 D 可视性、严谨性、层次性 2.业务流程的特点没有（A） A 双重性 B 目标性 C 逻辑性 D 层次性 3.业务流程的功能反映了活动间的关系。 4.业务流程具有目标性、逻辑性和层次性等特点。 5.业务流程是指一组共同为顾客创造价值而又互相关联的活动。 6.一组共同为顾客创造价值而又相互关联的活动指的是业务流程。 7.业务流程的特点。 业务流程的特点有：目标性，完成将投入转化为产出的特定的任务；逻辑性，组成流程的活动之间具有相互联系、相互作用的方式；层次性，流程复杂，可以由高至低一层一层分解。 8.业务流程的功能。 业务流程的功能：1.实现不同分工活动的结果链接；2.反映活动间的关系。 9.名词解释：流程。 流程由一系列的活动或者实践组成，可以渐变的连续性流程。 10.名词解释：业务流程。 业务流程的概念是指一组共同为顾客创造价值而又相互关联的活动。 业务流程分析的方法 1.属于业务流程分析步骤的有（D） A 编码设计 B 绘制实体 - 联系图 C 界面设计 D 业务流程优化 2.业务流程的主要步骤不包括（B） A 调查企业的组织结构 B 分工协作 C 调查企业的具体业务流程 D 绘制业务流程图 3.业务流程分析主要是为了描述现行系统的（C） A 数据模型 B 逻辑模型 C 物理模型 D 网络模型 4.在业务流程图中，表示业务和数据流程方向通常用（A） A 单箭头 B 圆点 C 矩形 D 横线 5.业务流程中，通常是指参与某项业务的部门或人、表达整个业务流程的起点和终点的符号称为（D） A 业务功能描述 B 数据流 C 业务和数据流动方向 D 外部实体 6.业务流程分析采用的是自顶向下的方法，首先画出高层管理的业务流程图，然后再对每一个功能部分进行分解。 7.业务流程分析主要是为了描述现行系统的物理模型。 8.调查企业的业务流程通常可按原有的信息流动过程，逐个调查当前系统中每个环节的处理任务，处理顺序和对时间的要求等情况，弄清每个环节的信息来源和去向。 9.业务流程图描述的内容包括：外部实体、业务功能描述和业务流程流动方向。 10.业务流程分析的主要步骤。 1.调查企业的组织结构；2.调查企业的具体业务流程；3.绘制业务流程图；4.业务流程优化； 业务流程重组。 1.以下关于业务流程重组的原则，正确的是（C） A 围绕具体任务而非最终结果来试试再造工作 B 不让后续过程的有关人员参与前端过程 C 将信息处理融入产生该信息的实际工作中去 D 资源分散化 2.BPR的核心基本特征不包括（C） A 根本性 B 彻底性 C 统一性 D 流程 3.业务流程重组的原则不包括（B） A 围绕最终结果而非具体任务来实施在造工作 B 让后续过程的有关人员参与后端过程 C 将信息处理融入产生该信息的实际工作中去 D 将平行工序链接起来而不是集成其结果 4.BPR是为了适应以顾客、竞争、变化为特征的现代企业经营环境。 5.BPR是为了适应以顾客、竞争、变化为特征的现代企业经营环境。 6.管理信息系统中，BPR指的是业务流程重组。 7.在BPR的定义中，“根本性”、“彻底性”、“显著改善”和“流程”是四个核心基本特征。 8.企业经营过程重构（BPR）集企业业务流程与管理信息系统开发于一体。 9.BPR的基本特征包括：“根本性”思考、“彻底性”再设计，“显著改善”和“流程”这四个核心基本特征。 10.在BPR的定义中，“根本性”思考、“彻底性”再设计，“显著改善”和“流程”是四个核心基本特征。 11.BPR的四个核心基本特征包括“根本性”、“彻底性”、“显著改善”和“流程”。 12.BPR的基本特征。 在BPR的定义中，“根本性”、“彻底性”、“显著改善”和“流程”是四个核心基本特征。 13.业务流程重组的原则。 业务流程重组的原则主要有：1.围绕最终结果而非具体任务来实施再造工作。2.让后续过程的有关人员参与前端过程。3.将信息处理融入产生该信息的实际工作中去。4.将地域上分散的资源集中化。5.将平行工序连接起来而不是集成其结果。6.决策点下移并将控制融入过程中。7.在源头获取信息。 14.名词解释：业务流程重组。 业务流程重组简称BPR，它是对企业的业务流程作根本性的思考和彻底重建，其目的是成本、质量、服务和速度等方面取得显著的改善，使得企业能最大限度地适应以顾客、竞争、变化为特征的现代企业经营环境。 "},"pages/信息系统开发与管理/第五章_系统分析/第四节_数据流程分析的概念.html":{"url":"pages/信息系统开发与管理/第五章_系统分析/第四节_数据流程分析的概念.html","title":"第四节：数据流程分析的概念","keywords":"","body":"第四节：数据流程分析的概念 数据流程分析的概念 "},"pages/信息系统开发与管理/第五章_系统分析/第五节_新系统逻辑模型.html":{"url":"pages/信息系统开发与管理/第五章_系统分析/第五节_新系统逻辑模型.html","title":"第五节：新系统逻辑模型","keywords":"","body":"第五节：新系统逻辑模型 "},"pages/信息系统开发与管理/第五章_系统分析/第六节_系统分析报告.html":{"url":"pages/信息系统开发与管理/第五章_系统分析/第六节_系统分析报告.html","title":"第六节：系统分析报告","keywords":"","body":"第六节：系统分析报告 "},"pages/持续化集成CI/01_使用shell打包.html":{"url":"pages/持续化集成CI/01_使用shell打包.html","title":"1.使用shell打包","keywords":"","body":"使用shell打包 需求 使用shell脚本，导出adhoc/appstore的包，然后上传至appstore/fir/pgy等平台。 一、准备adhoc/appstore的证书描述文件 新建iOS Distribution (App Store and Ad Hoc)证书。 新增(Ad Hoc/App Store) Provisioning Profile证书描述文件（该文件会关联App、Distribution证书、iPhone设备）。 二、准备导出plist配置文件 在我们手动使用Xcode打包的时候，导出完毕后可以得到对应ExportOptions.plist，直接使用即可。 注意：adhoc的plist请使用adhoc的证书描述文件（AdHocProvisioningProfile）打包，appstore的则使用appstore的证书描述文件（AppStoreProvisioningProfile）打包。 AdHocExportOptions.plist compileBitcode method ad-hoc provisioningProfiles com.******.packagewithscript AdHocProvisioningProfile signingCertificate Apple Distribution signingStyle manual stripSwiftSymbols teamID ****** thinning &lt;none&gt; AppStoreExportOptions compileBitcode method app-store provisioningProfiles com.******.packagewithscript AppStoreProvisioningProfile signingCertificate Apple Distribution signingStyle manual stripSwiftSymbols teamID ****** thinning &lt;none&gt; 三、配置表格 证书类型 证书描述文件类型 证书描述文件名称 plist文件 iOS Distribution (App Store and Ad Hoc) Ad Hoc AdHocProvisioningProfile AdHocExportOptions iOS Distribution (App Store and Ad Hoc) App Store AppStoreProvisioningProfile AppStoreExportOptions 四、xcodebuild和xcrun安装 xcodebuild和xcrun都是来自Command Line Tools，Xcode自带，如果没有可以通过以下命令安装： xcode-select --install 或者在下面的链接下载安装： https://developer.apple.com/downloads/ 安装完可在以下路径看到这两个工具： /Applications/Xcode.app/Contents/Developer/usr/bin/ 五、xcodebuild xcodebuild从入门到精通 脚本打包ipa会使用到如下命令： 清理 xcodebuild \\ clean -configuration ${development_mode} -quiet || rollbackIfNeed '清理失败' 编译 xcodebuild \\ archive -project ${project_name}.xcodeproj \\ -scheme ${scheme_name} \\ -configuration ${development_mode} \\ -archivePath ${buildPath}/${project_name}.xcarchive -quiet || rollbackIfNeed '编译失败' 导出 xcodebuild -exportArchive -archivePath ${buildPath}/${project_name}.xcarchive \\ -configuration ${development_mode} \\ -exportPath ${exportFilePath} \\ -exportOptionsPlist ${exportOptionsPlistPath} \\ -quiet || rollbackIfNeed '打包失败' 五、altool 脚本上传ipa至apptore会使用如下命令： # 验证并上传到App Store # ${exportFilePath}/${scheme_name}.ipa：ipa路径 # ******@126.com: 苹果账号 # ****-****-vlnc-hill：双重认证密码 xcrun altool --validate-app -f ${exportFilePath}/${scheme_name}.ipa -t ios -u ******@126.com -p ****-****-vlnc-hill --output-format xml || rollbackIfNeed 'ipa校验失败' xcrun altool --upload-app -f ${exportFilePath}/${scheme_name}.ipa -t ios -u ******@126.com -p ****-****-vlnc-hill --output-format xml || rollbackIfNeed 'ipa上传失败' 通过 altool 上传 App 的二进制文件 ipa上传 ipa上传stackflow 注：Xcode 11 的 altool 已经被命令 xcrun altool 替代。在终端运行xcrun altool -h可以查看说明。 六、fir-cli fir-cli安装 脚本上传ipa至fir会使用如下命令： # 上传到Fir # 将******替换成自己的Fir平台的token fir login -T ****** || rollbackIfNeed '登录fir失败' fir publish $exportFilePath/$scheme_name.ipa || rollbackIfNeed '发布ipa包至fir失败' 七、蒲公英 使用一条命令快速上传应用 脚本上传ipa至fir会使用如下命令： # 上传到蒲公英 # 蒲公英aipKey MY_PGY_API_K=****** # 蒲公英uKey MY_PGY_UK=****** curl -F \"file=@${exportFilePath}/${scheme_name}.ipa\" \\ -F \"uKey=${MY_PGY_UK}\" \\ -F \"_api_key=${MY_PGY_API_K}\" \\ https://www.pgyer.com/apiv1/app/upload || rollbackIfNeed '发布ipa包至pgy失败' 八、完整脚本 xcodebuild.sh 九、参考链接 蒲公英 fir While executing gem ... (Gem::FilePermissionError) 详解Shell脚本实现iOS自动化编译打包提交 "},"pages/持续化集成CI/02_使用fastlane打包.html":{"url":"pages/持续化集成CI/02_使用fastlane打包.html","title":"2.使用fastlane打包","keywords":"","body":"使用fastlane打包 fastlane理解 之前写了一篇使用shell打包的文章，从这篇文章打包是可以通过编写shell脚本完成，那么为什么还需要fastlane呢？ fastlane可以通过一个简单的通过简单命令来完成诸如截图、获取证书、编译、导出安装包，而不需要去关心如何去写大量的打包脚本。 fastlane可以定义多个任务，例如：打包到不同渠道包时，我们可以定义多个任务，只需要一行简单命令。 fastlane打包直接就包含了dsym等文件，而使用脚本还得自己去实现。 fastlane安装及配置 fastlane安装 sudo gem install fastlane -n /usr/local/bin fastlane升级 bundle update fastlane fastlane配置 cd 项目目录 fastlane init 执行以上命令，项目目录下会生成相应的Appfile、Fastfile。 执行fastlane init,bundle update卡住了 Appfile Appfile是用来配置一些类似于AppleID、BundleID参数(参数是fastlane已经定义好的，新增的并没有用，如果想新增变量需要使用.env方式)，可以在Fastfile中使用，AppleID、BundleID等其实会被一些actions直接调用，并不需要写出来传递。 普通配置方式 直接在Appfile里填写app_identifier、apple_id、team_id等，然后根据lane的不同可以设置成不同。 # 默认配置 app_identifier \"com.devhy.test\" apple_id \"devhy1@xxxx.com\" team_id \"xxxxxxxxx1\" # 如果lane是ent换成Dev的配置 for_lane :ent do app_identifier \"com.devhy.testDev\" apple_id \"devhy2@xxxx.com\" team_id \"xxxxxxxxx2\" end 使用.env配置方式 .env这个文件的作用是作为环境变量的配置文件，在fastlane init进行初始化后并不会自动生成，如果需要可以自己创建。 执行时默认会读取.env和.env.default文件里的配置。通过执行fastlane [lane-name] --env [envName]来指定使用配置文件.env.[envName]，读取顺序是.env -> .env.default -> .env.，相同的变量名会被后面的覆盖。 如我建了文件.env.myDev，里面写了一些参数，那在执行的时候使用fastlane [lane-name] --env myDev即可，想在Appfile、Deliverfile、Fastfile等调用，直接使用ENV['keyName']即可。 # .env.myDev文件 # bundle id App_Identifier = \"com.devhy.testDev\" # 开发者账号 Apple_Id = \"xx2@xxxx.com\" # 开发者TeamId Team_Id = \"xxxxxxxxx2\" # project的target scheme名称 Scheme = \"HYTestDev\" # Appfile使用.env方式直接读取变量即可 app_identifier ENV['App_Identifier'] apple_id ENV['Apple_Id'] team_id ENV['Team_Id'] 注意：因为是.env文件是.开头文件，默认是在finder中隐藏的，需要通过执行一下命令来显示： # 设置隐藏文件可见 defaults write com.apple.finder AppleShowAllFiles TRUE # 重启finder服务以生效 killall Finder 配置方式对比 普通配置方式：简单易懂，但不能自定义变量，且每个lane想不一样都要写一个for_lane .env配置方式：功能性强，但配置起来稍微麻烦一点。 Deliverfile Deliverfile是用来配置上传到iTunesConnect所需信息的，由于我们主要用fastlane来打包，发布是手动将ipa包提交审核，由于没有进行过尝试所以该文件配置方式就不叙述了。 Fastfile Fastfile是对流程进行控制的核心文件，需要设定支持的平台和在一些环节里需要做的事情。 基本结构 Fastfile主要是根据设定的平台，可以在before_all、after_all、error中做一些操作以及建立一些lane作为关键的执行逻辑，可以在其中使用fastlane内置的action，也可以调用自建action，还可以调用别的lane。 # 因为fastlane存在新老版本兼容问题，所以一般会指定fastlane版本 fastlane_version \"2.62.0\" default_platform :ios platform :ios do # 所有lane执行之前，可以做如执行cocoapods的pod install before_all do cocoapods end # 名字叫ent的lane，命令行里执行fastlane ent lane :ent do # 执行一些action，如cert下载证书，sigh下载pp文件，gym进行编译和导出包 end # 执行fastlane store即可 lane :store do # 调用一些action # 调用别的lane，比如send_msg send_msg end lane :send_msg do # 调用一些action end # 所有lane完成之后，可以适用参数lane来区分 after_all do |lane| end # 所有lane失败之后，可以适用参数lane来区分 error do |lane, exception| end end Fastfile样例 下面的Fastfile样例是配置了.env+Appfile后进行编写，因为这样在配置action时，可以省去一些入参。 因为使用了Appfile，cert的username、team_id 以及 sigh的app_identifier、username、team_id 可以不用传入了，fastlane在执行时会自己去从Appfile里取。以及之前在.env环境配置中设定了一个Scheme的字段，那么gym的scheme我们可以使用ENV['Scheme']来调用。 fastlane_version \"2.62.0\" default_platform :ios platform :ios do before_all do cocoapods end lane :store do # action(cert)，下载[开发者证书.cer] # 下载的文件会存在项目根目录的build文件夹下 # fastlane会让你在命令行登录开发者账号，登录成功后，会在你的[钥匙串]中创建一个 {deliver.[username]} 的登录账户 cert( # Appfile设置了这边就可以不用了 # username: \"devhy2@xxxx.com\", # team_id: \"xxxxxxxxx2\", # 下载.cer文件的位置 output_path: \"build\", ) # action(sigh)，下载[安装app匹配的Provision Profile文件(pp文件)] # 建议自己去苹果开发者网站证书中手动处理一波provision_profile # 建议用 bundleId_导出方式 来命名比如: # 企业包pp文件叫 testDev_InHouse.mobileprovision sigh( # Appfile设置了这边就可以不用了 # app_identifier: \"com.devhy.testDev\", # username: \"devhy2@xxxx.com\", # team_id: \"xxxxxxxxx2\", # 下载pp文件的位置 output_path: \"build\", # 自动下载签名时，adc里pp名字，不写也可以会根据你的bundle id、adhoc与否去下载最新的一个 # provisioning_name: \"testDev_InHouse\", # 仅下载不创建，默认是false readonly: true, # 因为是根据BundleID下载，导致adhoc和appstore会优先appstore，导致最后导出报错，如果是adhoc包请设置为true adhoc: true, ) # 编译配置，编译的scheme，导出包方式 gym( # 使用.env配置的环境变量 scheme: ENV['Scheme'], # app-store, ad-hoc, package, enterprise, development, developer-id export_method: \"enterprise\", # 输出日志的目录 buildlog_path: \"fastlanelog\", # 输出编译结果 output_directory: \"build\", include_bitcode: false ) end after_all do |lane| end error do |lane, exception| end end actions 在fastlane中使用的诸如cer()、sigh()、gym()都是action，其本质是预先写好的ruby脚本(如:sigh.rb)，fastlane中有很多已经写好的actions，当然也可以自己进行编写。 命令行常用的操作有： 1. 查看所有Action fastlane actions 2. 查看某个Action的参数说明 fastlane action [action_name]如(fastlane action gym) 版本自增及指定 # 版本处理 def setup_version_build(options) if \"#{options[:build]}\".empty? increment_build_number( xcodeproj: ENV['Xcodeproj'] ) else increment_build_number( xcodeproj: ENV['Xcodeproj'], build_number:options[:build] ) end unless \"#{options[:version]}\".empty? increment_version_number( xcodeproj: ENV['Xcodeproj'], version_number:options[:version] ) end end 使用fastlane上传App到蒲公英 https://www.pgyer.com/doc/view/fastlane 使用fastlane上传App到蒲公英 fastlane-plugin-firim Fastlane && AppStore Connect API Fastlane && AppStore Connect API 配置后的使用 编写完各种配置后怎么使用？其实使用方法还是比较简单的，不使用.env配置，执行fastlane [lane_name]即可。 使用某个.env配置，执行fastlane [lane_name] --env [env_name]即可 ，比如我在需要执行样例的Fastfile的store，并使用.env.myDev配置，那我可以执行fastlane store --env myDev。 完整脚本 Fastfile 参考链接 macOS Mojave 'ruby/config.h' file not found 和重复劳动说再见-使用fastlane进行iOS打包 iOS开发热门-自动打包fastlane fastlane文档 Mac 下 fastlane 安装 以及常见错误处理 Automating Version and Build Numbers Using agvtool iOS自动打包 fastlane 在mac上配置iOS自动化上架 deliver 使用fastlane deliver 自动上传App Store Connect 物料和截图 itunesconnect fastlane官网 "},"pages/持续化集成CI/03_使用jenkins打包.html":{"url":"pages/持续化集成CI/03_使用jenkins打包.html","title":"3.使用jenkins打包","keywords":"","body":"使用jenkins打包 安装jenkins 下载war，下载地址 启动jenkins java -jar jenkins.war --httpPort=8080 下载jdk8 请从本地复制密码并粘贴到下面。 /Users/mengru.tian/.jenkins/secrets/initialAdminPassword 创建第一个管理员用户 安装插件 点击Manage Jenkins 选择Manage Plugins 点击Available 安装证书插件 搜索keychain 点击“Download now and install after restart” 安装蒲公英插件 搜索pgyer 点击“Download now and install after restart” 安装Git Parameter插件 证书配置 点击Manage Jenkins 点击Keychains and Provisioning Profiles Management cp ~/Library/Keychains/login.keychain-db /Users/Shared/Jenkins/login.keychain 配置构建参数 Mac下使用命令行安装 jenkins 方法 Mac安装jenkins 利用Jenkins持续集成iOS项目 Jenkins安装与配置 关于jenkins 自动化打包部署的问题。 iOS Jenkins自动化打包上传到蒲公英 Git Parameter 插件 MacOS Jenkins卸载方法 Jenkins参数设置单选框、多选框、Git分支框 jenkins 构建后shell_如何/何时执行Shell标记一个构建失败在Jenkins？ 使用 Jenkins 插件上传应用到蒲公英 "},"pages/安全攻防/01_常用工具.html":{"url":"pages/安全攻防/01_常用工具.html","title":"1.常用工具","keywords":"","body":"1.常用工具 Alfred 3.2 Mac破解版 安装地址 “XXX.app 已损坏，打不开。您应该将它移到废纸篓”，Mac应用程序无法打开或文件损坏的处理方法 5分钟上手Mac效率神器Alfred以及Alfred常用操作 Setting the Terminal/Shell to \"Custom\" (iTerm). Mac OS 终端利器 iTerm2 安装教程 How do I hide the “user@hostname” info #39 MAC TERMINAL终端或ITERM2出现问号解决方案 git - 警告:不建议使用此脚本，请参阅git-completion.zsh SwitchHosts 安装地址 如何解决类似 curl: (7) Failed to connect to raw.githubusercontent.com port 443: Connection refused 的问题 #10 OpenInTerminal 安装教程 ios-app-signer 下载地址 idapro IDA Pro 7.0.zip（MAC版本，下载下来可直接打开即可，在10.15.4测试通过） IDA Pro Mac版 V7.0-pc6 IDA Pro 7.0 for Mac(静态反编译软件) v7.0.170914中文版 IDA Pro 7.0 macOS 10.15安装 cycript 安装Cycript报错找不到libruby.2.0.0.dylib 安装cycript遇到的问题 iOS 逆向必备工具和安装过程 csrutil disable sudo mkdir -p /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ sudo ln -s /System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/lib/libruby.2.6.dylib /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/libruby.2.0.0.dylib mac下安装ruby2.2.0 MacOS 下安装 Ruby pod update报错： Traceback (most recent call last): 2: from /usr/local/bin/pod:23:in `' 1: from /Library/Ruby/Site/2.6.0/rubygems.rb:296:in `activate_bin_path' 使用如下命令解决，参考链接： sudo gem install cocoapods "},"pages/安全攻防/02_MonkeyDev.html":{"url":"pages/安全攻防/02_MonkeyDev.html","title":"2.MonkeyDev","keywords":"","body":"2.MonkeyDev 一、安装 安装步骤 xcode 12 Types.xcspec not found #266 MonkeyDev插件的安装 二、运行工程 使用Monkey创建工程，导入.app包之后，配置好工程证书后，点击运行发现如下错误： error: Signing for “monkeyTestDylib” requires a development team 解决办法： 选中“monkeytestDylib”Target->Build Settings->搜索Code Sign Style->Manual。 Development Team再设置下。 源码 "},"pages/地图聚合SDK/01_地图显示.html":{"url":"pages/地图聚合SDK/01_地图显示.html","title":"1.地图显示","keywords":"","body":"1.地图显示 参考链接 百度地图SDK 百度定位SDK 高德地图SDK 高德定位SDK 高德地图Doc 用高德地图API 通过详细地址获得经纬度 Web服务API简介 IOS高德3D地图画多边形，以及判断某一经纬度是否在该多边形内 一、集成百度地图 第1步：注册和获取密钥 第2步：CocoaPods 自动配置，在.podspec文件中增加依赖 s.dependency 'BaiduMapKit', '6.3.0' 二、集成高德地图 第1步：获取Key 第2步：CocoaPods 自动配置，在.podspec文件中增加依赖 s.dependency 'AMap3DMap', '7.9.0' 三、抽象工厂 特点->比工厂方法产品种类多。 抽象产品 具体产品 抽象工厂 具体工厂 简单工厂 - 1 - N 工厂方法 1 N 1 N 抽象工厂 N N 1 N 四、地图SDK角色分析 抽象产品：MapViewProtocol、MapLocationProtocol 具体产品：BaiduMapView、GaodeMapView、BaiduMapLocation、GaodeMapLocation 抽象工厂：MapFactoryProtocol 具体工厂：BaiduMapFactory、GaodeMapFactory 地图引擎：MapEngine 五、显示地图 第1步：抽象地图协议 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { /// 初始化 /// - Parameter frame: init(frame: CGRect) /// 获取地图 func getView() -> UIView } 第2步：定义具体地图 高德地图 import MAMapKit /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { private var mapView: MAMapView! /// 初始化 /// - Parameter frame: required init(frame: CGRect) { super.init() mapView = MAMapView(frame: frame) } /// 获取地图 func getView() -> UIView { return mapView } } 百度地图 import BaiduMapAPI_Map /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { private var mapView: BMKMapView! /// 初始化 /// - Parameter frame: required init(frame: CGRect) { super.init() mapView = BMKMapView(frame: frame) } /// 获取地图 func getView() -> UIView { return mapView } } 第3步：抽象工厂 /// 地图工厂标准 public protocol MapFactoryProtocol: NSObjectProtocol { /// 初始化 /// - Parameter appKey: 第三方地图AppKey init(appKey: String) /// 获取地图 /// - Parameter frame: func getMapView(frame: CGRect) -> MapViewProtocol } 第4步：定义具体地图工厂 高德地图工厂 import MAMapKit /// 高德地图工厂 class GaodeMapFactory: NSObject, MapFactoryProtocol { /// 初始化 /// - Parameter appKey: 第三方地图AppKey required init(appKey: String) { super.init() AMapServices.shared()?.apiKey = appKey } /// 获取地图 /// - Parameter frame: func getMapView(frame: CGRect) -> MapViewProtocol { return GaodeMapView(frame: frame) } } 百度地图工厂 import BaiduMapAPI_Map /// 高德地图工厂 class BaiduMapFactory: NSObject, MapFactoryProtocol { private let mapManager = BMKMapManager() /// 初始化 /// - Parameter appKey: 第三方地图AppKey required init(appKey: String) { super.init() let result = mapManager.start(appKey, generalDelegate: self) if !result { print(\"manager start failed!!!\") } } /// 获取地图 /// - Parameter frame: func getMapView(frame: CGRect) -> MapViewProtocol { return BaiduMapView(frame: frame) } } 百度地图工厂需要实现下创建协议 extension BaiduMapFactory: BMKGeneralDelegate { func onGetNetworkState(_ iError: Int32) { if iError == 0 { print(\"联网成功\") } else { print(\"onGetNetworkState:\\(iError)\") } } func onGetPermissionState(_ iError: Int32) { if iError == 0 { print(\"授权成功\") } else { print(\"onGetPermissionState:\\(iError)\") } } } 第5步：定义地图引擎 通过读取工厂配置，获取激活的地图工厂。 注意：CQConfigManager可以读取config.xml,获取当前激活的地图工厂，这样可以不修改代码，无缝切换百度/高德地图。 /// 地图引擎 public class MapEngine: NSObject { /// 根据配置获取地图工厂 /// - Returns: 地图工厂 public func getFactory() -> MapFactoryProtocol? { let mapPlatform = CQConfigManager.shared.config.mapPlatform if let factoryName = mapPlatform?.factoryName, let appKey = mapPlatform?.appKey { // 百度地图工厂 if factoryName == \"BaiduMapFactory\" { return BaiduMapFactory(appKey: appKey) } // 高德地图工厂 if factoryName == \"GaodeMapFactory\" { return GaodeMapFactory(appKey: appKey) } } return nil } } 第6步：显示地图 注意：CQMapSDK是地图SDK，包含以上的源代码，而ViewController则是Demo工程的页面。 import CQMapSDK class ViewController: UIViewController { override func viewDidLoad() { super.viewDidLoad() // 显示地图 let engine = MapEngine() let factory = engine.getFactory() if let mapView = factory?.getMapView(frame: view.bounds) { view.addSubview(mapView.getView()) } } } "},"pages/地图聚合SDK/02_显示高德用户定位.html":{"url":"pages/地图聚合SDK/02_显示高德用户定位.html","title":"2.显示高德用户定位","keywords":"","body":"2.显示高德用户定位 六、显示高德用户定位 显示定位蓝点 第1步：地图协议增加显示用户定位属性 ......省略部分代码 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 设定是否显示定位图层 var showsUserLocation: Bool { get set } } /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... /// 设定是否显示定位图层 var showsUserLocation: Bool = false { didSet { mapView.showsUserLocation = showsUserLocation } } } /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ....... /// 设定是否显示定位图层 var showsUserLocation: Bool = false { didSet { mapView.showsUserLocation = showsUserLocation } } } 第2步：测试代码 class ViewController: UIViewController { override func viewDidLoad() { super.viewDidLoad() ...... if let mapView = factory?.getMapView(frame: view.bounds) { // 显示用户定位 mapView.showsUserLocation = true view.addSubview(mapView.getView()) } } } 增加如上代码后，高德/百度都没有显示用户定位。 发现问题1: [MAMapKit] 要在iOS 11及以上版本使用定位服务, 需要在Info.plist中添加NSLocationAlwaysAndWhenInUseUsageDescription和NSLocationWhenInUseUsageDescription字段。 解决办法：info.plist新增： NSLocationAlwaysAndWhenInUseUsageDescription CQMapSDK需要使用定位服务 NSLocationWhenInUseUsageDescription CQMapSDK需要使用定位服务 发现问题2: App Transport Security has blocked a cleartext HTTP (http://) resource load since it is insecure. Temporary exceptions can be configured via your app's Info.plist file. 解决办法：info.plist新增： NSAppTransportSecurity NSAllowsArbitraryLoads 发现问题3: [MAMapKit] 要在iOS 11及以上版本使用后台定位服务, 需要实现mapViewRequireLocationAuth: 代理方法 解决办法：实现mapViewRequireLocationAuth 1.定义地图代理协议 /// 地图代理协议 public protocol MapViewDelegateProtocol: NSObjectProtocol { } public extension MapViewDelegateProtocol { } 2.新增高德地图代理实现 import MAMapKit /// 高德地图代理实现 class MAMapViewDelegateImpl: NSObject, MAMapViewDelegate { weak var delegate: MapViewDelegateProtocol? /** * @brief 当plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription，并且[CLLocationManager authorizationStatus] == kCLAuthorizationStatusNotDetermined，会调用代理的此方法。 * 此方法实现调用后台权限API即可（ 该回调必须实现 [locationManager requestAlwaysAuthorization] ）; since 6.8.0 * - Parameter locationManager: 地图的CLLocationManager。 */ func mapViewRequireLocationAuth(_ locationManager: CLLocationManager!) { locationManager.requestAlwaysAuthorization() } } 3.高德地图实现增加代理 /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... private var mapDelegate = MAMapViewDelegateImpl() ...... /// 地图代理 var delegate: MapViewDelegateProtocol? { didSet { if let delegate = delegate { mapDelegate.delegate = delegate mapView.delegate = mapDelegate } } } ...... } 4.测试用户定位显示 class ViewController: UIViewController { override func viewDidLoad() { ...... if let mapView = factory?.getMapView(frame: view.bounds) { ...... // 设置代理 mapView.delegate = self ...... } } } 现在使用高德地图就可以正常显示用户定位啦！ "},"pages/地图聚合SDK/03_如何显示百度用户定位.html":{"url":"pages/地图聚合SDK/03_如何显示百度用户定位.html","title":"3.如何显示百度用户定位","keywords":"","body":"3.如何显示百度用户定位 七、如何显示百度用户定位？ 分析百度地图还是无法显示用户定位，经过查看百度Demo工程，必须完成以下步骤： 第1步：初始化定位SDK,设置showsUserLocation=true BMKLocationAuth.sharedInstance()?.checkPermision(withKey: appKey, authDelegate: self) 第2步：打开用户定位，并且确保定位成功 //开启定位服务 locationManager.startUpdatingHeading() locationManager.startUpdatingLocation() 第3步：在定位代理中更新用户位置 //MARK:BMKLocationManagerDelegate /** @brief 该方法为BMKLocationManager提供设备朝向的回调方法 @param manager 提供该定位结果的BMKLocationManager类的实例 @param heading 设备的朝向结果 */ func bmkLocationManager(_ manager: BMKLocationManager, didUpdate heading: CLHeading?) { NSLog(\"用户方向更新\") userLocation.heading = heading mapView.updateLocationData(userLocation) } /** @brief 连续定位回调函数 @param manager 定位 BMKLocationManager 类 @param location 定位结果，参考BMKLocation @param error 错误信息。 */ func bmkLocationManager(_ manager: BMKLocationManager, didUpdate location: BMKLocation?, orError error: Error?) { if let _ = error?.localizedDescription { NSLog(\"locError:%@;\", (error?.localizedDescription)!) } NSLog(\"用户定位更新\") userLocation.location = location?.location //实现该方法，否则定位图标不出现 mapView.updateLocationData(userLocation) } 第4步：在定位代理实现定位授权 func bmkLocationManager(_ manager: BMKLocationManager, doRequestAlwaysAuthorization locationManager: CLLocationManager) { locationManager.requestAlwaysAuthorization() } 下面我们集成百度定位SDK，在验证上面的步骤是否可以正确显示百度定位？ 八、集成百度定位 第1步：这里使用Cocoapod的配置，参考百度定位SDK-代码，以下是CQMapSDK.podspec关键配置： s.static_framework = true s.swift_version = '5.0' s.ios.deployment_target = '9.0' s.source_files = 'CQMapSDK/Classes/**/*',\"framework/*.framework/Headers/*.h\" s.public_header_files = \"framework/*.framework/Headers/*.h\" s.vendored_frameworks = \"framework/*.framework\" s.frameworks = \"CoreLocation\", \"Foundation\", \"UIKit\", \"SystemConfiguration\", \"AdSupport\", \"Security\", \"CoreTelephony\" s.libraries = \"sqlite3.0\",\"c++\" s.requires_arc = true s.pod_target_xcconfig = { 'EXCLUDED_ARCHS[sdk=iphonesimulator*]' => 'arm64' } s.user_target_xcconfig = { 'EXCLUDED_ARCHS[sdk=iphonesimulator*]' => 'arm64' } 增加了以上代码，执行pod update 第2步：定义定位相关协议：LocationProtocol、LocationManagerProtocol、LocationManagerDelegateProtocol 定位成功，返回的定位数据协议： /// 定位数据 public protocol LocationProtocol: NSObjectProtocol { /// 位置数据 var location: CLLocation? { get } /// 初始化LocationProtocol实例 /// - Parameter loc: CLLocation对象 init(location loc: CLLocation?) } 定位管理代理协议： /// 地图定位管理代理协议 public protocol LocationManagerDelegateProtocol : NSObjectProtocol { /// 为了适配app store关于新的后台定位的审核机制（app store要求如果开发者只配置了使用期间定位，则代码中不能出现申请后台定位的逻辑），当开发者在plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription时，需要在该delegate中调用后台定位api：[locationManager requestAlwaysAuthorization]。开发者如果只配置了NSLocationWhenInUseUsageDescription，且只有使用期间的定位需求，则无需在delegate中实现逻辑。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - locationManager: 系统 CLLocationManager 类 。 func locationManager(_ manager: LocationManagerProtocol, doRequestAlwaysAuthorization locationManager: CLLocationManager) /// 当定位发生错误时，会调用代理的此方法。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - error: 返回的错误，参考 CLError func locationManager(_ manager: LocationManagerProtocol, didFailWithError error: Error?) /// 连续定位回调函数。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - location: 定位结果 /// - error: 错误信息。 func locationManager(_ manager: LocationManagerProtocol, didUpdate location: LocationProtocol?, orError error: Error?) /// 提供设备朝向的回调方法。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - heading: 设备的朝向结果 func locationManager(_ manager: LocationManagerProtocol, didUpdate heading: CLHeading?) } extension LocationManagerDelegateProtocol { func locationManager(_ manager: LocationManagerProtocol, doRequestAlwaysAuthorization locationManager: CLLocationManager) { } func locationManager(_ manager: LocationManagerProtocol, didFailWithError error: Error?){ } func locationManager(_ manager: LocationManagerProtocol, didUpdate location: LocationProtocol?, orError error: Error?){ } func locationManager(_ manager: LocationManagerProtocol, didUpdate heading: CLHeading?){ } } 定位管理协议： /// 地图定位管理协议 public protocol LocationManagerProtocol: NSObjectProtocol { /// 实现了 LocationManagerDelegateProtocol 协议的类指针。 var delegate: LocationManagerDelegateProtocol? { get set } /// 开始连续定位。调用此方法会cancel掉所有的单次定位请求。 func startUpdatingLocation() /// 停止连续定位。调用此方法会cancel掉所有的单次定位请求，可以用来取消单次定位。 func stopUpdatingLocation() /// 开始设备朝向事件回调。 func startUpdatingHeading() /// r停止设备朝向事件回调。 func stopUpdatingHeading() } 第3步：百度定位SDK实现类：BaiduLocation、BaiduLocationManager、BMKLocationManagerDelegateImpl 百度定位数据实现： /// 百度定位数据 class BaiduLocation: NSObject, LocationProtocol { /// 位置数据 private(set) var location: CLLocation? /// 初始化LocationProtocol实例 /// - Parameter loc: CLLocation对象 required init(location loc: CLLocation?) { super.init() location = loc } } 百度定位管理代理实现： /// 百度地图定位管理代理实现类 class BMKLocationManagerDelegateImpl: NSObject, BMKLocationManagerDelegate { weak var delegate: LocationManagerDelegateProtocol? private var managerProtocol: LocationManagerProtocol! init(managerProtocol: LocationManagerProtocol) { super.init() self.managerProtocol = managerProtocol } func bmkLocationManager(_ manager: BMKLocationManager, doRequestAlwaysAuthorization locationManager: CLLocationManager) { delegate?.locationManager(managerProtocol, doRequestAlwaysAuthorization: locationManager) } func bmkLocationManager(_ manager: BMKLocationManager, didFailWithError error: Error?) { delegate?.locationManager(managerProtocol, didFailWithError: error) } func bmkLocationManager(_ manager: BMKLocationManager, didUpdate location: BMKLocation?, orError error: Error?) { let bmkLocation = BaiduLocation(location: location?.location) delegate?.locationManager(managerProtocol, didUpdate: bmkLocation, orError: error) } func bmkLocationManager(_ manager: BMKLocationManager, didUpdate heading: CLHeading?) { delegate?.locationManager(managerProtocol, didUpdate: heading) } } 百度定位管理： /// 百度地图定位管理 class BaiduLocationManager: NSObject, LocationManagerProtocol { private lazy var delegateImpl: BMKLocationManagerDelegateImpl = { BMKLocationManagerDelegateImpl(managerProtocol: self) }() private lazy var locationManager: BMKLocationManager = { //初始化BMKLocationManager的实例 let manager = BMKLocationManager() //设置定位管理类实例的代理 manager.delegate = delegateImpl //设定定位坐标系类型，默认为 BMKLocationCoordinateTypeGCJ02 manager.coordinateType = BMKLocationCoordinateType.BMK09LL //设定定位精度，默认为 kCLLocationAccuracyBest manager.desiredAccuracy = kCLLocationAccuracyBest //设定定位类型，默认为 CLActivityTypeAutomotiveNavigation manager.activityType = CLActivityType.automotiveNavigation //指定定位是否会被系统自动暂停，默认为NO manager.pausesLocationUpdatesAutomatically = false /** 是否允许后台定位，默认为NO。只在iOS 9.0及之后起作用。 设置为YES的时候必须保证 Background Modes 中的 Location updates 处于选中状态，否则会抛出异常。 由于iOS系统限制，需要在定位未开始之前或定位停止之后，修改该属性的值才会有效果。 */ manager.allowsBackgroundLocationUpdates = false /** 指定单次定位超时时间,默认为10s，最小值是2s。注意单次定位请求前设置。 注意: 单次定位超时时间从确定了定位权限(非kCLAuthorizationStatusNotDetermined状态) 后开始计算。 */ manager.locationTimeout = 10 return manager }() /// 实现了 LocationManagerDelegateProtocol 协议的类指针。 weak var delegate: LocationManagerDelegateProtocol? { didSet { delegateImpl.delegate = delegate } } /// 开始连续定位。调用此方法会cancel掉所有的单次定位请求。 func startUpdatingLocation() { locationManager.startUpdatingLocation() } /// 停止连续定位。调用此方法会cancel掉所有的单次定位请求，可以用来取消单次定位。 func stopUpdatingLocation() { locationManager.stopUpdatingLocation() } /// 开始设备朝向事件回调。 func startUpdatingHeading() { locationManager.startUpdatingHeading() } /// r停止设备朝向事件回调。 func stopUpdatingHeading() { locationManager.stopUpdatingHeading() } } 第4步：高德定位SDK实现类：GaodeLocation、GaodeLocationManager、AMapLocationManagerDelegateImpl，后续再做具体方法实现。 /// 高德定位数据 class GaodeLocation: NSObject, LocationProtocol { /// 位置数据 private(set) var location: CLLocation? /// 初始化LocationProtocol实例 /// - Parameter loc: CLLocation对象 required init(location loc: CLLocation?) { super.init() location = loc } } /// 高德地图定位管理代理实现类 class AMapLocationManagerDelegateImpl: NSObject { weak var delegate: LocationManagerDelegateProtocol? private var managerProtocol: LocationManagerProtocol! init(managerProtocol: LocationManagerProtocol) { super.init() self.managerProtocol = managerProtocol } } /// 高德地图定位管理 class GaodeLocationManager: NSObject, LocationManagerProtocol { private lazy var delegateImpl: BMKLocationManagerDelegateImpl = { BMKLocationManagerDelegateImpl(managerProtocol: self) }() /// 实现了 LocationManagerDelegateProtocol 协议的类指针。 weak var delegate: LocationManagerDelegateProtocol? { didSet { delegateImpl.delegate = delegate } } /// 开始连续定位。调用此方法会cancel掉所有的单次定位请求。 func startUpdatingLocation() { } /// 停止连续定位。调用此方法会cancel掉所有的单次定位请求，可以用来取消单次定位。 func stopUpdatingLocation() { } /// 开始设备朝向事件回调。 func startUpdatingHeading() { } /// r停止设备朝向事件回调。 func stopUpdatingHeading() { } } 第5步：新增及修改地图相关协议 1.新增UserLocationProtocol /// 用户定位协议 public protocol UserLocationProtocol: NSObjectProtocol { /// 位置更新状态，如果正在更新位置信息，则该值为YES var updating: Bool { get set } /// 位置信息，尚未定位成功，则该值为nil var location: CLLocation? { get set } /// heading信息，尚未定位成功，则该值为nil var heading: CLHeading? { get set } /// 定位标注点要显示的标题信息 var title: String? { get set } /// 定位标注点要显示的子标题信息 var subtitle: String? { get set } } 2.修改MapViewProtocol /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 设定是否显示定位图层 var showsUserLocation: Bool { get set } } 3.修改MapViewDelegateProtocol /// 地图代理协议 public protocol MapViewDelegateProtocol: NSObjectProtocol { /// 当plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription，并且[CLLocationManager authorizationStatus] == kCLAuthorizationStatusNotDetermined，会调用代理的此方法。此方法实现调用后台权限API即可（ 该回调必须实现 [locationManager requestAlwaysAuthorization] ）; since 6.8.0 /// - Parameter locationManager: 地图的CLLocationManager。 func mapViewRequireLocationAuth(_ mapView: MapViewProtocol, locationManager: CLLocationManager) } public extension MapViewDelegateProtocol { func mapViewRequireLocationAuth(_ mapView: MapViewProtocol, locationManager: CLLocationManager) { } } 第6步：新增及修改百度地图相关协议实现 /// 百度用户定位 class BaiduUserLocation: NSObject, UserLocationProtocol { /// 位置更新状态，如果正在更新位置信息，则该值为YES var updating: Bool = false /// 位置信息，尚未定位成功，则该值为nil var location: CLLocation? /// heading信息，尚未定位成功，则该值为nil var heading: CLHeading? /// 定位标注点要显示的标题信息 var title: String? /// 定位标注点要显示的子标题信息 var subtitle: String? } /// 百度地图代理实现 class BMKMapViewDelegateImpl: NSObject, BMKMapViewDelegate { weak var delegate: MapViewDelegateProtocol? private weak var mapViewProtocol: MapViewProtocol! init(mapViewProtocol: MapViewProtocol) { super.init() self.mapViewProtocol = mapViewProtocol } } /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ....... private lazy var mapDelegate: BMKMapViewDelegateImpl = { BMKMapViewDelegateImpl(mapViewProtocol: self) }() /// 初始化 /// - Parameter frame: required init(frame: CGRect) { super.init() mapView = BMKMapView(frame: frame) mapView.delegate = mapDelegate } ...... /// 地图代理 var delegate: MapViewDelegateProtocol? { didSet { if let delegate = delegate { mapDelegate.delegate = delegate } } } ...... /// 动态更新我的位置数据 /// - Parameter userLocation: 定位数据 func updateLocationData(_ userLocation: UserLocationProtocol) { let bmkUserLoc = BMKUserLocation() bmkUserLoc.location = userLocation.location mapView.updateLocationData(bmkUserLoc) } } 第7步：新增及修改高德地图相关协议实现 /// 高德用户定位 class GaodeUserLocation: NSObject, UserLocationProtocol { /// 位置更新状态，如果正在更新位置信息，则该值为YES var updating: Bool = false /// 位置信息，尚未定位成功，则该值为nil var location: CLLocation? /// heading信息，尚未定位成功，则该值为nil var heading: CLHeading? /// 定位标注点要显示的标题信息 var title: String? /// 定位标注点要显示的子标题信息 var subtitle: String? } /// 高德地图代理实现 class MAMapViewDelegateImpl: NSObject, MAMapViewDelegate { weak var delegate: MapViewDelegateProtocol? private weak var mapViewProtocol: MapViewProtocol! init(mapViewProtocol: MapViewProtocol) { super.init() self.mapViewProtocol = mapViewProtocol } /// 当plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription，并且[CLLocationManager authorizationStatus] == kCLAuthorizationStatusNotDetermined，会调用代理的此方法。此方法实现调用后台权限API即可（ 该回调必须实现 [locationManager requestAlwaysAuthorization] ）; since 6.8.0 /// - Parameter locationManager: 地图的CLLocationManager。 func mapViewRequireLocationAuth(_ locationManager: CLLocationManager!) { delegate?.mapViewRequireLocationAuth(mapViewProtocol, locationManager: locationManager) } } /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... private lazy var mapDelegate: MAMapViewDelegateImpl = { MAMapViewDelegateImpl(mapViewProtocol: self) }() /// 初始化 /// - Parameter frame: required init(frame: CGRect) { super.init() mapView = MAMapView(frame: frame) mapView.delegate = mapDelegate } ...... /// 地图代理 var delegate: MapViewDelegateProtocol? { didSet { if let delegate = delegate { mapDelegate.delegate = delegate } } } ...... /// 动态更新我的位置数据 /// - Parameter userLocation: 定位数据 func updateLocationData(_ userLocation: UserLocationProtocol) { } } 第8步：修改工厂协议及实现 /// 地图工厂标准 public protocol MapFactoryProtocol: NSObjectProtocol { ...... /// 获取定位管理对象 func getLocationManager() -> LocationManagerProtocol /// 获取用户定位数据 func getUserLocation() -> UserLocationProtocol } /// 高德地图工厂 class BaiduMapFactory: NSObject, MapFactoryProtocol { ...... /// 初始化 /// - Parameter appKey: 第三方地图AppKey required init(appKey: String) { super.init() BMKLocationAuth.sharedInstance()?.checkPermision(withKey: appKey, authDelegate: self) ...... } ...... /// 获取定位管理对象 func getLocationManager() -> LocationManagerProtocol { return BaiduLocationManager() } /// 获取用户定位数据 func getUserLocation() -> UserLocationProtocol { return BaiduUserLocation() } } /// 高德地图工厂 class GaodeMapFactory: NSObject, MapFactoryProtocol { ...... /// 获取定位管理对象 func getLocationManager() -> LocationManagerProtocol { return GaodeLocationManager() } /// 获取用户定位数据 func getUserLocation() -> UserLocationProtocol { return GaodeUserLocation() } } 第9步：验证百度地图显示定位 class ViewController: UIViewController { /// 地图实例 private var mapView: MapViewProtocol! /// 用户定位实例 private var userLocation: UserLocationProtocol! /// 定位管理实例 private var locationManager: LocationManagerProtocol! override func viewDidLoad() { super.viewDidLoad() // 获取地图工厂 let engine = MapEngine() let factory = engine.getFactory()! // 开启定位 userLocation = factory.getUserLocation() locationManager = factory.getLocationManager() locationManager.delegate = self locationManager.startUpdatingHeading() locationManager.startUpdatingLocation() // 显示地图 mapView = factory.getMapView(frame: view.bounds) // 设置代理 mapView.delegate = self // 显示用户定位(放在设置代理之后，确保可以调用locationManager.requestAlwaysAuthorization()) mapView.showsUserLocation = true view.addSubview(mapView.getView()) } ...... } extension ViewController: MapViewDelegateProtocol { /// 当plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription，并且[CLLocationManager authorizationStatus] == kCLAuthorizationStatusNotDetermined，会调用代理的此方法。此方法实现调用后台权限API即可（ 该回调必须实现 [locationManager requestAlwaysAuthorization] ）; since 6.8.0 /// - Parameter locationManager: 地图的CLLocationManager。 func mapViewRequireLocationAuth(_ mapView: MapViewProtocol, locationManager: CLLocationManager) { locationManager.requestAlwaysAuthorization() } } extension ViewController: LocationManagerDelegateProtocol { /// 为了适配app store关于新的后台定位的审核机制（app store要求如果开发者只配置了使用期间定位，则代码中不能出现申请后台定位的逻辑），当开发者在plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription时，需要在该delegate中调用后台定位api：[locationManager requestAlwaysAuthorization]。开发者如果只配置了NSLocationWhenInUseUsageDescription，且只有使用期间的定位需求，则无需在delegate中实现逻辑。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - locationManager: 系统 CLLocationManager 类 。 func locationManager(_ manager: LocationManagerProtocol, doRequestAlwaysAuthorization locationManager: CLLocationManager) { locationManager.requestAlwaysAuthorization() } /// 当定位发生错误时，会调用代理的此方法。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - error: 返回的错误，参考 CLError func locationManager(_ manager: LocationManagerProtocol, didFailWithError error: Error?) { NSLog(\"定位失败\") } /// 连续定位回调函数。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - location: 定位结果 /// - error: 错误信息。 func locationManager(_ manager: LocationManagerProtocol, didUpdate location: LocationProtocol?, orError error: Error?) { if let _ = error?.localizedDescription { print(\"locError:(error?.localizedDescription)!\") } print(\"用户定位更新\") userLocation.location = location?.location //实现该方法，否则定位图标不出现 mapView.updateLocationData(userLocation) } /// 提供设备朝向的回调方法。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - heading: 设备的朝向结果 func locationManager(_ manager: LocationManagerProtocol, didUpdate heading: CLHeading?) { print((\"用户方向更新\")) userLocation.heading = heading mapView.updateLocationData(userLocation) } } 这样就可以成功显示百度定位啦！ "},"pages/地图聚合SDK/04_集成高德定位.html":{"url":"pages/地图聚合SDK/04_集成高德定位.html","title":"4.集成高德定位","keywords":"","body":"4.集成高德定位 九、集成高德定位 第1步：修改CQMapSDK.podspec 新增依赖s.dependency 'AMapLocation', '2.6.8',执行pod update 第2步：修改高德定位实现 import AMapLocationKit /// 高德地图定位管理代理实现类 class AMapLocationManagerDelegateImpl: NSObject, AMapLocationManagerDelegate { weak var delegate: LocationManagerDelegateProtocol? private var managerProtocol: LocationManagerProtocol! init(managerProtocol: LocationManagerProtocol) { super.init() self.managerProtocol = managerProtocol } func amapLocationManager(_ manager: AMapLocationManager!, didFailWithError error: Error!) { delegate?.locationManager(managerProtocol, didFailWithError: error) } func amapLocationManager(_ manager: AMapLocationManager!, didUpdate location: CLLocation!) { let amapLocation = GaodeLocation(location: location) delegate?.locationManager(managerProtocol, didUpdate: amapLocation, orError: nil) } func amapLocationManager(_ manager: AMapLocationManager!, didUpdate newHeading: CLHeading!) { delegate?.locationManager(managerProtocol, didUpdate: newHeading) } } 第3步：更新定位管理 import AMapLocationKit /// 高德地图定位管理 class GaodeLocationManager: NSObject, LocationManagerProtocol { private lazy var delegateImpl: AMapLocationManagerDelegateImpl = { AMapLocationManagerDelegateImpl(managerProtocol: self) }() private lazy var locationManager: AMapLocationManager = { //初始化BMKLocationManager的实例 let manager = AMapLocationManager() //设置定位管理类实例的代理 manager.delegate = delegateImpl //设定定位坐标系类型，默认为 BMKLocationCoordinateTypeGCJ02 //manager.coordinateType = BMKLocationCoordinateType.BMK09LL //设定定位精度，默认为 kCLLocationAccuracyBest manager.desiredAccuracy = kCLLocationAccuracyBest //设定定位类型，默认为 CLActivityTypeAutomotiveNavigation //manager.activityType = CLActivityType.automotiveNavigation //指定定位是否会被系统自动暂停，默认为NO manager.pausesLocationUpdatesAutomatically = false /** 是否允许后台定位，默认为NO。只在iOS 9.0及之后起作用。 设置为YES的时候必须保证 Background Modes 中的 Location updates 处于选中状态，否则会抛出异常。 由于iOS系统限制，需要在定位未开始之前或定位停止之后，修改该属性的值才会有效果。 */ manager.allowsBackgroundLocationUpdates = false /** 指定单次定位超时时间,默认为10s，最小值是2s。注意单次定位请求前设置。 注意: 单次定位超时时间从确定了定位权限(非kCLAuthorizationStatusNotDetermined状态) 后开始计算。 */ manager.locationTimeout = 10 return manager }() /// 实现了 LocationManagerDelegateProtocol 协议的类指针。 weak var delegate: LocationManagerDelegateProtocol? { didSet { delegateImpl.delegate = delegate } } /// 开始连续定位。调用此方法会cancel掉所有的单次定位请求。 func startUpdatingLocation() { locationManager.startUpdatingLocation() } /// 停止连续定位。调用此方法会cancel掉所有的单次定位请求，可以用来取消单次定位。 func stopUpdatingLocation() { locationManager.stopUpdatingLocation() } /// 开始设备朝向事件回调。 func startUpdatingHeading() { locationManager.startUpdatingHeading() } /// r停止设备朝向事件回调。 func stopUpdatingHeading() { locationManager.stopUpdatingHeading() } } "},"pages/地图聚合SDK/05_新增定位属性.html":{"url":"pages/地图聚合SDK/05_新增定位属性.html","title":"5.新增定位属性","keywords":"","body":"5.新增定位属性 十、新增定位属性 第1步：修改定位管理协议 /// 地图定位管理协议 public protocol LocationManagerProtocol: NSObjectProtocol { ///设定期望的定位精度。单位米，默认为 kCLLocationAccuracyBest。定位服务会尽可能去获取满足desiredAccuracy的定位结果，但不保证一定会得到满足期望的结果。 ///注意：设置为kCLLocationAccuracyBest或kCLLocationAccuracyBestForNavigation时，单次定位会在达到locationTimeout设定的时间后，将时间内获取到的最高精度的定位结果返回。 ///⚠️ 当iOS14及以上版本，模糊定位权限下可能拿不到设置精度的经纬度 var desiredAccuracy: CLLocationAccuracy { get set } ///设定定位的最小更新距离。单位米，默认为 kCLDistanceFilterNone，表示只要检测到设备位置发生变化就会更新位置信息。 var distanceFilter: CLLocationDistance { get set } ///指定定位是否会被系统自动暂停。默认为NO。 var pausesLocationUpdatesAutomatically: Bool { get set } ///是否允许后台定位。默认为NO。只在iOS 9.0及之后起作用。设置为YES的时候必须保证 Background Modes 中的 Location updates 处于选中状态，否则会抛出异常。由于iOS系统限制，需要在定位未开始之前或定位停止之后，修改该属性的值才会有效果。 var allowsBackgroundLocationUpdates: Bool { get set } ///指定单次定位超时时间,默认为10s。最小值是2s。注意单次定位请求前设置。注意: 单次定位超时时间从确定了定位权限(非kCLAuthorizationStatusNotDetermined状态)后开始计算。 var locationTimeout: Int { get set } ///指定单次定位逆地理超时时间,默认为5s。最小值是2s。注意单次定位请求前设置。 var reGeocodeTimeout: Int { get set } ...... } 第2步：修改高德百度实现 ///设定期望的定位精度。单位米，默认为 kCLLocationAccuracyBest。定位服务会尽可能去获取满足desiredAccuracy的定位结果，但不保证一定会得到满足期望的结果。 ///注意：设置为kCLLocationAccuracyBest或kCLLocationAccuracyBestForNavigation时，单次定位会在达到locationTimeout设定的时间后，将时间内获取到的最高精度的定位结果返回。 ///⚠️ 当iOS14及以上版本，模糊定位权限下可能拿不到设置精度的经纬度 var desiredAccuracy: CLLocationAccuracy = kCLLocationAccuracyBest { willSet { locationManager.desiredAccuracy = newValue } } ///设定定位的最小更新距离。单位米，默认为 kCLDistanceFilterNone，表示只要检测到设备位置发生变化就会更新位置信息。 var distanceFilter: CLLocationDistance = kCLDistanceFilterNone { willSet { locationManager.distanceFilter = newValue } } ///指定定位是否会被系统自动暂停。默认为NO。 var pausesLocationUpdatesAutomatically: Bool = false { willSet { locationManager.pausesLocationUpdatesAutomatically = newValue } } ///是否允许后台定位。默认为NO。只在iOS 9.0及之后起作用。设置为YES的时候必须保证 Background Modes 中的 Location updates 处于选中状态，否则会抛出异常。由于iOS系统限制，需要在定位未开始之前或定位停止之后，修改该属性的值才会有效果。 var allowsBackgroundLocationUpdates: Bool = false { willSet { locationManager.allowsBackgroundLocationUpdates = newValue } } ///指定单次定位超时时间,默认为10s。最小值是2s。注意单次定位请求前设置。注意: 单次定位超时时间从确定了定位权限(非kCLAuthorizationStatusNotDetermined状态)后开始计算。 var locationTimeout: Int = 10 { willSet { locationManager.locationTimeout = newValue } } ///指定单次定位逆地理超时时间,默认为5s。最小值是2s。注意单次定位请求前设置。 var reGeocodeTimeout: Int = 5 { willSet { locationManager.reGeocodeTimeout = newValue } } "},"pages/地图聚合SDK/06_新增单次定位.html":{"url":"pages/地图聚合SDK/06_新增单次定位.html","title":"6.新增单次定位","keywords":"","body":"6.新增单次定位 十一、新增单次定位 第1步：定义定位回调 /// 单次定位回调 public typealias LocatingCompletionBlock = (LocationProtocol?, Error?) -> Void 第2步：修改定位管理协议 /// 地图定位管理协议 public protocol LocationManagerProtocol: NSObjectProtocol { ...... /// - Parameters: /// - withReGeocode: 是否带有逆地理信息(获取逆地理信息需要联网) /// - withNetworkState: 是否带有移动热点识别状态(需要联网) /// - completionBlock: 单次定位完成后的Block /// - return: 是否成功添加单次定位Request func requestLocation(withReGeocode code: Bool, withNetworkState state: Bool, completionBlock: @escaping LocatingCompletionBlock) -> Bool } 第3步：增加百度地图实现 /// 百度地图定位管理 class BaiduLocationManager: NSObject, LocationManagerProtocol { ...... /// - Parameters: /// - withReGeocode: 是否带有逆地理信息(获取逆地理信息需要联网) /// - withNetworkState: 是否带有移动热点识别状态(需要联网) /// - completionBlock: 单次定位完成后的Block /// - return: 是否成功添加单次定位Request func requestLocation(withReGeocode code: Bool, withNetworkState state: Bool, completionBlock: @escaping LocatingCompletionBlock) -> Bool { let block: BMKLocatingCompletionBlock = { (bmkLocation, state, error) -> Void in let location = BaiduLocation(location: bmkLocation?.location) completionBlock(location, error) } return locationManager.requestLocation(withReGeocode: code, withNetworkState: state, completionBlock: block) } } 十二、新增地图更多属性 第1步：地图协议修改 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 当前地图的中心点，改变该值时，地图的比例尺级别不会发生变化 var centerCoordinate: CLLocationCoordinate2D? { get set } /// 定位用户位置的模式 var userTrackingMode: UserTrackingMode { get set } /// 动态更新我的位置数据 /// - Parameter userLocation: 定位数据 func updateLocationData(_ userLocation: UserLocationProtocol?) } 第2步：更新地图实现 高德地图 /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... /// 当前地图的中心点，改变该值时，地图的比例尺级别不会发生变化 var centerCoordinate: CLLocationCoordinate2D? { didSet { if let centerCoordinate = centerCoordinate { mapView.centerCoordinate = centerCoordinate } } } /// 定位用户位置的模式 var userTrackingMode: UserTrackingMode = .none { didSet { switch userTrackingMode { case .none:/// 普通定位模式 mapView.userTrackingMode = .none break case .heading:/// 定位方向模式 mapView.userTrackingMode = .none break case .follow:/// 定位跟随模式 mapView.userTrackingMode = .follow break case .followWithHeading:/// 定位罗盘模式 mapView.userTrackingMode = .followWithHeading break } } } /// 动态更新我的位置数据 /// - Parameter userLocation: 定位数据 func updateLocationData(_ userLocation: UserLocationProtocol?) { } } 百度地图 /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ...... /// 当前地图的中心点，改变该值时，地图的比例尺级别不会发生变化 var centerCoordinate: CLLocationCoordinate2D? { didSet { if let centerCoordinate = centerCoordinate { mapView.centerCoordinate = centerCoordinate } } } /// 定位用户位置的模式 var userTrackingMode: UserTrackingMode = .none { didSet { switch userTrackingMode { case .none:/// 普通定位模式 mapView.userTrackingMode = BMKUserTrackingModeNone break case .heading:/// 定位方向模式 mapView.userTrackingMode = BMKUserTrackingModeHeading break case .follow:/// 定位跟随模式 mapView.userTrackingMode = BMKUserTrackingModeFollow break case .followWithHeading:/// 定位罗盘模式 mapView.userTrackingMode = BMKUserTrackingModeFollowWithHeading break } } } /// 动态更新我的位置数据 /// - Parameter userLocation: 定位数据 func updateLocationData(_ userLocation: UserLocationProtocol?) { guard let userLocation = userLocation else { return } let bmkUserLoc = BMKUserLocation() bmkUserLoc.location = userLocation.location mapView.updateLocationData(bmkUserLoc) } } 第3步：测试 单次定位完成设置定位为地图中心 /// 单次定位 private func singleRequestLocation() { _ = locationManager.requestLocation(withReGeocode: true, withNetworkState: true) {[weak self] (location, error) in if let error = error { print(\"单次定位失败：\", error.localizedDescription) } else { let latitude = location?.location?.coordinate.latitude ?? 0 let longitude = location?.location?.coordinate.longitude ?? 0 print(\"单次定位成功：altitude:\\(latitude),longitude:\\(longitude)\") self?.userLocation.location = location?.location // 实现该方法，否则定位图标不出现 if let userLocation = self?.userLocation { self?.mapView.updateLocationData(userLocation) } // 设置中心点 self?.mapView.centerCoordinate = location?.location?.coordinate } } } 第4步：增加高德地图实现 注意：高德地图要实现单次定位，需要保证\"Background Modes\"中的\"Location updates\"处于选中状态。 /// 高德地图定位管理 class GaodeLocationManager: NSObject, LocationManagerProtocol { ...... /// - Parameters: /// - withReGeocode: 是否带有逆地理信息(获取逆地理信息需要联网) /// - withNetworkState: 是否带有移动热点识别状态(需要联网) /// - completionBlock: 单次定位完成后的Block /// - return: 是否成功添加单次定位Request func requestLocation(withReGeocode code: Bool, withNetworkState state: Bool, completionBlock: @escaping LocatingCompletionBlock) -> Bool { let block: AMapLocatingCompletionBlock = { (amapLocation, regeocode, error) -> Void in let location = GaodeLocation(location: amapLocation) completionBlock(location, error) } return locationManager.requestLocation(withReGeocode: code, completionBlock: block) } } "},"pages/地图聚合SDK/07_丰富地图代理.html":{"url":"pages/地图聚合SDK/07_丰富地图代理.html","title":"7.丰富地图代理","keywords":"","body":"7.丰富地图代理 十三、丰富地图代理 第1步：修改地图代理协议 /// 地图代理协议 public protocol MapViewDelegateProtocol: NSObjectProtocol { ...... /// 地图区域即将改变时会调用此接口 /// - Parameters: /// - mapView: 地图实例 /// - animated: 是否动画 func mapView(_ mapView: MapViewProtocol, regionWillChangeAnimated animated: Bool) /// 地图区域改变完成后会调用此接口 /// - Parameters: /// - mapView: 地图实例 /// - animated: 是否动画 /// - wasUserAction: 标识是否是用户动作 func mapView(_ mapView: MapViewProtocol, regionDidChangeAnimated animated: Bool, wasUserAction: Bool) /// 位置或者设备方向更新后，会调用此函数 /// - Parameters: /// - mapView: 地图实例 /// - userLocation: 用户定位信息(包括位置与设备方向等数据) /// - updatingLocation: 标示是否是location数据更新, YES:location数据更新 NO:heading数据更新 func mapView(_ mapView: MapViewProtocol, didUpdate userLocation: UserLocationProtocol, updatingLocation: Bool) } 第2步：修改地图代理实现 高德地图 /// 高德地图代理实现 class MAMapViewDelegateImpl: NSObject, MAMapViewDelegate { ...... func mapView(_ mapView: MAMapView!, regionWillChangeAnimated animated: Bool) { delegate?.mapView(mapViewProtocol, regionWillChangeAnimated: animated) } func mapView(_ mapView: MAMapView!, regionDidChangeAnimated animated: Bool, wasUserAction: Bool) { delegate?.mapView(mapViewProtocol, regionDidChangeAnimated: animated, wasUserAction: wasUserAction) } func mapView(_ mapView: MAMapView!, didUpdate userLocation: MAUserLocation!, updatingLocation: Bool) { let gaodeUserLocation = GaodeUserLocation() gaodeUserLocation.location = userLocation.location gaodeUserLocation.heading = userLocation.heading gaodeUserLocation.updating = userLocation.isUpdating delegate?.mapView(mapViewProtocol, didUpdate: gaodeUserLocation, updatingLocation: updatingLocation) } } 百度地图 /// 百度地图代理实现 class BMKMapViewDelegateImpl: NSObject, BMKMapViewDelegate { ...... func mapView(_ mapView: BMKMapView!, regionWillChangeAnimated animated: Bool) { delegate?.mapView(mapViewProtocol, regionWillChangeAnimated: animated) } func mapView(_ mapView: BMKMapView!, regionDidChangeAnimated animated: Bool, reason: BMKRegionChangeReason) { let wasUserAction = reason == BMKRegionChangeReasonGesture delegate?.mapView(mapViewProtocol, regionDidChangeAnimated: animated, wasUserAction: wasUserAction) } } "},"pages/地图聚合SDK/08_地图新增缩放等级.html":{"url":"pages/地图聚合SDK/08_地图新增缩放等级.html","title":"8.地图新增缩放等级","keywords":"","body":"8.地图新增缩放等级 十四、地图新增缩放等级 第1步：修改地图协议 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 缩放级别（默认3-19，有室内地图时为3-20） var zoomLevel: CGFloat { get set } /// 最小缩放级别 var minZoomLevel: CGFloat { get set } /// 最大缩放级别（有室内地图时最大为20，否则为19） var maxZoomLevel: CGFloat { get set } } 第2步：修改地图实现 高德地图 /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... /// 缩放级别（默认3-19，有室内地图时为3-20） var zoomLevel: CGFloat { get { mapView.zoomLevel } set { mapView.zoomLevel = newValue } } /// 最小缩放级别 var minZoomLevel: CGFloat { get { mapView.minZoomLevel } set { mapView.minZoomLevel = newValue } } /// 最大缩放级别（有室内地图时最大为20，否则为19） var maxZoomLevel: CGFloat { get { mapView.maxZoomLevel } set { mapView.maxZoomLevel = newValue } } } 百度地图 /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ...... /// 缩放级别（默认3-19，有室内地图时为3-20） var zoomLevel: CGFloat { get { CGFloat(mapView.zoomLevel) } set { mapView.zoomLevel = Float(newValue) } } /// 最小缩放级别 var minZoomLevel: CGFloat { get { CGFloat(mapView.minZoomLevel) } set { mapView.minZoomLevel = Float(newValue) } } /// 最大缩放级别（有室内地图时最大为20，否则为19） var maxZoomLevel: CGFloat { get { CGFloat(mapView.maxZoomLevel) } set { mapView.maxZoomLevel = Float(newValue) } } } "},"pages/地图聚合SDK/09_地图标注实现.html":{"url":"pages/地图聚合SDK/09_地图标注实现.html","title":"9.地图标注实现","keywords":"","body":"9.地图标注实现 十五、地图标注实现 第1步：新增标注协议、标注view协议 AnnotationProtocol（通用标注协议）、AnnotationViewProtocol（通用标注view协议）、PinAnnotationViewProtocol（大头针标注view协议） /// 标注协议 public protocol AnnotationProtocol: NSObjectProtocol { /// 标注view中心坐标 var coordinate: CLLocationCoordinate2D? { get set} /// annotation标题 var title: String? { get set } /// annotation副标题 var subtitle: String? { get set } /// 具体地图标注实现 var annotationImpl: Any? { get } /// 构造方法 /// - Parameter annotationImpl: 具体地图标注实现 init(annotationImpl: Any?) } /// 标注view协议 public protocol AnnotationViewProtocol: NSObjectProtocol { /// 是否允许弹出callout var canShowCallout: Bool { get set } /// 显示在默认弹出框右侧的view var rightCalloutAccessoryView: UIView? { get set } /// 是否支持拖动 var draggable: Bool { get set } /// 具体地图标注view实现 var annotationViewImpl: UIView? { get } /// 构造方法 /// - Parameter annotationViewImpl: 具体地图标注view实现 init(annotationViewImpl: UIView?) } /// 大头针标注view颜色 public enum PinAnnotationColor: Int { /// 第2步：新增标注实现、标注view实现 GaodePointAnnotation（高德标注）、GaodePinAnnotationView（高德大头针） BaiduPointAnnotation（百度标注）、BaiduPinAnnotationView（百度大头针） PointAnnotation（通用标注）、PinAnnotationView（通用大头针） /// 高德标注 public class GaodePointAnnotation: MAPointAnnotation { /// 聚合标注 weak var annotation: PointAnnotation? } /// 高德大头针标准view public class GaodePinAnnotationView: MAPinAnnotationView { } /// 百度标注 public class BaiduPointAnnotation: BMKPointAnnotation { /// 聚合标注 weak var annotation: PointAnnotation? } /// 百度大头针标准view public class BaiduPinAnnotationView: BMKPinAnnotationView { } /// 标注实现 public class PointAnnotation: NSObject, AnnotationProtocol { /// 标注view中心坐标 public var coordinate: CLLocationCoordinate2D? { get { if let annotation = gaodeAnnotation { return annotation.coordinate } if let annotation = baiduAnnotation { return annotation.coordinate } return nil } set { if let coordinate = newValue, let annotation = gaodeAnnotation { annotation.coordinate = coordinate } if let coordinate = newValue, let annotation = baiduAnnotation { annotation.coordinate = coordinate } } } /// annotation标题 public var title: String? { get { if let annotation = gaodeAnnotation { return annotation.title } if let annotation = baiduAnnotation { return annotation.title } return nil } set { if let title = newValue, let annotation = gaodeAnnotation { annotation.title = title } if let title = newValue, let annotation = baiduAnnotation { annotation.title = title } } } /// annotation副标题 public var subtitle: String? { get { if let annotation = gaodeAnnotation { return annotation.subtitle } if let annotation = baiduAnnotation { return annotation.subtitle } return nil } set { if let subtitle = newValue, let annotation = gaodeAnnotation { annotation.subtitle = subtitle } if let subtitle = newValue, let annotation = baiduAnnotation { annotation.subtitle = subtitle } } } /// 具体地图标注实现 private(set) public var annotationImpl: Any? /// 高德标注 private var gaodeAnnotation: GaodePointAnnotation? { if let annotation = annotationImpl as? GaodePointAnnotation { return annotation } return nil } /// 百度标注 private var baiduAnnotation: BaiduPointAnnotation? { if let annotation = annotationImpl as? BaiduPointAnnotation { return annotation } return nil } /// 构造方法 /// - Parameter annotationImpl: 具体地图标注实现 public required init(annotationImpl: Any?) { super.init() self.annotationImpl = annotationImpl gaodeAnnotation?.annotation = self baiduAnnotation?.annotation = self } } /// 通用大头针标注view public class PinAnnotationView: NSObject, PinAnnotationViewProtocol { /// 是否允许弹出callout public var canShowCallout: Bool { get { if let annotationView = gaodeAnnotationView { return annotationView.canShowCallout } if let annotationView = baiduAnnotationView { return annotationView.canShowCallout } return false } set { if let annotationView = gaodeAnnotationView { annotationView.canShowCallout = newValue } if let annotationView = baiduAnnotationView { annotationView.canShowCallout = newValue } } } /// 显示在默认弹出框右侧的view public var rightCalloutAccessoryView: UIView? { get { if let annotationView = gaodeAnnotationView { return annotationView.rightCalloutAccessoryView } if let annotationView = baiduAnnotationView { return annotationView.rightCalloutAccessoryView } return nil } set { if let annotationView = gaodeAnnotationView { annotationView.rightCalloutAccessoryView = newValue } if let annotationView = baiduAnnotationView { annotationView.rightCalloutAccessoryView = newValue } } } /// 是否支持拖动 public var draggable: Bool { get { if let annotationView = gaodeAnnotationView { return annotationView.isDraggable } if let annotationView = baiduAnnotationView { return annotationView.isDraggable } return false } set { if let annotationView = gaodeAnnotationView { annotationView.isDraggable = newValue } if let annotationView = baiduAnnotationView { annotationView.isDraggable = newValue } } } /// 大头针的颜色 public var pinColor: PinAnnotationColor { get { if let annotationView = gaodeAnnotationView { return PinAnnotationColor(rawValue: annotationView.pinColor.rawValue) ?? .red } if let annotationView = baiduAnnotationView { return PinAnnotationColor(rawValue: Int(annotationView.pinColor)) ?? .red } return .red } set { if let annotationView = gaodeAnnotationView, let pinColor = MAPinAnnotationColor(rawValue: newValue.rawValue) { annotationView.pinColor = pinColor } if let annotationView = baiduAnnotationView { annotationView.pinColor = BMKPinAnnotationColor(newValue.rawValue) } } } /// 添加到地图时是否使用下落动画效果 public var animatesDrop: Bool { get { if let annotationView = gaodeAnnotationView { return annotationView.animatesDrop } if let annotationView = baiduAnnotationView { return annotationView.animatesDrop } return false } set { if let annotationView = gaodeAnnotationView { annotationView.animatesDrop = newValue } if let annotationView = baiduAnnotationView { annotationView.animatesDrop = newValue } } } /// 具体地图标注view实现 private(set) public var annotationViewImpl: UIView? /// 高德地图标注view private var gaodeAnnotationView: MAPinAnnotationView? { return annotationViewImpl as? MAPinAnnotationView } /// 百度地图标注view private var baiduAnnotationView: BMKPinAnnotationView? { return annotationViewImpl as? BMKPinAnnotationView } /// 构造方法 /// - Parameter annotationViewImpl: 具体地图标注view实现 public required init(annotationViewImpl: UIView?) { self.annotationViewImpl = annotationViewImpl } } 第3步：地图工厂标准修改及实现 /// 地图工厂标准 public protocol MapFactoryProtocol: NSObjectProtocol { ...... /// 获取地图标注 func getPointAnnotation() -> PointAnnotation /// 获取标注view /// - Parameters: /// - annotation: 标注 /// - reuseIdentifier: 重用ID func getPinAnnotationView(annotation: AnnotationProtocol, reuseIdentifier: String) -> PinAnnotationViewProtocol } /// 百度地图工厂 class BaiduMapFactory: NSObject, MapFactoryProtocol { ...... /// 获取地图标注 func getPointAnnotation() -> PointAnnotation { let annotationImpl = BaiduPointAnnotation() return PointAnnotation(annotationImpl: annotationImpl) } /// 获取标注view /// - Parameters: /// - annotation: 标注 /// - reuseIdentifier: 重用ID func getPinAnnotationView(annotation: AnnotationProtocol, reuseIdentifier: String) -> PinAnnotationViewProtocol { var annotationImpl = BaiduPointAnnotation() if let annotationImplParam = annotation.annotationImpl as? BaiduPointAnnotation { annotationImpl = annotationImplParam } let annotationViewImpl = BaiduPinAnnotationView(annotation: annotationImpl, reuseIdentifier: reuseIdentifier) return PinAnnotationView(annotationViewImpl: annotationViewImpl) } } /// 高德地图工厂 class GaodeMapFactory: NSObject, MapFactoryProtocol { ...... /// 获取地图标注 func getPointAnnotation() -> PointAnnotation { let annotationImpl = GaodePointAnnotation() return PointAnnotation(annotationImpl: annotationImpl) } /// 获取标注view /// - Parameters: /// - annotation: 标注 /// - reuseIdentifier: 重用ID func getPinAnnotationView(annotation: AnnotationProtocol, reuseIdentifier: String) -> PinAnnotationViewProtocol { var annotationImpl = GaodePointAnnotation() if let annotationImplParam = annotation.annotationImpl as? GaodePointAnnotation { annotationImpl = annotationImplParam } let annotationViewImpl = GaodePinAnnotationView(annotation: annotationImpl, reuseIdentifier: reuseIdentifier) return PinAnnotationView(annotationViewImpl: annotationViewImpl) } } 第4步：地图协议修改及实现 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 向地图窗口添加标注 /// - Parameter annotation: 要添加的标注 func addAnnotation(_ annotation: AnnotationProtocol) /// 向地图窗口添加一组标注，需要实现MAMapViewDelegate的-mapView:viewForAnnotation:函数来生成标注对应的View /// - Parameter annotations: 要添加的标注数组 func addAnnotations(_ annotations: [AnnotationProtocol]) /// 设置地图使其可以显示数组中所有的annotation, 如果数组中只有一个则直接设置地图中心为annotation的位置。 /// - Parameters: /// - annotations: 需要显示的annotation /// - animated: 是否执行动画 func showAnnotations(_ annotations: [AnnotationProtocol], animated: Bool) /// 从复用内存池中获取制定复用标识的annotation view /// - Parameter withIdentifier: 复用标识 /// - Returns: annotation view func dequeueReusableAnnotationView(withIdentifier: String) -> PinAnnotationViewProtocol? } /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... /// 向地图窗口添加标注 /// - Parameter annotation: 要添加的标注 func addAnnotation(_ annotation: AnnotationProtocol) { if let annotationImpl = annotation.annotationImpl as? MAAnnotation { mapView.addAnnotation(annotationImpl) } } /// 向地图窗口添加一组标注，需要实现MAMapViewDelegate的-mapView:viewForAnnotation:函数来生成标注对应的View /// - Parameter annotations: 要添加的标注数组 func addAnnotations(_ annotations: [AnnotationProtocol]) { var annotationImpls: [Any] = [] for annotation in annotations { if let annotationImpl = annotation.annotationImpl { annotationImpls.append(annotationImpl) } } mapView.addAnnotations(annotationImpls) } /// 设置地图使其可以显示数组中所有的annotation, 如果数组中只有一个则直接设置地图中心为annotation的位置。 /// - Parameters: /// - annotations: 需要显示的annotation /// - animated: 是否执行动画 func showAnnotations(_ annotations: [AnnotationProtocol], animated: Bool) { var annotationImpls: [Any] = [] for annotation in annotations { if let annotationImpl = annotation.annotationImpl { annotationImpls.append(annotationImpl) } } mapView.showAnnotations(annotationImpls, animated: animated) } /// 从复用内存池中获取制定复用标识的annotation view /// - Parameter withIdentifier: 复用标识 /// - Returns: annotation view func dequeueReusableAnnotationView(withIdentifier: String) -> PinAnnotationViewProtocol? { if let annotationViewImpl = mapView.dequeueReusableAnnotationView(withIdentifier: withIdentifier) as? GaodePinAnnotationView { return PinAnnotationView(annotationViewImpl: annotationViewImpl) } return nil } } /// 高德地图代理实现 class MAMapViewDelegateImpl: NSObject, MAMapViewDelegate { ...... func mapView(_ mapView: MAMapView!, viewFor annotation: MAAnnotation!) -> MAAnnotationView! { if let pointAnnotation = annotation as? GaodePointAnnotation, let anno = pointAnnotation.annotation { if let pinAnnotationView = delegate?.mapView(mapViewProtocol, viewFor: anno)?.annotationViewImpl as? MAAnnotationView { return pinAnnotationView } return nil } return nil } } /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ...... /// 向地图窗口添加标注 /// - Parameter annotation: 要添加的标注 func addAnnotation(_ annotation: AnnotationProtocol) { if let annotationImpl = annotation.annotationImpl as? BMKAnnotation { mapView.addAnnotation(annotationImpl) } } /// 向地图窗口添加一组标注，需要实现MAMapViewDelegate的-mapView:viewForAnnotation:函数来生成标注对应的View /// - Parameter annotations: 要添加的标注数组 func addAnnotations(_ annotations: [AnnotationProtocol]) { var annotationImpls: [BMKAnnotation] = [] for annotation in annotations { if let annotationImpl = annotation.annotationImpl as? BMKAnnotation { annotationImpls.append(annotationImpl) } } mapView.addAnnotations(annotationImpls) } /// 设置地图使其可以显示数组中所有的annotation, 如果数组中只有一个则直接设置地图中心为annotation的位置。 /// - Parameters: /// - annotations: 需要显示的annotation /// - animated: 是否执行动画 func showAnnotations(_ annotations: [AnnotationProtocol], animated: Bool) { var annotationImpls: [BMKAnnotation] = [] for annotation in annotations { if let annotationImpl = annotation.annotationImpl as? BMKAnnotation { annotationImpls.append(annotationImpl) } } mapView.showAnnotations(annotationImpls, animated: animated) } /// 从复用内存池中获取制定复用标识的annotation view /// - Parameter withIdentifier: 复用标识 /// - Returns: annotation view func dequeueReusableAnnotationView(withIdentifier: String) -> PinAnnotationViewProtocol? { if let annotationViewImpl = mapView.dequeueReusableAnnotationView(withIdentifier: withIdentifier) as? BaiduPinAnnotationView { return PinAnnotationView(annotationViewImpl: annotationViewImpl) } return nil } } /// 百度地图代理实现 class BMKMapViewDelegateImpl: NSObject, BMKMapViewDelegate { ...... func mapView(_ mapView: BMKMapView!, viewFor annotation: BMKAnnotation!) -> BMKAnnotationView! { if let pointAnnotation = annotation as? BaiduPointAnnotation, let anno = pointAnnotation.annotation { if let pinAnnotationView = delegate?.mapView(mapViewProtocol, viewFor: anno)?.annotationViewImpl as? BMKAnnotationView{ return pinAnnotationView } } return nil } } 第5步：测试页面 class PointAnnotationViewController: UIViewController { /// 地图工厂 private lazy var factory: MapFactoryProtocol = { let factory = MapEngine.shared.getFactory()! return factory }() /// 地图实例 private lazy var mapView: MapViewProtocol = { // 显示地图 let mapView = factory.getMapView(frame: view.bounds) // 设置代理 mapView.delegate = self // 显示用户定位(放在设置代理之后，确保可以调用locationManager.requestAlwaysAuthorization()) mapView.showsUserLocation = true // 跟踪模式 mapView.userTrackingMode = .followWithHeading // 设置缩放 mapView.zoomLevel = 16 return mapView }() /// 标注数组 private lazy var annotations: Array = { var annotations = Array() let coordinates: [CLLocationCoordinate2D] = [ CLLocationCoordinate2D(latitude: 39.992520, longitude: 116.336170), CLLocationCoordinate2D(latitude: 39.978234, longitude: 116.352343), CLLocationCoordinate2D(latitude: 39.998293, longitude: 116.348904), CLLocationCoordinate2D(latitude: 40.004087, longitude: 116.353915), CLLocationCoordinate2D(latitude: 40.001442, longitude: 116.353915), CLLocationCoordinate2D(latitude: 39.989105, longitude: 116.360200), CLLocationCoordinate2D(latitude: 39.989098, longitude: 116.360201), CLLocationCoordinate2D(latitude: 39.998439, longitude: 116.324219), CLLocationCoordinate2D(latitude: 39.979590, longitude: 116.352792)] for (idx, coor) in coordinates.enumerated() { var anno = factory.getPointAnnotation() anno.coordinate = coor anno.title = String(idx) annotations.append(anno) } return annotations }() override func viewDidLoad() { super.viewDidLoad() view.backgroundColor = .white view.addSubview(mapView.getView()) } override func viewDidAppear(_ animated: Bool) { super.viewDidAppear(animated) mapView.addAnnotations(annotations) mapView.showAnnotations(annotations, animated: false) } } extension PointAnnotationViewController: MapViewDelegateProtocol { func mapView(_ mapView: MapViewProtocol, viewFor annotation: AnnotationProtocol) -> PinAnnotationViewProtocol? { if annotation.isKind(of: PointAnnotation.self) { let pointReuseIndetifier = \"pointReuseIndetifier\" var annotationView = mapView.dequeueReusableAnnotationView(withIdentifier: pointReuseIndetifier) if annotationView == nil { annotationView = factory.getPinAnnotationView(annotation: annotation, reuseIdentifier: pointReuseIndetifier) } annotationView!.canShowCallout = true annotationView!.animatesDrop = true annotationView!.draggable = true annotationView!.rightCalloutAccessoryView = UIButton(type: .detailDisclosure) let idx = annotations.index(of: annotation as! PointAnnotation) annotationView!.pinColor = PinAnnotationColor(rawValue: (idx ?? 3) % 3)! return annotationView } return nil } } "},"pages/地图聚合SDK/10_地图多边形实现.html":{"url":"pages/地图聚合SDK/10_地图多边形实现.html","title":"10.地图多边形实现","keywords":"","body":"10.地图多边形实现 十六、地图多边形实现 第1步：新增地图覆盖物协议、地图覆盖物Renderer协议 OverlayProtocol（地图覆盖物协议）、OverlayRendererProtocol（地图覆盖物Renderer协议） /// 地图覆盖物协议，所有地图的覆盖物需要实现 public protocol OverlayProtocol: NSObjectProtocol { /// 具体地图覆盖物实现 var overlayImpl: Any? { get } /// 构造方法 /// - Parameter annotationImpl: 具体地图覆盖物实现 init(overlayImpl: Any?) } /// 地图覆盖物Renderer协议 public protocol OverlayRendererProtocol: NSObjectProtocol { /// 填充颜色,默认是kMAOverlayRendererDefaultFillColor var fillColor: UIColor { get set } /// 笔触颜色,默认是kMAOverlayRendererDefaultStrokeColor var strokeColor: UIColor { get set } ///笔触宽度, 单位屏幕点坐标，默认是0 var lineWidth: CGFloat { get set } /// 具体地图标注view实现 var overRendererImpl: Any? { get } /// 构造方法 /// - Parameter overRendererImpl: 具体地图覆盖物Renderer实现 init(overRendererImpl: Any?) } 第2步：新增地图多边形实现、地图多边形Renderer实现 GaodePolygon（高德地图多边形）、GaodePolygonRenderer（高德地图多边形Renderer） BaiduPolygon（百度地图多边形）、BaiduPolygonRenderer（百度地图多边形Renderer） Polygon（通用多边形）、PolygonRenderer（通用多边形Renderer） /// 高德地图多边形 public class GaodePolygon: MAPolygon { /// 聚合多边形区域l weak var polygon: Polygon? } /// 高德地图多边形Renderer public class GaodePolygonRenderer: MAPolygonRenderer { } /// 百度多边形 public class BaiduPolygon: BMKPolygon { /// 聚合多边形区域l weak var polygon: Polygon? } /// 百度多边形Renderer public class BaiduPolygonRenderer: BMKPolygonView { } /// 通用多边形 public class Polygon: NSObject, OverlayProtocol { /// 具体地图覆盖物实现 private(set) public var overlayImpl: Any? /// 高德覆盖物 private var gaodePolygon: GaodePolygon? { if let overlay = overlayImpl as? GaodePolygon { return overlay } return nil } /// 百度覆盖物 private var baiduPolygon: BaiduPolygon? { if let overlay = overlayImpl as? BaiduPolygon { return overlay } return nil } /// 构造方法 /// - Parameter overlayImpl: 具体地图覆盖物实现 public required init(overlayImpl: Any?) { super.init() self.overlayImpl = overlayImpl gaodePolygon?.polygon = self baiduPolygon?.polygon = self } } /// 通用多边形Renderer public class PolygonRenderer: NSObject, OverlayRendererProtocol { /// 填充颜色,默认是kMAOverlayRendererDefaultFillColor public var fillColor: UIColor { get { if let overRenderer = gaodeOverRenderer { return overRenderer.fillColor } if let overRenderer = baiduOverRenderer { return overRenderer.fillColor } return .red } set { if let overRenderer = gaodeOverRenderer { overRenderer.fillColor = newValue } if let overRenderer = baiduOverRenderer { overRenderer.fillColor = newValue } } } /// 笔触颜色,默认是kMAOverlayRendererDefaultStrokeColor public var strokeColor: UIColor { get { if let overRenderer = gaodeOverRenderer { return overRenderer.strokeColor } if let overRenderer = baiduOverRenderer { return overRenderer.strokeColor } return .red } set { if let overRenderer = gaodeOverRenderer { overRenderer.strokeColor = newValue } if let overRenderer = baiduOverRenderer { overRenderer.strokeColor = newValue } } } ///笔触宽度, 单位屏幕点坐标，默认是0 public var lineWidth: CGFloat { get { if let overRenderer = gaodeOverRenderer { return overRenderer.lineWidth } if let overRenderer = baiduOverRenderer { return overRenderer.lineWidth } return 0 } set { if let overRenderer = gaodeOverRenderer { overRenderer.lineWidth = newValue } if let overRenderer = baiduOverRenderer { overRenderer.lineWidth = newValue } } } /// 具体地图覆盖物实现 public private(set) var overRendererImpl: Any? /// 高德地图覆盖物 private var gaodeOverRenderer: GaodePolygonRenderer? { return overRendererImpl as? GaodePolygonRenderer } /// 百度地图覆盖物 private var baiduOverRenderer: BaiduPolygonRenderer? { return overRendererImpl as? BaiduPolygonRenderer } /// 构造方法 /// - Parameter overRendererImpl: 具体地图覆盖物Renderer实现 public required init(overRendererImpl: Any?) { self.overRendererImpl = overRendererImpl } } 第3步：地图工厂标准修改及实现 /// 地图工厂标准 public protocol MapFactoryProtocol: NSObjectProtocol { ...... /// 根据经纬度坐标数据生成闭合多边形 /// - Parameters: /// - coordinates: 经纬度坐标点数据,coords对应的内存会拷贝,调用者负责该内存的释放 /// - count: 经纬度坐标点数组个数 /// - Returns: 新生成的多边形 func getPolygon(coordinates: UnsafeMutablePointer, count: UInt) -> Polygon /// 根据指定的多边形生成一个多边形Renderer /// - Parameter overlay: 指定的多边形数据对象 /// - Returns: 新生成的多边形Renderer func getPolygonRenderer(overlay: OverlayProtocol) -> OverlayRendererProtocol } /// 高德地图工厂 class GaodeMapFactory: NSObject, MapFactoryProtocol { ...... /// 根据经纬度坐标数据生成闭合多边形 /// - Parameters: /// - coordinates: 经纬度坐标点数据,coords对应的内存会拷贝,调用者负责该内存的释放 /// - count: 经纬度坐标点数组个数 /// - Returns: 新生成的多边形 func getPolygon(coordinates: UnsafeMutablePointer, count: UInt) -> Polygon { let polygonImpl = GaodePolygon(coordinates: coordinates, count: count) return Polygon(overlayImpl: polygonImpl) } /// 根据指定的多边形生成一个多边形Renderer /// - Parameter overlay: 指定的多边形数据对象 /// - Returns: 新生成的多边形Renderer func getPolygonRenderer(overlay: OverlayProtocol) -> OverlayRendererProtocol { var polygonImpl = GaodePolygon() if let overlayImplParam = overlay.overlayImpl as? GaodePolygon { polygonImpl = overlayImplParam } let polygonRendererImpl = GaodePolygonRenderer(overlay: polygonImpl) return PolygonRenderer(overRendererImpl: polygonRendererImpl) } } /// 百度地图工厂 class BaiduMapFactory: NSObject, MapFactoryProtocol { ...... /// 根据经纬度坐标数据生成闭合多边形 /// - Parameters: /// - coordinates: 经纬度坐标点数据,coords对应的内存会拷贝,调用者负责该内存的释放 /// - count: 经纬度坐标点数组个数 /// - Returns: 新生成的多边形 func getPolygon(coordinates: UnsafeMutablePointer, count: UInt) -> Polygon { let polygonImpl = BaiduPolygon(coordinates: coordinates, count: count) return Polygon(overlayImpl: polygonImpl) } /// 根据指定的多边形生成一个多边形Renderer /// - Parameter overlay: 指定的多边形数据对象 /// - Returns: 新生成的多边形Renderer func getPolygonRenderer(overlay: OverlayProtocol) -> OverlayRendererProtocol { var polygonImpl = BaiduPolygon() if let overlayImplParam = overlay.overlayImpl as? BaiduPolygon { polygonImpl = overlayImplParam } let polygonRendererImpl = BaiduPolygonRenderer(overlay: polygonImpl) return PolygonRenderer(overRendererImpl: polygonRendererImpl) } } 第4步：地图协议修改及实现 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 向地图窗口添加一组Overlay，需要实现MAMapViewDelegate的-mapView:rendererForOverlay:函数来生成标注对应的Renderer /// - Parameter overlays: 要添加的overlay数组 func addOverlays(overlays: [OverlayProtocol]) /// 设置地图使其可以显示数组中所有的overlay, 如果数组中只有一个则直接设置地图中心为overlay的位置。 /// - Parameters: /// - overlays: 需要显示的overlays /// - animated: 是否执行动画 func showOverlays(overlays: [OverlayProtocol], animated: Bool) } /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... /// 向地图窗口添加一组Overlay，需要实现MAMapViewDelegate的-mapView:rendererForOverlay:函数来生成标注对应的Renderer /// - Parameter overlays: 要添加的overlay数组 func addOverlays(overlays: [OverlayProtocol]) { var overlayImpls: [Any] = [] for overlay in overlays { if let overlayImpl = overlay.overlayImpl { overlayImpls.append(overlayImpl) } } mapView.addOverlays(overlayImpls) } /// 设置地图使其可以显示数组中所有的overlay, 如果数组中只有一个则直接设置地图中心为overlay的位置。 /// - Parameters: /// - overlays: 需要显示的overlays /// - animated: 是否执行动画 func showOverlays(overlays: [OverlayProtocol], animated: Bool) { var overlayImpls: [Any] = [] for overlay in overlays { if let overlayImpl = overlay.overlayImpl { overlayImpls.append(overlayImpl) } } mapView.showOverlays(overlayImpls, animated: animated) } } /// 高德地图代理实现 class MAMapViewDelegateImpl: NSObject, MAMapViewDelegate { ...... func mapView(_ mapView: MAMapView!, rendererFor overlay: MAOverlay!) -> MAOverlayRenderer! { if let polygonOverlay = overlay as? GaodePolygon, let poly = polygonOverlay.polygon { if let polygonRenderer = delegate?.mapView(mapViewProtocol, rendererFor: poly)?.overRendererImpl as? MAPolygonRenderer { return polygonRenderer } } return nil } } /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ...... /// 向地图窗口添加一组Overlay，需要实现MAMapViewDelegate的-mapView:rendererForOverlay:函数来生成标注对应的Renderer /// - Parameter overlays: 要添加的overlay数组 func addOverlays(overlays: [OverlayProtocol]) { var overlayImpls: [BMKOverlay] = [] for overlay in overlays { if let overlayImpl = overlay.overlayImpl as? BMKOverlay { overlayImpls.append(overlayImpl) } } mapView.addOverlays(overlayImpls) } /// 设置地图使其可以显示数组中所有的overlay, 如果数组中只有一个则直接设置地图中心为overlay的位置。 /// - Parameters: /// - overlays: 需要显示的overlays /// - animated: 是否执行动画 func showOverlays(overlays: [OverlayProtocol], animated: Bool) { var overlayImpls: [BMKOverlay] = [] for overlay in overlays { if let overlayImpl = overlay.overlayImpl as? BMKOverlay { overlayImpls.append(overlayImpl) } } } } /// 百度地图代理实现 class BMKMapViewDelegateImpl: NSObject, BMKMapViewDelegate { ...... func mapView(_ mapView: BMKMapView!, viewFor overlay: BMKOverlay!) -> BMKOverlayView! { if let polygonOverlay = overlay as? BaiduPolygon, let poly = polygonOverlay.polygon { if let polygonRenderer = delegate?.mapView(mapViewProtocol, rendererFor: poly)?.overRendererImpl as? BMKPolygonView { return polygonRenderer } } return nil } } 第5步：测试页面 class PolygonViewController: UIViewController { /// 地图工厂 private lazy var factory: MapFactoryProtocol = { let factory = MapEngine.shared.getFactory()! return factory }() /// 地图实例 private lazy var mapView: MapViewProtocol = { // 显示地图 let mapView = factory.getMapView(frame: view.bounds) // 设置代理 mapView.delegate = self // 显示用户定位(放在设置代理之后，确保可以调用locationManager.requestAlwaysAuthorization()) mapView.showsUserLocation = true // 跟踪模式 mapView.userTrackingMode = .followWithHeading // 设置缩放 mapView.zoomLevel = 16 return mapView }() /// 标注数组 private lazy var polygons: Array = { var polygons = Array() var polygonCoordinates: [CLLocationCoordinate2D] = [ CLLocationCoordinate2D(latitude: 39.781892, longitude: 116.283413), CLLocationCoordinate2D(latitude: 39.787600, longitude: 116.391842), CLLocationCoordinate2D(latitude: 39.733187, longitude: 116.417932), CLLocationCoordinate2D(latitude: 39.704653, longitude: 116.338255)] let polygon = factory.getPolygon(coordinates: &polygonCoordinates, count: UInt(polygonCoordinates.count)) polygons.append(polygon) return polygons }() override func viewDidLoad() { super.viewDidLoad() view.backgroundColor = .white view.addSubview(mapView.getView()) } override func viewDidAppear(_ animated: Bool) { super.viewDidAppear(animated) mapView.addOverlays(overlays: polygons) mapView.showOverlays(overlays: polygons, animated: false) } } extension PolygonViewController: MapViewDelegateProtocol { func mapView(_ mapView: MapViewProtocol, rendererFor overlay: OverlayProtocol) -> OverlayRendererProtocol? { if overlay.isKind(of: Polygon.self) { let renderer = factory.getPolygonRenderer(overlay: overlay) renderer.lineWidth = 8.0 renderer.strokeColor = UIColor.magenta renderer.fillColor = UIColor.yellow.withAlphaComponent(0.4) return renderer } return nil } } "},"pages/第三方分享/微信分享.html":{"url":"pages/第三方分享/微信分享.html","title":"微信分享","keywords":"","body":"微信分享 微信开发平台 https://open.weixin.qq.com/ 官方Demo运行 1.在资源下载页面下载好最新的iOS开发工具包和范例代码。 2.将下载好的“OpenSDK1.8.9”拷贝至“ReleaseSample”下。 3.打开SDKSample，删除显示红色的“SDKExport”，右键工程文件通过“Add Files ...”将“OpenSDK1.8.9”导入工程。 4.点击SDKSample，选中Target,在Frameworks下增加WebKit.framework。 5.更新可用的bundleID及证书，真机就可以运行了。 当时最新的是（1.8.9版本，包含支付功能），这里选择包含支付版本的，因为官方Demo使用的是包含支付功能的。 “范例代码”不包含SDK，所以需要手动导入；“范例代码”没有依赖WebKit.framework,也需要手动导入；真机运行，需要更新可用的bundleID及证书。 新建工程 1.点击File，选择New->Project->App，一直Next直到完成。 2.删除SceneDelegate，直接Move To Trash。 3.删除AppDelegate中关于Scene的报错代码。 4.删除Info.plist的UIApplicationSceneManifest及Value。 5.在AppDelegate中增加如下代码。 var window: UIWindow? 没有第5步，工程运行会是黑屏，控制台会显示“The app delegate must implement the window property if it wants to use a main storyboard file.”。 集成OpenSDK（Swift） 官方接入点指南 1.首先新建Swift工程，工程名wechatshare，步骤如上。 2.右键“wechatshare”group，通过“Add Files ...”将“OpenSDK1.8.9”导入工程。 注意勾选“Copy items if needed”。 3.新建Swift桥接OC的文件，有一下两种方式： 3.1. 新建一个OC文件自动创建XXX-Bridging-Header.h。 3.2. 右键“wechatshare”group，New File...->Header File->wechatshare-Bridging-Header.h， 然后点击Target->Build Settings->搜索Objective-C Bridging Header->填写wechatshare/wecha tshare-Bridging-Header.h，这个是项目的相对路径。 4.测试sdk的使用 4.1.配置真机证书，设置iOS Deployment Target为真机支持的系统版本，新建工程的时候这里是最新的iOS版本，可能手机不是最新的，这里修改下就好。 4.2.wechatshare-Bridging-Header.h增加如下代码 #import \"WXApi.h\" 4.3.AppDelegate.swift增加如下代码 class AppDelegate: UIResponder, UIApplicationDelegate, WXApiDelegate { var window: UIWindow? func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool { // Override point for customization after application launch. WXApi.registerApp(\"\", universalLink: \"\") return true } } 4.4.运行工程，报错误： Undefined symbols for architecture arm64: \"operator delete[](void*)\", referenced from: +[WeChatApiUtil EncodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil NsDataEncodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil DecodeWithBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil DecodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) \"operator new[](unsigned long)\", referenced from: +[WeChatApiUtil EncodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil NsDataEncodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil DecodeWithBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil DecodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) \"_OBJC_CLASS_$_WKWebView\", referenced from: objc-class-ref in libWeChatSDK.a(WapAuthHandler.o) \"_OBJC_CLASS_$_WKWebViewConfiguration\", referenced from: objc-class-ref in libWeChatSDK.a(WapAuthHandler.o) ld: symbol(s) not found for architecture arm64 clang: error: linker command failed with exit code 1 (use -v to see invocation) 4.5.修复报错： 开发者需要在工程中链接上:Security.framework，CoreGraphics.framework，WebKit.framework，libc++.tbd。 官方文档是上没有写需要接入libc++.tbd，但是实际是需要的，要不然编译会遇到错误 5.准备appid及universalLink 5.1.登录开发者者中心，确保app开启了Associated Domains的功能。 5.2.更新证书描述文件，确保包含了Associated Domains的功能。 5.4.在项目中配置applinks:xxxx。 5.5.去微信开发平台注册用户获得appid。 5.6.更新AppDelegate.swift class AppDelegate: UIResponder, UIApplicationDelegate, WXApiDelegate { var window: UIWindow? func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool { // Override point for customization after application launch. WXApi.registerApp(\"wx536bce0bc6a71ffd\", universalLink: \"https://huya.gq/demo.html\") return true } } // 处理universallink extension AppDelegate { func application(_ application: UIApplication, handleOpen url: URL) -> Bool { return WXApi.handleOpen(url, delegate: self) } func application(_ application: UIApplication, open url: URL, sourceApplication: String?, annotation: Any) -> Bool { return WXApi.handleOpen(url, delegate: self) } func application(_ application: UIApplication, continue userActivity: NSUserActivity, restorationHandler: @escaping ([UIUserActivityRestoring]?) -> Void) -> Bool { return WXApi.handleOpenUniversalLink(userActivity, delegate: self) } } 6.分享超链接 6.1.在ViewController.swift增加如下代码，点击按钮触发分享： /// 发送Link消息给微信 @IBAction @objc private func sendLinkContent() { let webpageObject = WXWebpageObject() webpageObject.webpageUrl = Constant.kLinkURL let message = WXMediaMessage() message.title = Constant.kLinkTitle; message.description = Constant.kLinkDescription; message.setThumbImage(UIImage(named: \"res2.png\")!) //分享后展示图片，没有就显示大大的问号图片。 message.mediaObject = webpageObject; let req = SendMessageToWXReq(); req.bText = false; req.message = message; req.scene = Int32(WXSceneSession.rawValue);//WXSceneSession; WXApi.send(req) { (result) in} } 6.2.发现报错： -canOpenURL: failed for URL: \"weixinULAPI://\" - error: \"This app is not allowed to query for scheme weixinulapi\" 6.3.解决报错 打开info.plist,增加如下代码： LSApplicationQueriesSchemes weixin weixinULAPI CFBundleURLTypes CFBundleURLName weixin CFBundleURLSchemes wx536bce0bc6a7**** 6.4.接着报错： *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '+[WXApi genExtraUrlByReq:withAppData:]: unrecognized selector sent to class 0x100286398' 6.5.解决报错 在你的工程文件中选择 Build Setting，在\"Other Linker Flags\"中加入\"-ObjC -all_load\" 代码 "},"pages/第三方分享/Messenger分享.html":{"url":"pages/第三方分享/Messenger分享.html","title":"Messenger分享","keywords":"","body":"Messenger分享 分享到 iOS 和 Android 版 Messenger iOS分享到国外社交APP调研 检查用户是否安装了iOS 9的Facebook Messenger 应用的 iOS9 兼容准备 遇到个问题：我之前以为Facebook和Messe是一个App，导致在没有装Messenger的情况一下一直分享报错：“说弹窗没有意义”，后来才发现我没有装Messenger，问题解决！ "},"pages/第三方分享/LinkedIn分享.html":{"url":"pages/第三方分享/LinkedIn分享.html","title":"LinkedIn分享","keywords":"","body":"LinkedIn分享 iOS LinkedIn登录及信息获取 如何使用 OAuth 2.0 将 LinkedIn 集成入 iOS 应用 linkedin开发者中心 通过iOS应用在LinkedIn上共享URL 如何使用 OAuth 2.0 将 LinkedIn 集成入 iOS 应用 "},"pages/iOS组件化/参考链接.html":{"url":"pages/iOS组件化/参考链接.html","title":"参考链接","keywords":"","body":"参考链接 iOS组件化之架构通用设计 写iOS SDK注意事项 BeeHive CocoaPods 创建私有仓库（ObjC） iOS组件化（下篇）-加载XIB、图片资源 iOS开发——组件化及去Mode化方案 CTModule Github搜索BundleName发现了不少好的仓库 基于CTMediator的组件化中间件 LocalizedStringKit UCPlanKit YXResources.h ZJBundleRes.h ImageCachePool.m LBResourceBundleManager NSBundle+DFBundle.h ZUXBundle.h NSBundle+DFBundle.h NSBundle+Resource.h NSBundle+Extension.h UIImage+QIBundle.m CCCommentsHeader.h UCUIMacro.h "},"pages/iOS组件化/创建私有库.html":{"url":"pages/iOS组件化/创建私有库.html","title":"1.创建私有库","keywords":"","body":"创建私有库 一、创建存放podsepc的私有仓库 在自己的git服务器中创建一个保存podspec的仓库：[repo name]。 创建完仓库之后获取仓库地址 在terminal中执行pod的指令将仓库添加到本地的pod repo中。 pod repo add [repo name] [url] 添加完成之后，在~/.cocoapods/repos中就可以看到名称为[repo name]的文件夹，这就是我们的私有pod仓库。 当然也可以使用pod repo remove [repo name]移除repo。 二、创建存放源码的仓库并推送到私有仓库 在git服务器上再创建一个仓库用于存放源代码。 在terminal中执行pod lib create [lib name]创建一个cocoapods的demo工程。 执行之后会从git克隆一个模板，并会问几个问题，依次按照需求选择即可，完成之后会打开一个Xcode project。 三、编辑podspec文件 Pod::Spec.new do |s| s.name = '名字' s.version = '版本号 需要和git tag保持一致' s.summary = '简述' # This description is used to generate tags and improve search results. # * Think: What does it do? Why did you write it? What is the focus? # * Try to keep it short, snappy and to the point. # * Write the description between the DESC delimiters below. # * Finally, don't worry about the indent, CocoaPods strips it! s.description = 'MIT', :file => 'LICENSE' } s.author = { 'changqing.chen' => 'chenchangqing198@126.com' } s.source = { :git => 'git仓库地址', :tag => s.version.to_s } # s.social_media_url = 'https://twitter.com/' s.ios.deployment_target = '9.0' s.source_files = 'utlogin/Classes/**/*' # s.resource_bundles = { # 'utlogin' => ['utlogin/Assets/*.png'] # } # s.public_header_files = 'Pod/Classes/**/*.h' # s.frameworks = 'UIKit', 'MapKit' # s.dependency 'AFNetworking', '~> 2.3' end 四、校验podspec文件 编辑完成之后，使用pod lib lint来验证podspec填写的准确性，可以选择参数: --verbose查看整个过程 --allow-warnings允许一些警告通过验证，如果验证出错，而project build success可以尝试添加这个参数 --source如果依赖的库是一个私有仓库创建的库，可以使用这个参数指定私有仓库的podspec仓库，除此之外最好将cocoapods公有库的source也指定一下 指定source pod lib lint --sources='[私有podsepec仓库地址],https://github.com/CocoaPods/Specs' --verbose --allow-warnings --no-clean 不指定source pod lib lint --verbose --allow-warnings --no-clean 执行问pod lib lint，看到[lib name] passed validation后就算通过校验。 五、推送至私有仓库 编辑.gitignore，将# Pods/修改为Pods/，忽略Pods文件夹，第三方代码就不会上传。 通过验证后，将源码推送至git仓库： git init git add . git commit -am 'desc' git remote add origin 'url' git push origin master git tag 'tag' git push --tags 将podsepc添加到私有repo中使用命令： pod repo push [repo name] [name.podspec] --verbose --allow-warnings "},"pages/iOS组件化/使用中间件.html":{"url":"pages/iOS组件化/使用中间件.html","title":"2.使用中间件","keywords":"","body":"2.使用中间件 CTMediator 本文只介绍ObjC如何使用CTMediator。 一、新建业务组件Target 通过下载CTMediator的Demo，每个组件都会有一个Target的类，用于提供对外方法的实现。 Target类的命名规则：“Target_[lib name]”，例如：Target_A，A是组件名称，Target是固定不变的。 Target方法命名规则：“Action_[method name]”，例如：Action_ViewController，ViewController是方法名称，Action是固定不变的。 @implementation Target_testlib - (UIViewController *)Action_OneKeyLoginVC:(NSDictionary *)params { typedef void (^CallbackType)(NSString *); CallbackType callback = params[@\"callback\"]; if (callback) { callback(@\"success\"); } UTOneKeyLoginVC *viewController = [[UTOneKeyLoginVC alloc] init]; return viewController; } @end 二、新建业务组件对外接口 CTMediator是通过Target-Action的方式实现，所以我们只需要给CTMediator增加Category，在Category写组件对外的方法和实现即可。 在git服务器创建中间件仓库，通过pod lib create [中间件名称]创建中间件工程。 编辑podspec文件，增加s.dependency 'CTMediator'，然后执行pod install，完成CTMediator的依赖。 通过新增CTMediator的分类，实现组件对外的接口。 分类的命名：CTMediator+[lib name]，例如：CTMediator+testlib.h。 方法的命名：[lib name]_[method name]，例如：utlogin_OneKeyLoginVCWithCallback。 为CTMediator增加分类，编写业务组件的接口方法： @implementation CTMediator (utlogin) /// 一键登录页面 /// @param callback 回调 - (UIViewController *)utlogin_OneKeyLoginVCWithCallback:(void(^)(NSString *result))callback; { NSMutableDictionary *params = [[NSMutableDictionary alloc] init]; params[@\"callback\"] = callback; return [self performTarget:@\"utlogin\" action:@\"OneKeyLoginVC\" params:params shouldCacheTarget:NO]; } @end 这样就完成了组件对外的接口了，宿主工程可以通过依赖中间件直接调用到组件的Target_testlib的实现。 三、新建宿主工程 使用xcode创建宿主工程，在通过pod init创建Podfile文件。 using-cocoapods 编辑Podfile文件： # Uncomment the next line to define a global platform for your project # platform :ios, '9.0' source '私有仓库地址' source 'https://github.com/CocoaPods/Specs.git' target 'utopia' do # Comment the next line if you don't want to use dynamic frameworks use_frameworks! use_modular_headers! # Pods for utopia pod 'testlib'#业务组件 pod 'testmediatro'#中间件 end 执行pod install。 "},"pages/iOS组件化/加载图片资源.html":{"url":"pages/iOS组件化/加载图片资源.html","title":"3.加载图片资源","keywords":"","body":"3.加载图片资源 NSBundle+Resource.h #import NS_ASSUME_NONNULL_BEGIN @interface NSBundle (Resource) /** 获取资源的bundle @param className framework中的类名 @param bundleName 资源bundle的名字 */ + (instancetype)resourceBundleWithClassName:(NSString *)className bundleName:(NSString *)bundleName; /** 获取资源的bundle,默认bundle为当前类所在的bundle @param bundleName 资源bundle的名字 */ + (instancetype)resourceBundleWithBundleName:(NSString *)bundleName; /** 根据path来加载bundle 获取资源的bundle @param frameworkName 为资源所在的framework的名字 @param bundleName 资源bundle的名字 */ + (instancetype)resourceBundleWithFramework:(NSString *)frameworkName bundleName:(NSString *)bundleName; /** 这个方法会直接给出 framework的bundle @param className framework中的任意类名 */ + (instancetype)getFrameworkBundleWithClassName:(NSString *)className; /** 这个方法会直接给出 framework的bundle @param frameworkName framework的名称 */ + (instancetype)getFrameworkBundleWithFramework:(NSString *)frameworkName; /** 加载main bundle下 资源bundle @param bundleName 资源bundle名字 */ + (instancetype)getMainBundleWithResourceBundle:(NSString *)bundleName; @end NS_ASSUME_NONNULL_END NSBundle+Resource.m #import \"NSBundle+Resource.h\" @implementation NSBundle (Resource) + (instancetype)resourceBundleWithClassName:(NSString *)className bundleName:(NSString *)bundleName{ if (!className) { NSAssert(!className, @\"获取资源路径bundle时，类名为空\"); return nil; } NSBundle *frameworkBundle = [NSBundle bundleForClass:NSClassFromString(className)]; if (!frameworkBundle) { NSAssert(!className, @\"获取资源路径bundle时，获取framework的bundle为空\"); return nil; } NSURL *frameworkBundleUrl = [frameworkBundle URLForResource:bundleName withExtension:@\"bundle\"]; if (!frameworkBundleUrl) { NSAssert(!className, @\"获取资源路径bundle时，获取frameworkbundleURL为空\"); return nil; } return [self bundleWithURL:frameworkBundleUrl]; } + (instancetype)resourceBundleWithBundleName:(NSString *)bundleName{ return [self resourceBundleWithClassName:NSStringFromClass(self) bundleName:bundleName]; } + (instancetype)resourceBundleWithFramework:(NSString *)frameworkName bundleName:(NSString *)bundleName{ NSURL *frameworksURL = [[NSBundle mainBundle] URLForResource:@\"Frameworks\" withExtension:nil]; if (!frameworkName) { NSAssert(!frameworkName, @\"获取资源路径bundle时，frameworkName为空\"); } NSURL* libFrameworkURL = [frameworksURL URLByAppendingPathComponent:frameworkName]; libFrameworkURL = [libFrameworkURL URLByAppendingPathExtension:@\"framework\"]; NSBundle *libFrameworkBundle = [NSBundle bundleWithURL:libFrameworkURL]; if (!libFrameworkBundle) { NSAssert(!libFrameworkBundle, @\"获取资源路径bundle时，获取framework的bundle为空\"); } NSURL* libResourceBundleUrl = [libFrameworkBundle URLForResource:bundleName withExtension:@\"bundle\"]; return [self bundleWithURL:libResourceBundleUrl]; } + (instancetype)getFrameworkBundleWithClassName:(NSString *)className{ if (!className) { NSAssert(!className, @\"获取资源路径bundle时，类名为空\"); return nil; } NSBundle *frameworkBundle = [self bundleForClass:NSClassFromString(className)]; return frameworkBundle; } + (instancetype)getFrameworkBundleWithFramework:(NSString *)frameworkName{ NSURL *frameworksURL = [[NSBundle mainBundle] URLForResource:@\"Frameworks\" withExtension:nil]; if (!frameworkName) { NSAssert(!frameworkName, @\"获取资源路径bundle时，frameworkName为空\"); } NSURL* libFrameworkURL = [frameworksURL URLByAppendingPathComponent:frameworkName]; libFrameworkURL = [libFrameworkURL URLByAppendingPathExtension:@\"framework\"]; NSBundle *libFrameworkBundle = [self bundleWithURL:libFrameworkURL]; return libFrameworkBundle; } + (instancetype)getMainBundleWithResourceBundle:(NSString *)bundleName{ NSBundle *mainBundle = [NSBundle mainBundle]; NSURL *resourceUrl = [mainBundle URLForResource:bundleName withExtension:@\"bundle\"]; return [self bundleWithURL:resourceUrl]; } @end UIImage+Resource.h #import NS_ASSUME_NONNULL_BEGIN @interface UIImage (Resource) /** 通过图片名称，图片bundle所在framework的类名和图片所在bundle 来加载图片 @param imageName 图片名称 @param className 图片所在framework中任意一个类的名称 @param bundleName 图片所在 bundle */ + (UIImage *)imageNamed:(NSString *)imageName className:(NSString *)className bundleName:(NSString *)bundleName; /** 通过图片名称，图片所在framework的类名 此时图片直接位于framework bundle下 @param imageName 图片名称 @param className 图片所在framework中任意一个类的名称 */ + (UIImage *)imageNamed:(NSString *)imageName className:(NSString *)className; /** 通过图片名称，图片所在framework的类名 此时图片直接位于framework bundle下 @param imageName 图片名称 @param frameworkName 图片所在framework中任意一个类的名称 */ + (UIImage *)imageNamed:(NSString *)imageName frameworkName:(NSString *)frameworkName; /** 通过图片名称，图片bundle所在framework 和 图片所在的bundle 来加载图片 @param imageName 图片名称 @param frameworkName 框架名称 @param bundleName bundle 名字 */ + (UIImage *)imageNamed:(NSString *)imageName framework:(NSString *)frameworkName bundleName:(NSString *)bundleName; /** 加载main bundle下的图片 @param imageName 图片名称 @param bundleName bundle名称 */ + (UIImage *)imageNamed:(NSString *)imageName bundleName:(NSString *)bundleName; @end NS_ASSUME_NONNULL_END UIImage+Resource.m #import \"UIImage+Resource.h\" #import \"NSBundle+Resource.h\" @implementation UIImage (Resource) + (UIImage *)imageNamed:(NSString *)imageName className:(NSString *)className bundleName:(NSString *)bundleName{ NSBundle *bundle = [NSBundle resourceBundleWithClassName:className bundleName:bundleName]; UIImage *image = [UIImage imageNamed:imageName inBundle:bundle compatibleWithTraitCollection:nil]; return image; } + (UIImage *)imageNamed:(NSString *)imageName framework:(NSString *)frameworkName bundleName:(NSString *)bundleName{ NSBundle *bundle = [NSBundle resourceBundleWithFramework:frameworkName bundleName:bundleName]; UIImage *image = [UIImage imageNamed:imageName inBundle:bundle compatibleWithTraitCollection:nil]; return image; } + (UIImage *)imageNamed:(NSString *)imageName className:(NSString *)className{ NSBundle *bundle = [NSBundle getFrameworkBundleWithClassName:className]; UIImage *image = [UIImage imageNamed:imageName inBundle:bundle compatibleWithTraitCollection:nil]; return image; } + (UIImage *)imageNamed:(NSString *)imageName frameworkName:(NSString *)frameworkName{ NSBundle *bundle = [NSBundle getFrameworkBundleWithFramework:frameworkName]; UIImage *image = [UIImage imageNamed:imageName inBundle:bundle compatibleWithTraitCollection:nil]; return image; } + (UIImage *)imageNamed:(NSString *)imageName bundleName:(NSString *)bundleName{ if (!imageName || !bundleName) { NSAssert(!imageName || !bundleName, @\"image name 或者 bundle name 为空\"); return nil; } NSBundle *bundle = [NSBundle getMainBundleWithResourceBundle:bundleName]; UIImage *image = [UIImage imageNamed:imageName inBundle:bundle compatibleWithTraitCollection:nil]; return image; } @end "},"pages/其他/长城汽车Swift编程规约.html":{"url":"pages/其他/长城汽车Swift编程规约.html","title":"长城汽车Swift编程规约","keywords":"","body":"1.长城汽车Swift编程规约 前言 好的代码有一些特性：简明，自我解释，优秀的组织，良好的文档，良好的命名，优秀的设计以及可以被久经考验。参与长城系列APP开发的团队成员应严格遵照规约编写代码。规约会越来越完善，初期先按照以下规范。 第一次编辑时间:2020-01-28 核心原则 最重要的目标：每个元素都能够准确清晰的表达出它的含义。做出 API 设计、声明后要检查在上下文中是否足够清晰明白。 清晰比简洁重要。虽然 swift 代码可以被写得很简短，但是让代码尽量少不是 swift 的目标。简洁的代码来源于安全、强大的类型系统和其他一些语言特性减少了不必要的模板代码。而不是主观上写出最少的代码。 为每一个声明写注释文档。编写文档过程中获得的理解可以对设计产生深远的影响，所以不要回避拖延。 如果你不能很好的描述 API 的功能，很可能这个 API 的设计就是有问题的。 命名规约 不要使用约定命名样式代替访问控制 如果要控制访问权限应该使用访问控制（internal、fileprivate、private），不用使用自定义的命名方式来区分，比如在方法前前下划线表示私有。 只有在极端的情况下才会采用这种自定义命名表示。比如有一个方法只是为了某个模板调用才公开的，这种情况下本意是私有的，但是又必须声明成 public，可以使用自定义的命名惯例。 代码中的命名严禁使用拼音与英文混合的方式，更不允许直接使用中文的方式。 ✅ var productDiskDataArray: Array? var productDiskDataString: String! ❌ var chanpinDataArray: Array? var chanpinDataString: String! 类, 结构体, 枚举, 协议命名使用 UpperCamelCase 风格，必须遵从驼峰形式。特别注意类名开头大写。 ✅ class GWElecFenceFlowLayout: UICollectionViewFlowLayout ❌ class gwElecFenceFlowLayout: UICollectionViewFlowLayout 资源文件按照匈牙利命名法。 模块名+图片特征描述+状态 描述清晰有章法即可。 ✅ nav_add_normal@2X.png ❌ navAddNormal@2X.png NAV_ADD_NORMAL@2x.png 方法名、参数名、成员变量、局部变量都统一使用 lowerCamelCase 风格，必须遵从驼峰形式。 ✅ func viewDidLoad() func tableView(_ tableView: UITableView, numberOfRowsInSection section: Int) -> Int let automaticDimension: CGFloat let gradientShapeLayer = CAShapeLayer() ❌ func ViewDidLoad() func tableView(_ tableV: UITableView, numberOfRowsInSection section: Int) -> Int var FILLCOLOR: CGColor? let gradientshapeLayer = CAShapeLayer() 全局常量就正常变量一样使用匈牙利命名方式，不要在前面加上 g、k 或其他特别的格式。 ✅ let secondsPerMinute = 60 ❌ let SecondsPerMinute = 60 let kSecondsPerMinute = 60 let gSecondsPerMinute = 60 let SECONDS_PER_MINUTE = 60 杜绝完全不规范的缩写，避免望文不知义。 ❌ var abcAry: Array? 保证英文拼写正确 ❌ class AysncDat: NSObject 单例对象一般命名为 shared 或者 default。 ✅ /// 路由单例 public static let shared = GWNavigator() /// 屏蔽实现方式(可根据具体使用情况灵活调整) private override init() {} 格式规约 括号 非空的 block 花括号默认使用 K&R style。比如： while (x == y) { something() somethingelse() } 除了一些 Swift 特别要求的情况： 左花括号（ { ）前的代码不会换行，除非超过前面提到的代码长度超过限制。 左花括号后是一个换行，除非： 后面要声明闭包的参数，改为在 in 关键字后面换行。 符合每行只声明一件事里情况，忽略换行，把内容写在一行里。 如果是空的 block ，直接声明为 { }。 如果右花括号（ } ）结束了一个声明，后面接上一个换行。比如如果右花括号后面跟的是 else ，那么后面就不会跟换行，而会写成这样 } else { 衔接。 每行只声明一件事 每行最多只声明一件事，每行结尾用换行分隔。除非结尾跟的是一个总共只有一行声明的闭包。 ✅ guard let value = value else { return 0 } defer { file.close() } switch someEnum { case .first: return 5 case .second: return 10 case .third: return 20 } let squares = numbers.map { $0 * $0 } var someProperty: Int { get { return otherObject.property } set { otherObject.property = newValue } } var someProperty: Int { return otherObject.somethingElse() } required init?(coder aDecoder: NSCoder) { fatalError(\"no coder\") } 如果闭包是提前返回一个值，写在一行里可读性就会好一些。如果是一个正常的操作，可以视情况是否写在一行里。因为未来也有可能里面再增加代码的操作。 代码换行 代码中的空格 除了语言或者其他样式的要求，文字和注释之外，一个Unicode空格也只出现在以下地方: 条件关键字后面和跟着的括号 ✅ if (x == 0 && y == 0) || z == 0 { // ... } ❌ if(x == 0 && y == 0) || z == 0 { // ... } 如果闭包中的代码在同一行，左花括号的前面、后面，右花括号的前面有空格 ✅ let nonNegativeCubes = numbers.map { $0 * $0 * $0 }.filter { $0 >= 0 } ❌ let nonNegativeCubes = numbers.map { $0 * $0 * $0 } .filter { $0 >= 0 } ❌ let nonNegativeCubes = numbers.map{$0 * $0 * $0}.filter{$0 >= 0} 在任何二元或三元运算符的两边 还有以下的情况： 使用于赋值，初始化变量、属性，默认参数的等号两边。 ✅ var x = 5 func sum(_ numbers: [Int], initialValue: Int = 0) { // ... } ❌ var x=5 func sum(_ numbers: [Int], initialValue: Int=0) { // ... } 表示在协议中表示合成类型的 & 两边。 ✅ func sayHappyBirthday(to person: NameProviding & AgeProviding) { // ... } ❌ func sayHappyBirthday(to person: NameProviding&AgeProviding) { // ... } 自定义运算符的两边。 ✅ static func == (lhs: MyType, rhs: MyType) -> Bool { // ... } ❌ static func ==(lhs: MyType, rhs: MyType) -> Bool { // ... } 表示返回值的 -> 两边。 ✅ func sum(_ numbers: [Int]) -> Int { // ... } ❌ func sum(_ numbers: [Int])->Int { // ... } 例外：表示引用值、成员的点两边没有空格。 ✅ let width = view.bounds.width ❌ let width = view . bounds . width 例外：表示区域范围的 .. ✅ for number in 1...5 { // ... } ❌ let substring = string[index.. 参数列表、数组、tuple、字典里的逗号后面有一个空格 ✅ let numbers = [1, 2, 3] ❌ let numbers = [1,2,3] let numbers = [1 ,2 ,3] let numbers = [1 , 2 , 3] 冒号的后面有一个空格 ✅ // 类型声明 struct HashTable: Collection { // ... } struct AnyEquatable: Equatable { // ... } // 参数标签 let tuple: (x: Int, y: Int) func sum(_ numbers: [Int]) { // ... } // 变量声明 let number: Int = 5 // 字典声明 var nameAgeMap: [String: Int] = [] // 字典字面量 let nameAgeMap = [\"Ed\": 40, \"Timmy\": 9] 代码后的注释符号 // 与代码有两个空格距离 ✅ let initialFactor = 2 // Warm up the modulator. ❌ let initialFactor = 2 // Warm up the modulator. 表示字典、数组字面量的中括号外面有一个空格 ✅ let numbers = [1, 2, 3] ❌ let numbers = [ 1, 2, 3 ] 禁止变量、属性水平对齐 水平对齐是明确禁止的，除非是在写明显的表格数据时，省略对齐会损害可读性。引入水平对齐后，如果添加一个新的成员可能会需要其他成员再对齐一次，这给维护增加了负担。 ✅ struct DataPoint { var value: Int var primaryColor: UIColor } ❌ struct DataPoint { var value: Int var primaryColor: UIColor } 空行逻辑 在组织代码逻辑关系时，可以用空行隔开进行分组。 函数结尾不空行 函数内作用不同代码块空一行 规范里其他地方要求有空行的地方。 括号 最顶级的 if、guard、while、switch 的条件不使用括号。 ✅ if x == 0 { print(\"x is zero\") } if (x == 0 || y == 1) && z == 2 { print(\"...\") } ❌ if (x == 0) { print(\"x is zero\") } if ((x == 0 || y == 1) && z == 2) { print(\"...\") } 在有复杂的条件表达式，只有作者和 review 的人同时认为省略括号不会影响代码的可读性才会省略。不能假设每个读者都完全了解对 swift 的运算符优先级，所以这种情况下的括号提示用户的计算优先级是合理的。 集合处理 不要在 forin 循环里进行元素的 remove/add 操作。 ❌ var someInts:[Int] = [10, 20, 30] for index in someInts { someInts.insert(44 + index, at: index) } 并发处理 获取单例对象需要保证线程安全，其中的方法也要保证线程安全。 创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。 推荐: dispatch_queue_t gwhUpdateQueue = dispatch_queue_create(\"\", DISPATCH_QUEUE_SERIAL); let queue = DispatchQueue(label: \"com.gwh.app.update\", attributes: .concurrent) 高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁;能锁区块，就不要锁整个方法体;能用对象锁，就不要用类锁。 对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁。 并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用 version 作为更新依据。 控制语句 提前返回使用 guard ✅ func discombobulate(_ values: [Int]) throws -> Int { guard let first = values.first else { throw DiscombobulationError.arrayWasEmpty } guard first >= 0 else { throw DiscombobulationError.negativeEnergy } var result = 0 for value in values { result += invertedCombobulatoryFactory(of: value) } return result } ❌ func discombobulate(_ values: [Int]) throws -> Int { if let first = values.first { if first >= 0 { var result = 0 for value in values { result += invertedCombobulatoryFactor(of: value) } return result } else { throw DiscombobulationError.negativeEnergy } } else { throw DiscombobulationError.arrayWasEmpty } } for-where 循环 如果整个 for 循环在函数体顶部只有一个 if 判断，使用 for where 替换： ✅ for item in collection where item.hasProperty { // ... } ❌ for item in collection { if item.hasProperty { // ... } } Switch 中的 fallthrough Switch 中如果有几个 case 都对应相同的逻辑，case 使用逗号连接条件，而不是使用 fallthrough： ✅ switch value { case 1: print(\"one\") case 2...4: print(\"two to four\") case 5, 7: print(\"five or seven\") default: break } ❌ switch value { case 1: print(\"one\") case 2: fallthrough case 3: fallthrough case 4: print(\"two to four\") case 5: fallthrough case 7: print(\"five or seven\") default: break } 换句话说，不存在 case 中只有 fallthrough 的情况。如果 case 中有自己的代码逻辑再 fallthrough 是合理的。 注释规约 所使用的任何注释必须保持最新否则删除掉。代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。 类、类属性、类方法的注释必须使用 appledoc 规范，使用/*内容/格式。option+command+/。对于注释的要求:第一、能够准确反应设计思想和代码逻辑;第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。完全没有注释的大段代码对于阅读者形同天书，注释是给自己看的，即使隔很长时间，也能清晰理解当时的思路;注释也是给继任者看的，使其能够快速接替自己的工作。 好的命名、代码结构是自解释的，注释力求精简准确、表达到位。避免出现注释的一个极端:过多过滥的注释，代码的逻辑一旦修改，修改注释是相当大的负担。 补充规范 代码警告⚠️应该尽可能去除。除非明确为了提醒作用。 小代码块 保证逻辑的完整与连贯性 使用便于理解的API 无用注释与代码的删除尽量删除便于理解 优先使用工具类方法 服务器定义的字段为大写，客户端model 也应该使用小写 实现逻辑尽量才用普遍好理解的方法 import头文件的排版, 当import超过7个 ✅ #import \"GWHProductViewController.h\" //model #import \"GWHProductModel.h\" #import \"GWHCarModel.h\" #import \"GWHCommunityModel.h\" //view #import \"GWHProducPackageView.h\" #import \"GWHCarCell.h\" #import \"GWHCommunityCell.h\" //tool #import #import \"MJRefresh.h\" #import \"NSDateFormatter+Utility.h\" #import \"Masonry.h\" #import \"HttpUtils+GWHNetTool.h\" #import \"GWHUserManager.h\" //viewController #import \"GWHServiceProductRecViewController.h\" #import \"GWHPageController.h\" ❌ #import \"GWHProductViewController.h\" #import \"GWHProductModel.h\" #import \"GWHCarCell.h\" #import \"GWHCommunityCell.h\" #import #import \"MJRefresh.h\" #import \"NSDateFormatter+Utility.h\" #import \"HttpUtils+GWHNetTool.h\" #import \"GWHUserManager.h\" #import \"GWHServiceProductRecViewController.h\" #import \"GWHCarModel.h\" #import \"GWHCommunityModel.h\" #import \"GWHProducPackageView.h\" #import \"Masonry.h\" #import \"GWHPageController.h\" 代码提交逻辑 提交 commit 的类型 feat: 其他 fix: 修复bug 目前除了fix需要附带bug编号, 别的统一用feat. "},"pages/其他/WebView与iOS原生的交互.html":{"url":"pages/其他/WebView与iOS原生的交互.html","title":"WebView与iOS原生的交互","keywords":"","body":"1.WebView与iOS原生的交互 一、WKWebView的代理方法 1.1 WKNavigationDelegate 该代理提供的方法，可以用来追踪加载过程（页面开始加载、加载完成、加载失败）、决定是否执行跳转。 // 页面开始加载时调用 optional func webView(_ webView: WKWebView, didStartProvisionalNavigation navigation: WKNavigation!) // 当内容开始返回时调用 optional func webView(_ webView: WKWebView, didCommit navigation: WKNavigation!) // 页面加载完成之后调用 optional func webView(_ webView: WKWebView, didFinish navigation: WKNavigation!) // 页面加载失败时调用 optional func webView(_ webView: WKWebView, didFailProvisionalNavigation navigation: WKNavigation!, withError error: Error) 页面跳转的代理方法有三种，分为（收到跳转与决定是否跳转两种）： // 接收到服务器跳转请求之后调用 optional func webView(_ webView: WKWebView, didReceiveServerRedirectForProvisionalNavigation navigation: WKNavigation!) // 在收到响应后，决定是否跳转 optional func webView(_ webView: WKWebView, decidePolicyFor navigationResponse: WKNavigationResponse, decisionHandler: @escaping (WKNavigationResponsePolicy) -> Void) // 在发送请求之前，决定是否跳转 optional func webView(_ webView: WKWebView, decidePolicyFor navigationAction: WKNavigationAction, decisionHandler: @escaping (WKNavigationActionPolicy) -> Void) 1.2 WKUIDelegate optional func webView(_ webView: WKWebView, createWebViewWith configuration: WKWebViewConfiguration, for navigationAction: WKNavigationAction, windowFeatures: WKWindowFeatures) -> WKWebView? 下面代理方法全都是与界面弹出提示框相关的，针对于web界面的三种提示框（警告框、确认框、输入框）分别对应三种代理方法。下面只列举了警告框的方法。 optional func webView(_ webView: WKWebView, runJavaScriptAlertPanelWithMessage message: String, initiatedByFrame frame: WKFrameInfo, completionHandler: @escaping () -> Void) 1.3 WKScriptMessageHandler WKScriptMessageHandler其实就是一个遵循的协议，它能让网页通过JS把消息发送给OC。其中协议方法。 // 从web界面中接收到一个脚本时调用 func userContentController(_ userContentController: WKUserContentController, didReceive message: WKScriptMessage) 从协议中我们可以看出这里使用了两个类WKUserContentController和WKScriptMessage。WKUserContentController可以理解为调度器，WKScriptMessage则是携带的数据。 1.4 WKUserContentController WKUserContentController有两个核心方法，也是它的核心功能。 // js注入，即向网页中注入我们的js方法，这是一个非常强大的功能，开发中要慎用。 open func addUserScript(_ userScript: WKUserScript) // 添加供js调用oc的桥梁。这里的name对应WKScriptMessage中的name，多数情况下我们认为它就是方法名。 open func add(_ scriptMessageHandler: WKScriptMessageHandler, name: String) 1.5 WKScriptMessage WKScriptMessage就是js通知oc的数据。其中有两个核心属性用的很多。 open var name: String { get } 对应func add(_ scriptMessageHandler: WKScriptMessageHandler, name: String)添加的name。 open var body: Any { get } 携带的核心数据。js调用时只需window.webkit.messageHandlers.#name#.postMessage() 这里的name就是我们添加的name，是不是感觉很爽，就是这么简单，下面我们就来具体实现。 二、自定义CQWebView class CQWebView: UIView { // 增加webView属性 private lazy var webView: WKWebView = { let conf = WKWebViewConfiguration() conf.preferences.javaScriptEnabled = true conf.selectionGranularity = WKSelectionGranularity.character conf.allowsInlineMediaPlayback = true let webView = WKWebView(frame: .zero, configuration: conf) ... return webView }() } 2.1 增加js/oc交互方法 class CQWebView: UIView { ... // 增加js消息监听 func adddScriptMessageHandler(forName name: String) { webView.configuration.userContentController.add(self, name: name) } // 移除js消息监听 func removeScriptMessageHandler(forName name: String) { webView.configuration.userContentController.removeScriptMessageHandler(forName: name) } // oc执行js func evaluateJavaScript(_ javaScriptString: String, completionHandler: ((Any?, Error?) -> Void)? = nil) { webView.evaluateJavaScript(javaScriptString, completionHandler: completionHandler) } } 2.2 增加代理属性 // WebView代理 @objc protocol CQWebViewDelegate { // 接收 js 发来的消息 @objc optional func webView(_ webView: CQWebView, didReceiveMessage name: String, body: Any) } class CQWebView: UIView { ... weak var delegate: CQWebViewDelegate? } 2.3 实现WKScriptMessageHandler // js 和 swift 的交互 extension CQWebView: WKScriptMessageHandler { // 接收 js 发来的消息 func userContentController(_ userContentController: WKUserContentController, didReceive message: WKScriptMessage) { delegate?.webView?(self, didReceiveMessage: message.name, body: message.body) } } 三、JS调用Swift 2.1 完整html Untitled Document js调用swift // js调用swift function jsCallSwift(obj) { // 向 swift 发送数据，这里的‘msgBridge’就是 swift 中添加的消息通道的 name window.webkit.messageHandlers.msgBridge.postMessage(obj); } // swift调用js function swiftCallJs(msg){ document.getElementById('h').innerText+=msg; } 3.2 增加对js消息的监听 只需要调用CQWebView的adddScriptMessageHandler方法。 class ViewController: UIViewController { // MARK: - Properties private lazy var webView: CQWebView = { let webView = CQWebView(frame: .zero) webView.delegate = self webView.adddScriptMessageHandler(forName: \"msgBridge\") ... return webView }() } 3.2 实现对js消息的处理 extension ViewController: CQWebViewDelegate { func webView(_ webView: CQWebView, didReceiveMessage name: String, body: Any) { switch name { case \"msgBridge\": ... break default: break } } } 四、Swift调用JS 4.1 evaluateJavaScript方法使用 在html的js中已经定义了swiftCallJs方法等待调用，只需要调用CQWebView的evaluateJavaScript方法即可。 //swift 调 js函数 webView.evaluateJavaScript(\"swiftCallJs('\\( dic[\"msg\"] as! String)')\", completionHandler: { (any, error) in if (error != nil) { print(error ?? \"err\") } }) 4.2 WKWebView加载JS NSString *js = @\"\"; // 根据JS字符串初始化WKUserScript对象 WKUserScript *script = [[WKUserScript alloc] initWithSource:js injectionTime:WKUserScriptInjectionTimeAtDocumentEnd forMainFrameOnly:YES]; // 根据生成的WKUserScript对象，初始化WKWebViewConfiguration WKWebViewConfiguration *config = [[WKWebViewConfiguration alloc] init]; [config.userContentController addUserScript:script]; 参考文章 源码 Safari调试iOS中的JS iOS WKWebView 加载本地html文件（swift） 学习-WebKit(WKScriptMessageHandler) iOS下OC与JS的交互(WKWebview-MessageHandler实现) 自己动手打造基于 WKWebView 的混合开发框架（二）——js 向 Native 一句话传值并反射出 Swift 对象执行指定函数 WkWebKit - javascript on loaded page finds window.webkit is undefined WKWebview使用二三事 "},"pages/其他/理解MVVM架构模式.html":{"url":"pages/其他/理解MVVM架构模式.html","title":"理解MVVM架构模式","keywords":"","body":"理解MVVM架构模式 后续补充自己的理解与实战 参考文章 iOS 关于MVC和MVVM设计模式的那些事 iOS 关于MVVM Without ReactiveCocoa设计模式的那些事 iOS 关于MVVM With ReactiveCocoa设计模式的那些事 iOS 搭建App框架（MVVM+RAC+路由） 说说MVVM "},"pages/其他/Swift判断刘海屏幕.html":{"url":"pages/其他/Swift判断刘海屏幕.html","title":"Swift判断刘海屏幕","keywords":"","body":"Swift判断刘海屏幕 直接上代码 static var isFullScreen: Bool { if #available(iOS 11, *) { guard let w = UIApplication.shared.delegate?.window, let unwrapedWindow = w else { return false } if unwrapedWindow.safeAreaInsets.left > 0 || unwrapedWindow.safeAreaInsets.bottom > 0 { print(unwrapedWindow.safeAreaInsets) return true } } return false } 首先,刘海屏在iOS 11之后才推出,而重中之重的是safeAreaInsets属性 以下分别是竖屏与横屏的时候,safeAreaInsets打印的值 UIEdgeInsets(top: 44.0, left: 0.0, bottom: 34.0, right: 0.0) UIEdgeInsets(top: 0.0, left: 44.0, bottom: 21.0, right: 44.0) 其实单单判断bottom > 0 这个属性就完全可以解决问题了 static var kNavigationBarHeight: CGFloat { //return UIApplication.shared.statusBarFrame.height == 44 ? 88 : 64 return isFullScreen ? 88 : 64 } static var kBottomSafeHeight: CGFloat { //return UIApplication.shared.statusBarFrame.height == 44 ? 34 : 0 return isFullScreen ? 34 : 0 } 当然如果只是想简单适配 特别是竖屏的话 下面这段代码其实就能解决很多问题 UIApplication.shared.statusBarFrame.height == 44 "},"pages/其他/客户端网络逻辑整理及建议.html":{"url":"pages/其他/客户端网络逻辑整理及建议.html","title":"客户端网络逻辑整理及建议","keywords":"","body":"客户端网络逻辑整理及建议 第一次进入页面显示占位 下面是已有逻辑，以后保持目前逻辑，统一处理。 无网络（没有Wifi，没有4G），期望提示语：No network。 服务端报错，期望提示语：Loading failed。 服务端返回空，期望提示语：No data。 第一次进入页面loading显示 loading显示场景不明确，导致开发自己想象，比如进入“消息中心”有loading，进入“首页”没有。 loading样式根据场景做不同的区别，比如第一次进入页面与点击事件的loading可以有不同的视觉效果。 loading开始显示与结束的逻辑需要明确，比如一个页面加载有2个请求，是等一个请求完成后消失还是两个都完成后消失，比如：首页。 点击事件及反馈 明确是否需要增加loading等待，比如目前逻辑：点赞不显示loading，发布动态显示loading。 涉及网络请求的点击，反馈提示不明确，应该如何提示，个人认为如果产品没有定义，客户端不应该存在提示，因为客户端也不知道如何提示。 允许用户最大的等待时间不明确，目前客户端统一的超时时间是30s，是否合理，有待确定。 统一错误提示需要明确，比如：网络超时，没有网络，服务端报错，根据不同错误码给出指定提示。 重复点击处理，用户可能重复发出多个事件及请求。 网络监听处理 用户第一次进入页面，没有网络，页面显示了空占位，是否通过网络监听自动重新加载数据。 用户看视频的时候，是否提示用户正在使用4g。 上拉下拉刷新 当页面已经有数据的情况，这个时候下拉刷新，服务端报错，这个时候是显示空还是继续显示数据，是否给出错误提示。 当页面已经有数据的情况，这个时候上拉加载更多，服务端报错，是否给出错误提示。 Toast重叠 当用点击了两个有提示的按钮，可能导致Toast重叠。 App弹窗显示逻辑处理，当App首页存在多个窗口的时候，那么需要处理。 日志完善 确保每个请求都在控制台有日志打印，目前有些网络请求没有。 调试日志完善，GWLog增加上报或者支持沙盒。 "},"pages/其他/问题记录.html":{"url":"pages/其他/问题记录.html","title":"问题记录","keywords":"","body":"问题记录 iOS 13.1.3下，修复UITextView富文本点击产生多次回调 创建富文本 // 删除 let result = NSMutableAttributedString(string: \" \\(GWLocalized.delete)\") let range = NSRange(location: 0, length: result.string.count) result.addAttribute(NSAttributedString.Key.link, value: \"Delete://\", range: range) result.addAttribute(NSAttributedString.Key.font, value: UIFont(regularFontWithSize: level == 1 ? 16 : 14)!, range: range) textView?.tintColor = UIColor(hexString: \"#848484\") 点击富文本 // 点击事件 func textView(_ textView: UITextView, shouldInteractWith URL: URL, in characterRange: NSRange, interaction: UITextItemInteraction) -> Bool { // fix:iphone 7 plus 13.1.3活动的评论，点击删除，弹出三个关闭框 var recognizedTapGesture = false for ges in textView.gestureRecognizers ?? [] { if let tapGes = ges as? UITapGestureRecognizer, tapGes.state == .ended { recognizedTapGesture = true } } if !recognizedTapGesture { return true } if URL.scheme == \"Delete\" { let commentId = reactor?.currentState.comment.commentId let level = reactor?.currentState.level reactor?.action.onNext(.deleteComment(commentId: commentId, level: level)) } return true } "},"pages/其他/异常占位图的封装.html":{"url":"pages/其他/异常占位图的封装.html","title":"异常占位图的封装","keywords":"","body":"解决问题 统一的异常占位处理 代码复用性 一、占位图封装 import Foundation import GWI18n import MJRefresh extension UIView { private enum RuntimeKey { static let emptyView = UnsafeRawPointer(bitPattern: \"emptyView\".hashValue) } } public extension UIView { enum EmptyType { case custom(image: UIImage?, title: String?, top: CGFloat = 0) // 自定义 case empty(Bool = true) // 无数据 case netError // 无网络 case serverError // 服务器错误 } func showEmptyData(type: EmptyType = .empty(), reloadBlock: ((UIButton) -> Void)? = nil) { var emptyView: EmptyDataView! switch type { case let .empty(showReloadBt): emptyView = EmptyDataView(frame: CGRect(x: 0, y: 0, width: self.width, height: self.height)) emptyView.logo_ImgV.image = UIImage(inUtilCore: \"no_content\") emptyView.desc_Lb.text = GWI18n.R.string.localizable.base_date_empty() if !showReloadBt { emptyView.reload_Btn.isHidden = true } else { emptyView.reload_Btn.isHidden = false } case .netError: emptyView = EmptyDataView(frame: CGRect(x: 0, y: 0, width: self.width, height: self.height)) emptyView.logo_ImgV.image = UIImage(inUtilCore: \"no_network\") emptyView.desc_Lb.text = GWI18n.R.string.localizable.base_net_lost() case .serverError: emptyView = EmptyDataView(frame: CGRect(x: 0, y: 0, width: self.width, height: self.height)) emptyView.logo_ImgV.image = UIImage(inUtilCore: \"load_failure\") emptyView.desc_Lb.text = GWI18n.R.string.localizable.base_loading_failed() case let .custom(image, title, top): emptyView = EmptyDataView(frame: CGRect(x: 0, y: top, width: self.width, height: self.height - top)) emptyView.logo_ImgV.image = image emptyView.desc_Lb.text = title } emptyView.backgroundColor = .white emptyView.reload_block = reloadBlock emptyView.reload_Btn.setTitle(GWI18n.R.string.localizable.base_re_load(), for: .normal) if let scrollView = self as? UIScrollView { scrollView.mj_footer?.isHidden = true } self.emptyDataView = emptyView } func hideEmptyData() { if let scrollView = self as? UIScrollView { scrollView.mj_footer?.isHidden = false } self.emptyDataView = nil } private var emptyDataView: EmptyDataView? { set(newValue) { self.emptyDataView?.removeFromSuperview() objc_setAssociatedObject(self, RuntimeKey.emptyView!, newValue, .OBJC_ASSOCIATION_RETAIN_NONATOMIC) guard let newView = newValue else { return } self.addSubview(newView) self.bringSubviewToFront(newView) } get { return objc_getAssociatedObject(self, RuntimeKey.emptyView!) as? EmptyDataView } } } 二、使用方法 /// 请求失败时显示默认占位图 self.tableView.showEmptyData(type: .netError) { [weak self] _ in /// 重新请求 DispatchQueue.main.asyncAfter(deadline: DispatchTime.now() + 1) { /// 隐藏占位图 self?.tableView.hideEmptyData() } } 三、使用方法（新增） 基于GWNetwork的GWError重构，当网络请求时，如果没有网络会立马返回noNetwork的错误，所有增加传入error显示异常占位的方法: // 根据错误显示占位 func showEmptyData(error: Swift.Error, reloadBlock: ((UIButton) -> Void)? = nil) { if let gwError = error as? GWError { // 服务器报错 var emptyType: UIView.EmptyType = .serverError switch gwError { case let .client(cError): // 无网络 if cError == .noNetwork { emptyType = .netError } break default: break } self.showEmptyData(type: emptyType, reloadBlock: reloadBlock) } else { self.showEmptyData(type: .serverError, reloadBlock: reloadBlock) } } 如何使用： }).catchError {[weak tableView, weak self] (error) -> Observable in GWSwiftSpinner.hide() tableView?.endLoadMore() GWToast(error.localizedDescription) // 空视图显示 tableView?.showEmptyData(error: error, reloadBlock: { (_) in self?.action.onNext(.setup(messageTypeId: messageTypeId, page: page, tableView: tableView)) }) return PublishSubject() } "},"pages/其他/RxSwift的Timer.html":{"url":"pages/其他/RxSwift的Timer.html","title":"RxSwift的Timer","keywords":"","body":"RxSwift的Timer // MARK: - 消息功能 private let queryUnreadMsgTimerStopped = BehaviorRelay(value: false) // 开启计时器 private func startQueryUnreadMsgTimer() { guard let reactor = self.reactor else { return } // 定时请求未读消息 queryUnreadMsgTimerStopped.accept(false) Observable.interval(DispatchTimeInterval.seconds(60), scheduler: MainScheduler.instance).startWith(0).takeWhile{_ in UserManager.isLogin }.map{_ in GWHHomeReactor.Action.queryUnreadCount }.takeUntil(queryUnreadMsgTimerStopped.asObservable().filter{ $0 }).bind(to: reactor.action).disposed(by: disposeBag) } // 关闭计时器 private func stopQueryUnreadMsgTimer() { queryUnreadMsgTimerStopped.accept(true) } "},"pages/其他/AppTimer的使用.html":{"url":"pages/其他/AppTimer的使用.html","title":"AppTimer的使用","keywords":"","body":"AppTimer的使用 技术：RxSwift实现Timer。 场景：为了解决App多个组件（爱车、社区）分别使用Timer，造成资源浪费的问题，在GWCommonComponent写了一个AppTimer。 功能 在App启动时调用，跟随App生命周期，目前会发布1s,10s,60s的通知。 当App进入后台停止计时，当App将要进入前台重新开始计时。 如何使用 务必在每个国家的宿主工程AppLaunch（壳工程调用）。 func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool { ...... window?.rootViewController = tabBarController window?.makeKeyAndVisible() AppTimer.shared.start() return true } 源码 import UIKit import RxSwift import GWUtilCore /** 跟随App生命周期的Timer 业务：消息中心，爱车 */ public class AppTimer { public static let shared = AppTimer() private let timer = Observable.interval(.seconds(1), scheduler: MainScheduler.instance) private var disposable: Disposable? /// AppTimer到1s的序列 public let reach1sObservable = PublishSubject() /// AppTimer到10s的序列 public let reach10sObservable = PublishSubject() /// AppTimer到60s的序列 public let reach60sObservable = PublishSubject() init() { _ = NotificationCenter.default.rx.notification(UIApplication.willEnterForegroundNotification).subscribe(onNext: {[weak self] _ in GWLog(\"----- App进入前台 ------\") self?.start() }) _ = NotificationCenter.default.rx.notification(UIApplication.didEnterBackgroundNotification).subscribe(onNext: {[weak self] _ in GWLog(\"----- App进入后台 ------\") self?.stop() }) } /// 开启计时器 public func start() { stop() GWLog(\"----- 开始AppTimer订阅 ------\") disposable = timer.subscribe(onNext: {[weak self] count in GWLog(\"----- 当前时间\\(count+1)s ------\") // 1s通知 NotificationCenter.default.post(name: NSNotification.Name.appTimerReach1s, object: nil) self?.reach1sObservable.onNext(()) // 10s通知 if (count + 1) % 10 == 0 { NotificationCenter.default.post(name: NSNotification.Name.appTimerReach10s, object: nil) self?.reach10sObservable.onNext(()) } // 60s通知 if (count + 1) % 60 == 0 { NotificationCenter.default.post(name: NSNotification.Name.appTimerReach60s, object: nil) self?.reach60sObservable.onNext(()) } }, onDisposed: { GWLog(\"----- 释放AppTimer订阅 ------\") }) } /// 停止计时器 public func stop() { disposable?.dispose() } } "},"pages/其他/UIViewController的Rx扩展.html":{"url":"pages/其他/UIViewController的Rx扩展.html","title":"UIViewController的常用Rx扩展","keywords":"","body":"UIViewController的常用Rx扩展 https://www.hangge.com/blog/cache/detail_1943.html "},"pages/其他/Info.plist国际化.html":{"url":"pages/其他/Info.plist国际化.html","title":"Info.plist国际化","keywords":"","body":"Info.plist国际化 新建一个InfoPlist.stings文件，配置不同的语言。 如何新建 New File -> Resource下的Strings File。 参考链接 3分钟实现iOS语言本地化/国际化（图文详解） how to localise the ios-info-plist-file iOS调用系统相册\\相机 显示中文标题 iOS设置默认语言 解决iOS9/10简体中文,InfoPlist.string权限描述国际化显示问题 "},"pages/其他/统一设置页面title字体样式.html":{"url":"pages/其他/统一设置页面title字体样式.html","title":"统一设置页面title字体样式","keywords":"","body":"统一设置页面title字体样式 方案实现 给UINavigationBar增加setTitleFont方法的扩展。 extension UINavigationBar { /// Set Navigation Bar title, title color and font. /// /// - Parameters: /// - font: title font /// - color: title text color (default is .black). public func setTitleFont(_ font: UIFont, color: UIColor = .black) { var attrs = [NSAttributedString.Key: Any]() attrs[.font] = font attrs[.foregroundColor] = color self.titleTextAttributes = attrs } } 如何使用 在创建UINavigationController的时候，手动设置title字体样式。 navVc.navigationBar.setTitleFont(.appMediuFont(16)) 统一设置导航栏背景颜色、标题颜色和大小、状态栏文本颜色 "},"pages/其他/业务组件按国家子库化.html":{"url":"pages/其他/业务组件按国家子库化.html","title":"业务组件按国家子库化","keywords":"","body":"业务组件按国家子库化 如何解决多个国家并行开发？ 一、解决方案 方案一：不同国家不同分支 一个国家对应一个分支，对应国家的宿主工程依赖国家分支。 方案二：多个国家多个子库 按国家新建不同的子库，并且新建不同国家的target分别依赖子库。 对比两个方案，个人倾向方案二 方案一修改代码比较麻烦，不利于代码复用。 二、新建子库 下面以GWHModuleHome为例： 2.1 整理项目文件夹，按国家划分 在Classes下分别新建Core/Thailand/Russia文件夹。 将Classes原来所有文件及文件夹放入Core Thailand和Russia随便放一个swift文件，为了pod install可以显示这两个子库。 2.2 修改GWHModuleHome.podspec 设置默认子库Core。 新建Core/Thailand/Russia子库，源文件及资源文件路径一定要正确。 执行pod install，这样就完成子库新建。 Pod::Spec.new do |s| ..... s.default_subspec = 'Core' s.subspec 'Core' do |ss| ss.source_files = 'GWHModuleHome/Classes/Core/**/*' ss.resource_bundles = { 'GWHModuleHome' => [ 'GWHModuleHome/Assets/**/{*}', 'GWHModuleHome/Classes/**/{*.storyboard,*.xib}' ] } ss.dependency 'GWHModuleMine' ss.dependency 'GWNetWork' ss.dependency 'GWCommonComponent' ss.dependency 'GWUserCenterBase' ss.dependency 'Aquaman' ss.dependency 'Trident' end s.subspec 'Russia' do |ss| ss.source_files = 'GWHModuleHome/Classes/Russia/**/*' ss.dependency 'GWHModuleHome/Core' end s.subspec 'Thailand' do |ss| ss.source_files = 'GWHModuleHome/Classes/Thailand/**/*' ss.dependency 'GWHModuleHome/Core' end end 三、新建target 分别参考下面的链接： Xcode 一个项目下创建多个Target 配置多个target及多个Target的podfile文件配置 Xcode中Active Compilation Conditions和Preprocessor Macros的区别 "},"pages/其他/App权限管理.html":{"url":"pages/其他/App权限管理.html","title":"App权限管理","keywords":"","body":"App权限管理 GWUtilCore组件增加PrivacyManager，统一App内权限获取。 自定义隐私类型 目前App只涉及3种权限,定义如下： /* * 隐私权限类型 1. 在Info.plist文件中配置应用所需权限； 2. 在项目的Targets->Capabilities中开启相应开关，目前Siri、Health、NFC、HomeKit需要开启； 3. 引入相关库； 4. 使用代码获取对应的隐私权限。 */ public enum PrivacyType { // 相机 Authorized Denied Unsupported Unkonw case camera // 相册 NotDetermined Authorized Denied Restricted Unkonw case photos // 定位 NotDetermined Unkonw Denied Authorized:AuthorizedAlways||AuthorizedWhenInUse case location public var title: String { ...... } public var subTitle: String { ...... } public static var all :[PrivacyType] { return [.location , .camera , .photos] } } 自定义权限类型 将权限结果统一包装为自定义的类型，定义如下： /// 获取隐私权限结果 public enum AuthorizationStatus{ // 未知 case unknow // 授权 case authorized // 拒绝 case denied // 不支持 case unsupported // 未决定（相册，地理位置） case notDetermined public var title: String { ...... } public var titleColor: UIColor? { ...... } } 隐私权限管理 PrivacyManager提供两个请求隐私权限的类方法,定义如下： synRequestAccess方法为同步方法，可以同步获取指定隐私的权限。 /// 同步请求隐私权限 /// - Parameter type: 隐私权限类型 /// - Returns: 隐私权限 public static func synRequestAccess(_ type: PrivacyType) -> AuthorizationStatus { switch type { case .photos: return shared.requestAccessPhotos(nil) case .location: return shared.requestAccessLocation(nil) case .camera: return shared.requestAccessCamera(nil) } } asynRequestAccess方法为异步方法，通过传入completionHandle异步获取隐私的权限，如果是第一次会触发询问窗口，当用户决定后，completionHandle会将决定后的结果返回。 /// 异步请求隐私权限 /// - Parameters: /// - type: 隐私权限类型 /// - completionHandle: 隐私权限结果代码块 public static func asynRequestAccess(_ type: PrivacyType , completionHandle: ((AuthorizationStatus) -> Void)?) { switch type { case .photos: _ = shared.requestAccessPhotos(completionHandle) break case .location: _ = shared.requestAccessLocation(completionHandle) break case .camera: _ = shared.requestAccessCamera(completionHandle) break } 请求权限实现 extension PrivacyManager: CLLocationManagerDelegate { /// 在主线程回调隐私权限结果 /// - Parameters: /// - status: 隐私权限结果 /// - completionHandle: 隐私权限结果代码块 private func response(status: AuthorizationStatus, completionHandle: ((AuthorizationStatus) -> Void)?) { DispatchQueue.main.async { completionHandle?(status) } } // MARK: 请求相机访问权限 func requestAccessCamera(_ completionHandle: ((AuthorizationStatus) -> Void)?) -> AuthorizationStatus { if UIImagePickerController.isSourceTypeAvailable(.camera) { let status = AVCaptureDevice.authorizationStatus(for: .video) if status == .notDetermined { if completionHandle != nil { AVCaptureDevice.requestAccess(for: .video) { [weak self] (granted) in if granted{ self?.response(status: .authorized, completionHandle: completionHandle) } else { self?.response(status: .denied, completionHandle: completionHandle) } } } return .notDetermined } else if status == .authorized{ response(status: .authorized, completionHandle: completionHandle) return .authorized } else if status == .denied || status == .restricted{ response(status: .denied, completionHandle: completionHandle) return .denied } else { response(status: .unknow, completionHandle: completionHandle) return .unknow } } else{ response(status: .unsupported, completionHandle: completionHandle) return .unsupported } } // MARK: 请求相册权限 func requestAccessPhotos(_ completionHandle: ((AuthorizationStatus) -> Void)?) -> AuthorizationStatus { let status = PHPhotoLibrary.authorizationStatus() if status == .notDetermined { if completionHandle != nil { PHPhotoLibrary.requestAuthorization { [weak self] (status) in if status == .notDetermined{ self?.response(status: .notDetermined, completionHandle: completionHandle) } else if status == .denied || status == .restricted { self?.response(status: .denied, completionHandle: completionHandle) } else{ self?.response(status: .authorized, completionHandle: completionHandle) } } } return .notDetermined } else if status == .authorized { response(status: .authorized, completionHandle: completionHandle) return .authorized } else if status == .denied || status == .restricted { response(status: .denied, completionHandle: completionHandle) return .denied } else{ response(status: .unknow, completionHandle: completionHandle) return .unknow } } // MARK: 请求定位权限 func requestAccessLocation(_ completionHandle: ((AuthorizationStatus) -> Void)?) -> AuthorizationStatus { let status = CLLocationManager.authorizationStatus() let serviceEnabled = CLLocationManager.locationServicesEnabled() if !serviceEnabled { return .unsupported } else if status == .notDetermined { if completionHandle != nil { PrivacyManager.shared.locationCompletionBlock = completionHandle let loc = CLLocationManager() loc.delegate = PrivacyManager.shared loc.requestWhenInUseAuthorization() loc.startUpdatingLocation() PrivacyManager.shared.locationManager = loc } return .notDetermined } else if status == .denied || status == .restricted { response(status: .denied, completionHandle: completionHandle) return .denied } else if status == .authorizedAlways || status == .authorizedWhenInUse{ response(status: .authorized, completionHandle: completionHandle) return .authorized } else{ response(status: .unknow, completionHandle: completionHandle) return .unknow } } public func locationManager(_ manager: CLLocationManager, didChangeAuthorization status: CLAuthorizationStatus) { switch status { case .notDetermined: response(status: .notDetermined, completionHandle: PrivacyManager.shared.locationCompletionBlock) break case .denied, .restricted: response(status: .denied, completionHandle: PrivacyManager.shared.locationCompletionBlock) PrivacyManager.shared.locationCompletionBlock = nil break case .authorizedAlways, .authorizedWhenInUse: response(status: .authorized, completionHandle: PrivacyManager.shared.locationCompletionBlock) PrivacyManager.shared.locationCompletionBlock = nil break @unknown default: response(status: .unknow, completionHandle: PrivacyManager.shared.locationCompletionBlock) PrivacyManager.shared.locationCompletionBlock = nil break } } } "},"pages/其他/App定位管理.html":{"url":"pages/其他/App定位管理.html","title":"App定位管理","keywords":"","body":"App定位管理 项目中商城及爱车分别使用到了系统定位，故将定位管理类从地图中抽离出来。 定位管理作为单例，提供“开始定位”、“停止定位”、“定位是否可用”、“检索地址”等方法。 细节：开始定位的时候，需要使用到PrivacyManager的异步请求权限的方法，第一次使用的时候，会有询问弹窗，用户决定授权后，会继续定位，否则定位失败。 2021.1.21:新增func startLocating(subLocManager: SubCLLocationManager),支持同时调用多次定位,定位完成subLocManager的locatingCompletion会被调用。 import UIKit import CoreLocation /// 系统定位的子类 public class SubCLLocationManager: CLLocationManager { /// 定位完成 public var locatingCompletion: ((CLLocation?) -> Void)? public override init() { super.init() desiredAccuracy = kCLLocationAccuracyBest } } /// 自定义定位代理 public protocol LocationManagerDelegate: NSObjectProtocol { /// 更新定位 /// - Parameter location: 系统定位 func onLocationUpdated(location: CLLocation) /// 定位失败 func onLocationFailure() } public extension LocationManagerDelegate { func onLocationUpdated(location: CLLocation) { } func onLocationFailure() { } } /// 自定义定位管理 public class LocationManager: NSObject { /// 代理 public weak var delegate: LocationManagerDelegate? /// 系统用户定位 public var userLocation: CLLocation? /// 系统定位管理 private let locationManager = SubCLLocationManager() private var tempManagers = [SubCLLocationManager]() /// 单例 public static let shared = LocationManager() /// 开始定位 public func startLocating() { startLocating(subLocManager: locationManager) } /// 开始定位 /// - Parameter subLocManager: 指定定位管理 public func startLocating(subLocManager: SubCLLocationManager) { tempManagers.append(subLocManager) PrivacyManager.asynRequestAccess(.location) {[weak self] (status) in switch status { case .authorized: subLocManager.delegate = self subLocManager.startUpdatingLocation() break default: subLocManager.delegate = self subLocManager.locatingCompletion?(nil) self?.delegate?.onLocationFailure() self?.remove(manager: subLocManager) break } } } /// 停止定位 public func stopLocating() { stopLocating(subLocManager: locationManager) } /// 停止定位 /// - Parameter subLocManager: 指定定位管理 public func stopLocating(subLocManager: SubCLLocationManager) { subLocManager.stopUpdatingLocation() remove(manager: subLocManager) } /// 移除指定定位管理 /// - Parameter manager: 指定定位管理 private func remove(manager: SubCLLocationManager) { if let index = tempManagers.firstIndex(of: manager) { tempManagers.remove(at: index) } } /// 定位是否可用 /// - Returns: 是否可用 public func locationServiceEnable() -> Bool { return PrivacyManager.synRequestAccess(.location) == .authorized } /// 检索地址 /// - Parameter location: 定位 /// - Returns: 地址 public func reverseGeocodeLocation(_ location: CLLocation, completionHandler: ((String?) -> Void)?) { CLGeocoder().reverseGeocodeLocation(location) { (placemarks, error) -> Void in if error != nil { GWLog(\"----- 检索地址失败 -----\") completionHandler?(nil) } else { let pm = CLPlacemark(placemark: placemarks![0] as CLPlacemark) var address: String = \"\" func addSymbol() -> String { if !address.isEmpty { address += \",\" } return address } if let subThoroughtare = pm.subThoroughfare // 门牌号 { address = addSymbol() + subThoroughtare } if let thoroughfare = pm.thoroughfare // 街道 { address = addSymbol() + thoroughfare } if let subLocality = pm.subLocality // 区 { address = addSymbol() + subLocality } if let locality = pm.locality // 市 { address = addSymbol() + locality } if let administrativeArea = pm.administrativeArea // 省（州） { address = addSymbol() + administrativeArea } if let country = pm.country // 国家 { address = addSymbol() + country } GWLog(\"----- 检索地址：\\(address) -----\") completionHandler?(address) } } } } extension LocationManager: CLLocationManagerDelegate { /// 定位回调 public func locationManager(_ manager: CLLocationManager, didUpdateLocations locations: [CLLocation]) { guard let lastLocation = locations.last else { GWLog(\"----- 定位失败:没有找到最后一个位置 -----\") delegate?.onLocationFailure() if let manager = manager as? SubCLLocationManager { manager.locatingCompletion?(nil) } return } GWLog(\"----- 定位成功:latitude-\\(lastLocation.coordinate.latitude),longitude-\\(lastLocation.coordinate.longitude) -----\") userLocation = lastLocation delegate?.onLocationUpdated(location: lastLocation) if let manager = manager as? SubCLLocationManager { manager.locatingCompletion?(lastLocation) } } /// 定位失败 public func locationManager(_ manager: CLLocationManager, didFailWithError error: Error) { if let error = error as? CLError, error.code == .denied { GWLog(\"----- 用户拒绝定位 -----\") if let manager = manager as? SubCLLocationManager { stopLocating(subLocManager: manager) } } GWLog(\"----- 定位失败:\\(error) -----\") delegate?.onLocationFailure() if let manager = manager as? SubCLLocationManager { manager.locatingCompletion?(nil) } } } 附： placemark "},"pages/其他/资源包装类.html":{"url":"pages/其他/资源包装类.html","title":"资源包装类","keywords":"","body":"资源包装类 资源：项目中使用的图片、颜色、字体、常量、多语言等。 为了方便管理项目资源，UtilCore组件新增了几个资源包装类，后续请确保项目统一使用包装类调用资源。 如果只在业务组件中使用，请为以下几个包装类分别做扩展，确保资源调用的高度统一。 解决的问题 明确项目中所有使用的字体、颜色、图片等，资源统一管理，为后续主题做铺垫。 如果项目全部替换为通过包装类使用资源，那么一旦某个颜色、图片不用了，可以直接删除包装类的定义，再删除具体的资源，减少垃圾资源的存在。 代码风格统一，调用简单清晰。 GWColor.swift public extension UIColor { /// 根据模式（正常/暗黑）渲染颜色 /// - Parameters: /// - normal: 正常Hex /// - dark: 暗黑Hex /// - Returns: 颜色 static func renderColor(_ color: GWDynamicColor) -> UIColor { var result: UIColor? if #available(iOS 13, *) { result = UIColor(dynamicProvider: { (traitCollection) -> UIColor in (traitCollection.userInterfaceStyle == .dark ? UIColor(hexString: color.dark) ?? UIColor.white : UIColor(hexString: color.normal)) ?? UIColor.white }) } else { result = UIColor(hexString: color.normal) } return result ?? UIColor.white } } /** 颜色包装类 - UtilCore及其它业务组件通用的颜色在这里定义 - 通过使用包装类方便管理颜色 - 业务组件可以通过Extensions去扩展自己的颜色 - 使用：GWConstant.xC7C3C3 */ /// 动态颜色类型 public typealias GWDynamicColor = (normal: String, dark: String) public enum GWColor { public static let xC7C3C3 = UIColor.renderColor(GWDynamicColor(\"#C7C3C3\", \"#C7C3C3\")) public static let x2F2F2F = UIColor.renderColor(GWDynamicColor(\"#2F2F2F\", \"#2F2F2F\")) public static let xF6F6F6 = UIColor.renderColor(GWDynamicColor(\"#F6F6F6\", \"#F6F6F6\")) public static let x2D78FF = UIColor.renderColor(GWDynamicColor(\"#2D78FF\", \"#2D78FF\")) // ... } GWConstant.swift /** 常量包装类 - UtilCore及其它业务组件通用的常量在这里定义 - 通过使用包装类方便管理常量 - 业务组件可以通过Extensions去扩展自己的常量 - 使用：GWConstant.xxx */ public struct GWConstant { } GWFont.swift /** 字体包装类 - UtilCore及其它业务组件通用的字体在这里定义 - 通过使用包装类方便管理字体 - 业务组件可以通过Extensions去扩展自己的字体 - 使用：GWLocalized.r11 */ public struct GWFont { public static let r11 = UIFont.appRegularFontWith(size: 11) public static let r12 = UIFont.appRegularFontWith(size: 12) public static let r14 = UIFont.appRegularFontWith(size: 14) public static let r16 = UIFont.appRegularFontWith(size: 16) public static let r20 = UIFont.appRegularFontWith(size: 20) public static let m20 = UIFont(mediumFontWithSize: 20) } GWImage.swift /** 图片包装类 - UtilCore及其它业务组件通用的图片在这里定义 - 通过使用包装类方便管理图片 - 业务组件可以通过Extensions去扩展自己的图片 - 使用：GWImage.ruNoContent */ public struct GWImage { // MARK: - UIView+Empty /// 俄罗斯·没数据 static var ruNoContent: UIImage? { UIImage(inUtilCore: \"img_russia_common_no_content\") } /// 俄罗斯·无网络 static var ruNoNetwork: UIImage? { UIImage(inUtilCore: \"img_russia_common_no_network\") } /// 泰国·没数据 static var thNoContent: UIImage? { UIImage(inUtilCore: \"no_content\") } /// 泰国·无网络 static var thNoNetwork: UIImage? { UIImage(inUtilCore: \"no_network\") } /// 泰国·服务器错误 static var thServerError: UIImage? { UIImage(inUtilCore: \"load_failure\") } } GWLocalized.swift /** 多语言包装类 - UtilCore及其它业务组件通用的多语言在这里定义 - 通过使用包装类方便管理多语言 - 业务组件可以通过Extensions去扩展自己的多语言 - 使用：GWLocalized.loading */ public struct GWLocalized { /// 提交中 public static var loading: String { GWI18n.R.string.localizable.base_loading() } /// Yes public static var yes: String { GWI18n.R.string.localizable.base_yes() } /// No public static var no: String { GWI18n.R.string.localizable.base_no() } /// 无网络 public static var noNetwork: String { GWI18n.R.string.localizable.base_net_lost() } /// 发布 public static var publish: String { GWI18n.R.string.localizable.re_publish() } /// 确认 public static var confirm: String { GWI18n.R.string.localizable.base_sure() } /// 取消 public static var cancel: String { GWI18n.R.string.localizable.base_cancel() } } "},"pages/其他/同步未读消息.html":{"url":"pages/其他/同步未读消息.html","title":"同步未读消息","keywords":"","body":"同步未读消息 需求 只有用户已经登录才同步未读消息。 当收到消息推送，触发同步。 当App停留在前台，每个60s，并且停留在指定页面（首页、我的），触发同步。 当指定页面（首页、我的）第一次显示，触发同步。 当离开指定页面（首页、我的）超过60s再回来，触发同步。 一、先实现需求4、5 为vc增加rx的扩展序列visibleDetail，当vc显示或者离开时会发出“是否显示”、“是否第一次显示”、“离开时间”的元组信号。通过对visibleDetail的订阅，当发出信号时，去判断是否第一次，如果是就触发一次同步，如果不是，再判断离开时间是否超过60s，如果超过也需要触发一次同步。 1.1 如何为vc增加扩展序列visibleDetail 首先增加vc生命周期的基本扩展，代码来源。 public extension Reactive where Base: UIViewController { public var viewDidAppear: ControlEvent { let source = self.methodInvoked(#selector(Base.viewDidAppear)) .map { $0.first as? Bool ?? false } return ControlEvent(events: source) } public var viewWillDisappear: ControlEvent { let source = self.methodInvoked(#selector(Base.viewWillDisappear)) .map { $0.first as? Bool ?? false } return ControlEvent(events: source) } // 表示视图是否显示的可观察序列，当VC显示状态改变时会触发 public var isVisible: Observable { let viewDidAppearObservable = self.base.rx.viewDidAppear.map { _ in true } let viewWillDisappearObservable = self.base.rx.viewWillDisappear .map { _ in false } return Observable.merge(viewDidAppearObservable, viewWillDisappearObservable) } ...... } 继续给vc增加进入/离开时间的属性扩展，用于记录页面的进入/离开的时间撮。 private var enterStampKey: Void? private var leaveStampKey: Void? public extension UIViewController { /// 离开时间撮 var leaveStamp: TimeInterval? { set(newValue) { objc_setAssociatedObject(self, &leaveStampKey, newValue, .OBJC_ASSOCIATION_RETAIN) } get { return objc_getAssociatedObject(self, &leaveStampKey) as? TimeInterval } } /// 进入时间撮 var enterStamp: TimeInterval? { set(newValue) { objc_setAssociatedObject(self, &enterStampKey, newValue, .OBJC_ASSOCIATION_RETAIN) } get { return objc_getAssociatedObject(self, &enterStampKey) as? TimeInterval } } } 有了以上两个扩展，这个时候就可以为vc增加rx扩展序列visibleDetail了,可以看下面的实现，visibleDetail依赖于isVisible,每当页面进入/离开会记录时间撮，同时该序列返回的类型是元组，第一个元素是“是否显示”，第一个元素是“是否第一次显示”，第三个元素是“离开时间”。 public extension Reactive where Base: UIViewController { /// 显示详情 /// - Parameters: 参数 /// - visible：是否显示 /// - isFirst：是否第一次显示 /// - leaveTime：离开时间，单位s，默认返回0 typealias VisibleDetail = (visible: Bool,isFirst: Bool, leaveTime: Int) var visibleDetail: Observable { return self.base.rx.isVisible.map { (isVisible) -> VisibleDetail in if isVisible { let isFirst = self.base.enterStamp == nil// 是否第一次显示 var leaveTime: Int = 0// 离开时间 self.base.enterStamp = Date().timeIntervalSince1970 if let enterStamp = self.base.enterStamp, let leaveStamp = self.base.leaveStamp, enterStamp > leaveStamp { leaveTime = Int(enterStamp - leaveStamp) } return (true, isFirst, leaveTime) } else { self.base.leaveStamp = Date().timeIntervalSince1970 return (false, false, 0) } } } } 1.2 增加SynMessageTool单例 SynMessageTool是一个单例，提供了一个start、stop方法，start方法需要传入一个vc，意思是消息同步是需要根据vc的生命周期来控制的，当vc第一次显示或者离开vc超过60s再回来就触发同步。 /// 同步未读消息，发出通知 public class SynMessageTool { public static let shared = SynMessageTool() /// SynMessage订阅取消 private var disposable: Disposable? /// 同步消息（数字） /// - Parameters: /// - messageVc: 同步消息关联vc public func start(messageVc: UIViewController?) { // 传vc才同步消息 guard let vc = messageVc else { return } // 停止订阅 stop() GWLog(\"----- vc is \\(vc.classForCoder) -----\") /// 页面触发同步序列 let synMessageObservable = vc.rx.visibleDetail.flatMap { (detail) -> Observable in if detail.isFirst { GWLog(\"----- 第一次显示 -----\") return HomeViewModel.getUnreadCount } else if detail.leaveTime > 60 { GWLog(\"----- 非第一次显示，但是离开时间超过60s -----\") return HomeViewModel.getUnreadCount } return Observable.of() } GWLog(\"----- 开始SynMessage订阅 ------\") disposable = synMessageObservable.subscribe(onNext: { count in GWLog(\"----- 当前消息数数\\(count) ------\") // 发通知 NotificationCenter.default.post(name: .synMessageCount, object: count, userInfo: [\"count\":count]) }, onDisposed: { GWLog(\"----- 释放SynMessage订阅 ------\") }) } public func stop() { disposable?.dispose() } } 二、实现需求3 需求：当App停留在前台，每个60s，并且停留在指定页面（首页、我的），触发同步。 这里需要使用到AppTimer及vc的rx扩展序列isVisible,将AppTimer的到达60s的序列与isVisible通过Observable.combineLatest组合成一个新的序列，再通过flatMap转换成我们需要的序列，当然这里需要判断“页面是否正在显示”，如果显示才会触发同步。start方法修改如下： /// 同步消息（数字） /// - Parameters: /// - messageVc: 同步消息关联vc public func start(messageVc: UIViewController?) { // 传vc才同步消息 guard let vc = messageVc else { return } // 停止订阅 stop() // 同步消息序列 var synMessageObservable: Observable! GWLog(\"----- vc is \\(vc.classForCoder) -----\") /// 页面触发同步序列 let synMessageObservableByVc = vc.rx.visibleDetail.flatMap { (detail) -> Observable in if detail.isFirst { GWLog(\"----- 第一次显示 -----\") return HomeViewModel.getUnreadCount } else if detail.leaveTime > 60 { GWLog(\"----- 非第一次显示，但是离开时间超过60s -----\") return HomeViewModel.getUnreadCount } return Observable.of() } /// 计时器序列 let synMessageObservableByTimer = Observable.combineLatest(AppTimer.shared.reach60sObservable,vc.rx.isVisible).flatMap { (tuple) -> Observable in GWLog(\"----- AppTimer 触发同步 -----\") if tuple.1 { return HomeViewModel.getUnreadCount } return Observable.of() } synMessageObservable = Observable.merge(synMessageObservableByVc,synMessageObservableByTimer) GWLog(\"----- 开始SynMessage订阅 ------\") disposable = synMessageObservable.subscribe(onNext: { count in GWLog(\"----- 当前消息数数\\(count) ------\") // 发通知 NotificationCenter.default.post(name: .synMessageCount, object: count, userInfo: [\"count\":count]) }, onDisposed: { GWLog(\"----- 释放SynMessage订阅 ------\") }) } 三、实现需求2 需求：当收到消息推送，触发同步。 SynMessageTool只需要增加一个立即同步的方法，在收到通知的地方调用即可。 /// 立即同步 public func immediatelyStart() { immediatelySynDisposable = HomeViewModel.getUnreadCount.subscribe(onNext: {[weak self] count in GWLog(\"----- immediately:当前消息数数\\(count) ------\") // 发通知 NotificationCenter.default.post(name: .synMessageCount, object: count, userInfo: [\"count\":count]) // 释放订阅 self?.immediatelyStop() }, onDisposed: { GWLog(\"----- immediately:释放SynMessage订阅 ------\") }) } 四、实现需求1 需求：只有用户已经登录才同步未读消息。 只需要在start和immediatelyStart方法中加入如下登录判断即可。 // 登录后才同步消息 guard UserManager.isLogin else { return } 五、完整代码 /// 同步未读消息，发出通知 public class SynMessageTool { public static let shared = SynMessageTool() /// SynMessage订阅取消 private var disposable: Disposable? /// 确保同步一次后取消 private var immediatelySynDisposable: Disposable? /// 同步消息（数字） /// - Parameters: /// - messageVc: 同步消息关联vc public func start(messageVc: UIViewController?) { // 登录后才同步消息 guard UserManager.isLogin else { return } // 传vc才同步消息 guard let vc = messageVc else { return } // 停止订阅 stop() // 同步消息序列 var synMessageObservable: Observable! GWLog(\"----- vc is \\(vc.classForCoder) -----\") /// 页面触发同步序列 let synMessageObservableByVc = vc.rx.visibleDetail.flatMap { (detail) -> Observable in if detail.isFirst { GWLog(\"----- 第一次显示 -----\") return HomeViewModel.getUnreadCount } else if detail.leaveTime > 60 { GWLog(\"----- 非第一次显示，但是离开时间超过60s -----\") return HomeViewModel.getUnreadCount } return Observable.of() } /// 计时器序列 let synMessageObservableByTimer = Observable.combineLatest(AppTimer.shared.reach60sObservable,vc.rx.isVisible).flatMap { (tuple) -> Observable in GWLog(\"----- AppTimer 触发同步 -----\") if tuple.1 { return HomeViewModel.getUnreadCount } return Observable.of() } synMessageObservable = Observable.merge(synMessageObservableByVc,synMessageObservableByTimer) GWLog(\"----- 开始SynMessage订阅 ------\") disposable = synMessageObservable.subscribe(onNext: { count in GWLog(\"----- 当前消息数数\\(count) ------\") // 发通知 NotificationCenter.default.post(name: .synMessageCount, object: count, userInfo: [\"count\":count]) }, onDisposed: { GWLog(\"----- 释放SynMessage订阅 ------\") }) } public func stop() { disposable?.dispose() } /// 立即同步 public func immediatelyStart() { // 登录后才同步消息 guard UserManager.isLogin else { return } immediatelySynDisposable = HomeViewModel.getUnreadCount.subscribe(onNext: {[weak self] count in GWLog(\"----- immediately:当前消息数数\\(count) ------\") // 发通知 NotificationCenter.default.post(name: .synMessageCount, object: count, userInfo: [\"count\":count]) // 释放订阅 self?.immediatelyStop() }, onDisposed: { GWLog(\"----- immediately:释放SynMessage订阅 ------\") }) } /// 立即停止 public func immediatelyStop() { immediatelySynDisposable?.dispose() } } "},"pages/其他/自定义错误.html":{"url":"pages/其他/自定义错误.html","title":"自定义错误","keywords":"","body":"自定义错误 Swift中的Error 官方解释 A type representing an error value that can be thrown. Any type that declares conformance to the Error protocol can be used to represent an error in Swift’s error handling system. Because the Error protocol has no requirements of its own, you can declare conformance on any custom type you create. protocol Error Error在Swift中就是是个协议，而且不需要实现任何属性或者方法，因此任何声明符合Error协议的类型都可以用来表示Swift的错误。 使用枚举作为错误 通常错误都是有很多种，因此枚举类型是作为错误最适合不过，下面是官方给的例子： enum IntParsingError: Error { case overflow case invalidInput(Character) } 使用结构体作为错误 官方解释 Sometimes you may want different error states to include the same common data, such as the position in a file or some of your application’s state. When you do, use a structure to represent errors. 意思就是当不同错误会包含通用的属性，例如文件中的某些位置或应用的状态，这个时候就可以使用结构体作为错误了，下面还是官方的例子： struct XMLParsingError: Error { enum ErrorKind { case invalidCharacter case mismatchedTag case internalError } let line: Int let column: Int let kind: ErrorKind } func parse(_ source: String) throws -> XMLDoc { // ... throw XMLParsingError(line: 19, column: 5, kind: .mismatchedTag) // ... } 与NSError的关系 我们在Swift中自定义的Error是可以轻松通过as转为NSError的，但是我们支持NSError使用拥有localizedDescription、code、domain、UserInfo等属性的，只有实现了以下两个协议，转换后NSError的这些属性就可以生效。 LocalizedError 用来桥接原来NSError中的localizedDescription。 public protocol LocalizedError : Error { var errorDescription: String? { get } // ... } CustomNSError 用来桥接原来NSError中的code、domain、UserInfo。 public protocol CustomNSError : Error { /// The domain of the error. public static var errorDomain: String { get } /// The error code within the given domain. public var errorCode: Int { get } /// The user-info dictionary. public var errorUserInfo: [String : Any] { get } } 附： 苹果文档 Swift：Corelocation处理didFailWithError中的NSError Swift Error 的介绍和使用 Swift 3必看：Error与NSError的关系 fatalError "},"pages/其他/GWError的使用.html":{"url":"pages/其他/GWError的使用.html","title":"GWError的使用","keywords":"","body":"GWError的使用 根据业务，在项目中可以对GWClientError或GWSeverError增加case，自定义错误。 解决问题 客户端请求接口，我们需要弹出指定的提示，例如：“no network（没有网络）”、“network error（服务器错误）”。 客户端请求接口，对指定的错误码返回自定的错误，例如：动态不存在的时候，服务器会返回712200，我们需要按需求自定义提示。 编写校验逻辑的时候，我们可以根据不同的情况自定义不同的Error抛出，这样我们可以通过catch对错误统一处理。 GWError GWError是对GWClientError、GWSeverError、MoyaError的包装，也就是引发错误的来源是被包装的三种自定义错误： public enum GWError: Swift.Error { // Moya错误 case moya(MoyaError) // 客户端错误 case client(GWClientError) // 服务端错误 case server(GWSeverError) } GWError分别实现了LocalizedError、CustomNSError： extension GWError: LocalizedError { // 错误描述 public var errorDescription: String? { switch self { case let .moya(error): return error.gwErrorDescription ?? GWI18n.R.string.localizable.base_net_erroe() case let .client(error): return error.errorDescription case let .server(error): return error.errorDescription } } } extension GWError: CustomNSError { /// The domain of the error. public static var errorDomain: String = \"com.gwm.error\" // 错误码 public var errorCode: Int { switch self { case let .moya(error): return error.errorCode case let .client(error): return error.errorCode case let .server(error): return error.errorCode } } // 错误信息 public var errorUserInfo: [String: Any] { var userInfo: [String: Any] = [:] userInfo[NSLocalizedDescriptionKey] = errorDescription userInfo[NSUnderlyingErrorKey] = underlyingError return userInfo } // 内部错误 internal var underlyingError: Swift.Error? { switch self { case let .moya(error): return error case let .client(error): return error case let .server(error): return error } } } 为了兼容老代码，保留了statusCode、message、code三个属性： // 状态码 public var statusCode: Int? { switch self { case let .moya(error): return error.response?.statusCode case let .client(error): return error.statusCode case let .server(error): return error.statusCode } } // 错误提示 public var message: String { errorDescription ?? \"\" } // 错误码 public var code: String { switch self { case let .moya(error): return String(error.errorCode) case let .client(error): return String(error.code) case let .server(error): return String(error.code) } } GWClientError GWClientError是客户端错误，可以是校验逻辑中的任何一种异常的封装，同样分别实现了LocalizedError、CustomNSError，目前有如下定义：。 public enum GWClientError: Swift.Error { // 没有网络 case noNetwork // 网络异常 case networkException // 禁言错误 case muted // 提交评论失败 case postComment // 状态码 public var statusCode: Int? { // ... } // 错误提示 public var message: String { // ... } // 错误码 public var code: String { // ... } } extension GWClientError: LocalizedError { // 错误描述 public var errorDescription: String? { // ... } } extension GWClientError: CustomNSError {/// The domain of the error. public static var errorDomain: String = \"com.gwm.error.client\" // 错误码 public var errorCode: Int { // ... } // 错误信息 public var errorUserInfo: [String: Any] { // ... } } GWSeverError GWSeverError是服务端错误，是服务端异常的封装，同样分别实现了LocalizedError、CustomNSError，目前定义如下： public enum GWSeverError: Swift.Error { // 资讯评论不存在(回复评论) case newsCommentNotExist // 动态评论不存在(回复评论) case dynamicCommentNotExist // 动态评论不存在(查询评论) case dynamicCommentNotExistOnQuery // 动态不存在 case dynamicNotExist // 结果异常 case resultException(code: Int, msg: String?, statusCode: Int?) // 业务异常 static let businessErrors: [GWSeverError] = [ // ... ] // 根据code匹配业务异常 static func getError(code: Int, msg: String?, statusCode: Int?) -> GWSeverError { // ... } // 状态码 public var statusCode: Int? { // ... } // 错误提示 public var message: String { // ... } // 错误码 public var code: String { // ... } } 实现了CustomNSError的errorCode,errorCode应该和服务端错误code保持一致： extension GWSeverError: LocalizedError { // 错误描述 public var errorDescription: String? { // ... } } extension GWSeverError: CustomNSError {/// The domain of the error. public static var errorDomain: String = \"com.gwm.error.server\" // 错误码 public var errorCode: Int { switch self { // 资讯评论不存在(回复评论) case .newsCommentNotExist: return 710009 // 动态评论不存在(回复评论) case .dynamicCommentNotExist: return 712123 // 动态评论不存在(查询评论) case .dynamicCommentNotExistOnQuery: return 712197 // 动态不存在 case .dynamicNotExist: return 712200 case .resultException(let code, msg: _, statusCode: _): return code } } // 错误信息 public var errorUserInfo: [String: Any] { // ... } } 服务端通用错误 当服务端返回的错误码不匹配我们约定的errorCode时，我们统一使用resultException这个服务器错误，resultException有code、msg、statusCode三个关联属性。 // 业务异常 static let businessErrors: [GWSeverError] = [ .newsCommentNotExist, .dynamicCommentNotExist, .dynamicNotExist, .dynamicCommentNotExistOnQuery ] // 根据code匹配业务异常 static func getError(code: Int, msg: String?, statusCode: Int?) -> GWSeverError { if let customError = businessErrors.filter({ $0.errorCode == code }).first { return customError } return .resultException(code: code, msg: msg, statusCode: statusCode) } 如何使用 客户端错误的使用，下面是没有网络的时候返回了noNetwork异常： if NetworkReachabilityManager()?.isReachable == false { failedCallBack?(.client(.noNetwork)) return nil } 服务端错误的使用，下面是判断评论不存在返回了newsCommentNotExist异常： }, onError: { (error) in // 匹配自定义错误 if let gwError = error as? GWError { if gwError.code == GWSeverError.newsCommentNotExist.code { observer.onError(GWError.server(.newsCommentNotExist)) observer.onCompleted() return } } observer.onError(GWError.client(.postComment)) observer.onCompleted() }) "},"pages/其他/Swift注释.html":{"url":"pages/其他/Swift注释.html","title":"Swift注释","keywords":"","body":"Swift注释 快捷键： 光标放在方法那一行，option + command + / 可以自动生成相应的注释 单行注释： // 你要注释的内容 多行注释： /* 你要注释的内容 */ 多行嵌套注释： /*这是第一个多行注释的开头 /*这是第二个被嵌套的多行注释*/ 这是第一个多行注释的结尾*/ 文档注释： /// - Parameters: 参数 /// - item1: This is item1 /// - item2: This is item2 /// - Returns: the result string. 返回值 /// - Throws: `MyError.BothNilError` if both item1 and item2 are nil. 抛出异常 /// - Author: liuyubobobo 作者 - 无序列表 1. 有序列表 ``` 代码 # 标题 * _ 用于斜体 ** 粗体 其它： // MARK: - Methods // TODO: changeColor() // FIXME: Support Swift 2.2 附： Swift5.1—注释 Swift 注释规范和文档注释 "},"pages/其他/Universal_Links.html":{"url":"pages/其他/Universal_Links.html","title":"Universal Links","keywords":"","body":"Universal Links https://www.soga.ga/apple-app-site-association 参考文章 2020 iOS 微信分享/Universal Links 参考配置 参考配置2 测试链接(applinks:soga.ga) 官方文档 UniversalLinks和Web Credentials配置 iOS通用链接（UniversalLink）配置详细流程 iOS H5打开App(通用链接) https://cdn-h5-html.gwmcloud.com/.well-known/apple-app-site-association Universal Link 用企业证书build包，可以通过链接拉起绑定的App(客户端) iOS JMLink SDK 常见问题 iOS 9 通用链接（Universal Links） http://cdn-th-h5-html.gwmcloud.com/.well-known/outlink.html http://cdn-th-h5-html.gwmcloud.com/.well-known/apple-app-site-association "},"pages/其他/Configurations.html":{"url":"pages/其他/Configurations.html","title":"Configurations","keywords":"","body":"Configurations 用xcconfig文件配置iOS app环境变量 "},"pages/其他/iOS集成Flutter.html":{"url":"pages/其他/iOS集成Flutter.html","title":"iOS集成Flutter","keywords":"","body":"iOS集成Flutter flutter开源地址 flutter中文文档 flutter英文文档 一、flutter环境配置 参考 第1步：下载最新的SDK 下载地址 我这里SDK文件放在用户目录下，解压后可以看到在用户目录下多了一个flutter文件夹。 第2步：配置flutter命令 在用户目录下找到.bash_profile，可以直接用文本编辑器打开，或则使用vi命令： vi ~/.bash_profile 在.bash_profile中写入以下命令，完成配置 export PATH=~/flutter/bin:$PATH 第3步：检测flutter命令 帮助 flutter -h flutter安装地址 which flutter dart安装地址 which flutter dart 第4步：运行 flutter doctor 命令 flutter doctor 1.问题1 🎉  ~  flutter doctor Doctor summary (to see all details, run flutter doctor -v): [✓] Flutter (Channel stable, 2.2.1, on Mac OS X 10.15.7 19H114 darwin-x64, locale zh-Hans-CN) [✗] Android toolchain - develop for Android devices ✗ Unable to locate Android SDK. Install Android Studio from: https://developer.android.com/studio/index.html On first launch it will assist you in installing the Android SDK components. (or visit https://flutter.dev/docs/get-started/install/macos#android-setup for detailed instructions). If the Android SDK has been installed to a custom location, please use `flutter config --android-sdk` to update to that location. [✓] Xcode - develop for iOS and macOS [✓] Chrome - develop for the web [!] Android Studio (not installed) [✓] Connected device (1 available) ! Error: 陈长青的 iPhone is not connected. Xcode will continue when 陈长青的 iPhone is connected. (code -13) 将iPhone连接到电脑，并且解锁，解决问题： 🎉  ~  flutter doctor Doctor summary (to see all details, run flutter doctor -v): [✓] Flutter (Channel stable, 2.2.1, on Mac OS X 10.15.7 19H114 darwin-x64, locale zh-Hans-CN) [✗] Android toolchain - develop for Android devices ✗ Unable to locate Android SDK. Install Android Studio from: https://developer.android.com/studio/index.html On first launch it will assist you in installing the Android SDK components. (or visit https://flutter.dev/docs/get-started/install/macos#android-setup for detailed instructions). If the Android SDK has been installed to a custom location, please use `flutter config --android-sdk` to update to that location. [✓] Xcode - develop for iOS and macOS [✓] Chrome - develop for the web [!] Android Studio (not installed) [✓] Connected device (2 available) 1.问题2 [✗] Android toolchain - develop for Android devices ✗ Unable to locate Android SDK. Install Android Studio from: https://developer.android.com/studio/index.html On first launch it will assist you in installing the Android SDK components. (or visit https://flutter.dev/docs/get-started/install/macos#android-setup for detailed instructions). If the Android SDK has been installed to a custom location, please use `flutter config --android-sdk` to update to that location. 二、设置 iOS 开发环境 第1步：安装 Xcode 1.通过 直接下载 或者通过 Mac App Store 来安装最新稳定版 Xcode 2.通过在命令行中运行以下命令来配置 Xcode command-line tools: sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer sudo xcodebuild -runFirstLaunch 当你安装了最新版本的 Xcode，大部分情况下，上面的路径都是一样的。但如果你安装了不同版本的 Xcode，你可能要更改一下上述命令中的路径。 3.运行一次 Xcode 或者通过输入命令 sudo xcodebuild -license 来确保已经同意 Xcode 的许可协议。 第2步：配置 iOS 模拟器 open -a Simulator 三、创建并运行一个简单的 Flutter 应用 通过以下步骤来创建你的第一个 Flutter 应用并进行测试： 1.通过运行以下命令来创建一个新的 Flutter 应用： flutter create my_app 2.上述命令创建了一个 my_app 的目录，包含了 Flutter 初始的应用模版，切换路径到这个目录内： cd my_app 3.确保模拟器已经处于运行状态，输入以下命令来启动应用： flutter run 问题1: 🎉  ~/Desktop/代码/flutter/my_app  flutter run Multiple devices found: 陈长青的 iPhone (mobile) • ac82c56391c359c3313eb0f84cfe5f9c6fb45ca5 • ios • iOS 14.4.2 iPhone 12 Pro Max (mobile) • 4594BEED-D4F8-4CE4-A566-BF483F2A7565 • ios • com.apple.CoreSimulator.SimRuntime.iOS-14-4 (simulator) Chrome (web) • chrome • web-javascript • Google Chrome 90.0.4430.212 [1]: 陈长青的 iPhone (ac82c56391c359c3313eb0f84cfe5f9c6fb45ca5) [2]: iPhone 12 Pro Max (4594BEED-D4F8-4CE4-A566-BF483F2A7565) [3]: Chrome (chrome) Please choose one (To quit, press \"q/Q\"): 这里直接选择2，2是我刚才打开的模拟器，接下来会看到： Please choose one (To quit, press \"q/Q\"): 2 Launching lib/main.dart on iPhone 12 Pro Max in debug mode... Running Xcode build... └─Compiling, linking and signing... 10.8s Xcode build done. 39.8s Syncing files to device iPhone 12 Pro Max... 89ms Flutter run key commands. r Hot reload. 🔥🔥🔥 R Hot restart. h Repeat this help message. d Detach (terminate \"flutter run\" but leave application running). c Clear the screen q Quit (terminate the application on the device). 💪 Running with sound null safety 💪 An Observatory debugger and profiler on iPhone 12 Pro Max is available at: http://127.0.0.1:54602/2Xt-Ae85-qk=/ Activating Dart DevTools... 24.5s The Flutter DevTools debugger and profiler on iPhone 12 Pro Max is available at: http://127.0.0.1:9100?uri=http%3A%2F%2F127.0.0.1%3A54602%2F2Xt-Ae85-qk%3D%2F 这个时候，切换至模拟器，就可以看到应用的界面了！ 注意：如果选择1真机的话，需要打开Runner.xcworkspace，在xcode中配置好证书描述文件。 四、解决现有flutter项目问题 问题1 Invalid Podfile file: cannot load such file -- ../my_flutter/.ios/Flutter/podhelper.rb. 参考 问题2 Version solving failed. #18937 问题3 pub get failed (server unavailable) 问题4 Include of non-modular header inside framework module error "},"pages/其他/装机必备.html":{"url":"pages/其他/装机必备.html","title":"装机必备","keywords":"","body":"装机必备 一、升级系统 Mac系统如何显示隐藏文件 mac软件网址 xcode下载 如何生成 SSH 密钥 二、触摸板设置 轻点来点按：不用按压触摸板即可点击 三指拖拽：辅助功能->指针控制->触摸板选项->鼠标与触摸板->启用拖动->三指拖移 三、软件安装 Amphetamine：阻止mac睡眠 Sourcetree：代码管理 The Unarchiver：解压缩 Google Chrome：浏览器 ClashX：翻墙 DevCleaner：清理Xcode Xcode：开发 Sumblime Text：文本编辑 OmniDiskSweeper TeamViewer royaltsx：服务器上传 AE/AI/DW/InDesign/PR/Sketch/Principle/字魂：设计类 Word/Excel/PPT：微软 MenuMeters：监控CPU aifred3：红帽子 oh-my-zsh/iTerm：命令行 SwitchHosts：切Host My Hosts 185.199.108.133 raw.githubusercontent.com 199.232.68.133 user-images.githubusercontent.com 199.232.68.133 avatars2.githubusercontent.com 199.232.68.133 avatars1.githubusercontent.com Postman：请求测试 SwiftFormat for Xcode OBS：录屏幕 MachOView/iOS App Signer：越狱 DeepL：翻译 Wacom：数位板 Woodpecker：iOS屏幕测试 libreoffice：打开word Paintbrush：画板 Vysor：Android手机投屏电脑 Android File Transfer：Android数据传输 VLC media player Charles WiresShark 四、安装brew macos如何科学的安装homebrew跟npm? brew update 报错 \"fatal: Could not resolve HEAD to a revision\" 安装brew的正确姿势 Mac安装git，brew出现的问题 五、安装node MAC 安装BREW跟NODE mac管理及更新node版本 六、安装gitbook GitBook 安装以及使用 安装Gitbook GitBook快速生成PDF 配置ebook问题 $ sudo ln -s ~/Applications/calibre.app/Contents/MacOS/ebook-convert /usr/bin 七、安装CocoaPods ruby管理 iOS 安装CocoaPods 临时解决GitHub的raw.githubusercontent.com无法连接问题 八、安装PS Adobe Photoshop 2021 For Mac v22.3 PS中文破解版 九、Flutter flutter2.2.1 "},"pages/其他/常用链接.html":{"url":"pages/其他/常用链接.html","title":"常用链接","keywords":"","body":"常用链接 iOS·问题汇总 UITextField PlaceHolder居中显示问题 iOS 10、设置导航栏全透明 两种iOS隐藏导航栏的正确方法 让超出父视图范围的子视图响应事件，在UIView范围外响应点击 iOS时间的处理 ios 消除 字符串 首尾空格 iOS优美的侧滑返回FDFullscreenPopGesture iOS 设置屏幕常亮，延长休眠时间 arm64、armv7、armv7s iOS获取手机型号 Cannot find protocol definition for XXX ios – 奇怪的UIView-Encapsulated-Layout-Height错误 fix:iOS 13.1.3,textView点击回调3次 \"Double-quoted\" issue in latest version of Firebase rem布局在webview中页面错乱解决办法 解決UIWebView target = '_blank'不能彈出 Cocoapod with optional subspecs is not installing, if subspecs in the Podfile and another dependent Cocoapod podspec doesn't match Can I have an init func in a protocol? 构造方法不触发didSet iOS9中调用其他APP时出现This app is not allowed to query 微信怎么唤起app? facebook链接中分享的图片 line链接中分享的图片 Swift - encode URL Class Only Protocols In Swift 5 git删除远程分支报错:remote ref does not exist ERROR | [iOS] unknown: Encountered an unknown error (/usr/bin/xcrun simctl list -j devices Mac解疑：处理 Adobe Genuine Software Integrity Service assigning to ‘NSString *_strong’ from ‘const NSString’ discards qualifiers iOS10 NSLog限制为1024个字符字符串 Auto property synthesis will not synthesize property ‘delegate’;it will be implemented by its superc NSPredicate 谓词 更改 macOS 用户帐户和个人文件夹的名称 苹果 iOS 13 新增的 sign with Apple API 是如何实现隐私保护的？ Xcode 11新建项目多了Scenedelegate 使用 Xcode 13.2.1 和 iOS 15.4 设备 圆角和阴影并存（Swift和OC） Swiftlint warning : For Where Violation: where clauses are preferred over a single if inside a for. (for_where) 【ios】_UITemporaryLayoutWidth是什麼，為什麼它打破了我的約束？ Git 提交错了不用慌，这三招帮你修改记录 解决Xcode14 pod签名问题 iOS UILabel 添加查看更多 UILabel中的NSAttributedString尾部截断 UITableView或UICollectionView的cell中嵌套UICollectionView后,第二层的CollectionViewCell点击无响应的问题 ScrollView嵌套 iOS开发实战 - 解决UIScrollView嵌套滑动手势冲突 LYEmbedScrollView JXCategoryView MXParallaxHeader 关于嵌套滚动现实的讨论 iOS·新知识 使用 Swift Package Manager 集成依赖库 Swift化零为整：Reduce方法详解 swift3.0中fileprivate，private使用 Swift与泛型编程第四弹：类型擦除 iOS 插件化开发(动态库研究) 在 iOS 上下载文件 iOS归档看这篇就够了 ios - Swift URL查询字符串获取参数 Swift打印变量内存地址 swift小知识点之打印对象的地址 Swift 如何声明某个属性已过期 swift5.0 字符串截取 iOS开发之字符串(NSString)的拼接 iOS - NSDate分类-判断时间是否为今天,昨天,一周内,年月日 IOS 保留小数点后几位 Swift开发小技巧系列 - 浮点型数据的四舍五入 UIPresentationController简介 不要用子类！Swift的核心是面向协议 APP速度优化--启动优化 Swift只有上半部分圆角View XCode模拟器屏幕录制/录屏 UIView的autoresizingMask 关于Swift：如何对数组进行分组 SnapKit 关于数组Array的扩展—— 自适应宽度、垂直、水平、九宫格布局 do while妙用 iOS中取消延迟执行函数 iOS 阿拉伯语 RTL适配 Swift - 实现图片（UIImage）的水平翻转(镜像)，垂直翻转 UIView动画 Swift 图片拉伸，屡试不爽！！！ 为什么我们应该避免在 Swift 结构中使用闭包？ CocoaPods清理本地缓存 ExclusiveTouch OC中基于Runtime机制hook函数的多种方法 iOS自定义TableView索引 自定义模板 iOS 高效开发之 - 3分钟实现自定义 Xcode 初始化的模板 Xcode 自定义模板 iOS·审核 检查iOS项目中是否还有使用UIWebView iOS APP内测邀请之TestFlight iOS·多线程 实现自定义NSOperation 开始使用Operation Queue吧 iOS开发 自定义NSOPeration NSOperation property overrides (isExecuting / isFinished) 并发教程 iOS 多线程 - Operation iOS·性能优化 iOS 关于后台持续运行 iOS性能优化 iOS按钮倒计时在进入后台不继续计时的处理 iOS 后台数据处理 background fetch 动画 iOS动画事物（CATransaction） 工具 熊猫压缩 远程 udid获取 JSON格式化 代码格式化插件 App图标在线制作 压缩一个PDF文件 UIImage 二分压缩图片 Woodpecke‪r‬ 图片裁剪 临时邮箱、临时邮、临时电子邮箱、24小时邮箱 JSON格式化 Woodpecker使用 Xcode自定义代码块 时间戳cuo google play store 颜色代码表 阿里网盘资源 在线工具 语法检查 Gitbook 新版gitbook导出pdf gitbook教程 推荐12个实用的gitbook插件 mermaid插件 mermaid-gb3 mermaid文档 GitBook 插件 PlantUML 简介 GitBookW3C Gitbook详解（四）-配置和说明详解 定义右面页面的宽度 npmjs GitBook 本地使用排雷，及导出基本可用的 PDF 版本 案例 百度统计 Gitbook 静态站点 加入谷歌分析等网站统计代码 百度统计助手 百度统计后台 Markdown 用 Markdown 制作简历 Markdown Guide MarkDown代码块高亮 在 markdown 中生成并导出思维导图的 Gitbook 插件 gitbook 3.2.3及之后流程图解决方案 Markdown 中文文档 markdown编辑 学生、工作人士如何才能更优雅地记笔记、写文档?（Markdown教程，详细到超乎你想象） 使用 Markdown 时，如何为文字加下划线？ Markdown 简介 MarkDown中如何换行？ 简历 iOS程序员简历模板 好看的博客 bawn YouXianMing 笑忘书店 八年iOS架构师教你如何一举拿下iOS工程师的Offer,（附面试技巧） Cocoapods CocoaPods安装方法-2020.05.25 XCode 10中修改cocoapods中的源码编译不生效的解决方法 Cocoapods整理（三）——编写podspec文件 装机步骤 iOS 安装CocoaPods Mac系统如何显示隐藏文件 临时解决GitHub的raw.githubusercontent.com无法连接问题 Mac安装git，brew出现的问题 macos如何科学的安装homebrew跟npm? 安装brew的正确姿势 GitBook 安装以及使用 mac管理及更新node版本 安装Gitbook mac软件网址 xcode下载 开源项目 iOSExperience functionList GitBook 上有哪些十分优秀的已经完成的书？ 滴滴的哆啦A梦组件 iOS 打点 闪屏demo QGTextField URLNavigator 网络监控 Swift-30-Projects iOSKeyPointExploration 语法糖 TTBaseUIKit GAppFramework SwiftTheme NerdyUI Tangram-iOS YNSearch IOS网络请求的简单封装设计 YTKNetwork QMUI_iOS 代码分类 好库编程网 SDUserDefaults LCHelper geniusDemo 2018 iOS 三方库(仅供方面查看) macos 12.3.1 Xcode 13.3 最新objc 838源码更新 国家icon Stevia MVVM MVVMReactiveCocoa BigShow1949 Monkey MHDevelopExample_Objective_C MVVMDemo 地图 map BMKLocationKit iOS在APP中调用第三方地图地图（苹果,高德，百度，腾讯） 经纬度查询 学习 尚德 自考 潭州课堂 swift51 csdn 苹果官方文档 苹果官方文档-cn 苹果官方文档-en ios-resolution 查询序列号 OpenGL iOS-OpenGL-Tutorials OpenGLES RxSwift Swift - RxSwift的使用详解1（基本介绍、安装配置） RxSwift的学习之路（二）——Subjects 使用 RxTest 来建立基于 RxSwift 的自动化测试 RxSwift中文文档 RxSwift学习 - share(replay:scope:) RXSwift中Driver的使用 RxSwift+Moya网络请求之项目实战 RxMultiCounter RxSwift Error Handling RxDataSource 使用套路与解释 RxSwift 中的老司机 VPN cx vpn·周 vpn·李 有免费的梯子软件 导航栏 iOS系统中导航栏的转场解决方案与最佳实践 IOS-FDFullscreenPopGesture的使用 我的iOS开发笔记——右滑返回手势失效怎么办？ iOS 导航栏的那些事儿 Swift - 导航栏滑动透明渐变效果的实现（透明度随视图滚动而改变） iOS·UI 详解intrinsicContentSize 及 约束优先级／content Hugging／content Compression Resistance iOS开发笔记常用工具之文本宽度和高度计算 iOS 获取webview高度小结 iOS Swift 判断手机机型 已更新 至iPhone12 iOS判断刘海屏幕机型 所有机型 iOS 设置圆角、指定位置圆角及 iOS 11圆角设置 iOS9 Programming - Autolayout (I) 脚本 sudo pip install openpyxl 快捷键 注释：command+option+/ 打开xcode文档：command+shift+0 现在在Xcode中切换标签的快捷方式？ Android Studio常用快捷键汇总（mac） Flutter Flutter 开发文档 其它 医院等级查询 快速记 shell Shell 函数 shell脚本中判断上一个命令是否执行成功 shell bash判断文件或文件夹是否存在 shell脚本：删除当前目录下除了某几个文件之外的其他文件 ruby Ruby 教程 webview缓存 iOS html5使用缓存并及时更新方案总结 Cache-control iOS webView缓存，保证加载最新html iOS: 聊聊 UIWebView 缓存 H5 和移动端 WebView 缓存机制解析与实战 iOS-WKWebView缓存并保证实时性 iOS代码混淆工具 webview开启无图 iOS定制NSURLProtocol实现离线缓存 web离线技术原理 iOS 开发中使用 NSURLProtocol 拦截 HTTP 请求 使用 NSURLProtocol 拦截 APP 内的网络请求 移动 H5 首屏秒开优化方案探讨 JWNetAutoCache tableView https://www.jianshu.com/p/af4bc69839d8 优化UITableViewCell高度计算的那些事 代码规范 《Effective Objective-C》干货三部曲（二）：规范篇 Swift 如何声明某个属性已过期 单元测试 iOS 单元测试--异步测试 OC实现类似泛型效果的json数据解析 架构 iOS应用架构谈 网络层设计方案 iOS - AFNetWorking打印请求时的相关信息 一键登录 IOS客户端接入 gcd iOS GCD处理多个网络请求并发问题 iOS-GCD的串行队列和并行队列的任务及实现 iOS GCD线程同步以及AFNetworking多次请求线程依赖 c++ C++ 中的 inline 用法 在字符串string中实现一个判断函数，该函数功能是统计某一字符串类对象(仅由单词和空格组成)有多少个单词 IM聊天 环信·IM 环信·iOS SDK 快速集成 环信·集成 iOS SDK 前的准备工作 电影 你的名字 后端 docker 腾讯云 算法 算法学习笔记（目录） 菜鸟算法 Swift - Codable Swift - Codable使用小记 Swift - Codable textview iOS开发－UITextView文字排版 UITextView换行问题解决办法 android studio 用户指南 阿语适配 iOS 国际化 - 阿拉伯语的RTL 踩坑心得 Working with Unicode code points in Swift Swift进阶九:字符串 完整的Unicode地址 iOS检测字符串的语言（用于翻译之前检测） Detect Language of NSString 解决关于swift的Array repeating 初始化多个对象问题 简历优化 iOS开发求职者，写一份成功的简历？ 7年iOS开发经验，教你写一份脱颖而出的简历，进入大厂机会翻3倍！ 怎么样在简历中体现出自己的能力与价值? IOS开发工程师简历范文，【工作经历+项目经验+自我评价】怎么写 iOS高级面试简历指导 图片 头像 表 CJIABA这个牌子的手表一般的价格是多少 casio 浪琴efco是什么档次 https://wen.baidu.com/question/1839889959679620900.html Longines 浪琴 tianbo 天博手表 机械 wilon手表 机械 威斯登 swatch 劳力士（ROLEX） 美格尔(MEGIR)手表 moosie rosdn leier fila "},"pages/Git/相关链接.html":{"url":"pages/Git/相关链接.html","title":"相关链接","keywords":"","body":"相关链接 git如何删除远端不存在的本地分支？ 关于Git协作，除了知道merge，你可能还需要知道rebase git写错分支，如何将一个分支上的修改转移到另一个分支上 Xcode连接git@osc Rebase git rebase 还是 merge的使用场景最通俗的解释 git rebase -i合并多次提交 GIT上fork的项目获取最新源代码 Git 命令收集 Git 删除某一次提交 Git rebase合并多条commit记录 git cherry-pick 教程 git stash 用法总结和注意点 "},"pages/Git/rebase使用.html":{"url":"pages/Git/rebase使用.html","title":"rebase使用","keywords":"","body":"rebase使用 查看日志 git log --oneline -10 git reflog 拉取远程 git fetch team 基变 git rebase team/feature/v3.7.0 git rebase --continue git rebase -i cea405e6f 强推 git push origin feature/v3.7.0 -f git reflog git cherry-pick a89dbd17f git config --global pull.rebase true "},"pages/C++程序设计/第一章_C++语言简介.html":{"url":"pages/C++程序设计/第一章_C++语言简介.html","title":"第一章 C++语言简介","keywords":"","body":"第一章 C++语言简介 "},"pages/C++程序设计/第二章_面向对象的基本概念.html":{"url":"pages/C++程序设计/第二章_面向对象的基本概念.html","title":"第二章 面向对象的基本概念","keywords":"","body":"第二章 面向对象的基本概念 "},"pages/C++程序设计/第三章_类和对象进阶.html":{"url":"pages/C++程序设计/第三章_类和对象进阶.html","title":"第三章 类和对象进阶","keywords":"","body":"第三章 类和对象进阶 "},"pages/C++程序设计/第四章_运算符重载.html":{"url":"pages/C++程序设计/第四章_运算符重载.html","title":"第四章 运算符重载","keywords":"","body":"第四章 运算符重载 "},"pages/C++程序设计/第五章_类的继承与派生.html":{"url":"pages/C++程序设计/第五章_类的继承与派生.html","title":"第五章 类的继承与派生","keywords":"","body":"第五章 类的继承与派生 "},"pages/C++程序设计/第六章_多态与虚函数.html":{"url":"pages/C++程序设计/第六章_多态与虚函数.html","title":"第六章 多态与虚函数","keywords":"","body":"第六章 多态与虚函数 "},"pages/C++程序设计/第七章_输入_输出流.html":{"url":"pages/C++程序设计/第七章_输入_输出流.html","title":"第七章 输入/输出流","keywords":"","body":"第七章 输入/输出流 "},"pages/C++程序设计/第八章_文件操作.html":{"url":"pages/C++程序设计/第八章_文件操作.html","title":"第八章 文件操作","keywords":"","body":"第八章 文件操作 "},"pages/C++程序设计/第九章_函数模版与类模板.html":{"url":"pages/C++程序设计/第九章_函数模版与类模板.html","title":"第九章 函数模版与类模板","keywords":"","body":"第九章 函数模版与类模板 "},"pages/旅游安排/东极岛.html":{"url":"pages/旅游安排/东极岛.html","title":"东极岛","keywords":"","body":"东极岛 "},"pages/推送/相关链接.html":{"url":"pages/推送/相关链接.html","title":"相关链接","keywords":"","body":"相关链接 客户端技术：一文带你了解 iOS 消息推送机制 iOS开发——iOS静默推送介绍及使用场景 iOS10 推送extension之 Service Extension你玩过了吗？ 极光 "},"./":{"url":"./","title":"最近更新","keywords":"","body":"Mac下MYSQL的安装 环境：macos 11.7.4 一、MYSQL下载 1、进入官网，滑动至最下方，找到Downloads，点击MySQL Community Server。2、点击Archives，选择Product Version:5.7.31，选择Operating System:macOS。3、下载“macOS 10.14 (x86, 64-bit), Compressed TAR Archive”。4、解压缩： cd /usr/local/ sudo mkdir src sudo mv ~/Downloads/mysql-5.7.31-macos10.14-x86_64.tar.gz src sudo tar -xzvf mysql-5.7.31-macos10.14-x86_64.tar.gz sudo ln -sf mysql-5.7.31-macos10.14-x86_64 mysql sudo chown -R chenchangqing:staff mysql* 二、配置环境变量 1、 vi ~/.bash_profile，在 ~/.bashrc 中添加如下配置项。 MYSQL_HOME=/usr/local/mysql export PATH=$PATH:$MYSQL_HOME/bin:$MYSQL_HOME/support-files 2、source ~/.bash_profile。3、mysql --version。 chenchangqingdeMacBook-Pro-2:sdxy chenchangqing$ mysql --version mysql Ver 14.14 Distrib 5.7.31, for macos10.14 (x86_64) using EditLine wrapper 4、错误分析： dyld: Symbol not found: __ZTTNSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE Referenced from: /usr/local/mysql/bin/mysql (which was built for Mac OS X 12.0) Expected in: /usr/lib/libc++.1.dylib in /usr/local/mysql/bin/mysql Abort trap: 6 如果出现以上错误，说明下载的mysql版本和当前的macos系统不匹配，比如“macos 11.7.4”下载了“macOS 13 (x86, 64-bit), Compressed TAR Archive”，就会出现上面的错误。 -bash: /usr/local/mysql/bin/mysql: Bad CPU type in executable。 如果出现以上错误，说明下载的mysql版本与当前macos系统不匹配CPU架构不匹配，比如“macos 11.7.4”下载了“macOS 12 (ARM, 64-bit), Compressed TAR Archive”，就会出现上面的错误。 三、初始化root chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysqld --initialize-insecure 2023-03-24T17:25:46.055794Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details). 2023-03-24T17:25:46.057786Z 0 [Warning] Setting lower_case_table_names=2 because file system for /usr/local/mysql-5.7.31-macos10.14-x86_64/data/ is case insensitive 2023-03-24T17:25:46.237798Z 0 [Warning] InnoDB: New log files created, LSN=45790 2023-03-24T17:25:46.269409Z 0 [Warning] InnoDB: Creating foreign key constraint system tables. 2023-03-24T17:25:46.329085Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: e99f3ed4-ca68-11ed-b222-0a4a56d116f7. 2023-03-24T17:25:46.340828Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened. 2023-03-24T17:25:46.790867Z 0 [Warning] CA certificate ca.pem is self signed. 2023-03-24T17:25:46.937195Z 1 [Warning] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option. 从输出可以看到，mysqld 已经帮我们创建了一个 root 用户，且该 root 用户的 password 为空。 四、启动MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server start Starting MySQL . SUCCESS! 五、登录root chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql -uroot -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 2 Server version: 5.7.31 MySQL Community Server (GPL) Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> 六、给root用户创建密码 mysql> ALTER USER root@localhost IDENTIFIED WITH caching_sha2_password BY '123456'; -> ; ERROR 1524 (HY000): Plugin 'caching_sha2_password' is not loaded MySQL新版默认使用caching_sha2_password作为身份验证插件，而旧版是使用mysql_native_password。当连接MySQL时报错“plugin caching_sha2_password could not be loaded”时，可换回旧版插件。 mysql> ALTER USER root@localhost IDENTIFIED WITH mysql_native_password BY '123456'; Query OK, 0 rows affected (0.00 sec) mysql> FLUSH PRIVILEGES; Query OK, 0 rows affected (0.00 sec) mysql> quit Bye 七、常用命令 1、启动MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server start Starting MySQL SUCCESS! 2、停止MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server stop Shutting down MySQL .. SUCCESS! 3、重启MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server restart ERROR! MySQL server PID file could not be found! Starting MySQL . SUCCESS! 4、检查 MySQL 运行状态 chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server status SUCCESS! MySQL running (1725) 八、参考 https://learnku.com/articles/62379 https://blog.csdn.net/weixin_33728077/article/details/113902283 https://www.cnblogs.com/yjmyzz/p/how-to-install-mysql8-on-mac-using-tar-gz.html 备案号： -->沪ICP备2022002183号-1 "}}