{"pages/java/Domain_name_registration_and_filing.html":{"url":"pages/java/Domain_name_registration_and_filing.html","title":"域名的注册及备案","keywords":"","body":"域名的注册及备案 一、购买云服务器 打开腾讯云->产品->热门产品->轻量应用服务器 二、购买域名 腾讯云·DNSPod 公安备案指南 腾讯云·控制台 三、域名解析 腾讯云·DNS解析DNSPod DNS.TECH 域名检测 域名已经实名认证，但是显示状态还是注册局暂停解析 四、mac上登录服务器及上传 指定端口号运行gitbook serve --port 80 怎样在Mac上SSH和FTP？完美替代XShell是哪个软件？item2吗？Royal TSX! 没有比它更好 五、centos7.6上安装gitbook CentOS7.6安装Nodejs(Npm) CentOS7系统中node安装配置 make_unique 不是 'std'成员 Centos 7默认gcc版本为4.8，有时需要更高版本的，这里以升级至8.3.1版本为例，分别执行下面三条命令即可，无需手动下载源码编译 Centos7卸载nodejs 在centos7上安装gitbook debug of plugin-mathjax of gitbook 六、FTP Linux 云服务器搭建 FTP 服务 七、Ngnix 连前端都看得懂的《Nginx 入门指南》 Nginx 服务器 SSL 证书安装部署 Centos 安装 Gitbook Centos7.6安装和配置最新版Nginx服务 centOS7.6安装Nginx "},"pages/java/Java_Development_Tools.html":{"url":"pages/java/Java_Development_Tools.html","title":"Java开发工具","keywords":"","body":"Java开发工具 Spring Boot 我的Spring Boot学习之路！ IntelliJ IDEA IntelliJ IDEA 2022.2.3破解版图文教程mac,windows,linux均适用（2022.11.10亲测有效） "},"pages/java/configenv.html":{"url":"pages/java/configenv.html","title":"Mac下配置JDK","keywords":"","body":"Mac下配置JDK 如何下载JDK？ 官网地址：https://www.oracle.com/java/technologies/downloads/ 打开官网下载地址->点击“Java archive”->拉到最下面，点击目标版本（Java SE 7）->Mac，点击jdk-7u80-macosx-x64.dmg->登录下载->双击，然后一路next安装 查看JDK安装后的路径 /usr/libexec/java_home -V 输出： chenchangqingdeMacBook-Pro-2:sdxy chenchangqing$ /usr/libexec/java_home -V Matching Java Virtual Machines (3): 19.0.2 (x86_64) \"Oracle Corporation\" - \"OpenJDK 19.0.2\" /Users/chenchangqing/Library/Java/JavaVirtualMachines/openjdk-19.0.2/Contents/Home 1.7.80.15 (x86_64) \"Oracle Corporation\" - \"Java\" /Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home 1.7.0_80 (x86_64) \"Oracle Corporation\" - \"Java SE 7\" /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home /Users/chenchangqing/Library/Java/JavaVirtualMachines/openjdk-19.0.2/Contents/Home 记好即将要配置的JDK路径： /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 配置JDK环境变量 编辑.bash_profile文件 vi ~/.bash_profile 添加以下内容，JAVA_HOME就是上面的JDK路径。 # JDK JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home PATH=$JAVA_HOME/bin:$PATH:. CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:. export JAVA_HOME export PATH export CLASSPATH 输入以下命令使配置文件生效： source ~/.bash_profile 查看是否配置成功: 1.查看JDK路径： echo $JAVA_HOME 输出： /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 2.查看JDK的版本信息: java -version 输出： java version \"1.7.0_80\" Java(TM) SE Runtime Environment (build 1.7.0_80-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) 参考：https://juejin.cn/post/6844903878694010893参考：https://blog.csdn.net/chwshuang/article/details/54925950 "},"pages/mysql/Installation_of_MYSQL_under_Mac.html":{"url":"pages/mysql/Installation_of_MYSQL_under_Mac.html","title":"Mac下MYSQL的安装","keywords":"","body":"Mac下MYSQL的安装 环境：macos 11.7.4 一、MYSQL下载 1、进入官网，滑动至最下方，找到Downloads，点击MySQL Community Server。2、点击Archives，选择Product Version:5.7.31，选择Operating System:macOS。3、下载“macOS 10.14 (x86, 64-bit), Compressed TAR Archive”。4、解压缩： cd /usr/local/ sudo mkdir src sudo mv ~/Downloads/mysql-5.7.31-macos10.14-x86_64.tar.gz src sudo tar -xzvf mysql-5.7.31-macos10.14-x86_64.tar.gz sudo ln -sf mysql-5.7.31-macos10.14-x86_64 mysql sudo chown -R chenchangqing:staff mysql* 二、配置环境变量 1、 vi ~/.bash_profile，在 ~/.bashrc 中添加如下配置项。 MYSQL_HOME=/usr/local/mysql export PATH=$PATH:$MYSQL_HOME/bin:$MYSQL_HOME/support-files 2、source ~/.bash_profile。3、mysql --version。 chenchangqingdeMacBook-Pro-2:sdxy chenchangqing$ mysql --version mysql Ver 14.14 Distrib 5.7.31, for macos10.14 (x86_64) using EditLine wrapper 4、错误分析： dyld: Symbol not found: __ZTTNSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE Referenced from: /usr/local/mysql/bin/mysql (which was built for Mac OS X 12.0) Expected in: /usr/lib/libc++.1.dylib in /usr/local/mysql/bin/mysql Abort trap: 6 如果出现以上错误，说明下载的mysql版本和当前的macos系统不匹配，比如“macos 11.7.4”下载了“macOS 13 (x86, 64-bit), Compressed TAR Archive”，就会出现上面的错误。 -bash: /usr/local/mysql/bin/mysql: Bad CPU type in executable。 如果出现以上错误，说明下载的mysql版本与当前macos系统不匹配CPU架构不匹配，比如“macos 11.7.4”下载了“macOS 12 (ARM, 64-bit), Compressed TAR Archive”，就会出现上面的错误。 三、初始化root chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysqld --initialize-insecure 2023-03-24T17:25:46.055794Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details). 2023-03-24T17:25:46.057786Z 0 [Warning] Setting lower_case_table_names=2 because file system for /usr/local/mysql-5.7.31-macos10.14-x86_64/data/ is case insensitive 2023-03-24T17:25:46.237798Z 0 [Warning] InnoDB: New log files created, LSN=45790 2023-03-24T17:25:46.269409Z 0 [Warning] InnoDB: Creating foreign key constraint system tables. 2023-03-24T17:25:46.329085Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: e99f3ed4-ca68-11ed-b222-0a4a56d116f7. 2023-03-24T17:25:46.340828Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened. 2023-03-24T17:25:46.790867Z 0 [Warning] CA certificate ca.pem is self signed. 2023-03-24T17:25:46.937195Z 1 [Warning] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option. 从输出可以看到，mysqld 已经帮我们创建了一个 root 用户，且该 root 用户的 password 为空。 四、启动MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server start Starting MySQL . SUCCESS! 五、登录root chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql -uroot -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 2 Server version: 5.7.31 MySQL Community Server (GPL) Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> 六、给root用户创建密码 mysql> ALTER USER root@localhost IDENTIFIED WITH caching_sha2_password BY '123456'; -> ; ERROR 1524 (HY000): Plugin 'caching_sha2_password' is not loaded MySQL新版默认使用caching_sha2_password作为身份验证插件，而旧版是使用mysql_native_password。当连接MySQL时报错“plugin caching_sha2_password could not be loaded”时，可换回旧版插件。 mysql> ALTER USER root@localhost IDENTIFIED WITH mysql_native_password BY '123456'; Query OK, 0 rows affected (0.00 sec) mysql> FLUSH PRIVILEGES; Query OK, 0 rows affected (0.00 sec) mysql> quit Bye 七、常用命令 1、启动MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server start Starting MySQL SUCCESS! 2、停止MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server stop Shutting down MySQL .. SUCCESS! 3、重启MYSQL chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server restart ERROR! MySQL server PID file could not be found! Starting MySQL . SUCCESS! 4、检查 MySQL 运行状态 chenchangqingdeMacBook-Pro-2:local chenchangqing$ mysql.server status SUCCESS! MySQL running (1725) 创建数据库 http://c.biancheng.net/view/2413.html 八、参考 https://learnku.com/articles/62379 https://blog.csdn.net/weixin_33728077/article/details/113902283 https://www.cnblogs.com/yjmyzz/p/how-to-install-mysql8-on-mac-using-tar-gz.html "},"pages/idea/env.html":{"url":"pages/idea/env.html","title":"IDEA环境配置","keywords":"","body":"IDEA环境配置 链接地址 官方地址：https://www.jetbrains.com/idea/ 官方使用文档：https://www.jetbrains.com/idea/getting-started.html 安装目录 核心bin目录介绍 IDEA的VM配置，是指占用的机器内存。idea.vmoptions: -Xms128m// 占用最小内存 -Xmx750m// 张永最大内存 -XX:ReservedCodeCacheSize=512m// 代码占用的缓存大小 -XX:+UseG1GC -XX:SoftRefLRUPolicyMSPerMB=50 -XX:CICompilerCount=2 -XX:+HeapDumpOnOutOfMemoryError -XX:-OmitStackTraceInFastThrow -XX:+IgnoreUnrecognizedVMOptions -XX:CompileCommand=exclude,com/intellij/openapi/vfs/impl/FilePartNodeRoot,trieDescend -ea -Dsun.io.useCanonCaches=false -Dsun.java2d.metal=true -Djbr.catch.SIGABRT=true -Djdk.http.auth.tunneling.disabledSchemes=\"\" -Djdk.attach.allowAttachSelf=true -Djdk.module.illegalAccess.silent=true -Dkotlinx.coroutines.debug=off -XX:ErrorFile=$USER_HOME/java_error_in_idea_%p.log -XX:HeapDumpPath=$USER_HOME/java_error_in_idea.hprof -Dfile.encoding=UTF-8// 设置文件编码格式 -Dconsole.encoding=UTF-8// 设置控制台编码格式 如果电脑低于8G没有太多的修改必要，如果16G的内存，可以适当的修改最小内存和最大内存的值，调整最小内存可以提供Java程序的启动速度，调整最大内存可以减少内存回收的频率，提供程序性能。 常用配置 设置常规视图界面 注意：由于项目具体的不同，展示的界面也不尽相同。 设置主题 设置启动时是否打开项目 设置鼠标滚轮修改字体 设置自动打包 设置行号和分隔符 代码提示规则 取消单行显示 设置编辑区字体 设置编辑区主题 更多主题 修改控制台字体 修改注释颜色 修改类头的注释文档 设置项目文件编码 Build,Exeution,Deployment "},"pages/idea/createp.html":{"url":"pages/idea/createp.html","title":"IDEA创建项目","keywords":"","body":"IDEA创建项目 创建Java项目 打开IDEA 点击NewProject，显示NewProject窗口 左边菜单默认选中New Project 输入项目名称，指定项目路径、语言、Build System、JDK 点击Create 创建Java中的package 创建类、接口、枚举、注解 创建空项目和Module及相关操作 打开IDEA 点击NewProject，显示NewProject窗口 左边菜单默认选中Empty Project 输入项目名称 点击Create 创建Module 方式一： 点击File->New->Module 输入Module名称->点击Create 方式二： 点击右上角设置 点击Project Structure 选中Modules，点击+ 输入Module名称->点击Create 项目支持web 右键module名称 Add Framework Support 勾选Java EE->Web Application(4.0) 点击OK 删除Module 方式一： 右键module名称 Remove Module 方式二： 点击右上角设置 点击Project Structure 选中Modules，点击- 注意：Remove Module后，需要删除硬盘里的Module文件夹。 创建Java空项目和Module及相关操作 创建一个Java项目 删除src 右键项目名称，New->Module 创建Maven的Java项目 新建项目 编辑pom.xml 输入： org.springframework spring-context 5.3.16 com.alibaba druid 1.2.8 点击右上角的刷新，即可自动下载对应版本的jar，查看如下： 创建Maven的Web项目 新建Module 选中maven-archetype-webapp 新建java文件夹 运行 点击 Add new run configuration，选择Tomcat Local 点击fix，点击xxx war，点击apply，点击OK，点击运行。 使用Maven创建SpringBoot项目 IntelliJ IDEA右键无法创建Java Class文件 错误:(3, 32) java: 程序包org.springframework.boot不存在的解决访问 java 程序包org.springframework.boot不存在 HttpMessageNotWritableException: No converter for [...] with preset Content-Type 'null'] with OpenApi Spring generator HttpMessageNotWritableException: No Converter for [class …] With Preset Content-Type IDEA中用maven创建的Servlet项目 "},"pages/idea/quickkey.html":{"url":"pages/idea/quickkey.html","title":"IDEA快捷键","keywords":"","body":"IDEA快捷键 This browser does not support PDFs. Please download the PDF to view it: Download PDF. 设置快捷键风格 修改快捷键 常用快捷键 Alt + 7 改为 Alt + F12 "},"pages/idea/template.html":{"url":"pages/idea/template.html","title":"IDEA模版","keywords":"","body":"IDEA模版 官方介绍：https://www.jetbrains.com/help/idea/using-live-templates.html 代码模板是指，配置一些常用的代码字母缩写后，当输入缩写字母时，IDEA会根据输入的字母缩写，帮助自动完成预设的代码。从而提供编码效率，同时也可以进行个性化设置，例如：注释模板。 内置模版 在IDEA中，有很多内置的编码模板，使用者只需要敲击简单的前缀即可生成代码。但是以下模板是固定的无法修改。 鉴于Postfix Completion模板无法修改，IDEA提供了Live Templates模板，该模板用户可以自定义。 常用模板说明 自定义模板 首先创建一个模板组 创建模板 自定义模板 在自定义模板中，可以通过$名称$的形式声明变量，然后使用内置的函数为变量动态赋值。 效果 "},"pages/idea/tomcat.html":{"url":"pages/idea/tomcat.html","title":"IDEA配置Tomcat","keywords":"","body":"IDEA配置Tomcat 在IDEA中配置Tomcat，推荐使用Maven插件中的Tomcat插件启动项目。 配置Tomcat Setting -> Preferences -> Build,Execution,Deployment -> Application Servers 点击+，选择Tomcat Server，选择Tomcat根路径，点击OK 发布项目 项目配置信息 Module的配置信息 第三方Jar包 还有一种方式：右键Module名称->New Directory->拖jar进文件夹->选中jar列表->Add as Library "},"pages/idea/mavenconfig.html":{"url":"pages/idea/mavenconfig.html","title":"Maven的安装与配置","keywords":"","body":"Maven的安装与配置 Maven是一个免安装的程序，即解压则可以使用，但是Maven管理项目需要使用插件管理生命周期。而需要使用Maven的命令，所以需要配置Maven的环境变量。Maven本身使用Java开发，也依赖JDK的环境变量。 Maven的目录结构 Maven的下载 官方网站：https://maven.apache.org/ 下载地址：https://maven.apache.org/download.cgi 查看IDEA对Maven的版本要求： Setting->Preferences->Build,Execution,Deployment->Build Tools->Maven 环境变量 命令行输入以下命令 vi ~/bash_profile 编辑输入，然后保存： MAVEN_HOME=~/Documents/apps/apache-maven-3.9.3 PATH=$MAVEN_HOME/bin:$PATH:. export JAVA_HOME export PATH 命令行输入： source ~/.bash_profile 验证Maven： mvn -version settings.xml 打开：MAVEN_HOME/conf/settings.xml 配置仓库地址 找到localRepository，默认${user.home}/.m2/repository，可以修改。 配置阿里云镜像 maven-default-http-blocker external:http:* Pseudo repository to mirror external repositories initially using HTTP. http://0.0.0.0/ true --> alimaven central aliyun maven http://maven.aliyun.com/nexus/content/groups/public 配置JDK jdk1.8 true 1.8 1.8 1.8 1.8 项目中指定JDK IDEA集成Maven Setting->Preferences->Build,Execution,Deployment->Build Tools->Maven->修改Maven home path、setting.xml path "},"pages/jdbc/prepare.html":{"url":"pages/jdbc/prepare.html","title":"准备工作","keywords":"","body":"准备工作 工具 Navicat for MySQL PowerDesigner JDK API "},"pages/jdbc/configmysql.html":{"url":"pages/jdbc/configmysql.html","title":"Mac下配置MySQL驱动","keywords":"","body":"Mac下配置MySQL驱动 23.12.19 1:08更新 下载MySQL驱动jar 官网地址：https://dev.mysql.com/downloads/connector/j/ 打开官网地址->点击Select Operating System下拉->选中Platform Independent->找到Platform Independent (Architecture Independent), ZIP Archive，点击下载->解压ZIP，将mysql-connector-j-8.0.33.jar拷贝至如下路径： ~/Documents/code/java/mysql-connector-j-8.0.33.jar 以上是jar路径，也就是驱动所在位置。 配置驱动 如果没有配置过JDK，先配置JDK，参考：http://www.1221.site/pages/java/configenv.html vi ~/.bash_profile 修改classpath: CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:~/Documents/code/java/mysql-connector-j-8.0.33.jar:. 输入以下命令使配置文件生效： source ~/.bash_profile 参考：https://blog.csdn.net/pan_junbiao/article/details/86626741 "},"pages/jdbc/nature.html":{"url":"pages/jdbc/nature.html","title":"JDBC本质","keywords":"","body":"JDBC本质 JDBC是什么？ Java Database Connectivity（Java语言连接数据库）。 JDBC的本质是什么？ JDBC是SUN公司制定的一套接口（interface）。接口都有调用者和实现者，面向接口调用、面向接口写实现类，这都属于面向接口编程。java.sql.*，这个软件包下有很多的接口。 为什么要面向接口编程？ 解耦合：降低程序的耦合度，提供程序的扩展力。多态机制就是非常典型的面向抽象编程，而不是面向具体编程。 我们建议： Animal a = new Cat(); Animal b = new Dog(); // 喂养的方法 public void feed(Animal a) { // 面向父类型编程 } 我们不建议： Cat a = new Cat(); Dog b = new Dog(); // 喂养的方法 public void feed(Cat a) { // 面向实现类编程 } 为什么SUN制定一套JDBC接口呢？ 因为每一个数据库的底层实现原理不一样，Oracle数据库有自己的原理，MYSQL数据库也有自己的原理，MS SQLServer数据库也有自己的原理，每个数据库都有自己独特的实现原理。 编写程序模拟JDBC本质 SUN角色： /* SUN角色 SUN公司负责制定这套JDBC接口 */ public interface JDBC { /* 连接数据库的方法 */ void getConnectoin(); } 实现者角色： /* MySQL驱动 MySQL的数据库厂家负责写JDBC接口的实现类 */ public class MySQL implements JDBC { public void getConnectoin() { // 具体这里的代码怎么写，对于我们Java程序员来说没关系 // 这段代码涉及到MySQL底层数据库的实现原理 System.out.println(\"连接MySQL数据库成功\"); } } /* Oracle驱动 Oracle的数据库厂家负责写JDBC接口的实现类 */ public class Oracle implements JDBC { public void getConnectoin() { // 具体这里的代码怎么写，对于我们Java程序员来说没关系 // 这段代码涉及到Oracle底层数据库的实现原理 System.out.println(\"连接Oracle数据库成功\"); } } /* SqlServer驱动 SqlServer的数据库厂家负责写JDBC接口的实现类 */ public class SqlServer implements JDBC { public void getConnectoin() { // 具体这里的代码怎么写，对于我们Java程序员来说没关系 // 这段代码涉及到SqlServer底层数据库的实现原理 System.out.println(\"连接SqlServer数据库成功\"); } } Java程序员： /* Java程序员 不需要关心具体是哪个品牌的数据库，只需要面向JDBC接口写代码 面向接口编程，面向抽象编程，不要面向具体编程 */ public class JavaProgramer { public static void main(String[] args) { JDBC jdbc = new MySQL(); // 以下代码都是面向接口编程，不需要修改 jdbc.getConnectoin(); } } 编译： javac *.java 运行： java JavaProgramer 输出：连接MySQL数据库成功 使用“反射机制”： /* Java程序员 不需要关心具体是哪个品牌的数据库，只需要面向JDBC接口写代码 面向接口编程，面向抽象编程，不要面向具体编程 */ public class JavaProgramer { public static void main(String[] args) { // 创建对象使用反射机制 Class c = Class.forName(\"Oracle\"); JDBC jdbc = (JDBC)c.newInstance(); // 以下代码都是面向接口编程，不需要修改 jdbc.getConnectoin(); } } 使用配置文件jdbc.properties:className=Oracle import java.util.*; /* Java程序员 不需要关心具体是哪个品牌的数据库，只需要面向JDBC接口写代码 面向接口编程，面向抽象编程，不要面向具体编程 */ public class JavaProgramer { public static void main(String[] args) throws Exception { // 创建对象使用反射机制 ResourceBundle bundle = ResourceBundle.getBundle(\"jdbc\"); String className = bundle.getString(\"className\"); Class c = Class.forName(className); JDBC jdbc = (JDBC)c.newInstance(); // 以下代码都是面向接口编程，不需要修改 jdbc.getConnectoin(); } } 视频地址：https://www.bilibili.com/video/BV1Bt41137iB?p=2 "},"pages/jdbc/programming.html":{"url":"pages/jdbc/programming.html","title":"JDBC编程","keywords":"","body":"JDBC编程 JDBC编程六步 第一步：注册驱动（作用：告诉Java程序，即将要连接的是哪个品牌数据库）第二步：获取连接（表示JVM的进程和数据库进程之间的通道打开了，这属于进程之间的通信，重量级的，使用完毕之后一定要关闭连接）第三步：获取数据库操作对象（专门执行sql语句的对象）第四步：执行SQL语句（DML、DQL...）第五步：处理查询结果（只有当第四步执行的是select语句的时候，才有这第五步处理查询结果集）第六步：释放资源（使用完资源后一定要关闭资源） 编写测试类 import java.sql.Driver; import java.sql.DriverManager; import java.sql.SQLException; import java.sql.Connection; import java.sql.Statement; public class JDBCTest01 { public static void main(String[] args) { Statement stmt = null; Connection conn = null; try { // 第一步：注册驱动（作用：告诉Java程序，即将要连接的是哪个品牌数据库） Driver driver = new com.mysql.jdbc.Driver(); DriverManager.registerDriver(driver); // 第二步：获取连接（表示JVM的进程和数据库进程之间的通道打开了，这属于进程之间的通信，重量级的，使用完毕之后一定要关闭连接） String url = \"jdbc:mysql://127.0.0.1:3306/node\"; String user = \"root\"; String password = \"333\"; conn = DriverManager.getConnection(url, user, password); System.out.println(\"数据库连接对象 = \" + conn); // 第三步：获取数据库操作对象（专门执行sql语句的对象） stmt = conn.createStatement(); // 第四步：执行SQL语句（DML、DQL...） // executeUpdate：专门执行DML语句的（insert delte update) // 返回值是“影响数据库中的记录条数” // 增 String sql = \"insert into dept(deptno, dname, loc) values(50,'人事部','北京'\"; int count = stmt.executeUpdate(sql); System.out.println(count == 1 ? \"保存成功\" : \"保存失败\"); // 删 // String sql = \"delete from dept where deptno = 40\"; // int count = stmt.executeUpdate(sql); // System.out.println(count == 1 ? \"删除成功\" : \"删除失败\"); // 改 // String sql = \"update dept set dname = '销售部', loc = '天津' where deptno = 20\"; // int count = stmt.executeUpdate(sql); // System.out.println(count == 1 ? \"修改成功\" : \"修改失败\"); // 第五步：处理查询结果（只有当第四步执行的是select语句的时候，才有这第五步处理查询结果集） } catch(SQLException e) { e.printStackTrace(); } finally { // 第六步：释放资源（使用完资源后一定要关闭资源） // 为了保证资源一定释放，在finally语句中关闭资源 // 并且要遵循从小到大依次关闭 // 分别对其try catch if (conn != null) { try { conn.close() } catch(SQLException e) { e.printStackTrace(); } } if (conn != null) { try { conn.close() } catch(SQLException e) { e.printStackTrace(); } } } } } 执行： javac *.java java JDBCTest01 报错： chenchangqingdeMacBook-Pro-2:jdbc chenchangqing$ java JDBCTest01 Exception in thread \"main\" java.lang.UnsupportedClassVersionError: com/mysql/jdbc/Driver : Unsupported major.minor version 52.0 at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:800) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) at java.net.URLClassLoader.access$100(URLClassLoader.java:71) at java.net.URLClassLoader$1.run(URLClassLoader.java:361) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at JDBCTest01.main(JDBCTest01.java:10) 配置的驱动使用的是JDK1.8，但是本地编译代码的JDK是1.7，升级本地JDK为1.8，问题解决。 类加载的方式注册驱动 import java.sql.*; import java.util.*; public class JDBCTest02 { public static void main(String[] args) { try { // 注册驱动的第一种方式 // DriverManager.registerDriver(new com.mysql.jdbc.Driver()); // 注册驱动的第二种方式：常用的 Class.forName(\"com.mysql.jdbc.Driver\") } catch(SQLException e) { e.printStackTrace(); } catch(ClassNotFoundException e) { e.printStackTrace(); } } } 从属性资源文件中读取连接数据库信息 创建jdbc.properties： driver=com.mysql.jdbc.Driver url=jdbc:mysql://localhost:3306/mysql user=root password=123456 import java.sql.*; import java.util.*; public class JDBCTest03 { public static void main(String[] args) { // 使用资源绑定器绑定属性配置文件 ResourceBundle bundle = ResourceBundle.getBundle(\"jdbc\"); String driver = bundle.getString(\"driver\"); String url = bundle.getString(\"url\"); String user = bundle.getString(\"user\"); String user = bundle.getString(\"password\"); Connection conn = null; Statement stmt = null; try { // 1、注册驱动 Class.forName(driver); // 2、获取连接 conn = DriverManager.getConnection(url, user, password); // 3、获取数据库操作对象 stmt = conn.createStatement(); // 4、执行SQL语句 String sql = \"update dept set dname = '销售部', loc = '天津' where deptno = 20\"; int count = stmt.executeUpdate(sql); System.out.println(count == 1 ? \"修改成功\" : \"修改失败\"); } catch(SQLException e) { e.printStackTrace(); } catch(ClassNotFoundException e) { e.printStackTrace(); } finally { // 5、释放资源 } } } 处理查询结果集 import java.sql.*; import java.util.*; public class JDBCTest04 { public static void main(String[] args) { // 使用资源绑定器绑定属性配置文件 ResourceBundle bundle = ResourceBundle.getBundle(\"jdbc\"); String driver = bundle.getString(\"driver\"); String url = bundle.getString(\"url\"); String user = bundle.getString(\"user\"); String user = bundle.getString(\"password\"); Connection conn = null; Statement stmt = null; ResultSet rs = null; try { // 1、注册驱动 Class.forName(driver); // 2、获取连接 conn = DriverManager.getConnection(url, user, password); // 3、获取数据库操作对象 stmt = conn.createStatement(); // 4、执行SQL语句 String sql = \"select empno, ename, sal from emp\"; rs = stmt.executeQuery(sql); // 5、处理结果集 while(rs.next()) { String empno = rs.getString(\"empno\"); String ename = rs.getString(\"ename\"); String sal = rs.getString(\"sal\"); // 第二种方式取值 // String empno = rs.getString(1); // String ename = rs.getString(2); // String sal = rs.getString(3); System.out.println(empno + \",\" + ename + \",\" + sal); } } catch(SQLException e) { e.printStackTrace(); } catch(ClassNotFoundException e) { e.printStackTrace(); } finally { // 6、释放资源 if (rs != null) { try { rs.close() }catch(SQLException e) { e.printStackTrace(); } } if (stmt != null) { try { stmt.close() } catch(SQLException e) { e.printStackTrace(); } } if (conn != null) { try { conn.close() } catch(SQLException e) { e.printStackTrace(); } } } } } "},"pages/jdbc/login.html":{"url":"pages/jdbc/login.html","title":"JDBC登录","keywords":"","body":"JDBC登录 实现功能 需求：模拟用户登陆功能的实现。 业务描述：程序运行的时候，提供一个输入的入口，可以让用户输入用户名和密码。用户输入用户名和密码之后，提交信息，Java程序收集到用户信息，Java程序连接数据哭验证用户名和密码是否合法，合法，显示登录成功，不合法，显示登录失败。 数据准备 在实际开发中，表的设计会使用专业的建模工具，我们这里安装一个建模工具，PowerDesigner，使用PD工具来进行数据库表的设计。 编写程序 import java.sql.*; import java.util.*; public class JDBCTest05 { public static void main(String[] args) { // 初始化一个界面 Map userLoginInfo = initUI(); // 验证用户名和密码 boolean loginSuccess = login(userLoginInfo); // 最后输出结果 System.out.println(loginSuccess ? \"登录成功\" : \"登录失败\") } /** * 用户登录 * @param userLoginInfo 用户登录信息 * @return false表示失败， true表示成功 */ private static boolean login(Map userLoginInfo) { // JDBC代码 // 使用资源绑定器绑定属性配置文件 ResourceBundle bundle = ResourceBundle.getBundle(\"jdbc\"); String driver = bundle.getString(\"driver\"); String url = bundle.getString(\"url\"); String user = bundle.getString(\"user\"); String user = bundle.getString(\"password\"); Connection conn = null; Statement stmt = null; ResultSet rs = null; String loginName = userLoginInfo.get(\"loginName\"); String loginPwd = userLoginInfo.get(\"loginPwd\"); boolean loginSuccess = false; try { // 1、注册驱动 Class.forName(driver); // 2、获取连接 conn = DriverManager.getConnection(url, user, password); // 3、获取数据库操作对象 stmt = conn.createStatement(); // 4、执行SQL语句 String sql = \"select * from t user where loginName = '\"+ loginName +\"' and loginPwd = '\"+ loginPwd +\"'\"; rs = stmt.executeQuery(sql); // 5、处理结果集 if(rs.next()) { loginSuccess = true; } } catch(SQLException e) { e.printStackTrace(); } catch(ClassNotFoundException e) { e.printStackTrace(); } finally { // 6、释放资源 if (rs != null) { try { rs.close() }catch(SQLException e) { e.printStackTrace(); } } if (stmt != null) { try { stmt.close() } catch(SQLException e) { e.printStackTrace(); } } if (conn != null) { try { conn.close() } catch(SQLException e) { e.printStackTrace(); } } } return loginSuccess; } /** * 初始化用户界面 * @return 用户输入的用户名和密码登录信息 */ private static Map initUI() { Scanner s = new Scanner(System.in); System.out.println(\"用户名：\"); String loginName = s.nextLine(); System.out.println(\"密码：\"); String loginPwd = s.nextLine(); Map userLoginInfo = new HashMap<>(); userLoginInfo.put(\"loginName\", loginName); userLoginInfo.put(\"loginPwd\", loginPwd); return userLoginInfo; } } SQL注入 随意输入用户名，密码输入fsjdlf' or '1' = '1'；，用户登录成功，这种现象称为SQL注入。 1.导致SQL注入的根本原因是什么？ 用户输入的信息中含有sql语句的关键字，并且这些关键字参与了sql语句的变异过程，导致sql语句的原意被扭曲，进而达到sql注入。 2.解决SQL注入的问题？ 只要用户提供的信息不参与SQL语句的编译过程，问题就解决了。即使用户提供的信息中含有SQL语句的关键字，但是没有参与编译，不起作用。 3.代码修改 import java.sql.*; import java.util.*; public class JDBCTest06 { public static void main(String[] args) { // 初始化一个界面 Map userLoginInfo = initUI(); // 验证用户名和密码 boolean loginSuccess = login(userLoginInfo); // 最后输出结果 System.out.println(loginSuccess ? \"登录成功\" : \"登录失败\") } /** * 用户登录 * @param userLoginInfo 用户登录信息 * @return false表示失败， true表示成功 */ private static boolean login(Map userLoginInfo) { // JDBC代码 // 使用资源绑定器绑定属性配置文件 ResourceBundle bundle = ResourceBundle.getBundle(\"jdbc\"); String driver = bundle.getString(\"driver\"); String url = bundle.getString(\"url\"); String user = bundle.getString(\"user\"); String user = bundle.getString(\"password\"); Connection conn = null; PreparedStatement ps = null;// 这里使用PreparedStatement预编译的数据操作对象 ResultSet rs = null; String loginName = userLoginInfo.get(\"loginName\"); String loginPwd = userLoginInfo.get(\"loginPwd\"); boolean loginSuccess = false; try { // 1、注册驱动 Class.forName(driver); // 2、获取连接 conn = DriverManager.getConnection(url, user, password); // 3、获取预编译的数据库操作对象 String sql = \"select * from t user where loginName = ? and loginPwd = ?\"; ps = conn.prepareStatement(sql); // 给占位符？传值（第1个问号下标是1，第2个问号下标是2，JDBC中所有下标从1开始） ps.setString(1, loginName); ps.setString(2, loginPwd); // 4、执行SQL语句 // 程序执行到此处，会发送sql语句框子给DBMS，DBMS进行sql语句的预先编译 rs = ps.executeQuery(); // 5、处理结果集 if(rs.next()) { loginSuccess = true; } } catch(SQLException e) { e.printStackTrace(); } catch(ClassNotFoundException e) { e.printStackTrace(); } finally { // 6、释放资源 if (rs != null) { try { rs.close() }catch(SQLException e) { e.printStackTrace(); } } if (ps != null) { try { ps.close() } catch(SQLException e) { e.printStackTrace(); } } if (conn != null) { try { conn.close() } catch(SQLException e) { e.printStackTrace(); } } } return loginSuccess; } /** * 初始化用户界面 * @return 用户输入的用户名和密码登录信息 */ private static Map initUI() { Scanner s = new Scanner(System.in); System.out.println(\"用户名：\"); String loginName = s.nextLine(); System.out.println(\"密码：\"); String loginPwd = s.nextLine(); Map userLoginInfo = new HashMap<>(); userLoginInfo.put(\"loginName\", loginName); userLoginInfo.put(\"loginPwd\", loginPwd); return userLoginInfo; } } 4.对比一下Statement和PreparedStatement Statement存在sql注入问题，PreparedStatement解决sql注入问题。 Statement编译一次执行一次，PreparedStatement是编译一次，可执行N次。PreparedStatement效率较高一些。 PreparedStatement会在编译阶段做类型的安全检查 5.什么时候用Statement？ 当需要拼接sql的时候使用Statement，例如实现排序；当仅仅传值的时候使用PrepareStatement。 // 升序、降序 import java.sql.*; import java.util.*; public class JDBCTest07 { public static void main(String[] args) { // 用户在控制台输入desc就是降序，输入asc就是升序 Scanner s = new Scanner(System.in); System.out.println(\"输入desc或asc，desc表示降序，asc表示升序\"); System.out.print(\"请输入：\"); String keyWords = s.nextLine(); // 执行SQL Connection conn = null; Statement stmt = null; ResultSet rs = null; try { // 1、注册驱动 Class.forName(driver); // 2、获取连接 conn = DriverManager.getConnection(url, user, password); // 3、获取数据库操作对象 stmt = conn.createStatement(sql); // 4、执行SQL语句 String sql = \"select ename from emp order by ename \" + keyWords; rs = stmt.executeQuery(sql); // 5、处理结果集 while(rs.next()) { System.out.println(rs.getString(\"ename\")); } } catch(SQLException e) { e.printStackTrace(); } catch(ClassNotFoundException e) { e.printStackTrace(); } finally { // 6、释放资源 if (rs != null) { try { rs.close() }catch(SQLException e) { e.printStackTrace(); } } if (stmt != null) { try { stmt.close() } catch(SQLException e) { e.printStackTrace(); } } if (conn != null) { try { conn.close() } catch(SQLException e) { e.printStackTrace(); } } } } } PreparedStatement完成增删改 import java.sql.*; public class JDBCTest08 { public static void main(String[] args) { // 用户在控制台输入desc就是降序，输入asc就是升序 Scanner s = new Scanner(System.in); System.out.println(\"输入desc或asc，desc表示降序，asc表示升序\"); System.out.print(\"请输入：\"); String keyWords = s.nextLine(); // 执行SQL Connection conn = null; PreparedStatement ps = null; try { // 1、注册驱动 Class.forName(driver); // 2、获取连接 conn = DriverManager.getConnection(url, user, password); // 3、获取数据库操作对象 String sql = \"insert into dept (deptno, dname, loc) values (?, ?, ?)\"; ps = conn.prepareStatement(sql); ps.setInt(1, 60); ps.setString(2, \"销售部\"); ps.setString(3, \"上海\"); // String sql = \"delete from dept where deptno = ?\" // ps = conn.prepareStatement(sql); // ps.setInt(1, 60); // String sql = \"update dept set dname = ?, loc= ? where deptno = ?\"; // ps = conn.prepareStatement(sql); // ps.setString(1, \"研发1部\"); // ps.setString(2, \"北京\"); // ps.setInt(3, 60); // 4、执行SQL语句 int count = ps.executeUpdate(); System.out.println(count); } catch(SQLException e) { e.printStackTrace(); } catch(ClassNotFoundException e) { e.printStackTrace(); } finally { // 6、释放资源 if (rs != null) { try { rs.close() }catch(SQLException e) { e.printStackTrace(); } } if (ps != null) { try { ps.close() } catch(SQLException e) { e.printStackTrace(); } } if (conn != null) { try { conn.close() } catch(SQLException e) { e.printStackTrace(); } } } } } "},"pages/jdbc/transaction.html":{"url":"pages/jdbc/transaction.html","title":"JDBC事务","keywords":"","body":"JDBC事务 JDBC中的事务是自动提交的，什么事自动提交？ 只要执行任意一条DML语句，则自动提交一次，这是JDBC默认的事务行为。但是在实际的业务中，通常都是N条DML语句共同联合才能完成的，必须保证他们这些DML语句在同一个事务中同时成功或者同时失败。 账户转账事务 sql脚本： drop table if exists t_act; create table t_act { actno int, balance double(7, 2)// 注意，7表示有效数字的个数，2表示小数位的个数。 }; insert into t_act(actno, balance) values (111, 20000); insert into t_act(actno, balance) values (222, 0); commit; select * from t_act; java代码： import java.sql.*; public class JDBCTest08 { public static void main(String[] args) { Connection conn = null; PreparedStatement ps = null; try { // 1、注册驱动 Class.forName(driver); // 2、获取连接 conn = DriverManager.getConnection(url, user, password); // 将自动提交机制修改为手动提交 conn.setAutoCommit(false); // 3、获取数据库操作对象 String sql = \"update t_act set balance = ? where actno = ?\"; ps = conn.prepareStatement(sql); // 4、执行SQL语句 ps.setDouble(1, 10000); ps.setInt(2, 111); int count = ps.executeUpdate(); ps.setDouble(1, 10000); ps.setInt(2, 222); count += ps.executeUpdate(); System.out.println(count == 2 ? \"转账成功\" : \"转账失败\"); // 程序能够走到这里说明以上程序没有异常，事务结束，手动提交事务 conn.commit(); } catch(Exception e) { // 回滚事务 if(conn != null) { try { conn.rollback(); } catch (SQLException e1) { e1.printStackTrace(); } } e.printStackTrace(); } finally { // 6、释放资源 if (ps != null) { try { ps.close() } catch(SQLException e) { e.printStackTrace(); } } if (conn != null) { try { conn.close() } catch(SQLException e) { e.printStackTrace(); } } } } } 以上是单机事务，还有分布式事务。 "},"pages/jdbc/tool.html":{"url":"pages/jdbc/tool.html","title":"JDBC工具类","keywords":"","body":"JDBC工具类 23.12.18 22:46更新 23.12.19 23:32更新 DBUtil /** * JDBC工具类，简化JDBC编程 */ public class DBUtil { private static ResourceBundle bundle = ResourceBundle.getBundle(\"resources/jdbc\"); // com.mysql.jdbc.Driver private static String driver = bundle.getString(\"driver\"); // jdbc:mysql://localhost:3306/mysql private static String url = bundle.getString(\"url\"); private static String user = bundle.getString(\"user\"); private static String password = bundle.getString(\"password\"); /** * 工具类中的构造方法都是私有的 * 因为工具类当中的方法都是静态的，不需要new对象，直接采用类名调用 */ private DBUtil(){} // 静态代码块在类加载时执行，并且只执行一次 static { try { Class.forName(driver); } catch (ClassNotFoundException e) { e.printStackTrace(); } } // 这个对象实际上在服务器中只有一个 private static ThreadLocal local = new ThreadLocal<>(); /** * 获取数据库连接对象 * * @return 连接对象 * @throws SQLException */ public static Connection getConnection() throws SQLException { Connection conn = local.get(); if (conn == null) { conn = DriverManager.getConnection(url, user, password); local.set(conn); } return conn; } /** * 关闭资源 * @param conn 连接对象 * @param ps 数据库操作对象 * @param rs 结果集 */ public static void close(Connection conn, Statement ps, ResultSet rs) { if (rs != null) { try { rs.close() }catch(SQLException e) { e.printStackTrace(); } } if (ps != null) { try { ps.close() } catch(SQLException e) { e.printStackTrace(); } } if (conn != null) { try { conn.close() // Tomcat服务器是支持县城池的，也就是说一个人用过了t1线程，t1线程还有可能被其他用户使用。 local.remove(); } catch(SQLException e) { e.printStackTrace(); } } } } 模糊查询 /** * 测试DBUtil是否好用 * 模糊查询怎么写 */ public class JDBCTest09 { public static void main(String[] args) { Connection conn = null; PreparedStatement ps = null; ResultSet rs = null; try { // 获取连接 conn = DBUtil.getConnection(); // 获取预编译的数据库操作对象 // 错误的写法 /*String sql = \"select ename from emp where ename like '_?%'\"; ps = conn.prepareStatement(sql); ps.setString(1, \"A\");*/ String sql = \"select ename from emp where ename like ?\"; ps = conn.prepareStatement(sql); ps.setString(1, \"_A%\"); ps.executeQuery(); while(rs.next()) { System.out.println(rs.getString(\"ename\")); } } catch (Exception e) { e.printStackTrace(); } finally { DBUtil.close(conn, ps, rs); } } } 增 /** * 插入账户信息 * @param act 账户信息 * @return 1 表示插入成功 */ public int insert(Account act) { Connection conn = null; PreparedStatement ps = null; int count = 0; try { conn = DBUtil.getConnection(); String sql = \"insert into t_act(actno, balance) values (?, ?)\"; ps = conn.prepareStatement(sql); ps.setString(1, act.getActno); ps.setDouble(2, act.getBalance()); count = ps.executeUpdate(); } catch (SQLException e) { throw new RuntimeException(e); } finally { DBUtil.close(conn, ps, null); } return count; } 删 /** * 根据逐渐删除账户 * @param id 主键 * @return */ public int deleteById(Long id) { Connection conn = null; PreparedStatement ps = null; int count = 0; try { conn = DBUtil.getConnection(); String sql = \"delete from t_act where id = ?\"; ps = conn.prepareStatement(sql); ps.setLong(1, id); count = ps.executeUpdate(); } catch (SQLException e) { throw new RuntimeException(e); } finally { DBUtil.close(conn, ps, null); } } 改 /** * 更新账户 * @param act * @return */ public int update(Account act) { Connection conn = null; PreparedStatement ps = null; int count = 0; try { conn = DBUtil.getConnection(); String sql = \"update t_act set balance = ?, actno = ? where id = ?\"; ps = conn.prepareStatement(sql); ps.setDouble(1, act.getBalance()); ps.setString(2, act.getActno()); ps.setLong(3, act.getId()); count = ps.executeUpdate(); } catch (SQLException e) { throw new RuntimeException(e); } finally { DBUtil.close(conn, ps, null); } return count; } 查 /** * 根据账号查询账户 * @param actno * @return */ public Account selectByActno(String actno) { Connection conn = null; PreparedStatement ps = null; ResultSet rs = null; Account act = null; try { conn = DBUtil.getConnection(); String sql = \"select id, balance from t_act where actno = ?\"; ps = conn.prepareStatement(sql); ps.setString(1, actno); rs = ps.executeQuery(); if (rs.next()) { Long id = rs.getLong(\"id\"); Double balance = rs.getDouble(\"balance\"); // 将结果集封装成java对象 act = new Account(); act.setId(id); act.setActno(actno); act.setBalance(balance); } } catch (SQLException e) { throw new RuntimeException(e); } finally { DBUtil.close(conn, ps, null); } return act; } /** * 获取所有账户 * @return */ public List selectAll() { Connection conn = null; PreparedStatement ps = null; ResultSet rs = null; List list = new ArrayList<>(); try { conn = DBUtil.getConnection(); String sql = \"select id, actno, balance from t_act\"; ps = conn.prepareStatement(sql); rs = ps.executeQuery(); where (rs.next()) { Long id = rs.getLong(\"id\"); String actno = rs.getString(\"actno\"); Double balance = rs.getDouble(\"balance\"); // 将结果集封装成java对象 Account act = new Account(); act.setId(id); act.setActno(actno); act.setBalance(balance); list.add(act); } } catch (SQLException e) { throw new RuntimeException(e); } finally { DBUtil.close(conn, ps, null); } return list; } 视频 start://www.bilibili.com/video/BV1Z3411C7NZ?p=67 "},"pages/jdbc/lock.html":{"url":"pages/jdbc/lock.html","title":"悲观锁和乐观锁","keywords":"","body":"悲观锁和乐观锁 悲观锁（行级锁）：select结尾加for update。 乐观锁：多线程并发都可以对同一记录修改，只不过会对行数据记录版本号。有个线程发现版本号是v1.0，另一个线程发现版本号也是v1.0，这个时候第一个线程修改行数据后版本号为v2.0，另外一个线程也进行修改，在提交之前发现版本号是v2.0，和之前的v1.0不一致，发现数据被修改了，这个时候第二线程就回滚。 "},"pages/tomcat/tomcat-install.html":{"url":"pages/tomcat/tomcat-install.html","title":"Tomcat的安装","keywords":"","body":"Tomcat的安装 Tomcat介绍 servletapi：https://tomcat.apache.org/tomcat-7.0-doc/servletapi/index.html apache官网地址：https://www.apache.org/ tomcat官网地址：https://tomcat.apache.org tomcat开源免费的轻量级WEB服务器 tomcat还有另外一个名字：catalina（catalina是美国的一个岛屿，风景秀丽，据说作者在这个风景秀丽的小岛上开发了一个轻量级的WEB服务器，体积小、运行速度快，因此tomcat又被称为catalina）。 tomcat的logo是一只公猫（寓意表示Tomcat服务器是轻巧的，小巧的，果然，体积小，运行速度快，只实现了Servlet+JSP规范）。 tomcat是java语言写的 tomcat服务器要想运行，必须有jre（java的运行时环境） 下载Tomcat10.0.2 打开tomcat官网地址，点击左侧Download->Tomcat 10 点击Quick Navigation->Archives 点击v10.0.2/，点击bin，点击apache-tomcat-10.0.2.zip下载 点击v10.0.2/，点击src，点击apache-tomcat-10.0.2-src.zip下载 安装Tomcat10.0.2 将apache-tomcat-10.0.2.zip解压缩 将解压后的apache-tomcat-10.0.2文件夹，剪切至用户目录，方便管理mv apache-tomcat-10.0.2 ~/ Tomcat目录介绍 bin：这个目录是Tomcat服务器的命令文件存放的目录，比如：启动Tomcat、关闭Tomcat。 config：这个目录是Tomcat服务器的配置文件存放目录。 server.xml：可以配置端口号，默认Tomcat端口是8080。 lib：这个目录是Tomcat服务器核心程序目录，因为Tomcat服务器是Java语言编写的，这里的jar包里面都是class文件。 logs：Tomcat服务器的日志目录，Tomcat服务器启动等信息都会在这个目录下生成日志文件。 temp：Tomcat服务器的临时目录，存储临时文件。 webapps：这个目录当中就是用来存放大量的webapp（web application：web应用）。 work：这个目录是用来存放JSP文件翻译之后的java文件以及编译之后的class文件。 分析startup.bat Tomcat服务器提供了bat和sh文件，说明了这个Tomcat服务器的通用性。 分析startup.bat文件得出，执行这个命令，实际上最后是执行：catalina.bat文件。 catalina.bat文件中有这样一行配置：set MAINCLASS=org.apache.catalina.startup.Bootstrap（这个类就是main方法所在的类）。 Tomcat服务器就是JAVA语言写的，既然是JAVA语言写的，那么启动Tomcat服务器就是执行main方法。 配置CATALINA_HOME 编辑.bash_profilevi ~/.bash_profile 添加以下命令：# CATALINA CATALINA_HOME=~/apache-tomcat-10.0.2 PATH=$CATALINA_HOME/bin:$PATH:. export CATALINA_HOME export PATH sourcesource ~/.bash_profile 验证echo $CATALINA_HOME 启动Tomcat 修改命令权限startup.sh chmod 777 $CATALINA_HOME/bin/startup.sh catalina.sh chmod 777 $CATALINA_HOME/bin/catalina.sh shutdown.sh chmod 777 $CATALINA_HOME/bin/shutdown.sh 启动Tomcat startup.sh 关闭Tomcat shutdown.sh 视频地址 https://www.bilibili.com/video/BV1Z3411C7NZ/?p=4 "},"pages/tomcat/tomcat-first-webapp.html":{"url":"pages/tomcat/tomcat-first-webapp.html","title":"实现最基本的web应用","keywords":"","body":"实现最基本的web应用 第一步 找到CATALINA_HOME\\webapps目录。 因为所有的webapp要放到webapps目录下。（没有为什么，这是Tomcat服务器的要求，如果不放到这里，Tomcat服务器找不到你的应用。） 第二步 在CATALINA_HOME\\webapps目录下新建一个子目录，起名：oa。 这个目录oa就是你这个webapp的名字 第三步 在oa目录下新建资源文件，例如：index.html。 编写index.html文件的内容。 第四部 启动Tomcat服务器。 第五步 打开浏览器，在浏览器地址栏上输入这样的URL： http://127.0.0.1:8080/index.html 视频地址 https://www.bilibili.com/video/BV1Z3411C7NZ?p=5 "},"pages/tomcat/tomcat-servlet-webapp.html":{"url":"pages/tomcat/tomcat-servlet-webapp.html","title":"带有Servlet的web应用","keywords":"","body":"带有Servlet的web应用 webapproot |-----WEB-INF |-----classes（存放字节码） |-----lib（第三方jar包） |-----web.xml（注册Servlet） |-----html |-----css |-----javascript |-----image ... 第一步 在webapps目录下新建一个目录，起名crm（这个crm就是webapp的名字）。当然，也可以是其他项目，比如银行项目，可以创建一个目录bank，办工系统可以创建一个oa。 注意：crm就是这个webapp的根。 第二步 在webapp的根下新建一个目录：WEB-INF。 注意：这个目录的名字是Servlet规范中规定的，必须全部大写，必须一模一样。 第三步 在WEB-INF目录下新建一个目录：classes。 注意：这个目录的名字必须是全部小写的classes。这也是Servlet规范中规定的。另外这个目录下一定存放的是java程序编译之后的class文件（这里存放的字节文件）。 第四步 在WEB-INF目录下新建一个目录：lib 注意：这个目录不是必须的。但如果一个webapp需要第三方的jar包的话，这个jar包要放到这个lib目录下，这个目录的名字也不能随便编写，必须是全部小写的lib。例如java语言连接数据库需要的驱动jar包。那么这个jar包就一定要放到lib目录下。这是Servlet闺房中规定的。 第五步 在WEB-INF目录下新建一个文件：web.xml 注意：这个文件是必须的，这个文件名必须叫做web.xml。这个文件必须放在这里。一个合法的webapp，web.xml文件是必须的，这个web.xml文件是一个配置文件，在这个配置文件中描述了请求路径和Servlet类之间的对照关系。 这个文件最好从其他webapp中拷贝，最好别手写，没必要。复制粘切 第六步 编写一个java程序，这个小java程序也不能随意开发，这个小java程序必须实现Servlet接口。 这个Servlet接口不在JDK中。（因为Servlet不是JavaSE了，Servlet属于JavaEE，是另外一套类库） Servlet接口（Servlet.class文件）是Oracle提供的。（最原始的是sun公司提供的） Servlet接口是JavaEE规范中的一员。 Tomcat服务器实现了Servlet规范，所以Tomcat服务器也需要使用Servlet接口。Tomcat服务器中应该有这个接口，Tomcat服务器的CATALINA_HOME\\lib目录下又个servlet.api.jar，解压这个servlet.api.jar之后，你会看到里面有个Servlet.class文件。 重点：从JakartaEE9开始，Servlet接口的全名变了：jakarta.servlet.Servlet 注意：编写这个java小程序的时候，java源代码愿意在哪里就在哪里，位置无所谓，你只需要将java源代码编译之后的class文件放到classes目录下即可。 第七步 编译我们编写的HelloServlet 重点：你怎么能让你的HelloServlet编译通过呢？配置环境变量CLASSPATHCLASSPATH=.;C:\\dev\\apache-tomcat-10.0.12\\lib\\servlet.api.jar 第八步 将以上编译之后的HelloServlet.class文件拷贝到WEB-INF\\classes目录下。 第九步 在web.xml文件中编写配置信息，让“请求路径”和“Servlet类名”关联在一起。 这一步用专业术语描述：在web.xml文件中注册Servlet类。 mm com.xxxx.controller.OneServlet mm /one 第十步 启动Tomcat服务器。 第十一步 打开浏览器，在浏览器地址栏上输入这样的URL： http://127.0.0.1:8080/crm/one 视频地址 https://www.bilibili.com/video/BV1Z3411C7NZ/?p=8 "},"pages/servlet/rule.html":{"url":"pages/servlet/rule.html","title":"Servlet规范","keywords":"","body":"Servlet规范 Servlet规范来自于JAVAEE规范中的一种。 作用： 在Servlet规范中，指定【动态资源文件】开发步骤。 在Servlet规范中，指定Http服务器调用动态资源文件规则。 在Servlet规范中，指定Http服务器管理动态资源文件实例对象规则。 Servlet接口实现类 Servlet接口来自于Servlet-api.jar，存放在Http服务器，提供jar包。 Tomcat服务器下lib文件夹下有一个servlet-api.jar存放Servlet接口（javax.servlet.Servlet接口)。 Servlet规范中任务，Http服务器能调用的【动态资源文件】必须是一个Servlet接口实现类。 例子： class Student { // 不是动态资源文件，Tomcat无权调用 } class Teacher implements Servlet { // 合法动态资源文件，Tomcat有权利调用 Servlet obj = new Teacher(); obj.doGet(); } Servlet接口实现类开发步骤 第一步：创建一个Java类继承于HttpServlet父类，使之成为一个Servlet接口实现类。 public class OneServlet extends HttpServlet { } 子类-->父类-->A接口，此时，子类也是A接口实现类。 抽象类作用：降低接口实现类对接口实现过程难度，将接口中不需要使用抽象方法教给抽象类进行完成，这样接口实现类只需要对接口需要方法进行重写。 Servlet接口： init getServletConfig getServletInfo destory 四个方法对于Servlet接口实现类没用 service - 有用 Tomcat根据Servlet规范调用Servlet接口实现类规则： Tomcat有权利创建Servlet接口实现类实例对象。 Servlet oneServlet = new OneServlet(); Tomcat根据实例对象调用service方法处理当前请求。 oneServlet.service(); oneServlet-->(abstract)HttpServlet-->(abstract)GenericServlet-->Servlet 第二步：重写HttpServlet父类两个方法。doGet或doPost 浏览器get---->oneServlet.doGet() 浏览器post---->oneServlet.doPost() 通过父类决定在何种情况下调用子类中的方法 --- 设计模式 --- 模版设计模式 HttpServlet: service() { if (请求方式 == GET) { this.doGet } else if (请求方式 == POST) { this.doPost } } 第三步：将Servlet接口实现类信息【注册】到Tomcat服务器 【网站】---->【web】---->【WEB-INF】----> web.xml mm com.xxxx.controller.OneServlet Tomcat String mm = \"com.xxxx.controller.OneServlet\" 为了降低用户访问Servlet接口实现类的难度，需要设置简短请求别名 mm /one 如果现在浏览器向Tomcat所要OneServlet时，地址：http://localhost:8080/myWeb/one "},"pages/servlet/life.html":{"url":"pages/servlet/life.html","title":"Servlet生命周期","keywords":"","body":"Servlet生命周期 什么是Servlet对象生命周期 Servlet对象什么时候被创建 Servlet对象什么时候被销毁 Servlet对象创建了几个？ Servlet对象的生命周期表示：一个Servlet对象从出生在最后的死亡，整个过程是怎么样的。 Servlet对象是由谁来维护的？ Servlet对象的创建，对象上方法的调用，对象最终的销毁，Javaweb程序员是无权干预的 Servlet对象的生命周期是由Tomcat服务器（WEB Server）全权负责的。 Tomcat服务器通常我们又称为WEB容器。 WEB容器管理Servlet对象的死活。 思考：我们自己new的Servlet对象受WEB容器管理吗？ 我们自己new的Servlet对象是不受WEB容器管理的。 WEB容器创建的Servlet对象，这些Servlet对象都会被放到一个集合（HashMap），只有放到这个HashMap集合中的Servlet才能够被WEB容器管理，自己new的Servlet对象不会被WEB容器管理。（自己new的Servlet对象不在容器当中）。 研究：服务器在启动的时候，Servlet对象有没有创建出来？（默认情况下） 在Servlet中提供一个无参数的构造方法，启动服务器的时候看看构造方法是否执行。 经过测试得出结论：默认情况下，服务器在启动的时候Servlet对象并不会被实例化。 这个设计是合理的。用户没有发送请求之前，如果提前创建出来所有的Servlet对象，必然是耗费内存的，并且创建出来的Servlet如果一只没有用户访问，显然这个Servlet对象是一个废物，没必要先创建。 怎么让服务器启动的时候创建Servlet对象呢？ 在默认情况下，Http服务器接收到对于当前Servlet接口实现类第一次请求时，自动创建这个Servlet接口实现类的实例对象。 在手动配置情况下，要求Http服务器在启动时自动创建某个Servlet接口实现类的实例对象： mm com.xxxx.controller.OneServlet 30 Servlet对象生命周期 默认情况服务器启动的时候AServlet对象并没有被实例化。 用户发送第一次请求 用户发送第一次请求的时候，AServlet对象被实例化了。 AServlet对象被创建出来之后，Tomcat服务器马上调用了AServlet对象的init方法。 用户发送第一次请你去的时候，init方法执行之后，Tomcat服务器马上调用了AServlet对象的service方法。 用户发送第二次请求 Servlet对象并没有新建，还是使用之前创建好的Servlet对象，直接调用该Servlet对象的service方法。 Servlet对象是单例的（单实例，但是要注意：Servlet对象是单实例的，但是Servlet类并不符合单里模式，我们称为假单例。之所以单例是因为Servlet对象的创建我们javaweb程序员管不着，这个对象的创建只能是Tomcat来说了算，Tomcat只创建了一个，所以导致了单例，但是属于假单里，真单例模式，构造方法是私有化的。） 无参构造方法、init方法只在第一次用户发送请求的时候执行，也就是说无参数构造方法只能执行一次。init方法也只被tomcat服务器调用一次。 只要用户发送一次请求：service方法必然会被Tomcat服务器调用一次。发送100次请求，service方法会被调用100次。 关闭服务器的时候 服务器销毁AServlet的对象内存 服务器会自动调用AServlet对象的destroy方法。 关于Servlet类中方法的调用次数？ 构造方法只执行一次。 init方法只执行一次。 service方法：用户发送一次请求则执行一次，发送N次请求则执行N次。 destroy方法只执行一次。 当我们Servlet类中编写一个由参数的构造方法，如果没有编写无参数构造方法会出现什么问题？ 报错了：500错误。 注意：500是一个HTTP协议的错误状态码。 500一般情况下是因为服务器端的java程序出现了异常。 如果没有无参数的构造方法，会导致出现500错误，无法实例化Servlet对象。 所以，一定要注意：在Servlet开发当中，不建议程序员来定义构造方法，因为定义不当，一不小心就会导致无法实例化Servlet对象。 思考：Servlet的无参数构造方法是在对象第一次创建的时候执行，并且只执行一次，init方法也是在对象第一次创建的时候执行，并且只执行一次，那么这个无参数构造方法可以代替init方法吗？ 不能 Servlet规范中有要求，作为javaweb程序员，编写Servlet类的时候，不建议手动编写构造方法，因为编写构造方法，很容易让无参数构造方法消失，这个操作可能会导致Servlet对象无法实例化，所以init方法是有存在的必要的。 视频地址 https://www.bilibili.com/video/BV1Z3411C7NZ?p=11 "},"pages/servlet/exammanagementsystem.html":{"url":"pages/servlet/exammanagementsystem.html","title":"Servlet考试管理系统","keywords":"","body":"Servlet考试管理系统 准备工作 创建用户信息表Users 解决表不能存中文：https://blog.csdn.net/liyingjie2001/article/details/124602734 CREATE DATABASE c1221 CHARACTER SET utf8; CREATE TABLE Users( userId int primary key auto_increment, #用户编号 userName varchar(50), #用户名称 password varchar(50), #用户密码 sex char(1), #用户性别 ‘男’ 或 ‘女’ email varchar(50) ## 用户邮箱 ) CHARACTER SET utf8; 在src下新建com.c1221.entity.Users实体类 package com.c1221.entity; public class Users { private Integer userId; private String userName; private String password; private String sex; private String email; public Users(Integer userId, String userName, String password, String sex, String email) { this.userId = userId; this.userName = userName; this.password = password; this.sex = sex; this.email = email; } public void setUserId(Integer userId) { this.userId = userId; } public void setUserName(String userName) { this.userName = userName; } public void setPassword(String password) { this.password = password; } public void setSex(String sex) { this.sex = sex; } public void setEmail(String email) { this.email = email; } public Integer getUserId() { return userId; } public String getUserName() { return userName; } public String getPassword() { return password; } public String getSex() { return sex; } public String getEmail() { return email; } } 生成get、set、构造方法：右键类文件编辑区（Command+N）->Generate->Constructor、Getter、Setter 新增Mysql驱动 在web下WEB-INF下创建lib文件夹，存放mysql提供的JDBC实现jar包 在src下新建com.c1221.util.JdbcUtil工具类 package com.c1221.util; import java.sql.*; /** * JDBC工具类，简化JDBC编程 */ public class JdbcUtil { static final String URL = \"jdbc:mysql://localhost:3306/mysql\"; static final String USERNAME = \"root\"; static final String PASSWORD = \"333\"; /** * 工具类中的构造方法都是私有的 * 因为工具类当中的方法都是静态的，不需要new对象，直接采用类名调用 */ private JdbcUtil(){} // 静态代码块在类加载时执行，并且只执行一次 static { try { Class.forName(\"com.mysql.jdbc.Driver\"); } catch (ClassNotFoundException e) { e.printStackTrace(); } } /** * 获取数据库连接对象 * * @return 连接对象 * @throws SQLException */ public static Connection getConnection() throws SQLException { return DriverManager.getConnection(URL, USERNAME, PASSWORD); } /** * 关闭资源 * @param conn 连接对象 * @param ps 数据库操作对象 * @param rs 结果集 */ public static void close(Connection conn, Statement ps, ResultSet rs) { if (rs != null) { try { rs.close(); }catch(SQLException e) { e.printStackTrace(); } } if (ps != null) { try { ps.close(); } catch(SQLException e) { e.printStackTrace(); } } if (conn != null) { try { conn.close(); } catch(SQLException e) { e.printStackTrace(); } } } } 用户信息注册流程图 注册页面 在web下，新建user_add.html Title 用户姓名 用户密码 用户性别 男 女 用户邮箱 编写UserDao 在src下新建com.c1221.dao.UserDao package com.c1221.com.c1221.dao; import com.c1221.entity.Users; import com.c1221.util.JdbcUtil; import java.sql.*; public class UserDao { public int add(Users users) { Connection conn = null; PreparedStatement ps = null; int result = 0; try { // 2、获取连接 conn = JdbcUtil.getConnection(); // 将自动提交机制修改为手动提交 conn.setAutoCommit(false); // 3、获取数据库操作对象 String sql = \"insert into users(userName,password,sex,email)\" + \" values(?,?,?,?)\"; ps = conn.prepareStatement(sql); // 4、执行SQL语句 ps.setString(1, users.getUserName()); ps.setString(2, users.getPassword()); ps.setString(3, users.getSex()); ps.setString(4, users.getEmail()); result = ps.executeUpdate(); conn.commit(); } catch(Exception e) { // 回滚事务 if(conn != null) { try { conn.rollback(); } catch (SQLException e1) { e1.printStackTrace(); } } e.printStackTrace(); } finally { JdbcUtil.close(conn, ps, null); } return result; } } 注册Servlet 导入servlet-api.jar https://blog.51cto.com/laoshifu/4839810 修改web.xml UserAddServlet com.c1221.controller.UserAddServlet UserAddServlet /user/add 在src下新建com.c1221.controller.UserAddServlet @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { String userName,password,sex,email; UserDao dao = new UserDao(); Users user = null; int result = 0; PrintWriter out = null; // 1.【调用请求对象】读取【请求头】参数信息，得到用户的信息 userName = req.getParameter(\"userName\"); password = req.getParameter(\"password\"); sex = req.getParameter(\"sex\"); email = req.getParameter(\"email\"); // 2.【调用UserDao】将用户信息填充到INSERT命令借助JDBC规范发送到数据库服务器 user = new Users(null, userName, password, sex, email); result = dao.add(user); // 3.【调用响应对象】将【处理结果】以二进制形式写入到响应体 resp.setContentType(\"text/html;charset=utf-8\"); out = resp.getWriter(); if (result == 1) { out.print(\"用户信息注册成功\"); } else { out.print(\"用户信息注册失败\"); } out.close(); // Tomcat负责销毁【请求对象】和【响应对象】 // Tomcat负责将Http响应协议包推送到发起请求的浏览器上 // 浏览器根据响应头content-type指定编译器对响应体二进制内容编辑 // 浏览器将编辑后结果在窗口中展示给用户【结束】 } 查询Servlet 新增UserFindServlet @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { UserDao dao; PrintWriter out; // 1【调用DAO】将查询命令推送到数据服务器上，得到所有用户信息【List】 dao = new UserDao(); List userList = dao.findAll(); // 2【调用响应对象】将用户信息结合《table》标签命令以二进制形式写入到响应体 response.setContentType(\"text/html;charset=utf-8\"); out = response.getWriter(); out.print(\"\"); out.print(\"\"); out.print(\"用户编号\"); out.print(\"用户姓名\"); out.print(\"用户密码\"); out.print(\"用户性别\"); out.print(\"用户邮箱\"); out.print(\"\"); for (Users users:userList) { out.print(\"\"); out.print(\"\"+users.getUserId()+\"\"); out.print(\"\"+users.getUserName()+\"\"); out.print(\"\"+users.getPassword()+\"\"); out.print(\"\"+users.getSex()+\"\"); out.print(\"\"+users.getEmail()+\"\"); out.print(\"\"); } out.print(\"\"); } 修改web.xml UserFindServlet com.c1221.controller.UserFindServlet UserFindServlet /user/find 修改UserDao // 查询用户信息 public List findAll() { PreparedStatement ps = null; Connection conn = null; ResultSet rs = null; List userList = new ArrayList(); try { // 2、获取连接 conn = JdbcUtil.getConnection(); // 3、获取数据库操作对象 String sql = \"select * from users\"; ps = conn.prepareStatement(sql); rs = ps.executeQuery(); while (rs.next()) { Integer userId = rs.getInt(\"userId\"); String userName = rs.getString(\"userName\"); String password = rs.getString(\"password\"); String sex = rs.getString(\"sex\"); String email = rs.getString(\"email\"); Users users = new Users(userId, userName, password, sex, email); userList.add(users); } } catch(SQLException e) { e.printStackTrace(); } finally { JdbcUtil.close(conn, ps, rs); } return userList; } 导航栏 新建index.html Title 新建top.html Title 在线考试管理系统 新建left.html Title 用户信息管理、 用户信息注册 用户信息查询 试题信息管理 考试管理 UserDeleteServlet 修改UserFindServlet @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { UserDao dao; PrintWriter out; // 1【调用DAO】将查询命令推送到数据服务器上，得到所有用户信息【List】 dao = new UserDao(); List userList = dao.findAll(); // 2【调用响应对象】将用户信息结合《table》标签命令以二进制形式写入到响应体 response.setContentType(\"text/html;charset=utf-8\"); out = response.getWriter(); out.print(\"\"); out.print(\"\"); out.print(\"用户编号\"); out.print(\"用户姓名\"); out.print(\"用户密码\"); out.print(\"用户性别\"); out.print(\"用户邮箱\"); out.print(\"操作\"); out.print(\"\"); for (Users users:userList) { out.print(\"\"); out.print(\"\"+users.getUserId()+\"\"); out.print(\"\"+users.getUserName()+\"\"); out.print(\"\"+users.getPassword()+\"\"); out.print(\"\"+users.getSex()+\"\"); out.print(\"\"+users.getEmail()+\"\"); out.print(\"删除用户\"); out.print(\"\"); } out.print(\"\"); } 修改UserDAO，新增删除方法 // 根据用户编号删除用户信息 public int delete(String userId) { Connection conn = null; PreparedStatement ps = null; int result = 0; try { // 2、获取连接 conn = JdbcUtil.getConnection(); // 将自动提交机制修改为手动提交 conn.setAutoCommit(false); // 3、获取数据库操作对象 String sql = \"delete from users where userId=?\"; ps = conn.prepareStatement(sql); ps.setString(1, userId); result = ps.executeUpdate(); conn.commit(); } catch(Exception e) { // 回滚事务 if(conn != null) { try { conn.rollback(); } catch (SQLException e1) { e1.printStackTrace(); } } e.printStackTrace(); } finally { JdbcUtil.close(conn, ps, null); } return result; } 新增UserDeleteServlet @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { String userId; UserDao dao = new UserDao(); int result = 0; PrintWriter out = null; // 1.【调用请求对象】读取【请求头】参数（用户编号） userId = request.getParameter(\"userId\"); // 2.【调用DAO】将用户编号填充到delete命令并发送到数据库服务器 result = dao.delete(userId); // 3.【调用响应对象】将处理结果以二进制写入到响应体，交给浏览器 response.setContentType(\"text/html; charset=utf-8\"); out = response.getWriter(); if (result == 1) { out.print(\"用户信息删除成功\"); } else { out.print(\"用户信息删除失败\"); } } 登录验证 新建login.html Title 登录名 密码 修改UserDao，新增login方法 // 登录验证 public int login(String userName, String password) { PreparedStatement ps = null; Connection conn = null; ResultSet rs = null; int result = 0; try { // 2、获取连接 conn = JdbcUtil.getConnection(); // 3、获取数据库操作对象 String sql = \"select count(*) from users where userName=? and password=?\"; ps = conn.prepareStatement(sql); ps.setString(1, userName); ps.setString(2, password); rs = ps.executeQuery(); while (rs.next()) { result = rs.getInt(\"count(*)\"); } } catch(SQLException e) { e.printStackTrace(); } finally { JdbcUtil.close(conn, ps, rs); } return result; } 新增login_error.html Title 登录信息不存在，请重新登录 登录名 密码 新建LoginServlet @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { String userName,password; UserDao dao = new UserDao(); int result = 0; // 调用请求对象对请求体使用utf-8字符集进行重新编辑 request.setCharacterEncoding(\"utf-8\"); // 调用请求对象读取请求体参数信息 userName = request.getParameter(\"userName\"); password = request.getParameter(\"password\"); // 调用DAO将查询验证信息推送到数据库服务器上 result = dao.login(userName, password); // 调用响应对象，根据验证码结果将不同资源文件地址写入到响应体，交给浏览器 if (result == 1) { // 用户存在 response.sendRedirect(\"/examsystem/index.html\"); } else { response.sendRedirect(\"/examsystem/login_error.html\"); } } 修改web.xml LoginServlet com.c1221.controller.LoginServlet LoginServlet /user/login 欢迎资源文件 前提 用户可以记住网站名，但是不会记住网站资源文件名 默认欢迎资源文件 用户发送了一个针对某个网站的【默认请求】时，此时由Http服务器自动从当前网站返回的资源文件。 正常请求：http://localhost:8080/examsystem/index.html 默认请求：http://localhost:8080/examsystem Tomcat对默认欢迎资源文件定位规则 1）规则位置：Tomcat安装位置/conf/web.xml 2）规则命令： index.html index.htm index.jsp 设置当前网站的默认欢迎资源文件规则 1）规则位置：网站/web/WEB-INF/web.xml 2) 规则命令： login.html Http状态码 介绍 1）由三位数字组成的一个符号。2）Http服务器在推送响应包之前，根据本次请求处理情况将Http状态码写入到响应包中【状态行】上。3）如果Http服务器针对本次请求，返回了对应的资源文件。通过Http状态码通知浏览器应该如何处理这个结果。4）如果Http服务器针对本次请求，无法返回对应的资源文件。通过Http状态码向浏览器解释不能提供服务的原因。 分类 1）组成：100～599，分为5个大类 2）1XX 最有特征的是100：通知浏览器本次返回的资源文件并不是一个独立的资源文件，需要浏览器在接受响应包之后，继续向Http服务器所要依赖。 3）2XX最有特征的是200：通知浏览器本次返回的资源文件是一个完整独立资源文件，浏览器在接收到之后不需要所要其他关联文件。 4）3XX：最有特征的是302：通知浏览器本次返回的不是一个资源文件内容而是一个资源文件地址，需要浏览器根据这个地址自动发起请求来所要这个资源文件。 response.sendRedirect(\"资源文件地址\")写入到响应头中location，而这个行为导致Tomcat将302状态码写入到状态行。 5）4XX 404：通知浏览器，由于在服务器没有定位到被访问的资源文件，因此无法提供帮助。 405：通知浏览器，在服务器已经定位到被访问的资源文件（Servlet），但是这个Servlet对于浏览器采用的请求方式不能处理 6）5XX 500：通知浏览器，在服务端已经定位到被访问的资源文件（Servlet），这个Servlet可以接收浏览器采用请求方式，但是Servlet在处理请求期间，由于Java异常导致处理失败。 做个Servlet之间的调用规则 前提条件 某些来自于浏览器发送请求，往往需要服务端中多个Servlet协同处理。但是浏览器一次只能访问一个Servlet，导致用户需要手动通过浏览器发起多次请求才能得到服务。这样增加用户获得服务难度，导致用户放弃访问当前网站。 提高用户使用感受规则 无论本次请求涉及到多少个Servlet，用户只需要【手动】通知浏览器发起一次请求即可。 多个Servlet之间调用规则 1）重定向解决方案 2）请求转发解决方案 重定向解决方案 工作原理 用户第一次通过【手动方式】通知浏览器返回OneServlet。OneServlet工作完毕后，将TwoServlet地址写入到响应头location属性中，导致Tomcat将302状态码写入到状态行。 在浏览器接收到响应之后，会读取302状态。此时浏览器自动根据响应头中location属性地址发起第二次请求，访问TwoServlet去完成请求中剩余任务。 实现命令 response.sendRedirect(\"请求地址\")，将地址写入到响应包中响应头中的location属性。 特征 1）请求地址：既可以把当前网站内部的资源文件地址发送给浏览器（/网站名/资源文件名），也可以把其他网站资源文件地址发送给浏览器（http://ip地址：端口号/网站名称/资源文件名)。 2）请求次数：浏览器至少发送两次请求，但是只有第一次请求是用户手动发送。后续请求都是浏览器自动发送的。 3）请求方式：重定向解决方案中，通过地址栏通知浏览器发起下一次请求，因此通过重定向解决方案调用的资源文件接收的请求方式一定是【get】。 4）缺点：重定向解决方案需要在浏览器与服务器之间进行多次往返，大量时间消耗在往返次数上，增加用户等待服务时间。 请求转发解决方案 原理 用户第一次通过手动方式要求浏览器访问OneServlet，OneServlet工作完毕后，通过当前的请求对象代替浏览器向Tomcat发起请求，申请调用TwoServlet，Tomcat在接收到这个请求之后，自动调用TwoServlet来完成剩余任务。 实现命令 1) 通过当前请求对象生成资源文件申请报告对象 // 注意：一定要以“/”为开头 RequestDispatcher report = request.getRequestDispatcher(\"/资源文件名\"); 2) 将报告对象发送给Tomcat report.forward(当前请求对象, 当前响应对象); 优点 1）无论本次请求涉及到多少个Servlet，用户只需要手动通过浏览器发送一次请求。 2）Servlet之间调用发生在服务端计算上，节省服务器与浏览器之间往返次数，增加处理服务速度。 特征 1）请求次数：在请求转发过程中，浏览器只发送一次请求。 2）请求地址：知恩感向Tomcat服务器申请调用当前网站下资源文件地址。 3）请求方式：在请求转发过程中，浏览器只发送一个Http请求协议包，参与本次请求的所有Servlet共享同一个请求协议包，因此这些Servlet接收的请求方式与浏览器发送的请求方式保持一致。 多个Servlet之间数据共享实现方案 数据共享，OneServlet工作完毕后，将产生数据交给TwoServlet来使用。 Servlet规范中提供四种数据共享方案 ServletContext接口 Cookie类 HttpSession接口 HttpServletRequest接口 ServletContext接口 介绍 来自于Servlet规范中一个接口，在Tomcat中存在servlet-api.jar，在Tomcat中负责提供这个接口实现类。 如果两个Servlet来自于同一个网站，彼此之间通过网站的ServletContext实例对象实现数据共享。 开发人员习惯于将ServletContext对象称为【全局作用域对象】。 工作原理 每个网站都存在一个全局作用域对象，这个全局作用域对象【相当于】一个Map，在这个网站中OneServlet可以将一个数据存入到全局作用域对象，当前网站中其他Servlet此时都可以从全局作用域对象得到这个数据进行使用。 全局对象作用域的生命周期 1）在Http服务器启动过程中，自动为当前在内存中创建一个全局作用域对象。 2）在Http服务器运行期间时，一个网站只有一个全局作用域对象。 3）在Http服务器运行期间，全局作用域对象一致处理存活状态。 4）在Http服务器准备关闭时，负责将当前网站中全局作用域对象进行销毁处理。 全局作用域对象生命周期贯穿网站整个运行期间 命令实现 【同一个网站】OneServlet将数据共享给TwoServlet // 通过【请求对象】向Tomcat索要当前网站中【全局作用域对象】 ServletContext application = request.getServletContext(); // 将数据添加到全局作用域对象作为【共享数据】 application.setAttribute(\"key1\", 数据); 取数据 // 通过【请求对象】向Tomcat索要当前网站中【全局作用域对象】 ServletContext application = request.getServletContext(); // 从全局作用域对象得到指定关键字对应数据 Object 数据 = application.getAttribute(\"key1\"); Cookie 介绍 1）Cookie来自于Servlet规范中一个工具类，存在于Tomcat提供Servlet-api.jar中。 2）如果两个Servlet来自于同一个网站，并且为同一个浏览器/用户提供服务，此时借助于Cookie对象进行数据共享。 3）Cookie存放当前用户的私人数据，在共享数据过程中提供服务质量。 4）在现实生活场景中，Cookie相当于用户在服务端得到【会员卡】。 原理 用户通过浏览器第一次向MyWeb网站发送请求申请OneServlet，OneServlet在运行期间创建一个Cookie存储于当前用户相关数据，OneServlet工作完毕后，【将Cookie写入到响应头】交还给当前浏览。 浏览在接收到响应包之后，将Cookie存储在浏览器的缓存，一段时间后，用户通过【同一个浏览器】再次向【MyWeb网站】发送请求申请TwoServlet时，【浏览器需要无条件的将myWeb网站之前推送过来的Cookie，写入到【请求头】发送国旗。 此时TwoServlet在运行时，就可以通过读取请求头中的Cookie中信息，得到OneServlet提供的共享数据。 实现命令 同一个网站OneServlet于TwoServlet借助于Cookie实现数据共享 // 创建一个Cookie对象，保存共享数据（当前用户数据） // Cookie相当于一个map，一个cookie中只能存放一个键值对，这个键值对的key于value只能是String，键值对中key不能时中文 Cookie card = new Cookie(\"key1\", \"abc\"); Cookie card1 = new Cookie(\"key2\", \"abc2\");; // 2.【发卡】将cookie写入到响应头，交给浏览器 response.addCookie(card); response.addCookie(card1); 取数据 // 调用请求对象从请求头得到浏览器返回的cookie Cookie cookieArray[] = request.getCookies(); // 循环遍历数据得到每一个cookie的key与value for(Cookie card: cookieArray) { // 读取key “key1” String key = card.getName(); // 读取value “abc” String value = card.getValue(); } 会员卡订单 新建index.html Title 新会员申请开卡 用户名 预存金额 新建OneServlet package com.c1221.controller; import javax.servlet.*; import javax.servlet.http.*; import java.io.IOException; public class OneServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { String userName,money; // 调用请求对象读取【请求头】参数信息 userName = request.getParameter(\"userName\"); money = request.getParameter(\"money\"); // 开卡 Cookie card1 = new Cookie(\"userName\", userName); Cookie card2 = new Cookie(\"money\", money); // 发卡，将Cookie写入到响应头交给浏览器 response.addCookie(card1); response.addCookie(card2); // 通知浏览器【点餐页面】内容写入到响应体交给浏览器（请求转发） request.getRequestDispatcher(\"/index_2.html\").forward(request, response); } @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } } 新建index_2.html Title 点餐页面 食物类型饺子（30元） 面条（20元） 盖饭（15元） 新建TwoServlet package com.c1221.controller; import javax.servlet.*; import javax.servlet.http.*; import java.io.IOException; import java.io.PrintWriter; public class TwoServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { int jiaozi_money=30; int gaifan_money=15; int miantiao_money = 20; int money=0,xiaofei=0,balance=0; String food,userName = null; Cookie cookieArray[] = null; response.setContentType(\"text/html;charset=utf-8\"); PrintWriter out = response.getWriter(); Cookie newCard = null; // 读取请求头参数信息，得到用户点餐食物类型 food = request.getParameter(\"food\"); // 读取请求中的Cookie cookieArray = request.getCookies(); // 刷卡消费 for(Cookie card: cookieArray) { String key = card.getName(); String value = card.getValue(); if (\"userName\".equals(key)) { userName = value; } else if (\"money\".equals(key)) { money = Integer.valueOf(value); if (\"jiaozi\".equals(food)) { if (jiaozi_money > money) { out.print(\"用户\"+userName+\" 余额不足，请充值\"); } else { newCard = new Cookie(\"money\", (money-jiaozi_money)+\"\"); xiaofei = jiaozi_money; balance = money - jiaozi_money; } } else if (\"miantiao\".equals(food)) { if (miantiao_money > money) { out.print(\"用户\"+userName+\" 余额不足，请充值\"); } else { newCard = new Cookie(\"money\", (money-miantiao_money)+\"\"); xiaofei = miantiao_money; balance = money - miantiao_money; } } else if (\"gaifan\".equals(food)) { if (gaifan_money > money) { out.print(\"用户\"+userName+\" 余额不足，请充值\"); } else { newCard = new Cookie(\"money\", (money-gaifan_money)+\"\"); xiaofei = gaifan_money; balance = money - gaifan_money; } } } } // 将用户会员卡返还给用户 response.addCookie(newCard); // 将消费记录写入响应 out.print(\"用户\"+userName+\"本次消费 \"+xiaofei+\" 余额：\"+balance); } @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } } Cookie生命周期 Cookie销毁时机 1）在默认情况下，Cookie对象存放在浏览器的缓存中，因此只要浏览器关闭，Cookie对象就被销毁掉。 2）在手动设置情况下，可以要求浏览器将接收的Cookie存放在客户端计算机上硬盘上，同时需要指定Cookie在硬盘上存活时间。在存活时间范围内，关闭浏览器关闭客户端计算机，关闭服务器，都不会导致Cookie被销毁。在存活时间到达时，Cookie自动从硬盘上被删除。 // cookie在硬盘上存活1分钟 cookie.setMaxAge(60); HttpSession接口 介绍 1）HttpSession接口来自于Servlet规范下一个接口，存在于Tomcat中servlet-api.jar，其实现类由Http服务器提供。Tomcat体统实现类存在于servlet-api.jar。 2）如果两个Servlet来自于同一个网站，并且为同一个浏览器/用户提供服务，此时借助于HttpSession对象进行数据共享。 3）开发人员习惯于将HttpSession接口修饰对象称为【会话作用域对象】。 HttpSession于Cookie区别 1）存储位置：一个在天上，一个在地下 Cookie：存放在客户端计算机（浏览器内存/硬盘） HttpSession：存放在服务端计算机内存 2）数据类型 Cookie对象存储共享数据类型只能是String HttpSession对象可以存储任意类型的共享数据Object 3）数据数量 一个Cookie对象只能存储一个共享数据，HttpSession使用map集合存储共享数据，所以可以存储任意数量共享数据。 4）参照物 Cookie相当于客户在服务端【会员卡】，HttpSession相当于客户在服务端【私人保险柜】。 命令实现 同一个网站下OneServlet将数据传递给TwoServlet OneServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) { // 调用请求对象向Tomcat索要当前用户在服务端的私人存储柜 HttpSession session = request.getSession(); // 将数据添加到用户私人存储柜 session.setAttribute(\"key1\", 共享数据); } } 浏览器访问/myWeb中TwoServlet OneServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) { // 调用请求对象向Tomcat索要当前用户在服务端的私人存储柜 HttpSession session = request.getSession(); // 从会话作用域对象得到OneServlet提供的共享数据 Object 共享数据 = session.getAttribute(\"key1\"); } } Http服务器如何将用户于HttpSession关联起来 cookie getSession于getSession(false) 1）getSession()：如果当前用户在服务端已经拥有了自己的私人储物柜，邀请Tomcat将这个私人储物柜进行返回；如果当前用户在服务端尚未拥有自己的私人储物柜，邀请Tomcat为当前用户创建一个全新的私人储物柜。 2）getSession(false)：如果当亲啊用户在服务端已经拥有了自己的私人储物柜，要求Tomcat将这个私人储物柜进行返回；如果当前用户在服务端尚未拥有自己的的私人储物柜，此时Tomcat将返回null。 HttpSession的销毁时机 1）用户与HttpSession关联是使用的Cookie只能存放在浏览器缓存中。 2）在浏览器关闭时，意味着用户与他的HttpSession关联被切断 3）由于Tomcat无法检测浏览器何时关闭，因此在浏览器关闭时并不易导致Tomcat将浏览器关联的HttpSession进行销毁。 4）为了解决这个问题，Tomcat为每一个HttpSession对象设置【空闲时间】，这个空闲时间默认30分钟，如果当前HttpSession对象空闲时达到30分钟，此时Tomcat认为用户已经放弃了自己的HttpSession，此时Tomcat就会销毁掉这个HttpSession。 HttpSession空闲时间手动设置 在当前网站/web/WEB-INF/web.xml 5 Session购物车示例 新建index.html Title 商品名称 商品单价 供货商 放入购物车 华为笔记本电脑pro13 7000 华为 放入购物车 榴莲 300 泰国 放入购物车 男士内裤 1000 老催 放入购物车 查看我的购物车 新建OneServlet package com.c1221.controller; import javax.servlet.*; import javax.servlet.http.*; import java.io.IOException; public class OneServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { String goodsName; // 调用请求对象，读取请求头参数，得到用户选择商品名 goodsName = request.getParameter(\"goodsName\"); // 调用请求对象，向Tomcat索要当前用户服务端的私人储物柜 HttpSession session = request.getSession(); // 将用户选购商品添加到当前用户私人储物柜 Integer goodsNum = (Integer)session.getAttribute(goodsName); if (goodsNum == null) { session.setAttribute(goodsName, 1); } else { session.setAttribute(goodsName, goodsNum+1); } } @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } } 新建TwoServlet package com.c1221.controller; import javax.servlet.*; import javax.servlet.http.*; import java.io.IOException; import java.util.Enumeration; public class TwoServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { // 调用请求对象，向Tomcat索要当前用户在服务端私人储物柜 HttpSession session = request.getSession(); // 将session中所有的key读取出来，存放一个枚举对象 Enumeration goodsNames = session.getAttributeNames(); while (goodsNames.hasMoreElements()) { String goodsName = (String)goodsNames.nextElement(); int goodsNum = (int)session.getAttribute(goodsName); System.out.println(\"商品名称\"+goodsName+\" 商品数量\"+goodsNum); } } @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } } HttpServletRequest接口实现数据共享 介绍 1）在同一个网站中，如果两个Servlet之间通过【请求转发】方式进行调用，彼此之间共享同一个请求协议包。而一个请求协议包只对应一个请求对象，因此Servlet之间共享同一个请求对象，此时可以利用这个请求对象在两个Servlet之间实现数据共享。 2）在请求对象实现Servlet之间数据共享功能时，开发人员将请求数据对象称为【请求作用域对象】。 命令实现 OneServlet通过请求转发申请调用TwoServlet时，需要给TwoServlet提供共享数据 OneServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) { // 将数据添加到【请求作用域对象】中attribute属性 request.setAttribute(\"key1\", 数据);// 数据类型可以任意类型Object // 向Tomcat申请调用TwoServlet request.getRequestDispatcher(\"/two\").forward(request, response); } } TwoServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) { // 从当前请求对象得到OneServlet写入到共享数据 Object 数据 = request.getAttribute(\"key1\"); } } 监听器接口 介绍 1）一组来自于Servlet规范下接口，共有8个接口。在Tomcat存在servlet-api.jar包。 2）监听器接口需要由开发人员亲自实现，Http服务器提供jar包并没有对象的实现类。 3）监听器接口用于监控【作用域对象生命周期变化时刻】以及【作用域对象共享数据变化时刻】 作用域对象 1）在Servlet规范中，认为在服务端内存中可以在某些条件下为两个Servlet之间提供数据共享方案的对象，被称为【作用域对象】 2）Servlet规范下作用域对象： ServletContext：全局作用域对象 HttpServlet：会话作用域对象 HttpServletRequest：请求作用域对象 监听器接口实现类开发规范 根据监听的实际情况，选择对应监听器接口进行实现 充血监听器接口声明【监听事件处理方法】 在web.xml文件将监听器接口实现类注册到Http服务器 ServletContextListener接口： 1）作用：通过这个接口合法的检测全局作用域对象被初始化时刻及销毁时刻。 2）监听事件处理方法： public void contextInitlized()：在全局作用对象被Http服务器初始化被调用 public vaid contextDestory()：在全局作用域对象被Http服务器销毁事件触发调用 ServletContextAttributeListener接口 1）作用：通过这个接口合法的检测全局作用域对象共享数据变化时刻 2）监听事件处理方法： public void contextAdd()：在全局作用域对象添加共享数据 public void contextReplaced()：在全局作用域对象更新共享数据 public vlid contextRemove()：在全局作用域对象删除共享数据 全局作用域对象共享数据变化时刻 ServletContext application = request.getServletContext(); application.setAttribute(\"key1\", 100);// 新增共享数据 application.setAttribute(\"key1\", 200);// 更新共享数据 application.removeAttribute(\"key1\")；// 删除共享数据 监听器接口提高程序运行速度 测试用户注册时间 JDBC规范中，Connection创建与销毁最浪费时间，修改UserAddServlet，计算消耗时间： Date startDate = new Date(); result = dao.add(user); Date endDate = new Date(); System.out.println(\"添加消耗时间 = \"+ (endDate.getTime() - startDate.getTime())+\"毫秒\"); 新建JdbcUtil2 package com.c1221.util; import javax.servlet.ServletContext; import javax.servlet.http.HttpServletRequest; import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; import java.sql.SQLException; import java.util.Iterator; import java.util.Map; public class JdbcUtil2 { static final String URL = \"jdbc:mysql://localhost:3306/c1221\"; static final String USERNAME = \"root\"; static final String PASSWORD = \"123456\"; PreparedStatement ps = null; Connection con = null; // 静态代码块在类加载时执行，并且只执行一次 static { try { Class.forName(\"com.mysql.jdbc.Driver\"); } catch (ClassNotFoundException e) { e.printStackTrace(); } } public Connection getCon(HttpServletRequest request) { // 1. 通过请求对象，得到全局作用域对象 ServletContext application = request.getServletContext(); // 2. 从全局作用域得到map Map map = (Map)application.getAttribute(\"key1\"); // 3. 从map得到一个处于空闲状态Connection Iterator it = map.keySet().iterator(); while (it.hasNext()) { con = (Connection) it.next(); boolean flag = (boolean) map.get(con); if (flag == true) { break; } } return con; } public PreparedStatement createStatement(String sql, HttpServletRequest request) { try { ps = getCon(request).prepareStatement(sql); } catch (SQLException e) { e.printStackTrace(); } return ps; } public void close(HttpServletRequest request) { if (ps != null) { try { ps.close(); } catch(SQLException e) { e.printStackTrace(); } } ServletContext application = request.getServletContext(); Map map = (Map)application.getAttribute(\"key1\"); map.put(con, true); } public Connection getCon() { try { con = DriverManager.getConnection(URL, USERNAME, PASSWORD); } catch (SQLException e) { e.printStackTrace(); } return con; } public PreparedStatement createStatement(String sql) { try { ps = getCon().prepareStatement(sql); } catch (SQLException e) { e.printStackTrace(); } return ps; } public void close() { if (ps != null) { try { ps.close(); } catch(SQLException e) { e.printStackTrace(); } } if (con != null) { try { con.close(); } catch(SQLException e) { e.printStackTrace(); } } } } 新建OneListener package com.c1221.listener; import com.c1221.util.JdbcUtil; import com.c1221.util.JdbcUtil2; import javax.servlet.*; import javax.servlet.http.*; import java.sql.Connection; import java.util.HashMap; import java.util.Iterator; import java.util.Map; public class OneListener implements ServletContextListener, HttpSessionListener, HttpSessionAttributeListener { public OneListener() { } // 在Tomcat启动时，预先创建20个Connection，在userDao.add方法执行时 // 将实现建好connection交给add方法 @Override public void contextInitialized(ServletContextEvent sce) { /* This method is called when the servlet context is initialized(when the Web application is deployed). */ JdbcUtil2 util = new JdbcUtil2(); Map map = new HashMap(); for(int i=1;i 修改xml com.c1221.listener.OneListener 修改UserDao package com.c1221.com.c1221.dao; import com.c1221.entity.Users; import com.c1221.util.JdbcUtil; import com.c1221.util.JdbcUtil2; import javax.servlet.http.HttpServletRequest; import java.sql.*; import java.util.ArrayList; import java.util.List; public class UserDao { JdbcUtil2 util = new JdbcUtil2(); public int add(Users users) { Connection conn = null; PreparedStatement ps = null; int result = 0; try { // 2、获取连接 conn = JdbcUtil.getConnection(); // 将自动提交机制修改为手动提交 conn.setAutoCommit(false); // 3、获取数据库操作对象 String sql = \"insert into users(userName,password,sex,email)\" + \" values(?,?,?,?)\"; ps = conn.prepareStatement(sql); // 4、执行SQL语句 ps.setString(1, users.getUserName()); ps.setString(2, users.getPassword()); ps.setString(3, users.getSex()); ps.setString(4, users.getEmail()); result = ps.executeUpdate(); conn.commit(); } catch(Exception e) { // 回滚事务 if(conn != null) { try { conn.rollback(); } catch (SQLException e1) { e1.printStackTrace(); } } e.printStackTrace(); } finally { JdbcUtil.close(conn, ps, null); } return result; } public int add(Users users, HttpServletRequest request) { String sql = \"insert into users(userName,password,sex,email)\" + \" values(?,?,?,?)\"; PreparedStatement ps = util.createStatement(sql, request); int result = 0; try { // 4、执行SQL语句 ps.setString(1, users.getUserName()); ps.setString(2, users.getPassword()); ps.setString(3, users.getSex()); ps.setString(4, users.getEmail()); result = ps.executeUpdate(); } catch(SQLException e) { e.printStackTrace(); } finally { util.close(request); } return result; } // 查询用户信息 public List findAll() { PreparedStatement ps = null; Connection conn = null; ResultSet rs = null; List userList = new ArrayList(); try { // 2、获取连接 conn = JdbcUtil.getConnection(); // 3、获取数据库操作对象 String sql = \"select * from users\"; ps = conn.prepareStatement(sql); rs = ps.executeQuery(); while (rs.next()) { Integer userId = rs.getInt(\"userId\"); String userName = rs.getString(\"userName\"); String password = rs.getString(\"password\"); String sex = rs.getString(\"sex\"); String email = rs.getString(\"email\"); Users users = new Users(userId, userName, password, sex, email); userList.add(users); } } catch(SQLException e) { e.printStackTrace(); } finally { JdbcUtil.close(conn, ps, rs); } return userList; } // 根据用户编号删除用户信息 public int delete(String userId) { Connection conn = null; PreparedStatement ps = null; int result = 0; try { // 2、获取连接 conn = JdbcUtil.getConnection(); // 将自动提交机制修改为手动提交 conn.setAutoCommit(false); // 3、获取数据库操作对象 String sql = \"delete from users where userId=?\"; ps = conn.prepareStatement(sql); ps.setString(1, userId); result = ps.executeUpdate(); conn.commit(); } catch(Exception e) { // 回滚事务 if(conn != null) { try { conn.rollback(); } catch (SQLException e1) { e1.printStackTrace(); } } e.printStackTrace(); } finally { JdbcUtil.close(conn, ps, null); } return result; } // 登录验证 public int login(String userName, String password) { PreparedStatement ps = null; Connection conn = null; ResultSet rs = null; int result = 0; try { // 2、获取连接 conn = JdbcUtil.getConnection(); // 3、获取数据库操作对象 String sql = \"select count(*) from users where userName=? and password=?\"; ps = conn.prepareStatement(sql); ps.setString(1, userName); ps.setString(2, password); rs = ps.executeQuery(); while (rs.next()) { result = rs.getInt(\"count(*)\"); } } catch(SQLException e) { e.printStackTrace(); } finally { JdbcUtil.close(conn, ps, rs); } return result; } } 修改UserAddServlet package com.c1221.controller; import com.c1221.com.c1221.dao.UserDao; import com.c1221.entity.Users; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.io.IOException; import java.io.PrintWriter; import java.util.Date; public class UserAddServlet extends HttpServlet { @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { String userName,password,sex,email; UserDao dao = new UserDao(); Users user = null; int result = 0; PrintWriter out = null; // 1.【调用请求对象】读取【请求头】参数信息，得到用户的信息 userName = req.getParameter(\"userName\"); password = req.getParameter(\"password\"); sex = req.getParameter(\"sex\"); email = req.getParameter(\"email\"); // 2.【调用UserDao】将用户信息填充到INSERT命令借助JDBC规范发送到数据库服务器 user = new Users(null, userName, password, sex, email); Date startDate = new Date(); result = dao.add(user, req); Date endDate = new Date(); System.out.println(\"添加消耗时间 = \"+ (endDate.getTime() - startDate.getTime())+\"毫秒\"); // 3.【调用响应对象】将【处理结果】以二进制形式写入到响应体 resp.setContentType(\"text/html;charset=utf-8\"); out = resp.getWriter(); if (result == 1) { out.print(\"用户信息注册成功\"); } else { out.print(\"用户信息注册失败\"); } out.close(); // Tomcat负责销毁【请求对象】和【响应对象】 // Tomcat负责将Http响应协议包推送到发起请求的浏览器上 // 浏览器根据响应头content-type指定编译器对响应体二进制内容编辑 // 浏览器将编辑后结果在窗口中展示给用户【结束】 } } 过滤器接口 介绍 1）来自于Servlet规范下接口，在Tomcat中存在于servlet-api.jar包。 2）Filter接口实现类由开发人员负责提供，Http服务器不负责提供。 3）Filter接口在Http服务器调用资源文件之前，对Http服务器进行拦截。 具体作用 1）拦截Http服务器，帮助Http服务器检测当前请求合法性。 2）拦截Http服务器，对当前请求进行增强操作。 Filter接口实现类开发步骤：三步 1）创建一个Java类实现Filter接口。 2）充血Filter接口中doFilter方法。 3）web.xml将过滤器接口实现类注册到Http服务器。 过滤器示例一：拦截一张图片 在web目录下新增一张图片资源文件（fj.jpg）。 新建OneFilter package com.c1221.filter; import javax.servlet.*; import java.io.IOException; import java.io.PrintWriter; public class OneFilter implements Filter { public void init(FilterConfig config) throws ServletException { } public void destroy() { } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws ServletException, IOException { // 通过拦截请求对象得到请求包参数信息，从而得到来访用户的真实年龄 String age = request.getParameter(\"age\"); // 根据年龄，帮助Http服务器判断本次请求合法性 if (Integer.valueOf(age) 大爷，珍爱生命啊！\"); } } } 修改web.xml OneFilter com.c1221.filter.OneFilter OneFilter /fj.jpg 过滤器示例二：对request设置编码方式 新建index.html Title 参数： 参数： 新建OneServlet package com.c1221.controller; import javax.servlet.*; import javax.servlet.http.*; import java.io.IOException; public class OneServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { // 直接从请求体读取请求参数 String userName = request.getParameter(\"userName\"); System.out.println(\"OneServlet 从请求体得到参数 \"+userName); } } 新建TwoServlet package com.c1221.controller; import javax.servlet.*; import javax.servlet.http.*; import java.io.IOException; public class TwoServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { // 直接从请求体读取请求参数 String userName = request.getParameter(\"userName\"); System.out.println(\"TwoServlet 从请求体得到参数 \"+userName); } } 新建OneFilter package com.c1221.filter; import javax.servlet.*; import java.io.IOException; public class OneFilter implements Filter { public void init(FilterConfig config) throws ServletException { } public void destroy() { } // 通知拦截的请求对象，使用UTF-8字符集对当前请求体信息进行一次重新编辑 @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws ServletException, IOException { request.setCharacterEncoding(\"utf-8\");// 增强 chain.doFilter(request, response); } } 修改web.xml OneFilter com.c1221.filter.OneFilter OneFilter /* 过滤器拦截地址格式 命令格式 OneFilter 拦截地址 命令作用 拦截地址通知Tomcat在调用何种资源文件之前需要调用OneFilter过滤进行拦截。 拦截具体文件 要求Tomcat在调用某一个具体文件之前，来调用OneFilter拦截 /img/fj.jpg 拦截文件夹 要求Tomcat在调用某一个文件夹下所有的资源文件之前，来调用OneFilter拦截 /img/* 拦截某种类型 要求Tomcat在调用任意文件夹下某种类型文件之前，来调用OneFilter拦截 *.jpg 拦截所有 要求Tomcat在调用网站中任意文件时，来调用OneFilter拦截 /* 过滤器防止用户恶意登录行为 修改LoginServlet package com.c1221.controller; import com.c1221.com.c1221.dao.UserDao; import javax.servlet.*; import javax.servlet.http.*; import java.io.IOException; public class LoginServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { String userName,password; UserDao dao = new UserDao(); int result = 0; // 调用请求对象对请求体使用utf-8字符集进行重新编辑 request.setCharacterEncoding(\"utf-8\"); // 调用请求对象读取请求体参数信息 userName = request.getParameter(\"userName\"); password = request.getParameter(\"password\"); // 调用DAO将查询验证信息推送到数据库服务器上 result = dao.login(userName, password); // 调用响应对象，根据验证码结果将不同资源文件地址写入到响应体，交给浏览器 if (result == 1) { // 在判定来访用户身份合法后，通过请求对象向Tomcat申请为当前用户申请一个HttpSession HttpSession session = request.getSession(); // 用户存在 response.sendRedirect(\"/examsystem/index.html\"); } else { response.sendRedirect(\"/examsystem/login_error.html\"); } } } 修改UserFindServlet package com.c1221.controller; import com.c1221.com.c1221.dao.UserDao; import com.c1221.entity.Users; import javax.servlet.*; import javax.servlet.http.*; import java.io.IOException; import java.io.PrintWriter; import java.util.List; public class UserFindServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { UserDao dao; PrintWriter out; // 索要当前用户在服务端HttpSession HttpSession session = request.getSession(false); if (session == null) { response.sendRedirect(\"/login_error.html\"); return; } // 1【调用DAO】将查询命令推送到数据服务器上，得到所有用户信息【List】 dao = new UserDao(); List userList = dao.findAll(); // 2【调用响应对象】将用户信息结合《table》标签命令以二进制形式写入到响应体 response.setContentType(\"text/html;charset=utf-8\"); out = response.getWriter(); out.print(\"\"); out.print(\"\"); out.print(\"用户编号\"); out.print(\"用户姓名\"); out.print(\"用户密码\"); out.print(\"用户性别\"); out.print(\"用户邮箱\"); out.print(\"操作\"); out.print(\"\"); for (Users users:userList) { out.print(\"\"); out.print(\"\"+users.getUserId()+\"\"); out.print(\"\"+users.getUserName()+\"\"); out.print(\"\"+users.getPassword()+\"\"); out.print(\"\"+users.getSex()+\"\"); out.print(\"\"+users.getEmail()+\"\"); out.print(\"删除用户\"); out.print(\"\"); } out.print(\"\"); } @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } } 使用过滤器 问题示意图： 使用过滤器： 1）新建OneFilter package com.c1221; import javax.servlet.*; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import javax.servlet.http.HttpSession; import java.io.IOException; public class OneFilter implements Filter { public void init(FilterConfig config) throws ServletException { } public void destroy() { } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws ServletException, IOException { HttpServletRequest request2 = (HttpServletRequest)request; HttpServletResponse response2 = (HttpServletResponse)response; // 拦截后，通过请求对象向Tomcat索要当前用户的HttpSession HttpSession session = request2.getSession(false); // 判断来访用户身份合法性 if (session == null) { request2.getRequestDispatcher(\"/login_error.html\").forward(request, response); return; } // 放行 chain.doFilter(request, response); } } 修改web.xml OneFilter com.c1221.OneFilter OneFilter /* 互联网通信流程图 解决拦截所有后，无法登录问题 修改OneServlet package com.c1221; import javax.servlet.*; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import javax.servlet.http.HttpSession; import java.io.IOException; public class OneFilter implements Filter { public void init(FilterConfig config) throws ServletException { } public void destroy() { } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws ServletException, IOException { HttpServletRequest request2 = (HttpServletRequest)request; HttpServletResponse response2 = (HttpServletResponse)response; HttpSession session = null; // 调用请求对象读取请求包中请求行URI，了解用户访问的资源文件是谁 String uri = request2.getRequestURI();//【/网站名/资源文件名】/examsystem/login.html or /examsystem/login // 如果本次请求资源文件与登录相关【login.html or LoginServlet】此时应该无条件放行 if (uri.indexOf(\"login\") != -1 || \"/examsystem/\".equals(uri)) { chain.doFilter(request, response); return; } // 如果本次请求访问的是其他资源文件，需要得到用户在服务器HttpSession session = request2.getSession(false); if (session != null) {// 判断来访用户身份合法性 chain.doFilter(request, response);// 放行 return; } // 做拒绝请求 request2.getRequestDispatcher(\"/login_error.html\").forward(request, response); } } 参考 HTTP Status Code 304 状态码的详细讲解 mac chrome 强制刷新浏览器缓存 "},"pages/servlet/httpservletrequest.html":{"url":"pages/servlet/httpservletrequest.html","title":"HttpServletRequest","keywords":"","body":"HttpServletRequest 什么是HttpServletReqeust？ 1）HttpServletRequest是一个接口，全限定名称：jakarta.servlet.http.HttpServletRequest。2）HttpServletRequest接口是Servlet规范中的一员，在Tomcat中存在servlet-api.jar。3）HttpServletRequest接口实现类由Http服务器负责提供。4）HttpServletReqeust接口负责在doGet/doPost方法运行时读取http请求协议包中信息。5）开发人员习惯于将HttpServletRequest接口修饰的对象称为【请求对象】。 HttpServletRequest父接口 jakarta.servlet.ServletRequest： public interface HttpServletRequest extends ServletRequest {} HttpServletRequest实现类 org.apache.catalina.connector.RequestFacade public class RequestFacade implements HttpServletRequest {} Tomcat服务器实现了HttpServletRequest接口，也说明了了Tomcat服务器实现了Servlet规范。 HttpServletRequest对象中有什么信息？ 1）HttpServletRequest对象是Tomcat服务器负责创建的。封装了HTTP的请求协议。 实际上是用户发送请求的时候，遵循了HTTP协议，发送的是HTTP的请求协议，Tomcat服务器将HTTP协议中的信息以及数据全部解析出来，然后Tomcat服务器把这些信息封装到HttpServletRequest对象当中，传给了javaweb程序员。 request对象和response对象生命周期 1）http服务器接收到浏览器发送的【http请求协议包】之后，自动为当前的【Http请求协议包】生成1个【请求对象】和1一个【响应对象】。2）在http服务器调用doGet/doPost方法时，负责将【请求对象】和【响应对象】作为实参传递到方法，确保doGet/doPost正确执行。3）在http服务器准备推送http响应协议包之前，负责将本次请求关联的【请求对象】和【响应对象】销毁。 【请求对象】和【响应对象】生命周期贯穿一次请求的处理过程中，【请求对象】和【响应对象】相当于用户在服务端的代言人。 获取前端浏览器用户提交的数据 String getParameter(String name) MapgetParameterMap() EnumerationgetParameterNames() String[] getParameterValues(String name) 以上4个方法和获取用户提交的数据有关系。 注意：getParameterValues之所以使用String[]格式，是因为前端提交的数据格式可能是：name=张三&name=李四，解决同一个key对应了多个值的问题。 获取请求信息 javaweb程序员面向HttpServletRequest接口编程，调用方法就可以获取请求的信息了。 请求信息包括：请求行、请求头、请求体。 1）获取请求行信息： public class Servlet1 extends HttpServlet { protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { // 1. 通过请求对象，读取【请求行】中【url】信息 String url = request.getRequestURL().toString(); // 2. 通过请求对象，读取【请求行】中【method】信息 String method = request.getMethod(); // 3. 通过请求对象，读取【请求行】中uri信息 String uri = request.getRequestURI();// substring System.out.println(\"URL \"+url); System.out.println(\"method \"+method); System.out.println(\"URI \"+uri); } } URI：资源文件精准定位地址，在请求行并没有URL这个属性，实际上URL中截取一个字符串，这个字符串格式“/网站名/资源文件名”，URI用于让Http服务器对被访问的资源文件进行定位。 2）获取请求体信息： public class Servlet2 extends HttpServlet { protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { // 1. 通过请求对象，读取【请求行】中【所有请求参数名】信息 Enumeration paramNames = request.getParameterNames();// 将所有请求参数名称保存到一个枚举对象进行返回 while(paramNames.hasMoreElements()) { String paramName = (String)paramNames.nextElement(); // 2. 通过请求对象读取指定的参数名称的值 String value = request.getParameter(paramName); System.out.println(\"请求参数名 \"+paramName+\" 请求参数值\"+value); } } } 请求转发 可以代替浏览器向Http服务器申请资源文件调用。 // 第一步：获取请求转发器对象 RequestDispatcher dispatcher = request.getRequestDispatcher(\"/xxx\"); // 第二步：调用转发器的forward方法完成跳转/转发 dispatcher.forward(request, response); // 第一步和第二步代码可以联合在一起 request.getRequestDispatcher(\"/xxx\").forward(request, response); 转发的时候是一次请求，不管你转发了多少次，都是一次请求。 AServlet转发到BServlet，再转发到CServlet，再转发到DServlet，不管转发了多少次，都在同一request中。 这是因为调用forward方法的时候，会将当前的request和response对象传递给下一个Servlet。 转发的时候，转发的路径以“/”开始，不加项目名。 中文乱码 1）Post请求 public class Servlet3 extends HttpServlet { protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { // 通过请求对象，读取【请求体】参数信息 String value = request.getParameter(\"userName\"); Sytem.out.println(\"从请求体得到参数值 \"+value); } } 问题：以POST方式发送中文参数内容“劳瘁是个男人”，得到【乱码】“？？？？？” 原因：浏览器以POST发送请求，请求参数保存在【请求体】，在Http请求协议包到达http服务器之后，第一件事就是进行解码，请求体二进制内容由当前请求（request）负责解码，request默认使用【ISO-8859-1】字符集，一个东欧语系字符集，此时如果请求体参数内容是中文，将无法解码只能得到乱码。 解决：在Post请求方式下，在读取请求体内容时，应该通知请求对象使用utf-8字符集请求体内容进行一次重新解码。 request.setCHaracterEncoding(\"utf-8\"); Tomcat10不会乱码，Tomcat8、9都会乱码。 2）Get请求 public class Servlet3 extends HttpServlet { protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { // 通过请求对象，读取【请求头】参数信息 String userName = request.getParameter(\"userName\"); System.out.println(\"从请求头得到参数值 \"+userName); } } 问题：以GET方式发送中文参数内容“老杨是个正经人”时，得到正常值， 原因：浏览器以GET方式发送请求，请求参数保存在【请求头】，在Http请求协议包到达http服务器之后，第一件事就是进行解码请求头二进制内容由Tomcat9.0默认使用【utf-8】字符集，可以解析一切国家文字。 Get请求中文乱码怎么解决？修改CATALINA_HOME/conf/server.xml配置文件。 如何查看Tomcat默认使用什么字符集解析Get请求：CATALINA_HOME/webapps/docs/config/http.html找到URIEncoding说明。例如tomcat7: This specifies the character encoding used to decode the URI bytes, after %xx decoding the URL. If not specified, ISO-8859-1 will be used. 3) Response中文乱码在Tomcat9及之前，响应中文也是有乱码的，如何解决： response.setContentType(\"text/html;charset=UTF-8\") 请求域对象 1）请求域对象要比应用域对象范围小很多，生命周期短很多，请求域只在一次请求内有效。2）一个请求对象request对应一个请求域对象，一次请求结束之后，这个请求域就销毁了。3）请求域对象也有这三个方法： void setAttribute(String name, Object o)// 向请求域绑定数据 void removeAttribute(String name)// 从域当中根据name获取数据 Object getAttribute(String name)// 将域当中绑定的数据移出 4）请求域和应用域的选用原则 尽量使用小的域对象，因为小的域对象占用的资源较小。 其他常用方法 // 获取客户端的IP地址 String remoteAddr = request.getRemoteAddr(); // 获取应用的根路径 String contextPath = request.getContextPath(); // 获取请求方式 String method = request.getMethod(); // 获取请求的URI String requestURI = request.getRequestURI(); // 获取servlet路径 String servletPath = request.getServletPath(); 视频地址 start:https://www.bilibili.com/video/BV1Z3411C7NZ?p=22end:https://www.bilibili.com/video/BV1Z3411C7NZ?p=26 2023.12.4 00:38 "},"pages/servlet/httpservletresponse.html":{"url":"pages/servlet/httpservletresponse.html","title":"HttpServletResponse","keywords":"","body":"HttpServletResponse 什么是HttpServletResponse？ 1）HttpServletResponse是一个接口，全限定名称：jakarta.servlet.http.HttpServletResponse。2）HttpServletResponse接口Servlet规范中的一员，在Tomcat中存在servlet-api.jar。3）HttpServletResponse接口实现类由Http服务器负责提供。4）HttpServletResponse接口负责将doGet/doPost方法执行结果以二进制形式写入到【响应体】交给浏览器。5）开发人员习惯于将HttpServletResponse接口修饰的对象称为【响应对象】。6）设置响应头中【content-type】属性值，从而控制浏览器使用，对应编译器将响应体二进制数据编译为【文字、图片、视频、命令】。7）设置响应头中【location】属性，将一个请求地址赋值给location，从而控制浏览器向执行服务器发送请求。 写入Hello world响应体 public class OneServlet extends HttpServlet { protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { String result = \"Hello，world\"; // ----- 响应对象将结果写入到响应体 ---- start // 1. 通过响应对象，向Tomcat索要输出流 PrintWriter out = response.getWriter(); // 2. 通过输出流，将执行结果以二进制形式写入到响应体 out.write(result); // ----- 响应对象将结果写入到响应体 ---- end } } 写入50，浏览器显示的是2？ public class TwoServlet extends HttpServlet { protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { String result = 50; // ----- 响应对象将结果写入到响应体 ---- start // 1. 通过响应对象，向Tomcat索要输出流 PrintWriter out = response.getWriter(); // 2. 通过输出流，将执行结果以二进制形式写入到响应体 out.write(result); // ----- 响应对象将结果写入到响应体 ---- end } } 问题：浏览器显示不是50，而是2？ 原因：out.writer方法将【字符】、【字符串】、【ASCII码】写入到响应体，【ASCII码】 a ---> 97, 2 ---> 50，这里的50对应2，所以显示2。 修改：out.write(result) ---> out.print(result)，这样就会显示50。 解析HTML public class ThreeServlet extends HttpServlet { protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { String result = \"JavaMysqlHTML\"; // ----- 响应对象将结果写入到响应体 ---- start // 1. 通过响应对象，向Tomcat索要输出流 PrintWriter out = response.getWriter(); // 2. 通过输出流，将执行结果以二进制形式写入到响应体 out.print(result); // ----- 响应对象将结果写入到响应体 ---- end } } 问题：浏览器在接收到响应结果时，将作为文字内容在窗口展示出来，没有将当作HTML变迁命令来执行。 原因：浏览器在接收到响应包之后，根据【响应头中content-type】属性的值，来采用对应【编译器】对【响应体中二进制内容】进行编译处理。在默认的情况下，content-type的属性值为“text”，content-type=\"text\"，此时浏览器回采用【文本编译器】对响应体二进制数据进行解析。 修改：一定要在得到输出流之前，通过响应对象对应响应头中content-type的属性进行一次重新赋值用于指定浏览器采用正确编译器。 // 设置响应头content-type response.setContentType(\"text/html\"); 中文乱码 public class ThreeServlet extends HttpServlet { protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { // 设置响应头content-type response.setContentType(\"text/html\"); String result = \"JavaMysqlHTML\"; String result2 = \"红烧排骨梅菜扣肉糖处里脊\" // ----- 响应对象将结果写入到响应体 ---- start // 1. 通过响应对象，向Tomcat索要输出流 PrintWriter out = response.getWriter(); // 2. 通过输出流，将执行结果以二进制形式写入到响应体 out.print(result); out.print(result2); // ----- 响应对象将结果写入到响应体 ---- end } } 问题：中文字符不可以正确显示？出现？？？ 修改： response.setContentType(\"text/html;charset=utf-8\"); charset=ISO-8859-1：偏东欧的字符集，应该修改为charset=utf-8。 重定向 public class FourServlet extends HttpServlet { protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { // 设置响应头content-type response.setContentType(\"text/html;charset=utf-8;\"); String result = \"http://www.baidu.com\"; // 通过响应对象，将地址赋值给响应头中location属性 response.sendRedirect(result);// [响应头 location=\"http://www.baidu.com\"] } } 重定向：浏览器在接收到响应包之后，如果发现响应头中存在location属性，自动通过地址栏向location指定网站发送请求。 sendRedirect方法远程控制浏览器请求行为【请求地址，请求方式，请求参数】。 转发和重定向区别 1）代码区别 转发： request.getRequestDispatcher(\"/xxx\").forward(request, response); 重定向： response.sendRedirect(\"/xxx/b\"); 注意：路径上要加xxx项目名。因为浏览器发送请求，请求路径上需要添加项目名。 2）形式区别 转发（一次请求）：在浏览器地址栏上发送的请求是：http://localhost:8080/xxx/a ，最终请求结束之后，浏览器地址上的地址还是这个，没变； 重定向（两次请求）：在浏览器地址栏上发送的请求是：http://localhost:8080/xxx/a ，最终在浏览器地址栏上显示的地址是：http://localhost:8080/xxx/b 。 3）本质区别 转发：是由WEB服务器来控制的，A资源跳转到B资源，这个跳转动作是Tomcat服务器内部完成的； 重定向：是浏览器完成的，具体跳转到哪个资源，是浏览器说了算。 转发和重定向应该如何选择？ 如果在上一个Servlet当中向request域当中绑定了数据，希望从下一个Servlet当中把request域里面的数据取出来，使用转发机制；剩下所有的请求均使用重定向。 转发不会改变请求方法，比如doPost请求转发至doGet请求，会导致进入下个请求的doPost，出现405错误，这个时候需要考虑使用重定向。 视频 start:https://www.bilibili.com/video/BV1Z3411C7NZ?p=32 2023.12.5 11:52 "},"pages/servlet/genericservlet.html":{"url":"pages/servlet/genericservlet.html","title":"GenericServlet","keywords":"","body":"GenericServlet 编写一个GenericServlet 这个类是一个抽象类，其中有一个抽象方法service。 1）GenericServlet实现Servlet接口。2）GenericServlet是一个适配器。3）以后编写的所有Servlet类继承GenericServlet，重写service方法即可。 思考 GenericServlet是否需要改造一下？怎么改造？更便于子类程序的编写？ 1）我提供了一个GenericServlet之后，init方法还会执行吗？ 还会执行。会执行GenericServlet类中的init方法。 2）init方法是谁调用的？ Tomcat服务器调用的。 3）init方法中的ServletConfig对象是谁创建的？是谁传递过来的？ - 都是Tomcat干的。 - Tomcat服务器先创建了ServletConfig对象，然后调用init方法，将ServletConfig对象传递给了init方法。 4）Tomcat服务器为代码 public class Tomcat { public static void main(String[] args) { // 创建LoginServlet对象（通过反射机制，调用无参数构造方法来实例化LoginServlet对象） Class clazz = Class.forName(\"com.xxx.javaweb.servlet.LoginServlet\"); Object obj = clazz.newInstance(); // 向下转型 Servlet servlet = (Servlet)obj; // 创建ServletConfig对象 // Tomcat负责将ServletConfig对象实例化出来 ServletConfig servletConfig = new org.apache.catalina.core.StandardwrapperFacade(); // 调用Servlet的init方法 servlet.init(servletConfig); // 调用Servlet的service方法 ... } } 注意 以后我们编写Servlet类的时候，实际上是不会去直接继承GenericServlet类的，因为我们是B/S结构的系统，这种系统是基于HTTP超文本传输协议的，在Servlet规范当中，提供了一个类叫做HttpServlet，它是专门为HTTP协议准备的一个Servlet类。我们编写的Servlet类药即成HttpServlet。（HttpServlet是HTTP协议专用的）使用HttpServlet处理HTTP协议更便捷。但是你需要知道他的继承结构： jakarta.servlet.Servlet（接口）爷爷 jakarta.servlet.GenericServlet（抽象类）儿子 jakarta.servlet.http.HttpServlet（抽象类）孙子 视频地址 https://www.bilibili.com/video/BV1Z3411C7NZ?p=13 "},"pages/servlet/servletconfig.html":{"url":"pages/servlet/servletconfig.html","title":"ServletConfig","keywords":"","body":"ServletConfig 什么是ServletConfig？ 1）Servlet对象的配置信息对象2）ServletConfig对象中封装了标签中的配置信息。（web.xml文件中Servlet的配置信息） ServletConfig创建 1）一个Servlet对应一个ServletConfig对象。2）Servlet对象是Tomcat服务器创建，并且ServletConfig对象也是Tomcat服务器创建。并且默认情况下，它们都是在用户发送第一次请求的时候创建。3）Tomcat服务器调用Servlet对象的init方法的时候需要传一个ServletConfig对象的参数给init方法。4）ServletConfig接口的实现类是Tomcat服务器给实现的。（Tomcat服务器说的是WEB服务器） ServletConfig接口有哪些常用的方法？ public String getInitParameter(String name);// 通过初始化参数的name获取value public Enumeration getInitParameterNames();// 获取所有的初始化参数的name public ServletContext getServletContext();// 获取ServletContext对象 public String getServletName();// 获取Servlet的name 以上方法在Servlet类当中，都可以使用this去调用。因为GenericServlet实现了ServletConfig接口。 视频 https://www.bilibili.com/video/BV1Z3411C7NZ?p=16 "},"pages/servlet/servletcontext.html":{"url":"pages/servlet/servletcontext.html","title":"ServletContext","keywords":"","body":"ServletContext 什么是ServletContext？ 1）一个Servlet对象对应一个ServletConfig。100个Servlet对象则对应100个ServletConfig对象。2）只要在同一个webapp当中，只要在同一个应用当中，所有的Servlet对象都是共享同一个ServletContext对象的。3）ServletContext对象在服务器启动阶段创建，在服务器关闭的时候销毁。这就是ServletContext对象的生命周期。ServletContext对象是应用级对象。4）Tomcat服务器中有一个webapps，这个webapps下可以存放webapp，可以存放多个webapp，假设有100个webapp，那么就有100个ServletContext对象。但是，总之，一个应用，一个webapp肯定只有一个ServletContext对象。5）ServletContext被称为Servlet上下文对象。（Servlet对象的四周环境对象）6）一个ServletContext对象通常对应的是一个web.xml文件。7）ServletContext是一个接口，Tomcat服务器对ServletContext接口进行了实现。 ServletContext接口中有哪些常用的方法？ public String getInitParameter(String name);// 通过初始化参数的name获取value public Enumeration getInitParameterNames();// 获取所有的初始化参数的name 以上两个方法是ServletContext对象的方法，这个方法获取的是什么信息？是以下的配置信息。 pageSize 10 注意：1）以上的配置信息属于应用级的配置信息，一般一个项目中共享的配置信息会放到以上标签中。2）如果你的配置信息只是想给某一个Servlet作为参数，那么你配置到Servlet标签当中即可，使用ServletConfig对象来获取。 public String getContextPath() 1）获取应用的根路径（非常重要），因为在java源代码当中有一些地方可能会需要应用的根路径，这个方法可以动态获取应用的根路径。2）在java源码当中，不要将应用的根路径血丝，因为你永远不知道这个应用在最终部署的时候，起一个什么名字。 pubic String getRealPath(String path)// 获取文件的绝对路径（真实路径） // 通过ServletContext对象也是可以记录日志的 public void log(String message); public void log(String message, Throwable t); 这些日志信息记录到哪里了？localhost.20xx-xx-xx.log Tomcat服务器的logs目录下都有哪些日志文件？ 1）catalina.20xx-xx-xx.log：服务器端的java程序运行的控制台信息2）localhost.20xx-xx-xx.log：ServletContext对象的log方法记录的日志信息存储到这个文件中。3）localhost_access_log.20xx-xx-xx.txt 访问日志 // 存（怎么向ServletContext应用域中存数据） public void setAttribute(String name, Object value); // 取（怎么从ServletContext应用域中取数据） public Object getAttribute(String name); // 删（怎么删除ServletContext应用域中的数据） public void removeAttribute(String name); 视频 https://www.bilibili.com/video/BV1Z3411C7NZ?p=16 "},"pages/servlet/httpservlet.html":{"url":"pages/servlet/httpservlet.html","title":"HttpServlet","keywords":"","body":"HttpServlet HttpServlet在jakarta.servlet.http包里，HttpServlet类是专门为HTTP协议准备的，比GenericServlet更加适合HTTP协议下的开发。 Servlet规范中的接口 jakarta.servlet.Servlet 核心接口（接口）jakarta.servlet.ServletConfig Servlet配置信息接口（接口）jakarta.servlet.ServletContext Servlet上下文接口（接口）jakarta.servlet.ServletRequest Servlet请求接口（接口）jakarta.servlet.ServletResponse Servlet响应接口（接口）jakarta.servlet.ServletException Servlet异常（类）jakarta.servlet.GenericServlet 标准通用的Servlet类（抽象类） http包下都有哪些类和接口？ jakarta.servlet.http.HttpServlet HTTP协议专用的Servlet类，抽象类jakarta.servlet.http.HttpServletRequest HTTP协议专用的请求对象jakarta.servlet.http.HttpServletResponse HTTP协议专用的响应对象 HttpServlet Request对象中封装了什么信息？ HttpServletRequest，简称request对象，封装了请求协议的全部内容。Tomcat服务器（WEB容器）将“请求协议”中的数据全部解析出来，然后将这些数据全部封装到request对象当中了。也就是说，我们只要面向HttpServletRequest，就可以获取请求协议中的信息。 HttpServletResponse 专门用来响应HTTP协议到浏览器的。 Servlet生命周期 用户第一次请求 Tomcat服务器通过反射机制，调用无参数构造方法，创建Servlet对象。（web.xml文件中配置的Servlet类对应的对象。） Tomcat服务器调用Servlet对象的init方法完成初始化。 Tomcat服务器调用Servlet对象的service方法处理请求。 用户第二次请求 Tomcat服务器调用Servlet对象的service方法处理请求。 用户第三次请求 Tomcat服务器调用Servlet对象的service方法处理请求。 用户第N次请求 Tomcat服务器调用Servlet对象的service方法处理请求。 服务器关闭 Tomcat服务器调用Servlet的对象的destroy方法，做销毁之前的准备工作。 Tomcat服务器销毁Servlet对象。 HttpServlet源码分析 HelloServlet： public class HelloServlet extends HttpServlet { // 用户第一次请求，创建HelloServlet对象的时候，会执行这个无参数的方法。 public HttpServlet() { } // override 重写 doGet方法 // override 重写 doPost方法 } public abstract class GenericServlet implements Servlet, ServletConfig, Serializable { // 用户第一次请求的时候，HelloServlet对象第一次被创建之后，这个init方法会执行 public void init(ServletConfig config) throws ServletException { this.config = config; this.init(); } // 用户第一次请求的时候，带有参数的`init(ServletConfig config)`执行之后，会执行这个没有参数的init() public void init() throws ServletException { } } HttpServlet： // HttpServlet模板类 public abstract class HttpServlet extends GenericServlet { // 用户发送第一次请求的时候，这个service会执行 // 用户发送第N次请求的时候，这个service方法还是会执行 // 用户只要发送一次请求，这个service方法就会执行一次 public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException { if (req instanceof HttpServletRequest && res instanceof HttpServletResponse) { // 将ServletRequest和ServletResponse向下转型为带有Http的HttpServlet和HttpServletResponse HttpServletRequest request = (HttpServletRequest)req; HttpServletResponse response = (HttpServletResponse)res; // 调用重载的service方法 this.service(request, response); } else { throw new ServletException(\"non-HTTP request or response\"); } } // 这个service方法的两个参数都是带有Http的 // 这个service是一个模板方法。 // 在该方法中定义核心算法骨架，具体的实现步骤延迟到子类中去完成。 protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { // 获取请求方式 // 这个请求方式最终可能是：“” // 注意：request.getMethod()方法获取的时候请求方式，可能是七种之一 // GET POST PUT DELETE HEAD OPTIONS TRACE String method = req.getMethod(); long lastModified; if (method.equals(\"GET\")) { // 如果请求方式是GET，这执行doGet lastModified = this.getLastModified(req); if (lastModified == -1L) { this.doGet(req, resp); } else { long ifModifiedSince = req.getDateHeader(\"If-Modified-Since\"); if (ifModifiedSince 通过以上源代码分析： 1）假设前端发送的请求是get请求，后端程序员重写的方法是doPost；发生405这样的错误。2）假设前端发送的请求是post请求，后端程序员重写的方法是doGet；发生405这样的错误。3）只要HttpServlet类中的doGet方法或doPost方法执行了，必然405。4）HelloServlet继承HttpServelt，重写HttpServlet类中的service()方法，享受不到405错误，享受不到HTTP协议专属的东西。 405表示前端的错误，发送的请求方式不对。和服务器不一致。不是服务器需要的请求方式。 Servlet类的开发步骤 1）编写一个Servlet类，直接继承HttpServlet；2）重写doGet方法或者重写doPost方法，到底重写谁，javaweb程序员说了算；3）将Servlet类配置到web.xml文件当中。4）准备前端的页面（form表单），form表单中指定请求路径即可。 视频地址 https://www.bilibili.com/video/BV1Z3411C7NZ?p=20 "},"pages/servlet/welcome.html":{"url":"pages/servlet/welcome.html","title":"配置欢迎页","keywords":"","body":"配置欢迎页 什么是web站点的欢迎页 对于一个webapp来说，我们是可以设置他的欢迎页面，设置了欢迎页面之后，当你访问这个webapp的时候，或者访问这个web站点的时候，没有指定任何“资源路径”，这个时候会默认访问你的欢迎页面。 我们一般的访问方式是：http://localhost:4000/pages/servlet/welcome.html 这种方式是指定了要访问的就是welcome.html资源。 如果我们访问的方式是：http://localhost:4000/pages 没有指定具体的资源路径，它会默认访问设置的欢迎页面。 怎么设置欢迎页面 1）在IDEA工具的web目录下新建了一个文件login.html2）在web.xml文件中进行了以下的配置 login.html 注意：设置欢迎页面的时候，这个路径不需要以“/”开始，并且这个路径默认是从webapp的根下开始查找。 3）启动服务器，浏览器地址输入地址：http://localhost:4000/pages 设置多个欢迎页面 page1/page2/page.html login.html 注意：越靠上的优先级越高，找不到的继续向下找。 默认欢迎页配置 1）webapp内部的web.xml。（局部配置）2）CATALINA_HOME/conf/web.xml。（全局配置） index.html index.htm index.jsp 注意：局部优先原则（就近原则）。 WEB-INF目录 1）在WEB-INF目录下新建了一个文件，webcome.html2）打开浏览器访问：http://localhost:8080/project/WEB-INF/welcome.html ，出现了404错误。 注意：放在WEB-INF目录下的资源是受保护的。在浏览器上不能通过路径直接访问。所以像HTML、CSS、JS、Image等静态资源一定要放在WEB-INF目录之外。 视频地址 https://www.bilibili.com/video/BV1Z3411C7NZ?p=20 "},"pages/servlet/annotation.html":{"url":"pages/servlet/annotation.html","title":"注解","keywords":"","body":"注解 Servlet3.0版本之后，推出了各种Servlet基于注解式开发。优点： 1）开发效率高，不需要编写大量的配置信息。直接在java类上使用注解进行标注。2）web.xml文件体积变小了。 @WebServlet 类路径：jakarta.servlet.annotation.WebServlet，WebServlet注解有哪些属性？ 1）name：用来指定Servlet的名字。2）urlPatterns：用来制定Servlet的映射路径，可以指定多个字符串。3）loadOnStartUp：用来指定在服务器启动阶段是否加载该Servlet。4）value：当注解属性名称为value的时候，使用注解的时候，value属性名可以省略。 不是必须将所有属性都写上，只需要提供需要的。（需要什么用什么） 属性是一个数组，如果数组中只有一个元素，使用该注解的时候，属性值的大括号可以省略。 注解对象的使用格式：@注解名称{属性名=属性值} 解析注解 // 使用反射机制将类上面的注解进行解析 // 获取类Class对象 Class welcomeServletClass = Class.forName(\"com.xxx.WelcomeServlet\"); // 先判断这个类上面有没有这个注解对象，如果有这个注解对象，就获取该注解对象 if(welcomeServletClass.isAnnotationPresent(WebServlet.class)) { // 获取这个类上面的注解对象 WebServlet webServletAnnotation = welcomeServletClass.getAnnotation(WebServlet.class); // 获取注解的value属性值 String[] value = webServletAnnotation.value(); for(int i=0;i 解决类爆炸 一个请求对应一个方法，一个业务对应一个Servlet类： @WebServlet({\"/dept/list\", \"/dept/save\", \"/dept/edit\", \"/dept/detail\", \"/dept/delete\", \"/dept/modify\"}) public class DeptServlet extends HttpServlet { @Override protected void service(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { // 获取servlet path String servletPath = request.getServletPath(); if (\"/dept/list\".equals(servletPath)) { doList(request, response); } else if (\"/dept/save\".equals(servletPath)) { doSave(request, response); } else if (\"/dept/edit\".equals(servletPath)) { doEdit(request, response); } else if (\"/dept/detail\".equals(servletPath)) { doDetail(request, response); } else if (\"/dept/delete\".equals(servletPath)) { doDelete(request, response); } else if (\"/dept/modify\".equals(servletPath)) { doModify(request, response); } } private void doList(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { } private void doSave(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOExcepiton { } ... } @MultipartConfig Servlet文件上传||@MultipartConfig标注属性 视频 start:https://www.bilibili.com/video/BV1Z3411C7NZ?p=34end:https://www.bilibili.com/video/BV1Z3411C7NZ?p=35 "},"pages/jsp/principles.html":{"url":"pages/jsp/principles.html","title":"JSP原理","keywords":"","body":"JSP原理 23.12.16 1:16更新 我的第一个JSP程序 1）在WEB-INF目录之外创建一个index.jsp文件，然后这个文件没有任何内容。2）在上面的项目部署之后，启动服务器，打开浏览器访问以下路径 ，展示空白页面。 http://localhost:8080/jsp/index.jsp 3）实际上访问index.jsp，底层执行的是index_jsp.class这个java程序。4）这个index.jsp会被tomcat翻译生成index_jsp.java文件，然后tomcat服务器又会将index_jsp.java编译生成index_jsp.class文件。 控制台找到： CATALINA_BASE: C:\\Users\\Administrator\\AppData\\Local\\JetBrains\\IntelliJIdea2021.3\\tomcat\\xxx index_jsp.java路径： CATALINA_BASE\\work/Catalina\\localhost\\jsp\\org\\apache\\jsp\\index_jsp.java 5）访问index.jsp，实际上执行的是index_jsp.class中的方法。 JSP实际是一个Servlet 1）index.jsp访问的时候，会自动翻译生成index_jsp.java，会自动翻译生成index_jsp.class，那么index_jsp这就是一个类。2）index_jsp类继承HttpJspBase，而HttpJspBase类继承的是HttpServlet。所以index_jsp类就是一个Servlet类。3）jsp的生命周期和Servlet的生命周期完全相同。完全就是一个东西，没有任何区别。4）jsp和servlet一样，都是单例的。（假单例） JSP文件第一次访问的时候是比较慢的，为什么？ 1）要把jsp文件翻译生成java文件。2）java源文件要编译生成class字节码文件。3）然后通过class去创建servlet对象。4）然后调用servlet对象的init方法。5）最后调用servlet对象的service方法。 第二次就比较快了，为什么？因为第二次直接调用单例servlet对象的esrvice方法即可。 JSP是什么？ 1）JSP是java程序。（JSP本质还是一个Servlet）2）JSP是：JavaServer Pages的缩写。（基于java语言实现的服务器的页面）3）Servlet是JavaEE的13个子规范之一，那么JSP也是13个子规范之一。4）JSP是一套规范。所有的web容器/web服务器都是遵循这套规范的，都是按照这套规范进行的“翻译”。5）每一个web容器/web服务器都会内置一个JSP翻译引擎。 翻译JSP 在JSP文件中直接编写文件，都会自动被翻译到哪里？ 1）翻译到servlet类的service方法的out.writer(\"翻译到这里\")，直接翻译到双引号里，被java程序当做普通字符串打印输出到浏览器。2）在JSP中编写的HTML CSS JS代码，这些代码对于JSP来说只是一个普通的字符串。但是JSP把这个普通的字符串一旦输出到浏览器，浏览器就会对HTML CSS JS进行解释执行，展示页面效果。 中文乱码 通过page指令来设置响应的内容类型，在内容类型的最后面添加：charset=UTF-8。 表示响应的内容类型是text/html，采用字符集UTF-8。 1）在这个符号当中编写的被视为java程序，被翻译到Servlet类的service方法内部。 注意：在这个符号里面写java代码的时候，要时时刻刻地记住你正在“方法提”当中写代码，方法体中可以写什么，不可以写什么。 2）在service方法当中编写的代码是有顺序的，方法体当中的代码要遵循自上而下的顺序依次逐行执行。3）service方法当中不能写静态代码块，不能写方法，不能定义成员变量。4）在同一个JSP当中这个符号可以出现多个。 在这个符号中编写的java程序自动翻译到service方法之外。 这个语法很少用，因为在service方法外面写静态变量和实例变量，都会存在线程安全问题，JSP就是servlet，servlet是单例的，多线程并发的环境下，这个静态变量和实例变量一旦有修改操作，必然会存在线程安全问题。 JSP的专业注释，不会被翻译到java源代码当中。 这种注释属于HTML的注视，仍然会被翻译到java源代码当中。 JSP的输出语句 注意：以上代码中的out是JSP的九大内置对象之一。可以直接拿来用。当然，必须只能在service方法内部使用。 如果向浏览器傻姑娘输出的内容中没有“java代码”，例如输出的字符串是一个固定的字符串，可以直接在jsp中编写，不需要写到这里。 在等号后面编写要输出的内容，翻译成以下java代码，翻译到service方法当中了。 out.print(); 当输出的内容中含有java的变量，输出的内容是一个动态的内容，不是一个死的字符串。如果输出是一个固定的字符串，直接在JSP文件中编写科技。 JSP指令 指导JSP的翻译引擎如何工作。 include：包含，在JSP中完成静态包含。 taglib：引入标签库，例如：JSTL标签。 page：可以指定contetType等 语法： page指令 ：true表示启用JSP的内置对象session，false，反之不启用session。 ：contentType属性用来设置响应的内容类型。 ：设置字符集编码。 和 功能一样。 ：导入包。 ：当java程序出现错误后，会跳转到error.jsp页面。 注意：当配置了errorPage时，需要在errorPage输出错误堆栈信息，要不然程序员无法获取错误信息日志。 ：表示启用JSP九大内置对象之一：exception，默认是false。配置错误error.jsp页面： error 网络繁忙，稍后再试！！！ JSP的九大内置对象 pageContext：页面作用域。 request：请求作用域。 session：会话作用域。 application：应用作用域。 pageContext 以上四个作用域都用：setAttribute getAttribute removeAttribute class中的方法。 以上作用域的使用原则：尽可能使用小的域。 exception：打印异常堆栈信息。 config：获取web.xml的配置信息。 page：其实就是this，当前的servlet对象。 out：负责输出。 response：负责响应。 视频 start:https://www.bilibili.com/video/BV1Z3411C7NZ?p=36 end:https://www.bilibili.com/video/BV1Z3411C7NZ?p=37 end:https://www.bilibili.com/video/BV1Z3411C7NZ?p=50 "},"pages/jsp/servletjsp.html":{"url":"pages/jsp/servletjsp.html","title":"Servlet+JSP项目改造","keywords":"","body":"Servlet+JSP项目改造 使用Servlet处理业务，收集数据，使用JSP展示数据。 修改html为jsp 修改html为jsp，然后在jsp文件头步添加page指令（指定contentType防止中文乱码，将所有JSP直接拷贝至web目录下。 使用替换超链接的根路径。 修改Servlet 1）将数据集合list存储到request域当中。2）转发forward到jsp。 修改JSP 1）从request域取出List集合。2）遍历List集合，取出每个对象，动态生成tr。 如果只用JSP这一个技术，能不能开发web应用？ 当然可以使用JSP来完成所有的功能，因为JSP就是Servlet，在JSP的里面写的代码就是在service方法当中的，所以在当中完全可以编写JDBC代码，连接数据库，查询数据，也可以在这个方法当中编写业务逻辑代码，处理业务，都是可以的，所以使用单独的JSP开发web应用完全没问题。 虽然JSP一个技术就可以完成web应用，但是不建议，还是建议采用servlet+jsp的方式进行开发。这样都能将各自的优点发挥出来，JSP就是做数据展示，Servlet就是做数据的收集。 JSP中编写的java代码越少越好，一定要职责分明。 JSP文件的扩展名必须是xxx.jsp吗？ jsp文件的扩展名是可以配置的，不是固定的，在CATALINA_HOME/conf/web.xml可以配置jsp文件的扩展名： jsp *.jsp *.jspx xxx.jsp文件对于tomcat来说，只是一个普通的文本文件，web容器会将xxx.jsp文件最终生成java程序，最终调用的是java对象相关的方法，真正执行的时候，和 jsp文件就没有关系了。 包名bean是什么意思？ 1）javabean：java的logo是一杯冒着热气的咖啡，javabean被翻译为咖啡豆。2）java是一杯咖啡，咖啡又是由一粒一粒的咖啡研磨而成。3）整个java程序中有很多bean的存在，由很多bean组成。 4）javabean其实就是java中的实体类，负责数据的封装。5）由于javabean符合javabean规范，具有更强的通用性。 什么是javabean？ 1）有无参数构造方法2）属性私有化3）对外提供公开的set和get方法4）实现java.io.Serializable接口5）重写toString6）重写hashCode+equals "},"pages/jsp/session.html":{"url":"pages/jsp/session.html","title":"Session","keywords":"","body":"Session 什么是会话？ 用户打开浏览器，进行一系列操作，然后最终将浏览器关闭，这个整个过程叫做：一次会话。会话在服务器端也有一个对应的java对象，这个java对象叫做session。 什么是一次请求：用户在浏览器上点击了一下，然后在页面停下来，可以粗略认为是一次请求。请求对应的服务器的java对象是request。 一次会话当中包含多次请求。 在java的servlet规范当中，session对象类名：HttpSession(jarkata.servlet.http.HttpSession)。 session机制属于B/S结构的一部分。如果使用php语言开发WEB项目，同样也是有session这种机制的。session机制实际上是一种规范。然后不同的语言对这种会话机制都有实现。 Session的作用 保存会话状态：用户登录成功了，这是一种刚登陆成功的状态，你怎么把登录成功的状态一只保存起来？使用session对象可以保留会话状态。 为什么需要session对象来保存会话状态呢？ 因为HTTP协议是一种无状态协议。只要B和S断开了，那么关闭浏览器这个动作，服务器知道吗？不知道，服务器不知道浏览器关闭的。 什么是无状态：请求的时候，B和S是连接的，但是请求结束之后，连接就断了。为什么要这么做？HTTP协议为什么要设计成这样？因为这样的无状态协议，可以降低服务器的压力，请求的瞬间是连接的，请求结束后，连接断开，这样服务器压力小。 为什么不使用request对象保存会话状态？ 同问：为什么不是用ServletContext对象保存会话状态？ request是一次请求一个对象。 ServletContext对象是服务器启动的时候创建，服务器关闭的时候销毁，这个ServletContext对象只有一个。 ServletContext对象的域太大。 request请求域（HttpServletRequest)、session会话域（HttpSession）、application域（ServletContext) request小于session小于application。 思考一下 HttpSession session = request.getSession()； 这行代码很神奇：张三访问的时候获取的session对象就是张三的；李四访问的时候获取的session对象就是李四的。 session的实现原理 JSESSIONID=xxxx 这个是一Cookie的形式保存在浏览器的内存中的。浏览器只要关闭，这个cookie就没有了。 session列表是一个Map，map的key是sessionid，map的value是session对象。 用户第一次请求，服务器生成session对象，同时生成ID，将ID发送给浏览器。 用户第二次请求，自动将浏览器内存中的ID发送给服务器，服务器根据ID查找session对象。 关闭浏览器，内存消失，cookie消失，sessionid消失，会话等同于结束。 Cookie禁用了，session还能找到吗？ cookie禁用是什么意思？服务器正常发送cookie给浏览器，但是浏览器不要了，拒收了，并不是服务器不发了。 找不到了，每次请求都会获取到新的session对象。 cookie禁用了，session机制还能实现吗？可以，需要使用URL重写机制。 http://xxxx;jsessionid=DD5EC49931F2AC378DC68548C9E5?v=f050045a0c URL重写机制会提高开发者的成本，开发人员在编写任何请求路径的时候，后面都要添加一个sessionid，给开发带来了很大的难度，很大的成本，所以大部分的网站都是这样设计：你要是禁用cookie，你就别用了。 销毁session对象 session.invalidate(); JSP禁用session 视频 start:https://www.bilibili.com/video/BV1Z3411C7NZ?p=44 end:https://www.bilibili.com/video/BV1Z3411C7NZ?p=45 "},"pages/jsp/cookie.html":{"url":"pages/jsp/cookie.html","title":"Cookie","keywords":"","body":"Cookie 23.12.15 2:20 更新 什么是cookie？ 在session实现原理中，jsessionid=DD5EC49931F2AC378DC68548C9E5?v=f050045a0c，这个键值对数据就是cookie对象，是一串字符串。 cookie机制和sesion机制其实都不属于java中的机制，实际上cookie机制和session机制都是HTTP协议的一部分。php开发中也有cookie和session机制，只要做web开发，不管是什么编程语言，cookie和session机制都是需要的。 HTTP协议中规定：任何一个cookie都是name和value组成的，name和value都是字符串类型的。 cookie保存在哪里？ 可以保存在浏览器的运行内存中，浏览器只要关闭，cookie就消失了。 也可以保存在硬盘文件中，永久保存。 cookie有啥用？ cookie和session机制其实都是为了保存会话的状态。 cookie是将会话的状态保存在浏览器客户端上。 session是将会话的状态保存在服务器上。 为什么要有cookie和session机制呢？因为HTTP协议是无状态无连接协议。 十天免登录 在126邮箱中有一个功能：十天免登录。怎么实现的？ 用户输入正确的用户名和密码，并且同时选择十天内免登录。登录成功后，浏览器客户端会保存一个cookie，这个cookie中保存了用户名和密码等信息，这个cookie是保存在硬盘文件当中，十天有效，在十天内用户再次访问126的时候，浏览器自动提交126关联的cookie给服务器，服务器收到cookie之后，获取用户名和密码，验证，通过之后，自动登录成功。 怎么让cookie失效？ cookie过期 修改密码 在浏览器上清除cookie Cookie类 在java的servlet中，对cookie提供了哪些支持呢？ java提供了一个Cookie类类专门表示cookie数据，jakarta.servlet.http.Cookie java程序怎么把cookie数据发送给浏览器呢？response.addCookie(cookie) 在HTTP协议中是这样规定的：当浏览器发送请求的时候，会自动携带该path下的cookie数据给服务器。（URL） API：https://tomcat.apache.org/tomcat-7.0-doc/servletapi/index.html cookie的有效时间 // 设置cookie在一小时后失效，保存在硬盘中 cookie.setMaxAge(60*60); // 设置cookie的有效期为0，表示删除cookie，主要应用在：使用这种方式删除浏览器上的同名cookie cookie.setMaxAge(0); // 设置cookie的有效期 cookie的path 假设现在发送的请求路径是http://localhost:8080/servlet13/cookie/generate生成cookie，如果cookie没有设置path，默认的path是什么？ 默认的path是：http://localhost:8080/servlet13/cookie及它的子路径。 也就是说，以后只要浏览器请求的路径是http://localhost:8080/servlet13/cookie及它的子路径，cookie都会被发送到服务器。 手动设置cookie的path：cookie.setPath(\"servlet13\");表示只要是这个servlet13项目的请求路径，都会提交这个cookie给服务器。 服务器获取cookie Cookie[] cookies = request.getCookies(); // 如果不是null，表示一定有cookie if (cookies != null) { // 遍历数组 for (Cookie cookie: cookies) { // 获取cookie的name和value String name = cookie.getName(); String value = cookie.getValue(); System.out.println(name + \"=\" + value); } } 视频 start:https://www.bilibili.com/video/BV1Z3411C7NZ?p=47 "},"pages/jsp/el.html":{"url":"pages/jsp/el.html","title":"EL表达式","keywords":"","body":"EL表达式 23.12.16 15:8开始 23.12.16 22:39更新 EL表达式是什么？ Expression Language（表达式语言） EL表达式可以代替JSP中的java代码，让JSP文件中的程序看起来更加整洁，美观。 JSP中夹杂着各种java代码，例如：、等，导致JSP文件很混乱，不好看，不好维护，所以才有了后期的EL表达式。 EL表达式可以算是JSP语法的一部分，EL表达式归属于JSP。 EL表达式作用 从某个域中取数据。 将取出的数据转成字符串。 将字符串输出到浏览器。 EL表达式使用 语法：${表达式} ${userObj} ${这里位置写什么？这里写的一定是存储到域对象当中时的name} ${userObj} 等同于代码 不要这样写：${\"userObj\"} ${userObj}底层怎么做的？ 从域中取数据，取出user对象，然后调用user对象的toString方法，转成字符串，输出到浏览器。 怎么输出对象的属性值？ ${userObj.username}：使用这个语法的前提是：User对象有getUsername()方法。 ${userObj.password} ${userObj.age} ${userObj.email} ${userObj.address.street}：支持.语法 EL表达式对null进行了处理，如果是null，则在浏览器上显示空白。 EL表达式中这个语法，实际上调用了底层的getXXX()方法，getXXX方法的名称不使用驼峰命名也是可以的，但是不推荐。 EL表达式取数据优先级 在没有指定域范围的前提下，EL表达式优先从小范围中取数据. 域范围大小：pageContext EL表达式中有四个隐式的范围，可以指定域范围来读取数据： ${pageScope.data}：pageContext域 ${requestScope.data}：request域 ${sessionScope.data}：session域 ${applicationScope.data}：application域 在实际开发中，因为向某个域中存储数据的时候，name都是不同的，所以，xxxScope都是可以省略的。 EL表达式取数据的方式 ${user.username}：一般使用这种就够用了。 ${user[\"username\"]}：如果存储到域的时候，这个name中含有特殊字符，可以使用[]方式，例如：name=\"xxx.xxx\"。 怎么从Map取数据？${map.key} 怎么从Array取数据？${array[0]}：取出数组中第一个元素输出 取不出数据，在浏览器显示空白，不会出现下标越界问题。 忽略EL表达式 page指令当中，有一个属性，可以忽略EL表达式： isELIgnored=\"true\"：表示忽略EL表达式 isELIgnored=\"false\"：表示不忽略EL表达式，默认false。 isELIgnored=\"true\"表示忽略JSP中整个页面的EL表达式，如果想忽略其中某个，可以使用反斜杠：${username}。 如果页面中${pageContext.request.contextPath}不生效，请显示增加。 https://blog.csdn.net/qq_52108058/article/details/127145065 使用EL表达式获取应用的根 ${pageContext.request.contextPath} 获取请求参数、应用域配置参数 ${param.aihao}：获取request中参数为aihao的值。相当于以下代码： ${paramValues.aihao[0]}：当aihao对应多个值的时候使用。相当于以下代码： ${initParam.pageNum}：获取web.xml中context初始化参数。相当于以下代码： application.getInitParameter(\"pageNum\")； 算术运算符：+ - * / % ${10+20}：显示30。 ${10+\"20\"}：“20”会自动转成数字，再相加，显示30。 ${10+\"abc\"}：报数字格式化错误，NumberFormatException。 +号在EL表达式中，只会做求和，不会做字符串拼接。 关系运算符：== != > >= ${\"abc\"==\"abc\"}：显示true。 ${k1 == k2}：显示true。 ${s1 == s2}：显示true，因为String重写了equals方法。 ${obj1 == obj2}：显示false。 ==调用了equals方法。 == 和 eq效果一致。 ${!stu1 eq stu2}：错误的写法。 ${!(stu1 eq stu2)}：正确的写法。 ${not(stu1 eq stu2)}：正确的写法。 空运算符：empty ${empty param.username}：判空 ${!empty param.username}：判非空 ${not empty param.username}：判非空 ${empty param.password == null}：前半部分是boolean， false == null，显示false。 其他运算符 逻辑运算符：! && || not and or 条件运算符：? : ${empty param.username ? \"对不起，用户名不能为空\" : \"欢迎访问\"} 取值运算符：[] . 视频 start：https://www.bilibili.com/video/BV1Z3411C7NZ?p=51 end：https://www.bilibili.com/video/BV1Z3411C7NZ?p=53 问题 * "},"pages/jsp/jstl.html":{"url":"pages/jsp/jstl.html","title":"JSTL标签库","keywords":"","body":"JSTL标签库 23.12.16 22:45开始 23.12.17 21:39更新 什么是JSTL标签库？ Java Standard Tag Lib（Java标准的标签库） JSTL标签库通过结合EL表达式一起使用，目的是让JSP的java代码消失。 标签是写在JSP当中的，但是实际上最终还是要执行对应的java程序。 引入JSTL标签库对应的jar包 在tomcat10及之后引入的jar： jakarta.servlet.jsp.jstl-2.0.0.jar jakarta.servlet.jsp.jstl-api-2.0.0.jar tomcat10之前使用： javax.servlet.jsp.jstl-2.0.0.jar taglibs-standard-impl-1.2.5.jar taglibs-standard-spec-1.2.5.jar 在IDEA当中怎么引入？ 在WEB-INF下新建lib目录，然后将jar包拷贝到lib当中，然后将其“Add Lib...”。 一定是要和mysql的数据库驱动一样，都是放在WEB-INF/lib目录下的。 什么时候需要将jar包放到WEB-INF/lib目录下？如果这个jar是tomcat服务器没有的。 在JSP中引入要使用的标签库 使用taglib指令引入标签库： 这个是核心标签库。 以上uri后面的路径实际上指向了一个c.tld文件。 c.tld路径：在jakarta.servlet.jsp.jstl-2.0.0.jar里面META-INF目录下。 c.tld文件实际是一个xml配置文件。 配置文件tld解析 对该标签的描述 catch标签的名字 org。apache.taglibs.standard.tag.common.core.CatchTag标签对应的java类 JSP标签体当中可以出现的内容，如果是JSP，就表示标签体中可以出现符合JSP所有语法的代码，例如EL表达式。 对这个属性的描述 var属性名 falsefalse表示该属性不是必须的，true则反之。 false这个描述说明了该属性是否支持EL表达式，false表示不支持，true表示支持EL表达式。 catch JSP... forEach id:${varStatus.count},${s.id},name:${s.name} ${i} if 没有else标签，可以使用两个if。 if标签还有var属性，不是必须的。 if标签还有scope属性，用来指定var的存储域，也不是必须的 scope有四个值可以选：page、request、session、application 将var中的v存储到request域。 欢迎你${param.username}。 choose 青少年 青年 中年 老年 HTML中的base xxx 注意：html的base标签可能对JS代码不起作用，所以JS代码最好前面写项目根路径： document.location.href = \"${pageContext.request.contextPath}/dept/delete?deptno=\" + dno; 动态获取base： 视频 start：http://www.bilibili.com/video/BV1Z3411C7NZ?p=54 end：http://www.bilibili.com/video/BV1Z3411C7NZ?p=55 "},"pages/jsp/filter.html":{"url":"pages/jsp/filter.html","title":"过滤器","keywords":"","body":"过滤器 23.12.17 21:59开始 23.12.17 23:35更新 过滤器实现原理图 编写过滤器 第一步： 编写一个java类实现一个接口：jarkata.servlet.Filter，并且实现这个接口当中的doFilter方法。 init方法：在Filter对象第一次被创建之后调用，并且只调用一次。 doFilter方法：只要用户发送一次请求，则执行一次；发送N次请求，则执行N次；在这个方法中编写过滤规则。 destroy方法：在Filter对象被释放/销毁之前调用，并且只调用一次。 第二步： 在web.xml文件中对Filter进行配置。 filter com.xxx.javaweb.filter1 filter1 /abc 或则使用注解@WebFilter({\"*.do\"}) 生命周期 Servlet对象默认情况下，在服务器启动的时候是不会新建对象的。 Filter对象默认情况下，在服务器启动的时候新建对象。 Servlet是单例的，Filter也是单例的。 目标Servlet执行条件 在过滤器当中是否编写了：chain.doFilter(request, response); 用户发送的请求路径是否和Servlet的请求路径一致。 chain.doFilter(request, response);：执行下一个过滤器，如果下面没有过滤器了，执行最终的Servlet。 Filter的优先级高于Servlet：/a.do对应了一个Filter，也对应了一个Servlet，那么一定是先执行Filter。 Filter的配置路径 精确匹配：/a.do、/b.do、/dept/save 配置所有：/* 后缀匹配：*.do 前缀匹配：/dept/* Filter执行顺序 依靠filter-mapping标签的位置，越靠上优先级越高。 dofilter调用遵循栈结构，先进后出。 使用注解方式：比较Filter类名的顺序，例如：FilterA和FilterB，则执行FilterA。 Filter设计模式 责任链设计模式：在程序运行阶段，动态的组合程序调用顺序。 Filter优点 在程序编译阶段下不会确定调用顺序，因为Filter的调用顺序是配置到web.xml文件中的，只要修改web.xml配置文件中的filter-mapping的顺序就可以吊证Filter的执行顺序。 显然Filter的执行顺序是在程序运行阶段动态组合的，那么这种设计模式被称为责任链设计模式。 登录过滤器 配置文件 loginFilter com.xxx.javaweb.filter.LoginCheckFilter loginFilter /* 过滤器 public class LoginCheckFilter implements Filter { @Override public void doFilter(ServletRequest req, ServletResponse resp, FilterChina chain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest)req; HttpServletResponse response = (HttpServletResponse)resp; HttpSession session = request.getSession(false); String servletPath = request.getServletPath(); // 首页放行、欢迎页放行、去登录放行、退出放行、已登录放行 if(\"/index.jsp\".equals(servletPath) || \"/welcome\".equals(servletPath) || \"/user/login\".equals(servletPath) || \"/user/exit\".equals(servletPath) || (session != null && session.getAttribute(\"username\") != null)) { // 继续往下走 chain.doFilter(request, response); } else { response.sendRedirect(request.getContextPath() + \"/index.jsp\"); } } } 视频 start：https://www.bilibili.com/video/BV1Z3411C7NZ?p=57 end：https://www.bilibili.com/video/BV1Z3411C7NZ?p=58 "},"pages/jsp/listener.html":{"url":"pages/jsp/listener.html","title":"监听器","keywords":"","body":"监听器 23.12.17 23:41开始 23.12.18 0:50更新 23.12.18 21:25更新 什么是监听器 监听器是Servlet规范中的一员，就像Filter一样，Filter是Servlet规范中的一员。 在Servlet中，所有的监听器都是“Listener”结尾。 监听器作用 监听器实际上是Servlet规范留给我们javaweb程序员的特殊时机。 特殊的时刻如果想执行这段代码，你需要想到使用对应的监听器。 有哪些监听器 jakarta.servlet包下： ServletContextListener ServletContextAttributeListener ServletRequestListener ServletRequestAttributeListener jakarta.servlet.http包下： HttpSessionListener HttpSessionAttributeListener HttpSessionBindingListener HttpSessionIdListener：session的ID发生改变。 HttpSessionActivationListener 配置ContextListener 第一步：编写一个类，实现ServeltContextListener接口，并且实现里面的方法 public class MyServletContextListener implements ServeltContextListener { @Override public void contextInitialed(ServeltContextEvent sce) {// 服务器启动时间点 // 这个方法是在ServletContext对象被创建的时候调用 System.out.println(\"ervletContext对象被创建了\"); } @Override public void contextDestroyed(ServeltContextEvent sce) {// 服务器关闭时间点 // 这个方法是在ServletContext对象被销毁的时候调用 System.out.println(\"ervletContext对象被销毁了\"); } } 第二步：在web.xml文件中对ServeltContextListener进行配置，如下： com.xxx.javaweb.lister.MyServletContextListener 或者使用注解：@WebListener 配置HttpSessionAttributeListener @WebListener public class MyHttpSessionAttributeListener implements HttpSessionAttributeListener { // 向session域当中存储数据的时候，以下方法被WEB容器调用 @Override public void attributeAdded(HttpSessionBindingEvent se) { System.out.println(\"session data add\"); } // 将session域当中存储的数据删除的时候，以下方法被WEB容器调用。 @Override public void attributeRemoved(HttpSessionBindingEvent se) { System.out.println(\"session data remove\"); } // session域当中的某个数据被替换的时候，以下方法被WEB容器调用。 @Override public void attributeReplaced(HttpSessionBindingEvent se) { System.out.println(\"session data replace\"); } } 配置HttpSessionBindingListener public class User implements HttpSessionBindingListener { @Override public void valueBound(HttpSessionBindingEvent event) { System.out.println(\"绑定数据\"); } @Override public void valueUnBound(HttpSessionBindingEvent event) { System.out.println(\"解绑数据\"); } } // 将user存储到session域 session.setAttribute(\"user\", user); User实现了HttpSessionBindingListener，会触发监听方法。 统计在线用户个数 不考虑登录：可以使用HttpSessionAttributeListener实现。 考虑登录：需要使用HttpSessionBindingListener。 public class User implements HttpSessionBindingListener { @Override public void valueBound(HttpSessionBindingEvent event) { // 获取ServletContext对象 ServletContext application = event.getSession().getServletContext(); // 获取在线人数 Object onlinecount = application.getAttribute(\"onlinecount\"); if (onlinecount == null) { application.setAttribute(\"onlinecount\", 1); } else { int count = (Integer)onlinecount; count++; application.setAttribute(\"onlinecount\", count); } } @Override public void valueUnBound(HttpSessionBindingEvent event) { // 获取ServletContext对象 ServletContext application = event.getSession().getServletContext(); Integer onlinecount = (Integer)application.getAttribute(\"onlinecount\"); onlinecount--; application.setAttribute(\"onlinecount\", count); } } HttpSessionActivationListener 监听session对象的钝化和活化的。 钝化：session对象从内存存储到硬盘文件。 活化：从硬盘文件把session恢复到内存中。 视频 start:https://www.bilibili.com/video/BV1Z3411C7NZ?p=59 end:https://www.bilibili.com/video/BV1Z3411C7NZ?p=62 "},"pages/architecture/mvc.html":{"url":"pages/architecture/mvc.html","title":"MVC","keywords":"","body":"MVC 23.12.18 22:10 开始 MVC图解 视频 start：https://www.bilibili.com/video/BV1Z3411C7NZ?p=66 "},"pages/architecture/three_tier.html":{"url":"pages/architecture/three_tier.html","title":"三层架构","keywords":"","body":"三层架构 23.12.18 23:47开始 23.12.19 23:42更新 三层架构图解 代码结构 dao（包） AccountDao impl（包） AccountDaoImpl exceptions（包） AppException pojo（包） Account service（包） AccountService impl（包） AccountServiceImpl utils（包） DBUtils web（包） AccountServlet 视频 start：https://www.bilibili.com/video/BV1Z3411C7NZ?p=71 "},"pages/springboot/springandspringboot.html":{"url":"pages/springboot/springandspringboot.html","title":"Spring与SpringBoot","keywords":"","body":"Spring与SpringBoot 23.12.23 20:55开始 23.12.23 23:17更新 23.12.24 22:50更新 Spring的能力 https://spring.io/ Microservices（微服务）：一个项目功能模块很多，将每一个功能模块拆解成一个微小的服务，独立运行。 Reactive（响应式编程）：构建异步数据流，占用少量的线程，少量cpu及内存资源，构建高吞吐量的应用。 Cloud（分布式云开发） Web apps（Web应用）：SpringMVC Serverless（无服务开发）：函数式服务，无需购买服务器。 Event Driver（事件驱动）：实时数据流。 Batch：批处理。 Spring生态圈 https://spring.io/projects/spring-boot/ Spring Framework（Web开发） Spring Data（数据访问） Spring Security（安全控制） Spring Cloud(分布式) Spring Session(分布式Session的存储问题) Spring AMQP（消息队列） Spring Mobile（移动开发） Spring Batch（批处理） Spring Android Spring Shell ...... Spring5升级 响应式编程：https://spring.io/reactive/ 内部源码设计：基于Java8的一些新特性，如：接口默认实现，重新设计源码架构。 接口默认实现：不在需要使用适配器模式。 什么是SpringBoot SpringBoot是整合Spring技术栈的一站式框架。 SpringBoot是简化Spring技术栈的快速开发脚手架。 SpringBoot要求 https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started.html#getting-started-system-requirements Java8及以上。 Maven3.3及以上。 为什么用SpringBoot Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can \"just run\". 能快速创建出生产级别的Spring应用 SpringBoot优点 Create stand-alone Spring applications 创建独立Spring应用 Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files) 内嵌web服务器 Provide opinionated 'starter' dependencies to simplify your build configuration 自动starter依赖，简化构建配置 Automatically configure Spring and 3rd party libraries whenever possible 自动配置Spring以及第三方功能 Provide production-ready features such as metrics, health checks, and externalized configuration 提供生产级别的监控、健康检查及外部化配置 Absolutely no code generation and no requirement for XML configuration 无代码生成、无需编写XML SpringBoot缺点 人称版本帝，迭代快，需要时刻关注变化 封装太深，内部原理复杂，不容易精通 微服务 https://martinfowler.com/microservices/ 微服务是一种架构风格 一个应用拆分为一组小型服务 每个服务运行在自己的进程内，也就是可独立部署和升级 服务之间使用轻量级HTTP交互 服务围绕业务功能拆分 可以由全自动部署机制独立部署 去中心化，服务自治。服务可以使用不同的语言、不同的存储技术 分布式 分布式的困难 远程调用：Http方式 服务发现：找到服务可用的机器 负载均衡 服务容错：A服务调用B服务失败，如何处理。 配置管理：建立配置中心，相同服务去配置中心更新。 服务监控：对服务健康状况的监测。 链路追踪：服务A调B服务，B服务调用C服务，C服务出现问题，如何排查。 日志管理 任务调度：服务A有个定时任务，所有机器怎么同步，并发还是串行。 ...... 分布式的解决 SpringBoot + SpringCloud 云原生 原生应用如何上云。 Cloud Native 上云的困难 服务自愈：服务A的一台机器挂了，自动拉起一台新机器，部署服务A。 弹性伸缩：流量高峰自动增加服务部署，流量低峰家少服务器部署。 服务隔离：一台机器同时部署了服务A、B、C，A服务挂了，不影响其他服务。 自动化部署 灰度发布：更新服务只更新其中一台机器，其他机器的服务依然保持旧版本，经过时间验证，更新的服务没有问题，在全部更新。 流量治理：机器A性能不好，让A负载低一些。 ...... 上云的解决 初识云原生 深入Docker-容器化技术 掌握星际容器编排Kubernetes DevOps-实战企业CI/CD，构建企业云平台 拥抱新一代架构Service Mesh与Serverless 云上架构与场景方案实战 如何学习SpringBoot 进入官网：https://spring.io/projects/spring-boot/ 点击LEARN，CURRENT当前版本，xxx-SNAPSHOT快照版本，RELEASE已发布。 点击Reference Doc. Documentation Overview（文档） https://docs.spring.io/spring-boot/docs/current/reference/html/documentation.html#documentation 可以下载PDF Getting Started（入门） Using Spring Boot（使用Spring Boot） Core Features（核心特性） 查看版本更新日志：https://github.com/spring-projects/spring-boot/wiki#release-notes 学习资料 文档地址：https://www.yuque.com/atguigu/springboot 源码地址：https://gitee.com/leifengyang/springboot2 视频地址 start：https://www.bilibili.com/video/BV19K4y1L7MT?p=1 end：https://www.bilibili.com/video/BV19K4y1L7MT?p=4 "},"pages/springboot/gettingstarted.html":{"url":"pages/springboot/gettingstarted.html","title":"SpringBoot入门","keywords":"","body":"SpringBoot入门 23.12.25 2:30更新 官方地址 https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started.html#getting-started.introducing-spring-boot SpringBoot2系统要求 Java 8 & 兼容java14 . Maven 3.3+ idea 2019.1.2 命令 监测java版本 java -version java version \"1.8.0_361\" Java(TM) SE Runtime Environment (build 1.8.0_361-b09) Java HotSpot(TM) 64-Bit Server VM (build 25.361-b09, mixed mode) 查看maven版本 mvn -v Apache Maven 3.9.3 (21122926829f1ead511c958d89bd2f672198ae9f) Maven home: /Users/chenchangqing/Documents/apps/apache-maven-3.9.3 Java version: 1.8.0_361, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk1.8.0_361.jdk/Contents/Home/jre Default locale: zh_CN, platform encoding: UTF-8 OS name: \"mac os x\", version: \"11.7.10\", arch: \"x86_64\", family: \"mac\" 修改setting.xml 打开Maven home/conf/settings.xml 替换配置文件： nexus-aliyun central Nexus aliyun http://maven.aliyun.com/nexus/content/groups/public jdk-1.8 true 1.8 1.8 1.8 1.8 HelloWorld 打开官方文档 https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started.html#getting-started.first-application 创建maven的springboot项目 http://1221.site/pages/idea/createp.html#%E4%BD%BF%E7%94%A8maven%E5%88%9B%E5%BB%BAspringboot%E9%A1%B9%E7%9B%AE 1）点击New Project 2）项目设置 3）Create 4）pom.xml 默认会有一个Main入口 @SpringBootApplication public class Springboot01HelloworldApplication { public static void main(String[] args) { SpringApplication.run(Springboot01HelloworldApplication.class, args); } } 创建类HelloController 右键包名，New->Java Class，输入HelloController： @RestController public class HelloController { @RequestMapping(\"/hello\") public String handle01(){ return \"Hello, Spring Boot!\"; } } 运行项目 在Springboot01HelloworldApplication运行main方法： 发现错误 springboot3.2.1不支持jdk1.8，需要更新到java17 配置文件 application.properties https://docs.spring.io/spring-boot/docs/current/reference/html/application-properties.html#appendix.application-properties 例如：Server Properties -> server.port 简化部署 springboot支持通过.jar文件直接启动服务。官方文档： https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started.html#getting-started.first-application.executable-jar 配置pom.xml org.springframework.boot spring-boot-maven-plugin 打包 找到jar target->xxx-1.0-SNAPSHOT.jar 运行项目 cd到jar目录，执行： java -jar xxx-1.0-SNAPSHOT.jar xxx-1.0-SNAPSHOT.jar/BOOT-INF/lib：这里目录下有tomcat-embed-core-9.0.38.jar、tomcat-embed-websocket-9.0.38，带有tomcat环境。 视频地址 start：https://www.bilibili.com/video/BV19K4y1L7MT?p=5 end：https://www.bilibili.com/video/BV19K4y1L7MT?p=5 "},"pages/springboot/autoconfig.html":{"url":"pages/springboot/autoconfig.html","title":"自动配置","keywords":"","body":"自动配置 24.1.5 00:21 开始 24.1.5 01:13 更新 24.1.6 19:36 更新 自动版本仲裁机制 在pom.xml文件可以找到以上parent配置： org.springframework.boot spring-boot-starter-parent 2.3.4.RELEASE 点击spring-boot-starter-parent，可以看到： org.springframework.boot spring-boot-dependencies 2.3.4.RELEASE 点击spring-boot-dependencies，可以看到： 5.18.3 2.0.2 2.31.2 1.9.21 3.24.2 ... 1.1.0 2.9.1 3.0.3 在这里声明了几乎所有常用的jar依赖，称为自动版本仲裁机制。 修改mysql默认版本号 点击：https://mvnrepository.com/ 搜索mysql，点击mysql-connector-j，找到需要的版本，比如：5.1.43 修改pom.xml： 5.1.43 org.springframework.boot mysql-connector-java starter场景启动器 官方地址：https://docs.spring.io/spring-boot/docs/current/reference/html/using.html#using.build-systems.starters 官方starter：spring-boot-starter-*。 第三方starter：thirdpartyproject-spring-boot-starter。 只要引入starter，这个场景的所有常规需要的依赖我们都自动引入。 核心依赖： org.springframework.boot spring-boot-starter 2.3.4.RELEASE compile 查看依赖树：右键点击artifactId，点击Diagrams->Show Dependences。 自动配置 在spring-boot-starter-web-3.2.1.pom，自动配置了Tomcatstarter，SpringMVCstarter。 Tomcat org.springframework.boot spring-boot-starter-tomcat 3.2.1 compile SpringMVC org.springframework spring-web 6.1.2 compile org.springframework spring-webmvc 6.1.2 compile 默认包扫描 https://docs.spring.io/spring-boot/docs/current/reference/html/using.html#using.structuring-your-code.using-the-default-package 默认扫描：主程序所在包及其下面的所有子包里面的组件都会被默认扫描进来。 修改包扫描路径：在Application类上增加注解： @SpringBootApplication(scanBasePackages=\"com.xxx\") 或 @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(\"com.xxx\") 各种配置 tomcat默认端口：server.port 文件上传：spring.servlet.multipart.max-file-size 配置都是绑定了java类，比入文件上传：MultipartProperties，并且这个java类在容器中存在对应对象。 所有配置都可以通过application.properties修改。 配置是按场景starter加载的。 SpringBoot所有自动配置都在spring-boot-autoconfigure`。 点击spring-boot-starter-web，点击spring-boot-starter，找到spring-boot-autoconfigure 视频地址 start：https://www.bilibili.com/video/BV19K4y1L7MT/?p=6 end：https://www.bilibili.com/video/BV19K4y1L7MT/?p=7 "},"pages/springboot/annotation.html":{"url":"pages/springboot/annotation.html","title":"注解","keywords":"","body":"注解 24.1.6 19:38 开始 24.1.6 21:13 更新 @Configuration 告诉SpringBoot这是一个配置类，相当于配置文件。 @Configuration public class MyConfig {} proxyBeanMethods属性 Full(proxyBeanMethods=true)：保证每个@Bean方法被调用多少次返回的组件都是单实例的。 Lite(proxyBeanMethods=false)：每个@Bean方法被调用多少次返回的组件都是新创建的。 组件依赖必须使用Full模式，默认就是这个模式。 @Configuration(proxyBeanMethods = false) public class MyConfig {} 不需要组件依赖时使用Lite模式，也就是说不需要创建新组件时使用Lite，使用Full模式来保证取得的组件为ioc中的同一组件，而这两个模式在getBean时都是从ioc容器中拿的同一个组件。 @Bean 给容器中添加组件，以方法名作为组件的id，返回类型就是组件类型； 返回的值，就是组件在容器中的实例，默认单实例。 @Configuration public class MyConfig { @Bean public User user01(){ User zhangsan = new User(\"zhangsan\", 18); return zhangsan; } } 注册依赖 @Configuration public class MyConfig { @Bean public User user01(){ User zhangsan = new User(\"zhangsan\", 18); zhangsan.setPet(tomcatPet()); return zhangsan; } @Bean(\"tom\") public Pet tomcatPet(){ return new Pet(\"tomcat\"); } } SpringMVC注解 @Component、@Controller、@Service、@Repository 根据它们的源码可以看到，Controller、Service、Repository其本质就是Component。 它存在的本质只是给开发者看的，对Spring而言它们就都是Component。 @Controller 控制层类，@Service 业务层类，@Repository 持久层类。 @Component 无法归类到前3种时就称为组件。 原文：https://blog.csdn.net/nutony/article/details/118670662 @Import 给容器中自动创建出这两个类型的组件、默认组件的名字就是全类名。 @Import({User.class, DBHelper.class}) @Configuration(proxyBeanMethods = false) public class MyConfig {} @Import 高级用法： https://www.bilibili.com/video/BV1gW411W7wy?p=8 @Conditional 该注解及其扩展来的注解的关键是实现Condition接口重写其matches方法。 @Conditional，中派生了很多的子注解，它们可以添加在@Bean注解的方法上也可以放在配置类上，在方法上满足所需条件时则执行方法中内容并注册到IOC。 容器中如果不满足条件则不注册，在配置类中满足需求时则执行配置类中所有的@Bean方法并注册到 IOC。 容器中如果不满足条件则不注册，以@ConditionalOnBean(name=\"tom\")为例，当 IOC 容器中拥有id为tom的组件时才会满足条件，否则不满足条件。 @Configuration(proxyBeanMethods = false) public class MyConfig { @Bean @ConditionalOnBean(name = \"tom\") //@ConditionalOnMissingBean(name = \"tom\") public User user01(){ User zhangsan = new User(\"zhangsan\", 18); return zhangsan; } @Bean(\"tom\") public Pet tomcatPet(){ return new Pet(\"tomcat\"); } } @ImportResource 配置文件： ======================beans.xml========================= 加载配置文件： @ImportResource(\"classpath:beans.xml\") public class MyConfig {} 测试： boolean haha = run.containsBean(\"haha\"); boolean hehe = run.containsBean(\"hehe\"); System.out.println(\"haha：\"+haha);//true System.out.println(\"hehe：\"+hehe);//true 配置绑定 原始方式 public class getProperties { public static void main(String[] args) throws FileNotFoundException, IOException { Properties pps = new Properties(); pps.load(new FileInputStream(\"a.properties\")); Enumeration enum1 = pps.propertyNames();//得到配置文件的名字 while(enum1.hasMoreElements()) { String strKey = (String) enum1.nextElement(); String strValue = pps.getProperty(strKey); System.out.println(strKey + \"=\" + strValue); //封装到JavaBean。 } } } 第一种方式 @Component + @ConfigurationProperties：@Component@ConfigurationProperties(prefix = \"mycar\")声明在要绑定的类的上方。 /** * 只有在容器中的组件，才会拥有SpringBoot提供的强大功能 */ @Component @ConfigurationProperties(prefix = \"mycar\") public class Car { private String brand; private Integer price; public String getBrand() { return brand; } public void setBrand(String brand) { this.brand = brand; } public Integer getPrice() { return price; } public void setPrice(Integer price) { this.price = price; } @Override public String toString() { return \"Car{\" + \"brand='\" + brand + '\\'' + \", price=\" + price + '}'; } } 第二种方式 @EnableConfigurationProperties + @ConfigurationProperties： @ConfigurationProperties(prefix = \"mycar\")声明在要绑定的类的上方； 在配置类的上方声明@EnableConfigurationProperties(Car.class)，开启对应类的配置绑定功能，把Car这个组件自动注入到容器中。 @EnableConfigurationProperties(Car.class) // 1、开启Car配置绑定功能 // 2、把这个Car这个组件自动注册到容器中 // 说明一下为什么需要第二种方法： // 如果@ConfigurationProperties是在第三方包中， // 那么@component是不能注入到容器的， // 只有@EnableConfigurationProperties才可以注入到容器。 public class MyConfig {} 视频地址 start：https://www.bilibili.com/video/BV19K4y1L7MT/?p=8 end：https://www.bilibili.com/video/BV19K4y1L7MT/?p=12 "},"pages/springboot/autoconfigprinciple.html":{"url":"pages/springboot/autoconfigprinciple.html","title":"自动配置原理","keywords":"","body":"自动配置原理 24.1.6 21:59 开始 24.1.7 00:39 更新 @SpringBootApplication @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan( excludeFilters = {@Filter( type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class} ), @Filter( type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class} )} ) @SpringBootConfiguration：也就是@Configuration，代表当前是一个配置类。 @ComponentScan：指定扫描哪些。 @EnableAutoConfiguration：激活自动配置。 @AutoConfigurationPackage// 下面分析 @Import({AutoConfigurationImportSelector.class})// 下面分析 public @interface EnableAutoConfiguration @AutoConfigurationPackage // 给容器中导入一个组件 @Import({AutoConfigurationPackages.Registrar.class}) public @interface AutoConfigurationPackage 利用Registrar给容器中导入一系列组件。 指定的一个包下的所有组件导入进来，默认xxxApplication所在包下。 Registrar： static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports { Registrar() { } public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { AutoConfigurationPackages.register(registry, (String[])(new PackageImports(metadata)).getPackageNames().toArray(new String[0])); } public Set determineImports(AnnotationMetadata metadata) { return Collections.singleton(new PackageImports(metadata)); } } metadata：注解元信息，注解指的是@AutoConfigurationPackage。 @AutoConfigurationPackage是标注在xxxApplication类上。 计算包名：new PackageImports(metadata)).getPackageNames() 所以Registrar将xxxAplication所在包下的所有组件注册了。 @Import @Import({AutoConfigurationImportSelector.class}) public class AutoConfigurationImportSelector { ... @Override public String[] selectImports(AnnotationMetadata annotationMetadata) { if (!isEnabled(annotationMetadata)) { return NO_IMPORTS; } AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations()); } ... } 调用关系 利用getAutoConfigurationEntry(annotationMetadata)给容器中批量导入一些组件。 protected AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata) { if (!isEnabled(annotationMetadata)) { return EMPTY_ENTRY; } AnnotationAttributes attributes = getAttributes(annotationMetadata); // 寻找需要加载的候选配置类数组 List configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); Set exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = getConfigurationClassFilter().filter(configurations); fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationEntry(configurations, exclusions); } 调用getCandidateConfigurations，获取到所有需要导入到容器中的配置类。 List configurations = getCandidateConfigurations(annotationMetadata, attributes); Debug调试，查看有127个候选配置组件： getCandidateConfigurations getCandidateConfigurations调用了ImportCandidates.load： protected List getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { List configurations = ImportCandidates.load(AutoConfiguration.class, getBeanClassLoader()) .getCandidates(); Assert.notEmpty(configurations, \"No auto configuration classes found in \" + \"META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports. If you \" + \"are using a custom packaging, make sure that file is correct.\"); return configurations; } ImportCandidates.load从META-INF/spring/%s.imports位置来加载一个文件： public static ImportCandidates load(Class annotation, ClassLoader classLoader) { Assert.notNull(annotation, \"'annotation' must not be null\"); ClassLoader classLoaderToUse = decideClassloader(classLoader); String location = String.format(\"META-INF/spring/%s.imports\", annotation.getName()); Enumeration urls = findUrlsInClasspath(classLoaderToUse, location); List importCandidates = new ArrayList(); while(urls.hasMoreElements()) { URL url = (URL)urls.nextElement(); importCandidates.addAll(readCandidateConfigurations(url)); } return new ImportCandidates(importCandidates); } 查看spring-boot-autoconfigure-3.2.1jar的META-INF: spring.boot2.7起使用META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports文件来指定需要加载自动配置类 按需开启自动配置项 虽然我们127个场景的所有自动配置启动的时候默认全部加载。xxxxAutoConfiguration 按照条件装配规则（@Conditional），最终会按需配置。 AopAutoConfiguration // 我是一个配置类 @AutoConfiguration // 判断配置文件中是否存在“spring.aop”，且名字为“auto”，且值为true，则配置类生效； // matchIfMissing = true：如果没有配，也认为配置了，并且值为true，所以默认不配也是生效。 @ConditionalOnProperty(prefix = \"spring.aop\", name = \"auto\", havingValue = \"true\", matchIfMissing = true) public class AopAutoConfiguration { // 我是一个配置类 @Configuration(proxyBeanMethods = false) // 如果整个应用没有`Advice`类（org.aspectj.weaver.Advice），则不生效； // 默认没有`Advice`类，所以默认不生效。 @ConditionalOnClass(Advice.class) static class AspectJAutoProxyingConfiguration { ... } @Configuration(proxyBeanMethods = false) // 如果没有“org.aspectj.weaver.Advice”类，则生效； // 默认是没有的，所有默认生效。 @ConditionalOnMissingClass(\"org.aspectj.weaver.Advice\") // 默认开启aop功能 @ConditionalOnProperty(prefix = \"spring.aop\", name = \"proxy-target-class\", havingValue = \"true\", matchIfMissing = true) static class ClassProxyingConfiguration { ... } } CacheAutoConfiguration @AutoConfiguration(after = { CouchbaseDataAutoConfiguration.class, HazelcastAutoConfiguration.class, HibernateJpaAutoConfiguration.class, RedisAutoConfiguration.class }) // 判断是否存在`CacheManager.class`类，不存在，则不生效； // 默认是有的，条件通过； @ConditionalOnClass(CacheManager.class) // 默认是没有的，条件不通过，配置不生效。 @ConditionalOnBean(CacheAspectSupport.class) @ConditionalOnMissingBean(value = CacheManager.class, name = \"cacheResolver\") @EnableConfigurationProperties(CacheProperties.class) @Import({ CacheConfigurationImportSelector.class, CacheManagerEntityManagerFactoryDependsOnPostProcessor.class }) public class CacheAutoConfiguration { ... } DispatcherServletAutoConfiguration @AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE) // 判断是否在`ServletWebServerFactoryAutoConfiguration`配置完成之后 @AutoConfiguration(after = ServletWebServerFactoryAutoConfiguration.class) // 判断是否是原生servlet的应用，还有一种是响应式编程`WebFlux` @ConditionalOnWebApplication(type = Type.SERVLET) // 判断是否导入了`DispatcherServlet`类，因为注册了springmvc的starter，所以导入了。 @ConditionalOnClass(DispatcherServlet.class) public class DispatcherServletAutoConfiguration { ... @Configuration(proxyBeanMethods = false) @Conditional(DefaultDispatcherServletCondition.class) // 判断是否导入`ServletRegistration` @ConditionalOnClass(ServletRegistration.class) // 开启配置绑定功能，配置文件中以“spring.mvc”开头的属性， // 会被绑定至`WebMvcProperties`实例。 @EnableConfigurationProperties(WebMvcProperties.class) protected static class DispatcherServletConfiguration { // 配置`DispatcherServlet`组件，指定名字。 @Bean(name = DEFAULT_DISPATCHER_SERVLET_BEAN_NAME) public DispatcherServlet dispatcherServlet(WebMvcProperties webMvcProperties) { ... return dispatcherServlet; } @SuppressWarnings({ \"deprecation\", \"removal\" }) private void configureThrowExceptionIfNoHandlerFound(WebMvcProperties webMvcProperties, DispatcherServlet dispatcherServlet) { dispatcherServlet.setThrowExceptionIfNoHandlerFound(webMvcProperties.isThrowExceptionIfNoHandlerFound()); } // 配置文件上传解析器组件 @Bean @ConditionalOnBean(MultipartResolver.class) // 容器中没有名字为“multipartResolver”的组件 @ConditionalOnMissingBean(name = DispatcherServlet.MULTIPART_RESOLVER_BEAN_NAME) public MultipartResolver multipartResolver(MultipartResolver resolver) { // 给@Bean标注的方法传入了对象参数，这个参数的值就会从容器中找。 // SpringMVC multipartResolver。防止有些用户配置的文件上传解析器不符合规范 // Detect if the user has created a MultipartResolver but named it incorrectly return resolver; } } ... } HttpEncodingAutoConfiguration @AutoConfiguration // 配置绑定至`ServerProperties`类 @EnableConfigurationProperties(ServerProperties.class) // 是否是Servlet应用 @ConditionalOnWebApplication(type = ConditionalOnWebApplication.Type.SERVLET) // 是否存在`CharacterEncodingFilter`类 @ConditionalOnClass(CharacterEncodingFilter.class) // 默认开启encoding @ConditionalOnProperty(prefix = \"server.servlet.encoding\", value = \"enabled\", matchIfMissing = true) public class HttpEncodingAutoConfiguration { private final Encoding properties; public HttpEncodingAutoConfiguration(ServerProperties properties) { this.properties = properties.getServlet().getEncoding(); } @Bean // 容器中如果没有配CharacterEncodingFilter，自动配置 // SpringBoot默认会在底层配好所有的组件。但是如果用户自己配置了以用户的优先 @ConditionalOnMissingBean public CharacterEncodingFilter characterEncodingFilter() { CharacterEncodingFilter filter = new OrderedCharacterEncodingFilter(); filter.setEncoding(this.properties.getCharset().name()); filter.setForceRequestEncoding(this.properties.shouldForce(Encoding.Type.REQUEST)); filter.setForceResponseEncoding(this.properties.shouldForce(Encoding.Type.RESPONSE)); return filter; } ... } 总结 SpringBoot先加载所有的自动配置类（xxxxxAutoConfiguration）。 每个自动配置类按照条件进行生效，默认都会绑定配置文件指定的值。 xxxxProperties里面拿，xxxProperties和配置文件进行了绑定。 生效的配置类就会给容器中装配很多组件。 只要容器中有这些组件，相当于这些功能就有了。 定制化配置 用户直接自己@Bean替换底层的组件。 用户去看这个组件是获取的配置文件什么值就去修改。 xxxxxAutoConfiguration ---> 组件 ---> xxxxProperties里面拿值 ----> application.properties 视频地址 start：https://www.bilibili.com/video/BV19K4y1L7MT?p=13 end：https://www.bilibili.com/video/BV19K4y1L7MT?p=15 "},"pages/springboot/commonlyusedtechniques.html":{"url":"pages/springboot/commonlyusedtechniques.html","title":"常用技巧","keywords":"","body":"常用技巧 24.1.7 1:33 开始 24.4.26 12:01 开始 引入场景依赖 https://docs.spring.io/spring-boot/docs/current/reference/html/using-spring-boot.html#using-boot-starter 查看自动配置了哪些 视频地址 start：https://www.bilibili.com/video/BV19K4y1L7MT/?p=16 "},"pages/http/http.html":{"url":"pages/http/http.html","title":"HTTP协议","keywords":"","body":"HTTP协议 什么是协议？ 1）协议实际上是某些人，或者某些组织提前制定好的一套规则，大家按照这个规范来，这样可以做到沟通无障碍。2）协议就是一套规范，就是一套标准。由其他人或其他组织来负责制定的。3）我说的话你能听懂，你说的话，我也能听懂，这说明我们之间是有一套规范的，一套协议的，这套协议就是：中国普通话协议。我们都遵守这套协议，我们之间就可以沟通无障碍。 什么是HTTP协议？ 1）HTTP协议：是W3C制定的一种超文本传输协议。（通信协议：发送消息的模版提前被制定好。）2）浏览器 向 WEB服务器发送数据，叫做请求（request）；WEB服务器 向 浏览器发送数据，叫做响应（response）。 W3C 1）万维网联盟组织2）负责制定标准的：HTTP HTML4.0 HTML5 XML DOM等规范都是W3C制定的。3）万维网之父：蒂姆·伯纳斯·李 什么是超文本？ 1）超文本说的就是：不是普通文本，比如流媒体：声音、视频、图片等。2）HTTP协议支持：不但可以传送普通字符串，同样支持传递声音、视频、图片等流媒体信息。 这种协议游走在B和S之间。B向S发数据药遵循HTTP协议，S向B发数据同样需要遵循HTTP协议。这样B和S才能解耦合。 什么是解耦合？B不依赖S，S也不依赖B。B/S表示：B/S结构的系统（浏览器访问WEB服务器的系统）。 HTTP请求协议 请求行 第一部分：请求方式 get post delete put head options trace 第二部分：URI 什么是URI？统一资源标识符。代表网络中某个资源的名字。但是通过URI是无法定位资源的。 什么是uRL？统一资源定位符。代表网络中某个资源，同时，通过URL是可以定位到该资源的。 URI和URL什么关系，有什么区别？ URL包括URI http://localhost:8080/servlet05/index.html 这是URL。 /servlet05/index.html 这是URI。 第三部分：HTTP协议版本号 请求头 请求的主机 主机的端口 浏览器信息 平台信息 cookie等信息 ... 空白行 空白行是用来区分请求头和请求体 请求体 向服务器发送的具体数据 HTTP响应协议 状态行 第一部分：协议版本号（HTTP/1.1） 第二部分：状态码（HTTP协议中规定的响应状态号。不同的响应结果对应不同的号码。） 200：表示请求响应成功，正常结束。 404：表示访问的资源不存在。 405:表示发送请求方式与后端请求的处理方式不一致 以4开始的，一般是浏览器端错误导致的。 以5开头的，一般是服务器端错误导致的。 第三部分 ok 表示正常成功结束 not found 表示资源找不到 响应头 响应的内容类型 响应的内容长度 响应的时间 ... 空白行 用来分割“响应头”和“响应体” 响应体 响应体就是响应的正文，这些内容是一个长的字符串，这个字符串被浏览器渲染，解释并执行，最终展示出效果。 怎么查看协议内容？ 使用chrome浏览器：F12，然后找到network，通过这个面板可以查看协议的具体内容。 怎么向服务器发送GET、POST请求？ 1）到目前为止，只有一种情况可以发送POST请求：使用form表单，并且form标签中的method属性值为：method=“post”。2）其他所有情况一律都是get请求： - 在浏览器地址栏直接输入URL，敲回车，属于get请求。 - 在浏览器上直接点击超链接，属于get请求。 - 使用form表单提交数据时，form标签中没有写method属性，默认就是get。 - 或者使用form表单的时候，form标签中method属性值为：method=“get”。 GET请求和POST请求有什么区别？ 1）get请求发送数据的时候，数据会挂在URI的后面，并且在URI后面添加一个“?”，“?”后面是数据。这样会导致发送的数据会显示在浏览器的地址栏上。（get请求在“请求行”上发送数据） 2）post请求发送数据的时候，在请求体当中发送。不会回显到浏览器的地址栏上。也就是说post发送的数据，在浏览器地址栏上看不到。（post在“请求体”当中发数据） 3）不管你是get请求还是post请求，发送的请求数据格式是完全相同的，只不过位置不同，格式都是统一的： name=value&name1=value1&name2=value2 4）get请求只能发送普通的字符串，并且发送的字符串长度有限制，不同的浏览器限制不同，这个没有明确的规范。 5）get请求无法发送大数据量。 6）post请求可以发送任何类型的数据，包括普通字符串，流媒体等信息：视频、声音、图片。 7）post请求可以发送大量数据，理论上没有长度限制。 8）get请求在W3C中是这样说的：get请求比较适合从服务器获取数据。 9）post请求在W3C中是这样说的：post请求比较适合向服务器传送数据。 10）get请求是安全的。get请求是绝对安全的。为什么？因为get请求只是为了从服务器上获取数据，不会对服务器造成威胁。 11）post请求是危险的，为什么？因为post请求是向服务器提交数据，如果这些数据通过后门的方式进入到服务器当中，服务器是很危险的。另外post是为了提交数据，所有一般情况下拦截请求的时候，大部分回选择拦截（监听）post请求。 12）get请求支持缓存，post请求支持缓存。 - 任何一个get请求最终的“响应结果”都会被浏览器缓存起来，在浏览器缓存当中，一个get请求的路径，对应一个资源。 - 实际上，你只要发送get请求，浏览器做的第一件事都是先从本地浏览器缓存中找，找不到的时候才会去服务器上获取，这种缓存机制目的是为了提高用户体验。 - get请求每次都去服务器上找资源： - 只需要每一次get请求的路径不同即可。 - 在路径的后面增加一个每时每刻在变化的“时间戳”。 GET请求和POST请求如何选择，什么时候使用GET请求，什么时候使用POST请求？ 1）怎么选择GET请求和POST请求呢？衡量标准是什么呢？你这个请求是想获取服务器端的数据，还是想向服务器发送数据。如果你是想从服务器上获取资源，建议使用GET请求，如果你这个请求是为了向服务器提交数据，建议使用POST请求。 2）大部分的form表单提交，都是POST方式，因为form表单中要填写回显敏感信息到浏览器地址栏上。（例如：密码信息） 3）做文件上传，一定是POST请求。要传的数据不是普通文本。 4）其他情况都可以使用GET请求。 视频 start：https://www.bilibili.com/video/BV1Z3411C7NZ?p=18 end：https://www.bilibili.com/video/BV1Z3411C7NZ?p=19 19:54 "},"pages/swift/Swift_annotation.html":{"url":"pages/swift/Swift_annotation.html","title":"Swift注释","keywords":"","body":"Swift注释 快捷键： 光标放在方法那一行，option + command + / 可以自动生成相应的注释 单行注释： // 你要注释的内容 多行注释： /* 你要注释的内容 */ 多行嵌套注释： /*这是第一个多行注释的开头 /*这是第二个被嵌套的多行注释*/ 这是第一个多行注释的结尾*/ 文档注释： /// - Parameters: 参数 /// - item1: This is item1 /// - item2: This is item2 /// - Returns: the result string. 返回值 /// - Throws: `MyError.BothNilError` if both item1 and item2 are nil. 抛出异常 /// - Author: liuyubobobo 作者 - 无序列表 1. 有序列表 ``` 代码 # 标题 * _ 用于斜体 ** 粗体 其它： // MARK: - Methods // TODO: changeColor() // FIXME: Support Swift 2.2 附： Swift5.1—注释 Swift 注释规范和文档注释 "},"pages/swift/Great_Wall_Motors_Swift_Programming_Protocol.html":{"url":"pages/swift/Great_Wall_Motors_Swift_Programming_Protocol.html","title":"长城汽车Swift编程规约","keywords":"","body":"1.长城汽车Swift编程规约 前言 好的代码有一些特性：简明，自我解释，优秀的组织，良好的文档，良好的命名，优秀的设计以及可以被久经考验。参与长城系列APP开发的团队成员应严格遵照规约编写代码。规约会越来越完善，初期先按照以下规范。 第一次编辑时间:2020-01-28 核心原则 最重要的目标：每个元素都能够准确清晰的表达出它的含义。做出 API 设计、声明后要检查在上下文中是否足够清晰明白。 清晰比简洁重要。虽然 swift 代码可以被写得很简短，但是让代码尽量少不是 swift 的目标。简洁的代码来源于安全、强大的类型系统和其他一些语言特性减少了不必要的模板代码。而不是主观上写出最少的代码。 为每一个声明写注释文档。编写文档过程中获得的理解可以对设计产生深远的影响，所以不要回避拖延。 如果你不能很好的描述 API 的功能，很可能这个 API 的设计就是有问题的。 命名规约 不要使用约定命名样式代替访问控制 如果要控制访问权限应该使用访问控制（internal、fileprivate、private），不用使用自定义的命名方式来区分，比如在方法前前下划线表示私有。 只有在极端的情况下才会采用这种自定义命名表示。比如有一个方法只是为了某个模板调用才公开的，这种情况下本意是私有的，但是又必须声明成 public，可以使用自定义的命名惯例。 代码中的命名严禁使用拼音与英文混合的方式，更不允许直接使用中文的方式。 ✅ var productDiskDataArray: Array? var productDiskDataString: String! ❌ var chanpinDataArray: Array? var chanpinDataString: String! 类, 结构体, 枚举, 协议命名使用 UpperCamelCase 风格，必须遵从驼峰形式。特别注意类名开头大写。 ✅ class GWElecFenceFlowLayout: UICollectionViewFlowLayout ❌ class gwElecFenceFlowLayout: UICollectionViewFlowLayout 资源文件按照匈牙利命名法。 模块名+图片特征描述+状态 描述清晰有章法即可。 ✅ nav_add_normal@2X.png ❌ navAddNormal@2X.png NAV_ADD_NORMAL@2x.png 方法名、参数名、成员变量、局部变量都统一使用 lowerCamelCase 风格，必须遵从驼峰形式。 ✅ func viewDidLoad() func tableView(_ tableView: UITableView, numberOfRowsInSection section: Int) -> Int let automaticDimension: CGFloat let gradientShapeLayer = CAShapeLayer() ❌ func ViewDidLoad() func tableView(_ tableV: UITableView, numberOfRowsInSection section: Int) -> Int var FILLCOLOR: CGColor? let gradientshapeLayer = CAShapeLayer() 全局常量就正常变量一样使用匈牙利命名方式，不要在前面加上 g、k 或其他特别的格式。 ✅ let secondsPerMinute = 60 ❌ let SecondsPerMinute = 60 let kSecondsPerMinute = 60 let gSecondsPerMinute = 60 let SECONDS_PER_MINUTE = 60 杜绝完全不规范的缩写，避免望文不知义。 ❌ var abcAry: Array? 保证英文拼写正确 ❌ class AysncDat: NSObject 单例对象一般命名为 shared 或者 default。 ✅ /// 路由单例 public static let shared = GWNavigator() /// 屏蔽实现方式(可根据具体使用情况灵活调整) private override init() {} 格式规约 括号 非空的 block 花括号默认使用 K&R style。比如： while (x == y) { something() somethingelse() } 除了一些 Swift 特别要求的情况： 左花括号（ { ）前的代码不会换行，除非超过前面提到的代码长度超过限制。 左花括号后是一个换行，除非： 后面要声明闭包的参数，改为在 in 关键字后面换行。 符合每行只声明一件事里情况，忽略换行，把内容写在一行里。 如果是空的 block ，直接声明为 { }。 如果右花括号（ } ）结束了一个声明，后面接上一个换行。比如如果右花括号后面跟的是 else ，那么后面就不会跟换行，而会写成这样 } else { 衔接。 每行只声明一件事 每行最多只声明一件事，每行结尾用换行分隔。除非结尾跟的是一个总共只有一行声明的闭包。 ✅ guard let value = value else { return 0 } defer { file.close() } switch someEnum { case .first: return 5 case .second: return 10 case .third: return 20 } let squares = numbers.map { $0 * $0 } var someProperty: Int { get { return otherObject.property } set { otherObject.property = newValue } } var someProperty: Int { return otherObject.somethingElse() } required init?(coder aDecoder: NSCoder) { fatalError(\"no coder\") } 如果闭包是提前返回一个值，写在一行里可读性就会好一些。如果是一个正常的操作，可以视情况是否写在一行里。因为未来也有可能里面再增加代码的操作。 代码换行 代码中的空格 除了语言或者其他样式的要求，文字和注释之外，一个Unicode空格也只出现在以下地方: 条件关键字后面和跟着的括号 ✅ if (x == 0 && y == 0) || z == 0 { // ... } ❌ if(x == 0 && y == 0) || z == 0 { // ... } 如果闭包中的代码在同一行，左花括号的前面、后面，右花括号的前面有空格 ✅ let nonNegativeCubes = numbers.map { $0 * $0 * $0 }.filter { $0 >= 0 } ❌ let nonNegativeCubes = numbers.map { $0 * $0 * $0 } .filter { $0 >= 0 } ❌ let nonNegativeCubes = numbers.map{$0 * $0 * $0}.filter{$0 >= 0} 在任何二元或三元运算符的两边 还有以下的情况： 使用于赋值，初始化变量、属性，默认参数的等号两边。 ✅ var x = 5 func sum(_ numbers: [Int], initialValue: Int = 0) { // ... } ❌ var x=5 func sum(_ numbers: [Int], initialValue: Int=0) { // ... } 表示在协议中表示合成类型的 & 两边。 ✅ func sayHappyBirthday(to person: NameProviding & AgeProviding) { // ... } ❌ func sayHappyBirthday(to person: NameProviding&AgeProviding) { // ... } 自定义运算符的两边。 ✅ static func == (lhs: MyType, rhs: MyType) -> Bool { // ... } ❌ static func ==(lhs: MyType, rhs: MyType) -> Bool { // ... } 表示返回值的 -> 两边。 ✅ func sum(_ numbers: [Int]) -> Int { // ... } ❌ func sum(_ numbers: [Int])->Int { // ... } 例外：表示引用值、成员的点两边没有空格。 ✅ let width = view.bounds.width ❌ let width = view . bounds . width 例外：表示区域范围的 .. ✅ for number in 1...5 { // ... } ❌ let substring = string[index.. 参数列表、数组、tuple、字典里的逗号后面有一个空格 ✅ let numbers = [1, 2, 3] ❌ let numbers = [1,2,3] let numbers = [1 ,2 ,3] let numbers = [1 , 2 , 3] 冒号的后面有一个空格 ✅ // 类型声明 struct HashTable: Collection { // ... } struct AnyEquatable: Equatable { // ... } // 参数标签 let tuple: (x: Int, y: Int) func sum(_ numbers: [Int]) { // ... } // 变量声明 let number: Int = 5 // 字典声明 var nameAgeMap: [String: Int] = [] // 字典字面量 let nameAgeMap = [\"Ed\": 40, \"Timmy\": 9] 代码后的注释符号 // 与代码有两个空格距离 ✅ let initialFactor = 2 // Warm up the modulator. ❌ let initialFactor = 2 // Warm up the modulator. 表示字典、数组字面量的中括号外面有一个空格 ✅ let numbers = [1, 2, 3] ❌ let numbers = [ 1, 2, 3 ] 禁止变量、属性水平对齐 水平对齐是明确禁止的，除非是在写明显的表格数据时，省略对齐会损害可读性。引入水平对齐后，如果添加一个新的成员可能会需要其他成员再对齐一次，这给维护增加了负担。 ✅ struct DataPoint { var value: Int var primaryColor: UIColor } ❌ struct DataPoint { var value: Int var primaryColor: UIColor } 空行逻辑 在组织代码逻辑关系时，可以用空行隔开进行分组。 函数结尾不空行 函数内作用不同代码块空一行 规范里其他地方要求有空行的地方。 括号 最顶级的 if、guard、while、switch 的条件不使用括号。 ✅ if x == 0 { print(\"x is zero\") } if (x == 0 || y == 1) && z == 2 { print(\"...\") } ❌ if (x == 0) { print(\"x is zero\") } if ((x == 0 || y == 1) && z == 2) { print(\"...\") } 在有复杂的条件表达式，只有作者和 review 的人同时认为省略括号不会影响代码的可读性才会省略。不能假设每个读者都完全了解对 swift 的运算符优先级，所以这种情况下的括号提示用户的计算优先级是合理的。 集合处理 不要在 forin 循环里进行元素的 remove/add 操作。 ❌ var someInts:[Int] = [10, 20, 30] for index in someInts { someInts.insert(44 + index, at: index) } 并发处理 获取单例对象需要保证线程安全，其中的方法也要保证线程安全。 创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。 推荐: dispatch_queue_t gwhUpdateQueue = dispatch_queue_create(\"\", DISPATCH_QUEUE_SERIAL); let queue = DispatchQueue(label: \"com.gwh.app.update\", attributes: .concurrent) 高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁;能锁区块，就不要锁整个方法体;能用对象锁，就不要用类锁。 对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁。 并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用 version 作为更新依据。 控制语句 提前返回使用 guard ✅ func discombobulate(_ values: [Int]) throws -> Int { guard let first = values.first else { throw DiscombobulationError.arrayWasEmpty } guard first >= 0 else { throw DiscombobulationError.negativeEnergy } var result = 0 for value in values { result += invertedCombobulatoryFactory(of: value) } return result } ❌ func discombobulate(_ values: [Int]) throws -> Int { if let first = values.first { if first >= 0 { var result = 0 for value in values { result += invertedCombobulatoryFactor(of: value) } return result } else { throw DiscombobulationError.negativeEnergy } } else { throw DiscombobulationError.arrayWasEmpty } } for-where 循环 如果整个 for 循环在函数体顶部只有一个 if 判断，使用 for where 替换： ✅ for item in collection where item.hasProperty { // ... } ❌ for item in collection { if item.hasProperty { // ... } } Switch 中的 fallthrough Switch 中如果有几个 case 都对应相同的逻辑，case 使用逗号连接条件，而不是使用 fallthrough： ✅ switch value { case 1: print(\"one\") case 2...4: print(\"two to four\") case 5, 7: print(\"five or seven\") default: break } ❌ switch value { case 1: print(\"one\") case 2: fallthrough case 3: fallthrough case 4: print(\"two to four\") case 5: fallthrough case 7: print(\"five or seven\") default: break } 换句话说，不存在 case 中只有 fallthrough 的情况。如果 case 中有自己的代码逻辑再 fallthrough 是合理的。 注释规约 所使用的任何注释必须保持最新否则删除掉。代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。 类、类属性、类方法的注释必须使用 appledoc 规范，使用/*内容/格式。option+command+/。对于注释的要求:第一、能够准确反应设计思想和代码逻辑;第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。完全没有注释的大段代码对于阅读者形同天书，注释是给自己看的，即使隔很长时间，也能清晰理解当时的思路;注释也是给继任者看的，使其能够快速接替自己的工作。 好的命名、代码结构是自解释的，注释力求精简准确、表达到位。避免出现注释的一个极端:过多过滥的注释，代码的逻辑一旦修改，修改注释是相当大的负担。 补充规范 代码警告⚠️应该尽可能去除。除非明确为了提醒作用。 小代码块 保证逻辑的完整与连贯性 使用便于理解的API 无用注释与代码的删除尽量删除便于理解 优先使用工具类方法 服务器定义的字段为大写，客户端model 也应该使用小写 实现逻辑尽量才用普遍好理解的方法 import头文件的排版, 当import超过7个 ✅ #import \"GWHProductViewController.h\" //model #import \"GWHProductModel.h\" #import \"GWHCarModel.h\" #import \"GWHCommunityModel.h\" //view #import \"GWHProducPackageView.h\" #import \"GWHCarCell.h\" #import \"GWHCommunityCell.h\" //tool #import #import \"MJRefresh.h\" #import \"NSDateFormatter+Utility.h\" #import \"Masonry.h\" #import \"HttpUtils+GWHNetTool.h\" #import \"GWHUserManager.h\" //viewController #import \"GWHServiceProductRecViewController.h\" #import \"GWHPageController.h\" ❌ #import \"GWHProductViewController.h\" #import \"GWHProductModel.h\" #import \"GWHCarCell.h\" #import \"GWHCommunityCell.h\" #import #import \"MJRefresh.h\" #import \"NSDateFormatter+Utility.h\" #import \"HttpUtils+GWHNetTool.h\" #import \"GWHUserManager.h\" #import \"GWHServiceProductRecViewController.h\" #import \"GWHCarModel.h\" #import \"GWHCommunityModel.h\" #import \"GWHProducPackageView.h\" #import \"Masonry.h\" #import \"GWHPageController.h\" 代码提交逻辑 提交 commit 的类型 feat: 其他 fix: 修复bug 目前除了fix需要附带bug编号, 别的统一用feat. "},"pages/swift/Swift_determines_bangs_screen.html":{"url":"pages/swift/Swift_determines_bangs_screen.html","title":"Swift判断刘海屏幕","keywords":"","body":"Swift判断刘海屏幕 直接上代码 static var isFullScreen: Bool { if #available(iOS 11, *) { guard let w = UIApplication.shared.delegate?.window, let unwrapedWindow = w else { return false } if unwrapedWindow.safeAreaInsets.left > 0 || unwrapedWindow.safeAreaInsets.bottom > 0 { print(unwrapedWindow.safeAreaInsets) return true } } return false } 首先,刘海屏在iOS 11之后才推出,而重中之重的是safeAreaInsets属性 以下分别是竖屏与横屏的时候,safeAreaInsets打印的值 UIEdgeInsets(top: 44.0, left: 0.0, bottom: 34.0, right: 0.0) UIEdgeInsets(top: 0.0, left: 44.0, bottom: 21.0, right: 44.0) 其实单单判断bottom > 0 这个属性就完全可以解决问题了 static var kNavigationBarHeight: CGFloat { //return UIApplication.shared.statusBarFrame.height == 44 ? 88 : 64 return isFullScreen ? 88 : 64 } static var kBottomSafeHeight: CGFloat { //return UIApplication.shared.statusBarFrame.height == 44 ? 34 : 0 return isFullScreen ? 34 : 0 } 当然如果只是想简单适配 特别是竖屏的话 下面这段代码其实就能解决很多问题 UIApplication.shared.statusBarFrame.height == 44 "},"pages/webview/01_Interaction_between_WebView_and_iOS_native.html":{"url":"pages/webview/01_Interaction_between_WebView_and_iOS_native.html","title":"1.WebView与iOS原生的交互","keywords":"","body":"1.WebView与iOS原生的交互 一、WKWebView的代理方法 1.1 WKNavigationDelegate 该代理提供的方法，可以用来追踪加载过程（页面开始加载、加载完成、加载失败）、决定是否执行跳转。 // 页面开始加载时调用 optional func webView(_ webView: WKWebView, didStartProvisionalNavigation navigation: WKNavigation!) // 当内容开始返回时调用 optional func webView(_ webView: WKWebView, didCommit navigation: WKNavigation!) // 页面加载完成之后调用 optional func webView(_ webView: WKWebView, didFinish navigation: WKNavigation!) // 页面加载失败时调用 optional func webView(_ webView: WKWebView, didFailProvisionalNavigation navigation: WKNavigation!, withError error: Error) 页面跳转的代理方法有三种，分为（收到跳转与决定是否跳转两种）： // 接收到服务器跳转请求之后调用 optional func webView(_ webView: WKWebView, didReceiveServerRedirectForProvisionalNavigation navigation: WKNavigation!) // 在收到响应后，决定是否跳转 optional func webView(_ webView: WKWebView, decidePolicyFor navigationResponse: WKNavigationResponse, decisionHandler: @escaping (WKNavigationResponsePolicy) -> Void) // 在发送请求之前，决定是否跳转 optional func webView(_ webView: WKWebView, decidePolicyFor navigationAction: WKNavigationAction, decisionHandler: @escaping (WKNavigationActionPolicy) -> Void) 1.2 WKUIDelegate optional func webView(_ webView: WKWebView, createWebViewWith configuration: WKWebViewConfiguration, for navigationAction: WKNavigationAction, windowFeatures: WKWindowFeatures) -> WKWebView? 下面代理方法全都是与界面弹出提示框相关的，针对于web界面的三种提示框（警告框、确认框、输入框）分别对应三种代理方法。下面只列举了警告框的方法。 optional func webView(_ webView: WKWebView, runJavaScriptAlertPanelWithMessage message: String, initiatedByFrame frame: WKFrameInfo, completionHandler: @escaping () -> Void) 1.3 WKScriptMessageHandler WKScriptMessageHandler其实就是一个遵循的协议，它能让网页通过JS把消息发送给OC。其中协议方法。 // 从web界面中接收到一个脚本时调用 func userContentController(_ userContentController: WKUserContentController, didReceive message: WKScriptMessage) 从协议中我们可以看出这里使用了两个类WKUserContentController和WKScriptMessage。WKUserContentController可以理解为调度器，WKScriptMessage则是携带的数据。 1.4 WKUserContentController WKUserContentController有两个核心方法，也是它的核心功能。 // js注入，即向网页中注入我们的js方法，这是一个非常强大的功能，开发中要慎用。 open func addUserScript(_ userScript: WKUserScript) // 添加供js调用oc的桥梁。这里的name对应WKScriptMessage中的name，多数情况下我们认为它就是方法名。 open func add(_ scriptMessageHandler: WKScriptMessageHandler, name: String) 1.5 WKScriptMessage WKScriptMessage就是js通知oc的数据。其中有两个核心属性用的很多。 open var name: String { get } 对应func add(_ scriptMessageHandler: WKScriptMessageHandler, name: String)添加的name。 open var body: Any { get } 携带的核心数据。js调用时只需window.webkit.messageHandlers.#name#.postMessage() 这里的name就是我们添加的name，是不是感觉很爽，就是这么简单，下面我们就来具体实现。 二、自定义CQWebView class CQWebView: UIView { // 增加webView属性 private lazy var webView: WKWebView = { let conf = WKWebViewConfiguration() conf.preferences.javaScriptEnabled = true conf.selectionGranularity = WKSelectionGranularity.character conf.allowsInlineMediaPlayback = true let webView = WKWebView(frame: .zero, configuration: conf) ... return webView }() } 2.1 增加js/oc交互方法 class CQWebView: UIView { ... // 增加js消息监听 func adddScriptMessageHandler(forName name: String) { webView.configuration.userContentController.add(self, name: name) } // 移除js消息监听 func removeScriptMessageHandler(forName name: String) { webView.configuration.userContentController.removeScriptMessageHandler(forName: name) } // oc执行js func evaluateJavaScript(_ javaScriptString: String, completionHandler: ((Any?, Error?) -> Void)? = nil) { webView.evaluateJavaScript(javaScriptString, completionHandler: completionHandler) } } 2.2 增加代理属性 // WebView代理 @objc protocol CQWebViewDelegate { // 接收 js 发来的消息 @objc optional func webView(_ webView: CQWebView, didReceiveMessage name: String, body: Any) } class CQWebView: UIView { ... weak var delegate: CQWebViewDelegate? } 2.3 实现WKScriptMessageHandler // js 和 swift 的交互 extension CQWebView: WKScriptMessageHandler { // 接收 js 发来的消息 func userContentController(_ userContentController: WKUserContentController, didReceive message: WKScriptMessage) { delegate?.webView?(self, didReceiveMessage: message.name, body: message.body) } } 三、JS调用Swift 2.1 完整html Untitled Document js调用swift // js调用swift function jsCallSwift(obj) { // 向 swift 发送数据，这里的‘msgBridge’就是 swift 中添加的消息通道的 name window.webkit.messageHandlers.msgBridge.postMessage(obj); } // swift调用js function swiftCallJs(msg){ document.getElementById('h').innerText+=msg; } 3.2 增加对js消息的监听 只需要调用CQWebView的adddScriptMessageHandler方法。 class ViewController: UIViewController { // MARK: - Properties private lazy var webView: CQWebView = { let webView = CQWebView(frame: .zero) webView.delegate = self webView.adddScriptMessageHandler(forName: \"msgBridge\") ... return webView }() } 3.2 实现对js消息的处理 extension ViewController: CQWebViewDelegate { func webView(_ webView: CQWebView, didReceiveMessage name: String, body: Any) { switch name { case \"msgBridge\": ... break default: break } } } 四、Swift调用JS 4.1 evaluateJavaScript方法使用 在html的js中已经定义了swiftCallJs方法等待调用，只需要调用CQWebView的evaluateJavaScript方法即可。 //swift 调 js函数 webView.evaluateJavaScript(\"swiftCallJs('\\( dic[\"msg\"] as! String)')\", completionHandler: { (any, error) in if (error != nil) { print(error ?? \"err\") } }) 4.2 WKWebView加载JS NSString *js = @\"\"; // 根据JS字符串初始化WKUserScript对象 WKUserScript *script = [[WKUserScript alloc] initWithSource:js injectionTime:WKUserScriptInjectionTimeAtDocumentEnd forMainFrameOnly:YES]; // 根据生成的WKUserScript对象，初始化WKWebViewConfiguration WKWebViewConfiguration *config = [[WKWebViewConfiguration alloc] init]; [config.userContentController addUserScript:script]; 参考文章 源码 Safari调试iOS中的JS iOS WKWebView 加载本地html文件（swift） 学习-WebKit(WKScriptMessageHandler) iOS下OC与JS的交互(WKWebview-MessageHandler实现) 自己动手打造基于 WKWebView 的混合开发框架（二）——js 向 Native 一句话传值并反射出 Swift 对象执行指定函数 WkWebKit - javascript on loaded page finds window.webkit is undefined WKWebview使用二三事 "},"pages/webview/02_DSBridge_usage.html":{"url":"pages/webview/02_DSBridge_usage.html","title":"2.DSBridge的使用","keywords":"","body":"2.DSBridge的使用 参考文章 DSBridge框架使用说明 使用DSBridge同H5交互 "},"pages/webview/Interaction_between_WebView_and_iOS_native.html":{"url":"pages/webview/Interaction_between_WebView_and_iOS_native.html","title":"WebView与iOS原生的交互","keywords":"","body":"1.WebView与iOS原生的交互 一、WKWebView的代理方法 1.1 WKNavigationDelegate 该代理提供的方法，可以用来追踪加载过程（页面开始加载、加载完成、加载失败）、决定是否执行跳转。 // 页面开始加载时调用 optional func webView(_ webView: WKWebView, didStartProvisionalNavigation navigation: WKNavigation!) // 当内容开始返回时调用 optional func webView(_ webView: WKWebView, didCommit navigation: WKNavigation!) // 页面加载完成之后调用 optional func webView(_ webView: WKWebView, didFinish navigation: WKNavigation!) // 页面加载失败时调用 optional func webView(_ webView: WKWebView, didFailProvisionalNavigation navigation: WKNavigation!, withError error: Error) 页面跳转的代理方法有三种，分为（收到跳转与决定是否跳转两种）： // 接收到服务器跳转请求之后调用 optional func webView(_ webView: WKWebView, didReceiveServerRedirectForProvisionalNavigation navigation: WKNavigation!) // 在收到响应后，决定是否跳转 optional func webView(_ webView: WKWebView, decidePolicyFor navigationResponse: WKNavigationResponse, decisionHandler: @escaping (WKNavigationResponsePolicy) -> Void) // 在发送请求之前，决定是否跳转 optional func webView(_ webView: WKWebView, decidePolicyFor navigationAction: WKNavigationAction, decisionHandler: @escaping (WKNavigationActionPolicy) -> Void) 1.2 WKUIDelegate optional func webView(_ webView: WKWebView, createWebViewWith configuration: WKWebViewConfiguration, for navigationAction: WKNavigationAction, windowFeatures: WKWindowFeatures) -> WKWebView? 下面代理方法全都是与界面弹出提示框相关的，针对于web界面的三种提示框（警告框、确认框、输入框）分别对应三种代理方法。下面只列举了警告框的方法。 optional func webView(_ webView: WKWebView, runJavaScriptAlertPanelWithMessage message: String, initiatedByFrame frame: WKFrameInfo, completionHandler: @escaping () -> Void) 1.3 WKScriptMessageHandler WKScriptMessageHandler其实就是一个遵循的协议，它能让网页通过JS把消息发送给OC。其中协议方法。 // 从web界面中接收到一个脚本时调用 func userContentController(_ userContentController: WKUserContentController, didReceive message: WKScriptMessage) 从协议中我们可以看出这里使用了两个类WKUserContentController和WKScriptMessage。WKUserContentController可以理解为调度器，WKScriptMessage则是携带的数据。 1.4 WKUserContentController WKUserContentController有两个核心方法，也是它的核心功能。 // js注入，即向网页中注入我们的js方法，这是一个非常强大的功能，开发中要慎用。 open func addUserScript(_ userScript: WKUserScript) // 添加供js调用oc的桥梁。这里的name对应WKScriptMessage中的name，多数情况下我们认为它就是方法名。 open func add(_ scriptMessageHandler: WKScriptMessageHandler, name: String) 1.5 WKScriptMessage WKScriptMessage就是js通知oc的数据。其中有两个核心属性用的很多。 open var name: String { get } 对应func add(_ scriptMessageHandler: WKScriptMessageHandler, name: String)添加的name。 open var body: Any { get } 携带的核心数据。js调用时只需window.webkit.messageHandlers.#name#.postMessage() 这里的name就是我们添加的name，是不是感觉很爽，就是这么简单，下面我们就来具体实现。 二、自定义CQWebView class CQWebView: UIView { // 增加webView属性 private lazy var webView: WKWebView = { let conf = WKWebViewConfiguration() conf.preferences.javaScriptEnabled = true conf.selectionGranularity = WKSelectionGranularity.character conf.allowsInlineMediaPlayback = true let webView = WKWebView(frame: .zero, configuration: conf) ... return webView }() } 2.1 增加js/oc交互方法 class CQWebView: UIView { ... // 增加js消息监听 func adddScriptMessageHandler(forName name: String) { webView.configuration.userContentController.add(self, name: name) } // 移除js消息监听 func removeScriptMessageHandler(forName name: String) { webView.configuration.userContentController.removeScriptMessageHandler(forName: name) } // oc执行js func evaluateJavaScript(_ javaScriptString: String, completionHandler: ((Any?, Error?) -> Void)? = nil) { webView.evaluateJavaScript(javaScriptString, completionHandler: completionHandler) } } 2.2 增加代理属性 // WebView代理 @objc protocol CQWebViewDelegate { // 接收 js 发来的消息 @objc optional func webView(_ webView: CQWebView, didReceiveMessage name: String, body: Any) } class CQWebView: UIView { ... weak var delegate: CQWebViewDelegate? } 2.3 实现WKScriptMessageHandler // js 和 swift 的交互 extension CQWebView: WKScriptMessageHandler { // 接收 js 发来的消息 func userContentController(_ userContentController: WKUserContentController, didReceive message: WKScriptMessage) { delegate?.webView?(self, didReceiveMessage: message.name, body: message.body) } } 三、JS调用Swift 2.1 完整html Untitled Document js调用swift // js调用swift function jsCallSwift(obj) { // 向 swift 发送数据，这里的‘msgBridge’就是 swift 中添加的消息通道的 name window.webkit.messageHandlers.msgBridge.postMessage(obj); } // swift调用js function swiftCallJs(msg){ document.getElementById('h').innerText+=msg; } 3.2 增加对js消息的监听 只需要调用CQWebView的adddScriptMessageHandler方法。 class ViewController: UIViewController { // MARK: - Properties private lazy var webView: CQWebView = { let webView = CQWebView(frame: .zero) webView.delegate = self webView.adddScriptMessageHandler(forName: \"msgBridge\") ... return webView }() } 3.2 实现对js消息的处理 extension ViewController: CQWebViewDelegate { func webView(_ webView: CQWebView, didReceiveMessage name: String, body: Any) { switch name { case \"msgBridge\": ... break default: break } } } 四、Swift调用JS 4.1 evaluateJavaScript方法使用 在html的js中已经定义了swiftCallJs方法等待调用，只需要调用CQWebView的evaluateJavaScript方法即可。 //swift 调 js函数 webView.evaluateJavaScript(\"swiftCallJs('\\( dic[\"msg\"] as! String)')\", completionHandler: { (any, error) in if (error != nil) { print(error ?? \"err\") } }) 4.2 WKWebView加载JS NSString *js = @\"\"; // 根据JS字符串初始化WKUserScript对象 WKUserScript *script = [[WKUserScript alloc] initWithSource:js injectionTime:WKUserScriptInjectionTimeAtDocumentEnd forMainFrameOnly:YES]; // 根据生成的WKUserScript对象，初始化WKWebViewConfiguration WKWebViewConfiguration *config = [[WKWebViewConfiguration alloc] init]; [config.userContentController addUserScript:script]; 参考文章 源码 Safari调试iOS中的JS iOS WKWebView 加载本地html文件（swift） 学习-WebKit(WKScriptMessageHandler) iOS下OC与JS的交互(WKWebview-MessageHandler实现) 自己动手打造基于 WKWebView 的混合开发框架（二）——js 向 Native 一句话传值并反射出 Swift 对象执行指定函数 WkWebKit - javascript on loaded page finds window.webkit is undefined WKWebview使用二三事 "},"pages/cicd/01_Use_shell_to_package.html":{"url":"pages/cicd/01_Use_shell_to_package.html","title":"1.使用shell打包","keywords":"","body":"使用shell打包 需求 使用shell脚本，导出adhoc/appstore的包，然后上传至appstore/fir/pgy等平台。 一、准备adhoc/appstore的证书描述文件 新建iOS Distribution (App Store and Ad Hoc)证书。 新增(Ad Hoc/App Store) Provisioning Profile证书描述文件（该文件会关联App、Distribution证书、iPhone设备）。 二、准备导出plist配置文件 在我们手动使用Xcode打包的时候，导出完毕后可以得到对应ExportOptions.plist，直接使用即可。 注意：adhoc的plist请使用adhoc的证书描述文件（AdHocProvisioningProfile）打包，appstore的则使用appstore的证书描述文件（AppStoreProvisioningProfile）打包。 AdHocExportOptions.plist compileBitcode method ad-hoc provisioningProfiles com.******.packagewithscript AdHocProvisioningProfile signingCertificate Apple Distribution signingStyle manual stripSwiftSymbols teamID ****** thinning &lt;none&gt; AppStoreExportOptions compileBitcode method app-store provisioningProfiles com.******.packagewithscript AppStoreProvisioningProfile signingCertificate Apple Distribution signingStyle manual stripSwiftSymbols teamID ****** thinning &lt;none&gt; 三、配置表格 证书类型 证书描述文件类型 证书描述文件名称 plist文件 iOS Distribution (App Store and Ad Hoc) Ad Hoc AdHocProvisioningProfile AdHocExportOptions iOS Distribution (App Store and Ad Hoc) App Store AppStoreProvisioningProfile AppStoreExportOptions 四、xcodebuild和xcrun安装 xcodebuild和xcrun都是来自Command Line Tools，Xcode自带，如果没有可以通过以下命令安装： xcode-select --install 或者在下面的链接下载安装： https://developer.apple.com/downloads/ 安装完可在以下路径看到这两个工具： /Applications/Xcode.app/Contents/Developer/usr/bin/ 五、xcodebuild xcodebuild从入门到精通 脚本打包ipa会使用到如下命令： 清理 xcodebuild \\ clean -configuration ${development_mode} -quiet || rollbackIfNeed '清理失败' 编译 xcodebuild \\ archive -project ${project_name}.xcodeproj \\ -scheme ${scheme_name} \\ -configuration ${development_mode} \\ -archivePath ${buildPath}/${project_name}.xcarchive -quiet || rollbackIfNeed '编译失败' 导出 xcodebuild -exportArchive -archivePath ${buildPath}/${project_name}.xcarchive \\ -configuration ${development_mode} \\ -exportPath ${exportFilePath} \\ -exportOptionsPlist ${exportOptionsPlistPath} \\ -quiet || rollbackIfNeed '打包失败' 五、altool 脚本上传ipa至apptore会使用如下命令： # 验证并上传到App Store # ${exportFilePath}/${scheme_name}.ipa：ipa路径 # ******@126.com: 苹果账号 # ****-****-vlnc-hill：双重认证密码 xcrun altool --validate-app -f ${exportFilePath}/${scheme_name}.ipa -t ios -u ******@126.com -p ****-****-vlnc-hill --output-format xml || rollbackIfNeed 'ipa校验失败' xcrun altool --upload-app -f ${exportFilePath}/${scheme_name}.ipa -t ios -u ******@126.com -p ****-****-vlnc-hill --output-format xml || rollbackIfNeed 'ipa上传失败' 通过 altool 上传 App 的二进制文件 ipa上传 ipa上传stackflow 注：Xcode 11 的 altool 已经被命令 xcrun altool 替代。在终端运行xcrun altool -h可以查看说明。 六、fir-cli fir-cli安装 脚本上传ipa至fir会使用如下命令： # 上传到Fir # 将******替换成自己的Fir平台的token fir login -T ****** || rollbackIfNeed '登录fir失败' fir publish $exportFilePath/$scheme_name.ipa || rollbackIfNeed '发布ipa包至fir失败' 七、蒲公英 使用一条命令快速上传应用 脚本上传ipa至fir会使用如下命令： # 上传到蒲公英 # 蒲公英aipKey MY_PGY_API_K=****** # 蒲公英uKey MY_PGY_UK=****** curl -F \"file=@${exportFilePath}/${scheme_name}.ipa\" \\ -F \"uKey=${MY_PGY_UK}\" \\ -F \"_api_key=${MY_PGY_API_K}\" \\ https://www.pgyer.com/apiv1/app/upload || rollbackIfNeed '发布ipa包至pgy失败' 八、完整脚本 xcodebuild.sh 九、参考链接 蒲公英 fir While executing gem ... (Gem::FilePermissionError) 详解Shell脚本实现iOS自动化编译打包提交 "},"pages/cicd/02_Use_fastlane_to_package.html":{"url":"pages/cicd/02_Use_fastlane_to_package.html","title":"2.使用fastlane打包","keywords":"","body":"使用fastlane打包 fastlane理解 之前写了一篇使用shell打包的文章，从这篇文章打包是可以通过编写shell脚本完成，那么为什么还需要fastlane呢？ fastlane可以通过一个简单的通过简单命令来完成诸如截图、获取证书、编译、导出安装包，而不需要去关心如何去写大量的打包脚本。 fastlane可以定义多个任务，例如：打包到不同渠道包时，我们可以定义多个任务，只需要一行简单命令。 fastlane打包直接就包含了dsym等文件，而使用脚本还得自己去实现。 fastlane安装及配置 fastlane安装 sudo gem install fastlane -n /usr/local/bin fastlane升级 bundle update fastlane fastlane配置 cd 项目目录 fastlane init 执行以上命令，项目目录下会生成相应的Appfile、Fastfile。 执行fastlane init,bundle update卡住了 Appfile Appfile是用来配置一些类似于AppleID、BundleID参数(参数是fastlane已经定义好的，新增的并没有用，如果想新增变量需要使用.env方式)，可以在Fastfile中使用，AppleID、BundleID等其实会被一些actions直接调用，并不需要写出来传递。 普通配置方式 直接在Appfile里填写app_identifier、apple_id、team_id等，然后根据lane的不同可以设置成不同。 # 默认配置 app_identifier \"com.devhy.test\" apple_id \"devhy1@xxxx.com\" team_id \"xxxxxxxxx1\" # 如果lane是ent换成Dev的配置 for_lane :ent do app_identifier \"com.devhy.testDev\" apple_id \"devhy2@xxxx.com\" team_id \"xxxxxxxxx2\" end 使用.env配置方式 .env这个文件的作用是作为环境变量的配置文件，在fastlane init进行初始化后并不会自动生成，如果需要可以自己创建。 执行时默认会读取.env和.env.default文件里的配置。通过执行fastlane [lane-name] --env [envName]来指定使用配置文件.env.[envName]，读取顺序是.env -> .env.default -> .env.，相同的变量名会被后面的覆盖。 如我建了文件.env.myDev，里面写了一些参数，那在执行的时候使用fastlane [lane-name] --env myDev即可，想在Appfile、Deliverfile、Fastfile等调用，直接使用ENV['keyName']即可。 # .env.myDev文件 # bundle id App_Identifier = \"com.devhy.testDev\" # 开发者账号 Apple_Id = \"xx2@xxxx.com\" # 开发者TeamId Team_Id = \"xxxxxxxxx2\" # project的target scheme名称 Scheme = \"HYTestDev\" # Appfile使用.env方式直接读取变量即可 app_identifier ENV['App_Identifier'] apple_id ENV['Apple_Id'] team_id ENV['Team_Id'] 注意：因为是.env文件是.开头文件，默认是在finder中隐藏的，需要通过执行一下命令来显示： # 设置隐藏文件可见 defaults write com.apple.finder AppleShowAllFiles TRUE # 重启finder服务以生效 killall Finder 配置方式对比 普通配置方式：简单易懂，但不能自定义变量，且每个lane想不一样都要写一个for_lane .env配置方式：功能性强，但配置起来稍微麻烦一点。 Deliverfile Deliverfile是用来配置上传到iTunesConnect所需信息的，由于我们主要用fastlane来打包，发布是手动将ipa包提交审核，由于没有进行过尝试所以该文件配置方式就不叙述了。 Fastfile Fastfile是对流程进行控制的核心文件，需要设定支持的平台和在一些环节里需要做的事情。 基本结构 Fastfile主要是根据设定的平台，可以在before_all、after_all、error中做一些操作以及建立一些lane作为关键的执行逻辑，可以在其中使用fastlane内置的action，也可以调用自建action，还可以调用别的lane。 # 因为fastlane存在新老版本兼容问题，所以一般会指定fastlane版本 fastlane_version \"2.62.0\" default_platform :ios platform :ios do # 所有lane执行之前，可以做如执行cocoapods的pod install before_all do cocoapods end # 名字叫ent的lane，命令行里执行fastlane ent lane :ent do # 执行一些action，如cert下载证书，sigh下载pp文件，gym进行编译和导出包 end # 执行fastlane store即可 lane :store do # 调用一些action # 调用别的lane，比如send_msg send_msg end lane :send_msg do # 调用一些action end # 所有lane完成之后，可以适用参数lane来区分 after_all do |lane| end # 所有lane失败之后，可以适用参数lane来区分 error do |lane, exception| end end Fastfile样例 下面的Fastfile样例是配置了.env+Appfile后进行编写，因为这样在配置action时，可以省去一些入参。 因为使用了Appfile，cert的username、team_id 以及 sigh的app_identifier、username、team_id 可以不用传入了，fastlane在执行时会自己去从Appfile里取。以及之前在.env环境配置中设定了一个Scheme的字段，那么gym的scheme我们可以使用ENV['Scheme']来调用。 fastlane_version \"2.62.0\" default_platform :ios platform :ios do before_all do cocoapods end lane :store do # action(cert)，下载[开发者证书.cer] # 下载的文件会存在项目根目录的build文件夹下 # fastlane会让你在命令行登录开发者账号，登录成功后，会在你的[钥匙串]中创建一个 {deliver.[username]} 的登录账户 cert( # Appfile设置了这边就可以不用了 # username: \"devhy2@xxxx.com\", # team_id: \"xxxxxxxxx2\", # 下载.cer文件的位置 output_path: \"build\", ) # action(sigh)，下载[安装app匹配的Provision Profile文件(pp文件)] # 建议自己去苹果开发者网站证书中手动处理一波provision_profile # 建议用 bundleId_导出方式 来命名比如: # 企业包pp文件叫 testDev_InHouse.mobileprovision sigh( # Appfile设置了这边就可以不用了 # app_identifier: \"com.devhy.testDev\", # username: \"devhy2@xxxx.com\", # team_id: \"xxxxxxxxx2\", # 下载pp文件的位置 output_path: \"build\", # 自动下载签名时，adc里pp名字，不写也可以会根据你的bundle id、adhoc与否去下载最新的一个 # provisioning_name: \"testDev_InHouse\", # 仅下载不创建，默认是false readonly: true, # 因为是根据BundleID下载，导致adhoc和appstore会优先appstore，导致最后导出报错，如果是adhoc包请设置为true adhoc: true, ) # 编译配置，编译的scheme，导出包方式 gym( # 使用.env配置的环境变量 scheme: ENV['Scheme'], # app-store, ad-hoc, package, enterprise, development, developer-id export_method: \"enterprise\", # 输出日志的目录 buildlog_path: \"fastlanelog\", # 输出编译结果 output_directory: \"build\", include_bitcode: false ) end after_all do |lane| end error do |lane, exception| end end actions 在fastlane中使用的诸如cer()、sigh()、gym()都是action，其本质是预先写好的ruby脚本(如:sigh.rb)，fastlane中有很多已经写好的actions，当然也可以自己进行编写。 命令行常用的操作有： 1. 查看所有Action fastlane actions 2. 查看某个Action的参数说明 fastlane action [action_name]如(fastlane action gym) 版本自增及指定 # 版本处理 def setup_version_build(options) if \"#{options[:build]}\".empty? increment_build_number( xcodeproj: ENV['Xcodeproj'] ) else increment_build_number( xcodeproj: ENV['Xcodeproj'], build_number:options[:build] ) end unless \"#{options[:version]}\".empty? increment_version_number( xcodeproj: ENV['Xcodeproj'], version_number:options[:version] ) end end 使用fastlane上传App到蒲公英 https://www.pgyer.com/doc/view/fastlane 使用fastlane上传App到蒲公英 fastlane-plugin-firim Fastlane && AppStore Connect API Fastlane && AppStore Connect API 配置后的使用 编写完各种配置后怎么使用？其实使用方法还是比较简单的，不使用.env配置，执行fastlane [lane_name]即可。 使用某个.env配置，执行fastlane [lane_name] --env [env_name]即可 ，比如我在需要执行样例的Fastfile的store，并使用.env.myDev配置，那我可以执行fastlane store --env myDev。 完整脚本 Fastfile 参考链接 macOS Mojave 'ruby/config.h' file not found 和重复劳动说再见-使用fastlane进行iOS打包 iOS开发热门-自动打包fastlane fastlane文档 Mac 下 fastlane 安装 以及常见错误处理 Automating Version and Build Numbers Using agvtool iOS自动打包 fastlane 在mac上配置iOS自动化上架 deliver 使用fastlane deliver 自动上传App Store Connect 物料和截图 itunesconnect fastlane官网 "},"pages/cicd/03_Use_jenkins_to_package.html":{"url":"pages/cicd/03_Use_jenkins_to_package.html","title":"3.使用jenkins打包","keywords":"","body":"使用jenkins打包 安装jenkins 下载war，下载地址 启动jenkins java -jar jenkins.war --httpPort=8080 下载jdk8 请从本地复制密码并粘贴到下面。 /Users/mengru.tian/.jenkins/secrets/initialAdminPassword 创建第一个管理员用户 安装插件 点击Manage Jenkins 选择Manage Plugins 点击Available 安装证书插件 搜索keychain 点击“Download now and install after restart” 安装蒲公英插件 搜索pgyer 点击“Download now and install after restart” 安装Git Parameter插件 证书配置 点击Manage Jenkins 点击Keychains and Provisioning Profiles Management cp ~/Library/Keychains/login.keychain-db /Users/Shared/Jenkins/login.keychain 配置构建参数 Mac下使用命令行安装 jenkins 方法 Mac安装jenkins 利用Jenkins持续集成iOS项目 Jenkins安装与配置 关于jenkins 自动化打包部署的问题。 iOS Jenkins自动化打包上传到蒲公英 Git Parameter 插件 MacOS Jenkins卸载方法 Jenkins参数设置单选框、多选框、Git分支框 jenkins 构建后shell_如何/何时执行Shell标记一个构建失败在Jenkins？ 使用 Jenkins 插件上传应用到蒲公英 "},"pages/map/01_Map_display.html":{"url":"pages/map/01_Map_display.html","title":"1.地图显示","keywords":"","body":"1.地图显示 参考链接 百度地图SDK 百度定位SDK 高德地图SDK 高德定位SDK 高德地图Doc 用高德地图API 通过详细地址获得经纬度 Web服务API简介 IOS高德3D地图画多边形，以及判断某一经纬度是否在该多边形内 一、集成百度地图 第1步：注册和获取密钥 第2步：CocoaPods 自动配置，在.podspec文件中增加依赖 s.dependency 'BaiduMapKit', '6.3.0' 二、集成高德地图 第1步：获取Key 第2步：CocoaPods 自动配置，在.podspec文件中增加依赖 s.dependency 'AMap3DMap', '7.9.0' 三、抽象工厂 特点->比工厂方法产品种类多。 抽象产品 具体产品 抽象工厂 具体工厂 简单工厂 - 1 - N 工厂方法 1 N 1 N 抽象工厂 N N 1 N 四、地图SDK角色分析 抽象产品：MapViewProtocol、MapLocationProtocol 具体产品：BaiduMapView、GaodeMapView、BaiduMapLocation、GaodeMapLocation 抽象工厂：MapFactoryProtocol 具体工厂：BaiduMapFactory、GaodeMapFactory 地图引擎：MapEngine 五、显示地图 第1步：抽象地图协议 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { /// 初始化 /// - Parameter frame: init(frame: CGRect) /// 获取地图 func getView() -> UIView } 第2步：定义具体地图 高德地图 import MAMapKit /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { private var mapView: MAMapView! /// 初始化 /// - Parameter frame: required init(frame: CGRect) { super.init() mapView = MAMapView(frame: frame) } /// 获取地图 func getView() -> UIView { return mapView } } 百度地图 import BaiduMapAPI_Map /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { private var mapView: BMKMapView! /// 初始化 /// - Parameter frame: required init(frame: CGRect) { super.init() mapView = BMKMapView(frame: frame) } /// 获取地图 func getView() -> UIView { return mapView } } 第3步：抽象工厂 /// 地图工厂标准 public protocol MapFactoryProtocol: NSObjectProtocol { /// 初始化 /// - Parameter appKey: 第三方地图AppKey init(appKey: String) /// 获取地图 /// - Parameter frame: func getMapView(frame: CGRect) -> MapViewProtocol } 第4步：定义具体地图工厂 高德地图工厂 import MAMapKit /// 高德地图工厂 class GaodeMapFactory: NSObject, MapFactoryProtocol { /// 初始化 /// - Parameter appKey: 第三方地图AppKey required init(appKey: String) { super.init() AMapServices.shared()?.apiKey = appKey } /// 获取地图 /// - Parameter frame: func getMapView(frame: CGRect) -> MapViewProtocol { return GaodeMapView(frame: frame) } } 百度地图工厂 import BaiduMapAPI_Map /// 高德地图工厂 class BaiduMapFactory: NSObject, MapFactoryProtocol { private let mapManager = BMKMapManager() /// 初始化 /// - Parameter appKey: 第三方地图AppKey required init(appKey: String) { super.init() let result = mapManager.start(appKey, generalDelegate: self) if !result { print(\"manager start failed!!!\") } } /// 获取地图 /// - Parameter frame: func getMapView(frame: CGRect) -> MapViewProtocol { return BaiduMapView(frame: frame) } } 百度地图工厂需要实现下创建协议 extension BaiduMapFactory: BMKGeneralDelegate { func onGetNetworkState(_ iError: Int32) { if iError == 0 { print(\"联网成功\") } else { print(\"onGetNetworkState:\\(iError)\") } } func onGetPermissionState(_ iError: Int32) { if iError == 0 { print(\"授权成功\") } else { print(\"onGetPermissionState:\\(iError)\") } } } 第5步：定义地图引擎 通过读取工厂配置，获取激活的地图工厂。 注意：CQConfigManager可以读取config.xml,获取当前激活的地图工厂，这样可以不修改代码，无缝切换百度/高德地图。 /// 地图引擎 public class MapEngine: NSObject { /// 根据配置获取地图工厂 /// - Returns: 地图工厂 public func getFactory() -> MapFactoryProtocol? { let mapPlatform = CQConfigManager.shared.config.mapPlatform if let factoryName = mapPlatform?.factoryName, let appKey = mapPlatform?.appKey { // 百度地图工厂 if factoryName == \"BaiduMapFactory\" { return BaiduMapFactory(appKey: appKey) } // 高德地图工厂 if factoryName == \"GaodeMapFactory\" { return GaodeMapFactory(appKey: appKey) } } return nil } } 第6步：显示地图 注意：CQMapSDK是地图SDK，包含以上的源代码，而ViewController则是Demo工程的页面。 import CQMapSDK class ViewController: UIViewController { override func viewDidLoad() { super.viewDidLoad() // 显示地图 let engine = MapEngine() let factory = engine.getFactory() if let mapView = factory?.getMapView(frame: view.bounds) { view.addSubview(mapView.getView()) } } } "},"pages/map/02_Show_Amap_user_positioning.html":{"url":"pages/map/02_Show_Amap_user_positioning.html","title":"2.显示高德用户定位","keywords":"","body":"2.显示高德用户定位 六、显示高德用户定位 显示定位蓝点 第1步：地图协议增加显示用户定位属性 ......省略部分代码 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 设定是否显示定位图层 var showsUserLocation: Bool { get set } } /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... /// 设定是否显示定位图层 var showsUserLocation: Bool = false { didSet { mapView.showsUserLocation = showsUserLocation } } } /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ....... /// 设定是否显示定位图层 var showsUserLocation: Bool = false { didSet { mapView.showsUserLocation = showsUserLocation } } } 第2步：测试代码 class ViewController: UIViewController { override func viewDidLoad() { super.viewDidLoad() ...... if let mapView = factory?.getMapView(frame: view.bounds) { // 显示用户定位 mapView.showsUserLocation = true view.addSubview(mapView.getView()) } } } 增加如上代码后，高德/百度都没有显示用户定位。 发现问题1: [MAMapKit] 要在iOS 11及以上版本使用定位服务, 需要在Info.plist中添加NSLocationAlwaysAndWhenInUseUsageDescription和NSLocationWhenInUseUsageDescription字段。 解决办法：info.plist新增： NSLocationAlwaysAndWhenInUseUsageDescription CQMapSDK需要使用定位服务 NSLocationWhenInUseUsageDescription CQMapSDK需要使用定位服务 发现问题2: App Transport Security has blocked a cleartext HTTP (http://) resource load since it is insecure. Temporary exceptions can be configured via your app's Info.plist file. 解决办法：info.plist新增： NSAppTransportSecurity NSAllowsArbitraryLoads 发现问题3: [MAMapKit] 要在iOS 11及以上版本使用后台定位服务, 需要实现mapViewRequireLocationAuth: 代理方法 解决办法：实现mapViewRequireLocationAuth 1.定义地图代理协议 /// 地图代理协议 public protocol MapViewDelegateProtocol: NSObjectProtocol { } public extension MapViewDelegateProtocol { } 2.新增高德地图代理实现 import MAMapKit /// 高德地图代理实现 class MAMapViewDelegateImpl: NSObject, MAMapViewDelegate { weak var delegate: MapViewDelegateProtocol? /** * @brief 当plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription，并且[CLLocationManager authorizationStatus] == kCLAuthorizationStatusNotDetermined，会调用代理的此方法。 * 此方法实现调用后台权限API即可（ 该回调必须实现 [locationManager requestAlwaysAuthorization] ）; since 6.8.0 * - Parameter locationManager: 地图的CLLocationManager。 */ func mapViewRequireLocationAuth(_ locationManager: CLLocationManager!) { locationManager.requestAlwaysAuthorization() } } 3.高德地图实现增加代理 /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... private var mapDelegate = MAMapViewDelegateImpl() ...... /// 地图代理 var delegate: MapViewDelegateProtocol? { didSet { if let delegate = delegate { mapDelegate.delegate = delegate mapView.delegate = mapDelegate } } } ...... } 4.测试用户定位显示 class ViewController: UIViewController { override func viewDidLoad() { ...... if let mapView = factory?.getMapView(frame: view.bounds) { ...... // 设置代理 mapView.delegate = self ...... } } } 现在使用高德地图就可以正常显示用户定位啦！ "},"pages/map/03_How_to_display_Baidu_user_positioning.html":{"url":"pages/map/03_How_to_display_Baidu_user_positioning.html","title":"3.如何显示百度用户定位","keywords":"","body":"3.如何显示百度用户定位 七、如何显示百度用户定位？ 分析百度地图还是无法显示用户定位，经过查看百度Demo工程，必须完成以下步骤： 第1步：初始化定位SDK,设置showsUserLocation=true BMKLocationAuth.sharedInstance()?.checkPermision(withKey: appKey, authDelegate: self) 第2步：打开用户定位，并且确保定位成功 //开启定位服务 locationManager.startUpdatingHeading() locationManager.startUpdatingLocation() 第3步：在定位代理中更新用户位置 //MARK:BMKLocationManagerDelegate /** @brief 该方法为BMKLocationManager提供设备朝向的回调方法 @param manager 提供该定位结果的BMKLocationManager类的实例 @param heading 设备的朝向结果 */ func bmkLocationManager(_ manager: BMKLocationManager, didUpdate heading: CLHeading?) { NSLog(\"用户方向更新\") userLocation.heading = heading mapView.updateLocationData(userLocation) } /** @brief 连续定位回调函数 @param manager 定位 BMKLocationManager 类 @param location 定位结果，参考BMKLocation @param error 错误信息。 */ func bmkLocationManager(_ manager: BMKLocationManager, didUpdate location: BMKLocation?, orError error: Error?) { if let _ = error?.localizedDescription { NSLog(\"locError:%@;\", (error?.localizedDescription)!) } NSLog(\"用户定位更新\") userLocation.location = location?.location //实现该方法，否则定位图标不出现 mapView.updateLocationData(userLocation) } 第4步：在定位代理实现定位授权 func bmkLocationManager(_ manager: BMKLocationManager, doRequestAlwaysAuthorization locationManager: CLLocationManager) { locationManager.requestAlwaysAuthorization() } 下面我们集成百度定位SDK，在验证上面的步骤是否可以正确显示百度定位？ 八、集成百度定位 第1步：这里使用Cocoapod的配置，参考百度定位SDK-代码，以下是CQMapSDK.podspec关键配置： s.static_framework = true s.swift_version = '5.0' s.ios.deployment_target = '9.0' s.source_files = 'CQMapSDK/Classes/**/*',\"framework/*.framework/Headers/*.h\" s.public_header_files = \"framework/*.framework/Headers/*.h\" s.vendored_frameworks = \"framework/*.framework\" s.frameworks = \"CoreLocation\", \"Foundation\", \"UIKit\", \"SystemConfiguration\", \"AdSupport\", \"Security\", \"CoreTelephony\" s.libraries = \"sqlite3.0\",\"c++\" s.requires_arc = true s.pod_target_xcconfig = { 'EXCLUDED_ARCHS[sdk=iphonesimulator*]' => 'arm64' } s.user_target_xcconfig = { 'EXCLUDED_ARCHS[sdk=iphonesimulator*]' => 'arm64' } 增加了以上代码，执行pod update 第2步：定义定位相关协议：LocationProtocol、LocationManagerProtocol、LocationManagerDelegateProtocol 定位成功，返回的定位数据协议： /// 定位数据 public protocol LocationProtocol: NSObjectProtocol { /// 位置数据 var location: CLLocation? { get } /// 初始化LocationProtocol实例 /// - Parameter loc: CLLocation对象 init(location loc: CLLocation?) } 定位管理代理协议： /// 地图定位管理代理协议 public protocol LocationManagerDelegateProtocol : NSObjectProtocol { /// 为了适配app store关于新的后台定位的审核机制（app store要求如果开发者只配置了使用期间定位，则代码中不能出现申请后台定位的逻辑），当开发者在plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription时，需要在该delegate中调用后台定位api：[locationManager requestAlwaysAuthorization]。开发者如果只配置了NSLocationWhenInUseUsageDescription，且只有使用期间的定位需求，则无需在delegate中实现逻辑。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - locationManager: 系统 CLLocationManager 类 。 func locationManager(_ manager: LocationManagerProtocol, doRequestAlwaysAuthorization locationManager: CLLocationManager) /// 当定位发生错误时，会调用代理的此方法。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - error: 返回的错误，参考 CLError func locationManager(_ manager: LocationManagerProtocol, didFailWithError error: Error?) /// 连续定位回调函数。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - location: 定位结果 /// - error: 错误信息。 func locationManager(_ manager: LocationManagerProtocol, didUpdate location: LocationProtocol?, orError error: Error?) /// 提供设备朝向的回调方法。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - heading: 设备的朝向结果 func locationManager(_ manager: LocationManagerProtocol, didUpdate heading: CLHeading?) } extension LocationManagerDelegateProtocol { func locationManager(_ manager: LocationManagerProtocol, doRequestAlwaysAuthorization locationManager: CLLocationManager) { } func locationManager(_ manager: LocationManagerProtocol, didFailWithError error: Error?){ } func locationManager(_ manager: LocationManagerProtocol, didUpdate location: LocationProtocol?, orError error: Error?){ } func locationManager(_ manager: LocationManagerProtocol, didUpdate heading: CLHeading?){ } } 定位管理协议： /// 地图定位管理协议 public protocol LocationManagerProtocol: NSObjectProtocol { /// 实现了 LocationManagerDelegateProtocol 协议的类指针。 var delegate: LocationManagerDelegateProtocol? { get set } /// 开始连续定位。调用此方法会cancel掉所有的单次定位请求。 func startUpdatingLocation() /// 停止连续定位。调用此方法会cancel掉所有的单次定位请求，可以用来取消单次定位。 func stopUpdatingLocation() /// 开始设备朝向事件回调。 func startUpdatingHeading() /// r停止设备朝向事件回调。 func stopUpdatingHeading() } 第3步：百度定位SDK实现类：BaiduLocation、BaiduLocationManager、BMKLocationManagerDelegateImpl 百度定位数据实现： /// 百度定位数据 class BaiduLocation: NSObject, LocationProtocol { /// 位置数据 private(set) var location: CLLocation? /// 初始化LocationProtocol实例 /// - Parameter loc: CLLocation对象 required init(location loc: CLLocation?) { super.init() location = loc } } 百度定位管理代理实现： /// 百度地图定位管理代理实现类 class BMKLocationManagerDelegateImpl: NSObject, BMKLocationManagerDelegate { weak var delegate: LocationManagerDelegateProtocol? private var managerProtocol: LocationManagerProtocol! init(managerProtocol: LocationManagerProtocol) { super.init() self.managerProtocol = managerProtocol } func bmkLocationManager(_ manager: BMKLocationManager, doRequestAlwaysAuthorization locationManager: CLLocationManager) { delegate?.locationManager(managerProtocol, doRequestAlwaysAuthorization: locationManager) } func bmkLocationManager(_ manager: BMKLocationManager, didFailWithError error: Error?) { delegate?.locationManager(managerProtocol, didFailWithError: error) } func bmkLocationManager(_ manager: BMKLocationManager, didUpdate location: BMKLocation?, orError error: Error?) { let bmkLocation = BaiduLocation(location: location?.location) delegate?.locationManager(managerProtocol, didUpdate: bmkLocation, orError: error) } func bmkLocationManager(_ manager: BMKLocationManager, didUpdate heading: CLHeading?) { delegate?.locationManager(managerProtocol, didUpdate: heading) } } 百度定位管理： /// 百度地图定位管理 class BaiduLocationManager: NSObject, LocationManagerProtocol { private lazy var delegateImpl: BMKLocationManagerDelegateImpl = { BMKLocationManagerDelegateImpl(managerProtocol: self) }() private lazy var locationManager: BMKLocationManager = { //初始化BMKLocationManager的实例 let manager = BMKLocationManager() //设置定位管理类实例的代理 manager.delegate = delegateImpl //设定定位坐标系类型，默认为 BMKLocationCoordinateTypeGCJ02 manager.coordinateType = BMKLocationCoordinateType.BMK09LL //设定定位精度，默认为 kCLLocationAccuracyBest manager.desiredAccuracy = kCLLocationAccuracyBest //设定定位类型，默认为 CLActivityTypeAutomotiveNavigation manager.activityType = CLActivityType.automotiveNavigation //指定定位是否会被系统自动暂停，默认为NO manager.pausesLocationUpdatesAutomatically = false /** 是否允许后台定位，默认为NO。只在iOS 9.0及之后起作用。 设置为YES的时候必须保证 Background Modes 中的 Location updates 处于选中状态，否则会抛出异常。 由于iOS系统限制，需要在定位未开始之前或定位停止之后，修改该属性的值才会有效果。 */ manager.allowsBackgroundLocationUpdates = false /** 指定单次定位超时时间,默认为10s，最小值是2s。注意单次定位请求前设置。 注意: 单次定位超时时间从确定了定位权限(非kCLAuthorizationStatusNotDetermined状态) 后开始计算。 */ manager.locationTimeout = 10 return manager }() /// 实现了 LocationManagerDelegateProtocol 协议的类指针。 weak var delegate: LocationManagerDelegateProtocol? { didSet { delegateImpl.delegate = delegate } } /// 开始连续定位。调用此方法会cancel掉所有的单次定位请求。 func startUpdatingLocation() { locationManager.startUpdatingLocation() } /// 停止连续定位。调用此方法会cancel掉所有的单次定位请求，可以用来取消单次定位。 func stopUpdatingLocation() { locationManager.stopUpdatingLocation() } /// 开始设备朝向事件回调。 func startUpdatingHeading() { locationManager.startUpdatingHeading() } /// r停止设备朝向事件回调。 func stopUpdatingHeading() { locationManager.stopUpdatingHeading() } } 第4步：高德定位SDK实现类：GaodeLocation、GaodeLocationManager、AMapLocationManagerDelegateImpl，后续再做具体方法实现。 /// 高德定位数据 class GaodeLocation: NSObject, LocationProtocol { /// 位置数据 private(set) var location: CLLocation? /// 初始化LocationProtocol实例 /// - Parameter loc: CLLocation对象 required init(location loc: CLLocation?) { super.init() location = loc } } /// 高德地图定位管理代理实现类 class AMapLocationManagerDelegateImpl: NSObject { weak var delegate: LocationManagerDelegateProtocol? private var managerProtocol: LocationManagerProtocol! init(managerProtocol: LocationManagerProtocol) { super.init() self.managerProtocol = managerProtocol } } /// 高德地图定位管理 class GaodeLocationManager: NSObject, LocationManagerProtocol { private lazy var delegateImpl: BMKLocationManagerDelegateImpl = { BMKLocationManagerDelegateImpl(managerProtocol: self) }() /// 实现了 LocationManagerDelegateProtocol 协议的类指针。 weak var delegate: LocationManagerDelegateProtocol? { didSet { delegateImpl.delegate = delegate } } /// 开始连续定位。调用此方法会cancel掉所有的单次定位请求。 func startUpdatingLocation() { } /// 停止连续定位。调用此方法会cancel掉所有的单次定位请求，可以用来取消单次定位。 func stopUpdatingLocation() { } /// 开始设备朝向事件回调。 func startUpdatingHeading() { } /// r停止设备朝向事件回调。 func stopUpdatingHeading() { } } 第5步：新增及修改地图相关协议 1.新增UserLocationProtocol /// 用户定位协议 public protocol UserLocationProtocol: NSObjectProtocol { /// 位置更新状态，如果正在更新位置信息，则该值为YES var updating: Bool { get set } /// 位置信息，尚未定位成功，则该值为nil var location: CLLocation? { get set } /// heading信息，尚未定位成功，则该值为nil var heading: CLHeading? { get set } /// 定位标注点要显示的标题信息 var title: String? { get set } /// 定位标注点要显示的子标题信息 var subtitle: String? { get set } } 2.修改MapViewProtocol /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 设定是否显示定位图层 var showsUserLocation: Bool { get set } } 3.修改MapViewDelegateProtocol /// 地图代理协议 public protocol MapViewDelegateProtocol: NSObjectProtocol { /// 当plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription，并且[CLLocationManager authorizationStatus] == kCLAuthorizationStatusNotDetermined，会调用代理的此方法。此方法实现调用后台权限API即可（ 该回调必须实现 [locationManager requestAlwaysAuthorization] ）; since 6.8.0 /// - Parameter locationManager: 地图的CLLocationManager。 func mapViewRequireLocationAuth(_ mapView: MapViewProtocol, locationManager: CLLocationManager) } public extension MapViewDelegateProtocol { func mapViewRequireLocationAuth(_ mapView: MapViewProtocol, locationManager: CLLocationManager) { } } 第6步：新增及修改百度地图相关协议实现 /// 百度用户定位 class BaiduUserLocation: NSObject, UserLocationProtocol { /// 位置更新状态，如果正在更新位置信息，则该值为YES var updating: Bool = false /// 位置信息，尚未定位成功，则该值为nil var location: CLLocation? /// heading信息，尚未定位成功，则该值为nil var heading: CLHeading? /// 定位标注点要显示的标题信息 var title: String? /// 定位标注点要显示的子标题信息 var subtitle: String? } /// 百度地图代理实现 class BMKMapViewDelegateImpl: NSObject, BMKMapViewDelegate { weak var delegate: MapViewDelegateProtocol? private weak var mapViewProtocol: MapViewProtocol! init(mapViewProtocol: MapViewProtocol) { super.init() self.mapViewProtocol = mapViewProtocol } } /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ....... private lazy var mapDelegate: BMKMapViewDelegateImpl = { BMKMapViewDelegateImpl(mapViewProtocol: self) }() /// 初始化 /// - Parameter frame: required init(frame: CGRect) { super.init() mapView = BMKMapView(frame: frame) mapView.delegate = mapDelegate } ...... /// 地图代理 var delegate: MapViewDelegateProtocol? { didSet { if let delegate = delegate { mapDelegate.delegate = delegate } } } ...... /// 动态更新我的位置数据 /// - Parameter userLocation: 定位数据 func updateLocationData(_ userLocation: UserLocationProtocol) { let bmkUserLoc = BMKUserLocation() bmkUserLoc.location = userLocation.location mapView.updateLocationData(bmkUserLoc) } } 第7步：新增及修改高德地图相关协议实现 /// 高德用户定位 class GaodeUserLocation: NSObject, UserLocationProtocol { /// 位置更新状态，如果正在更新位置信息，则该值为YES var updating: Bool = false /// 位置信息，尚未定位成功，则该值为nil var location: CLLocation? /// heading信息，尚未定位成功，则该值为nil var heading: CLHeading? /// 定位标注点要显示的标题信息 var title: String? /// 定位标注点要显示的子标题信息 var subtitle: String? } /// 高德地图代理实现 class MAMapViewDelegateImpl: NSObject, MAMapViewDelegate { weak var delegate: MapViewDelegateProtocol? private weak var mapViewProtocol: MapViewProtocol! init(mapViewProtocol: MapViewProtocol) { super.init() self.mapViewProtocol = mapViewProtocol } /// 当plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription，并且[CLLocationManager authorizationStatus] == kCLAuthorizationStatusNotDetermined，会调用代理的此方法。此方法实现调用后台权限API即可（ 该回调必须实现 [locationManager requestAlwaysAuthorization] ）; since 6.8.0 /// - Parameter locationManager: 地图的CLLocationManager。 func mapViewRequireLocationAuth(_ locationManager: CLLocationManager!) { delegate?.mapViewRequireLocationAuth(mapViewProtocol, locationManager: locationManager) } } /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... private lazy var mapDelegate: MAMapViewDelegateImpl = { MAMapViewDelegateImpl(mapViewProtocol: self) }() /// 初始化 /// - Parameter frame: required init(frame: CGRect) { super.init() mapView = MAMapView(frame: frame) mapView.delegate = mapDelegate } ...... /// 地图代理 var delegate: MapViewDelegateProtocol? { didSet { if let delegate = delegate { mapDelegate.delegate = delegate } } } ...... /// 动态更新我的位置数据 /// - Parameter userLocation: 定位数据 func updateLocationData(_ userLocation: UserLocationProtocol) { } } 第8步：修改工厂协议及实现 /// 地图工厂标准 public protocol MapFactoryProtocol: NSObjectProtocol { ...... /// 获取定位管理对象 func getLocationManager() -> LocationManagerProtocol /// 获取用户定位数据 func getUserLocation() -> UserLocationProtocol } /// 高德地图工厂 class BaiduMapFactory: NSObject, MapFactoryProtocol { ...... /// 初始化 /// - Parameter appKey: 第三方地图AppKey required init(appKey: String) { super.init() BMKLocationAuth.sharedInstance()?.checkPermision(withKey: appKey, authDelegate: self) ...... } ...... /// 获取定位管理对象 func getLocationManager() -> LocationManagerProtocol { return BaiduLocationManager() } /// 获取用户定位数据 func getUserLocation() -> UserLocationProtocol { return BaiduUserLocation() } } /// 高德地图工厂 class GaodeMapFactory: NSObject, MapFactoryProtocol { ...... /// 获取定位管理对象 func getLocationManager() -> LocationManagerProtocol { return GaodeLocationManager() } /// 获取用户定位数据 func getUserLocation() -> UserLocationProtocol { return GaodeUserLocation() } } 第9步：验证百度地图显示定位 class ViewController: UIViewController { /// 地图实例 private var mapView: MapViewProtocol! /// 用户定位实例 private var userLocation: UserLocationProtocol! /// 定位管理实例 private var locationManager: LocationManagerProtocol! override func viewDidLoad() { super.viewDidLoad() // 获取地图工厂 let engine = MapEngine() let factory = engine.getFactory()! // 开启定位 userLocation = factory.getUserLocation() locationManager = factory.getLocationManager() locationManager.delegate = self locationManager.startUpdatingHeading() locationManager.startUpdatingLocation() // 显示地图 mapView = factory.getMapView(frame: view.bounds) // 设置代理 mapView.delegate = self // 显示用户定位(放在设置代理之后，确保可以调用locationManager.requestAlwaysAuthorization()) mapView.showsUserLocation = true view.addSubview(mapView.getView()) } ...... } extension ViewController: MapViewDelegateProtocol { /// 当plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription，并且[CLLocationManager authorizationStatus] == kCLAuthorizationStatusNotDetermined，会调用代理的此方法。此方法实现调用后台权限API即可（ 该回调必须实现 [locationManager requestAlwaysAuthorization] ）; since 6.8.0 /// - Parameter locationManager: 地图的CLLocationManager。 func mapViewRequireLocationAuth(_ mapView: MapViewProtocol, locationManager: CLLocationManager) { locationManager.requestAlwaysAuthorization() } } extension ViewController: LocationManagerDelegateProtocol { /// 为了适配app store关于新的后台定位的审核机制（app store要求如果开发者只配置了使用期间定位，则代码中不能出现申请后台定位的逻辑），当开发者在plist配置NSLocationAlwaysUsageDescription或者NSLocationAlwaysAndWhenInUseUsageDescription时，需要在该delegate中调用后台定位api：[locationManager requestAlwaysAuthorization]。开发者如果只配置了NSLocationWhenInUseUsageDescription，且只有使用期间的定位需求，则无需在delegate中实现逻辑。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - locationManager: 系统 CLLocationManager 类 。 func locationManager(_ manager: LocationManagerProtocol, doRequestAlwaysAuthorization locationManager: CLLocationManager) { locationManager.requestAlwaysAuthorization() } /// 当定位发生错误时，会调用代理的此方法。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - error: 返回的错误，参考 CLError func locationManager(_ manager: LocationManagerProtocol, didFailWithError error: Error?) { NSLog(\"定位失败\") } /// 连续定位回调函数。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - location: 定位结果 /// - error: 错误信息。 func locationManager(_ manager: LocationManagerProtocol, didUpdate location: LocationProtocol?, orError error: Error?) { if let _ = error?.localizedDescription { print(\"locError:(error?.localizedDescription)!\") } print(\"用户定位更新\") userLocation.location = location?.location //实现该方法，否则定位图标不出现 mapView.updateLocationData(userLocation) } /// 提供设备朝向的回调方法。 /// - Parameters: /// - manager: 定位 LocationManagerProtocol 实现类。 /// - heading: 设备的朝向结果 func locationManager(_ manager: LocationManagerProtocol, didUpdate heading: CLHeading?) { print((\"用户方向更新\")) userLocation.heading = heading mapView.updateLocationData(userLocation) } } 这样就可以成功显示百度定位啦！ "},"pages/map/04_Integrated_Amap_positioning.html":{"url":"pages/map/04_Integrated_Amap_positioning.html","title":"4.集成高德定位","keywords":"","body":"4.集成高德定位 九、集成高德定位 第1步：修改CQMapSDK.podspec 新增依赖s.dependency 'AMapLocation', '2.6.8',执行pod update 第2步：修改高德定位实现 import AMapLocationKit /// 高德地图定位管理代理实现类 class AMapLocationManagerDelegateImpl: NSObject, AMapLocationManagerDelegate { weak var delegate: LocationManagerDelegateProtocol? private var managerProtocol: LocationManagerProtocol! init(managerProtocol: LocationManagerProtocol) { super.init() self.managerProtocol = managerProtocol } func amapLocationManager(_ manager: AMapLocationManager!, didFailWithError error: Error!) { delegate?.locationManager(managerProtocol, didFailWithError: error) } func amapLocationManager(_ manager: AMapLocationManager!, didUpdate location: CLLocation!) { let amapLocation = GaodeLocation(location: location) delegate?.locationManager(managerProtocol, didUpdate: amapLocation, orError: nil) } func amapLocationManager(_ manager: AMapLocationManager!, didUpdate newHeading: CLHeading!) { delegate?.locationManager(managerProtocol, didUpdate: newHeading) } } 第3步：更新定位管理 import AMapLocationKit /// 高德地图定位管理 class GaodeLocationManager: NSObject, LocationManagerProtocol { private lazy var delegateImpl: AMapLocationManagerDelegateImpl = { AMapLocationManagerDelegateImpl(managerProtocol: self) }() private lazy var locationManager: AMapLocationManager = { //初始化BMKLocationManager的实例 let manager = AMapLocationManager() //设置定位管理类实例的代理 manager.delegate = delegateImpl //设定定位坐标系类型，默认为 BMKLocationCoordinateTypeGCJ02 //manager.coordinateType = BMKLocationCoordinateType.BMK09LL //设定定位精度，默认为 kCLLocationAccuracyBest manager.desiredAccuracy = kCLLocationAccuracyBest //设定定位类型，默认为 CLActivityTypeAutomotiveNavigation //manager.activityType = CLActivityType.automotiveNavigation //指定定位是否会被系统自动暂停，默认为NO manager.pausesLocationUpdatesAutomatically = false /** 是否允许后台定位，默认为NO。只在iOS 9.0及之后起作用。 设置为YES的时候必须保证 Background Modes 中的 Location updates 处于选中状态，否则会抛出异常。 由于iOS系统限制，需要在定位未开始之前或定位停止之后，修改该属性的值才会有效果。 */ manager.allowsBackgroundLocationUpdates = false /** 指定单次定位超时时间,默认为10s，最小值是2s。注意单次定位请求前设置。 注意: 单次定位超时时间从确定了定位权限(非kCLAuthorizationStatusNotDetermined状态) 后开始计算。 */ manager.locationTimeout = 10 return manager }() /// 实现了 LocationManagerDelegateProtocol 协议的类指针。 weak var delegate: LocationManagerDelegateProtocol? { didSet { delegateImpl.delegate = delegate } } /// 开始连续定位。调用此方法会cancel掉所有的单次定位请求。 func startUpdatingLocation() { locationManager.startUpdatingLocation() } /// 停止连续定位。调用此方法会cancel掉所有的单次定位请求，可以用来取消单次定位。 func stopUpdatingLocation() { locationManager.stopUpdatingLocation() } /// 开始设备朝向事件回调。 func startUpdatingHeading() { locationManager.startUpdatingHeading() } /// r停止设备朝向事件回调。 func stopUpdatingHeading() { locationManager.stopUpdatingHeading() } } "},"pages/map/05_New_positioning_attribute.html":{"url":"pages/map/05_New_positioning_attribute.html","title":"5.新增定位属性","keywords":"","body":"5.新增定位属性 十、新增定位属性 第1步：修改定位管理协议 /// 地图定位管理协议 public protocol LocationManagerProtocol: NSObjectProtocol { ///设定期望的定位精度。单位米，默认为 kCLLocationAccuracyBest。定位服务会尽可能去获取满足desiredAccuracy的定位结果，但不保证一定会得到满足期望的结果。 ///注意：设置为kCLLocationAccuracyBest或kCLLocationAccuracyBestForNavigation时，单次定位会在达到locationTimeout设定的时间后，将时间内获取到的最高精度的定位结果返回。 ///⚠️ 当iOS14及以上版本，模糊定位权限下可能拿不到设置精度的经纬度 var desiredAccuracy: CLLocationAccuracy { get set } ///设定定位的最小更新距离。单位米，默认为 kCLDistanceFilterNone，表示只要检测到设备位置发生变化就会更新位置信息。 var distanceFilter: CLLocationDistance { get set } ///指定定位是否会被系统自动暂停。默认为NO。 var pausesLocationUpdatesAutomatically: Bool { get set } ///是否允许后台定位。默认为NO。只在iOS 9.0及之后起作用。设置为YES的时候必须保证 Background Modes 中的 Location updates 处于选中状态，否则会抛出异常。由于iOS系统限制，需要在定位未开始之前或定位停止之后，修改该属性的值才会有效果。 var allowsBackgroundLocationUpdates: Bool { get set } ///指定单次定位超时时间,默认为10s。最小值是2s。注意单次定位请求前设置。注意: 单次定位超时时间从确定了定位权限(非kCLAuthorizationStatusNotDetermined状态)后开始计算。 var locationTimeout: Int { get set } ///指定单次定位逆地理超时时间,默认为5s。最小值是2s。注意单次定位请求前设置。 var reGeocodeTimeout: Int { get set } ...... } 第2步：修改高德百度实现 ///设定期望的定位精度。单位米，默认为 kCLLocationAccuracyBest。定位服务会尽可能去获取满足desiredAccuracy的定位结果，但不保证一定会得到满足期望的结果。 ///注意：设置为kCLLocationAccuracyBest或kCLLocationAccuracyBestForNavigation时，单次定位会在达到locationTimeout设定的时间后，将时间内获取到的最高精度的定位结果返回。 ///⚠️ 当iOS14及以上版本，模糊定位权限下可能拿不到设置精度的经纬度 var desiredAccuracy: CLLocationAccuracy = kCLLocationAccuracyBest { willSet { locationManager.desiredAccuracy = newValue } } ///设定定位的最小更新距离。单位米，默认为 kCLDistanceFilterNone，表示只要检测到设备位置发生变化就会更新位置信息。 var distanceFilter: CLLocationDistance = kCLDistanceFilterNone { willSet { locationManager.distanceFilter = newValue } } ///指定定位是否会被系统自动暂停。默认为NO。 var pausesLocationUpdatesAutomatically: Bool = false { willSet { locationManager.pausesLocationUpdatesAutomatically = newValue } } ///是否允许后台定位。默认为NO。只在iOS 9.0及之后起作用。设置为YES的时候必须保证 Background Modes 中的 Location updates 处于选中状态，否则会抛出异常。由于iOS系统限制，需要在定位未开始之前或定位停止之后，修改该属性的值才会有效果。 var allowsBackgroundLocationUpdates: Bool = false { willSet { locationManager.allowsBackgroundLocationUpdates = newValue } } ///指定单次定位超时时间,默认为10s。最小值是2s。注意单次定位请求前设置。注意: 单次定位超时时间从确定了定位权限(非kCLAuthorizationStatusNotDetermined状态)后开始计算。 var locationTimeout: Int = 10 { willSet { locationManager.locationTimeout = newValue } } ///指定单次定位逆地理超时时间,默认为5s。最小值是2s。注意单次定位请求前设置。 var reGeocodeTimeout: Int = 5 { willSet { locationManager.reGeocodeTimeout = newValue } } "},"pages/map/06_Add_single_positioning.html":{"url":"pages/map/06_Add_single_positioning.html","title":"6.新增单次定位","keywords":"","body":"6.新增单次定位 十一、新增单次定位 第1步：定义定位回调 /// 单次定位回调 public typealias LocatingCompletionBlock = (LocationProtocol?, Error?) -> Void 第2步：修改定位管理协议 /// 地图定位管理协议 public protocol LocationManagerProtocol: NSObjectProtocol { ...... /// - Parameters: /// - withReGeocode: 是否带有逆地理信息(获取逆地理信息需要联网) /// - withNetworkState: 是否带有移动热点识别状态(需要联网) /// - completionBlock: 单次定位完成后的Block /// - return: 是否成功添加单次定位Request func requestLocation(withReGeocode code: Bool, withNetworkState state: Bool, completionBlock: @escaping LocatingCompletionBlock) -> Bool } 第3步：增加百度地图实现 /// 百度地图定位管理 class BaiduLocationManager: NSObject, LocationManagerProtocol { ...... /// - Parameters: /// - withReGeocode: 是否带有逆地理信息(获取逆地理信息需要联网) /// - withNetworkState: 是否带有移动热点识别状态(需要联网) /// - completionBlock: 单次定位完成后的Block /// - return: 是否成功添加单次定位Request func requestLocation(withReGeocode code: Bool, withNetworkState state: Bool, completionBlock: @escaping LocatingCompletionBlock) -> Bool { let block: BMKLocatingCompletionBlock = { (bmkLocation, state, error) -> Void in let location = BaiduLocation(location: bmkLocation?.location) completionBlock(location, error) } return locationManager.requestLocation(withReGeocode: code, withNetworkState: state, completionBlock: block) } } 十二、新增地图更多属性 第1步：地图协议修改 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 当前地图的中心点，改变该值时，地图的比例尺级别不会发生变化 var centerCoordinate: CLLocationCoordinate2D? { get set } /// 定位用户位置的模式 var userTrackingMode: UserTrackingMode { get set } /// 动态更新我的位置数据 /// - Parameter userLocation: 定位数据 func updateLocationData(_ userLocation: UserLocationProtocol?) } 第2步：更新地图实现 高德地图 /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... /// 当前地图的中心点，改变该值时，地图的比例尺级别不会发生变化 var centerCoordinate: CLLocationCoordinate2D? { didSet { if let centerCoordinate = centerCoordinate { mapView.centerCoordinate = centerCoordinate } } } /// 定位用户位置的模式 var userTrackingMode: UserTrackingMode = .none { didSet { switch userTrackingMode { case .none:/// 普通定位模式 mapView.userTrackingMode = .none break case .heading:/// 定位方向模式 mapView.userTrackingMode = .none break case .follow:/// 定位跟随模式 mapView.userTrackingMode = .follow break case .followWithHeading:/// 定位罗盘模式 mapView.userTrackingMode = .followWithHeading break } } } /// 动态更新我的位置数据 /// - Parameter userLocation: 定位数据 func updateLocationData(_ userLocation: UserLocationProtocol?) { } } 百度地图 /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ...... /// 当前地图的中心点，改变该值时，地图的比例尺级别不会发生变化 var centerCoordinate: CLLocationCoordinate2D? { didSet { if let centerCoordinate = centerCoordinate { mapView.centerCoordinate = centerCoordinate } } } /// 定位用户位置的模式 var userTrackingMode: UserTrackingMode = .none { didSet { switch userTrackingMode { case .none:/// 普通定位模式 mapView.userTrackingMode = BMKUserTrackingModeNone break case .heading:/// 定位方向模式 mapView.userTrackingMode = BMKUserTrackingModeHeading break case .follow:/// 定位跟随模式 mapView.userTrackingMode = BMKUserTrackingModeFollow break case .followWithHeading:/// 定位罗盘模式 mapView.userTrackingMode = BMKUserTrackingModeFollowWithHeading break } } } /// 动态更新我的位置数据 /// - Parameter userLocation: 定位数据 func updateLocationData(_ userLocation: UserLocationProtocol?) { guard let userLocation = userLocation else { return } let bmkUserLoc = BMKUserLocation() bmkUserLoc.location = userLocation.location mapView.updateLocationData(bmkUserLoc) } } 第3步：测试 单次定位完成设置定位为地图中心 /// 单次定位 private func singleRequestLocation() { _ = locationManager.requestLocation(withReGeocode: true, withNetworkState: true) {[weak self] (location, error) in if let error = error { print(\"单次定位失败：\", error.localizedDescription) } else { let latitude = location?.location?.coordinate.latitude ?? 0 let longitude = location?.location?.coordinate.longitude ?? 0 print(\"单次定位成功：altitude:\\(latitude),longitude:\\(longitude)\") self?.userLocation.location = location?.location // 实现该方法，否则定位图标不出现 if let userLocation = self?.userLocation { self?.mapView.updateLocationData(userLocation) } // 设置中心点 self?.mapView.centerCoordinate = location?.location?.coordinate } } } 第4步：增加高德地图实现 注意：高德地图要实现单次定位，需要保证\"Background Modes\"中的\"Location updates\"处于选中状态。 /// 高德地图定位管理 class GaodeLocationManager: NSObject, LocationManagerProtocol { ...... /// - Parameters: /// - withReGeocode: 是否带有逆地理信息(获取逆地理信息需要联网) /// - withNetworkState: 是否带有移动热点识别状态(需要联网) /// - completionBlock: 单次定位完成后的Block /// - return: 是否成功添加单次定位Request func requestLocation(withReGeocode code: Bool, withNetworkState state: Bool, completionBlock: @escaping LocatingCompletionBlock) -> Bool { let block: AMapLocatingCompletionBlock = { (amapLocation, regeocode, error) -> Void in let location = GaodeLocation(location: amapLocation) completionBlock(location, error) } return locationManager.requestLocation(withReGeocode: code, completionBlock: block) } } "},"pages/map/07_Rich_Map_Agent.html":{"url":"pages/map/07_Rich_Map_Agent.html","title":"7.丰富地图代理","keywords":"","body":"7.丰富地图代理 十三、丰富地图代理 第1步：修改地图代理协议 /// 地图代理协议 public protocol MapViewDelegateProtocol: NSObjectProtocol { ...... /// 地图区域即将改变时会调用此接口 /// - Parameters: /// - mapView: 地图实例 /// - animated: 是否动画 func mapView(_ mapView: MapViewProtocol, regionWillChangeAnimated animated: Bool) /// 地图区域改变完成后会调用此接口 /// - Parameters: /// - mapView: 地图实例 /// - animated: 是否动画 /// - wasUserAction: 标识是否是用户动作 func mapView(_ mapView: MapViewProtocol, regionDidChangeAnimated animated: Bool, wasUserAction: Bool) /// 位置或者设备方向更新后，会调用此函数 /// - Parameters: /// - mapView: 地图实例 /// - userLocation: 用户定位信息(包括位置与设备方向等数据) /// - updatingLocation: 标示是否是location数据更新, YES:location数据更新 NO:heading数据更新 func mapView(_ mapView: MapViewProtocol, didUpdate userLocation: UserLocationProtocol, updatingLocation: Bool) } 第2步：修改地图代理实现 高德地图 /// 高德地图代理实现 class MAMapViewDelegateImpl: NSObject, MAMapViewDelegate { ...... func mapView(_ mapView: MAMapView!, regionWillChangeAnimated animated: Bool) { delegate?.mapView(mapViewProtocol, regionWillChangeAnimated: animated) } func mapView(_ mapView: MAMapView!, regionDidChangeAnimated animated: Bool, wasUserAction: Bool) { delegate?.mapView(mapViewProtocol, regionDidChangeAnimated: animated, wasUserAction: wasUserAction) } func mapView(_ mapView: MAMapView!, didUpdate userLocation: MAUserLocation!, updatingLocation: Bool) { let gaodeUserLocation = GaodeUserLocation() gaodeUserLocation.location = userLocation.location gaodeUserLocation.heading = userLocation.heading gaodeUserLocation.updating = userLocation.isUpdating delegate?.mapView(mapViewProtocol, didUpdate: gaodeUserLocation, updatingLocation: updatingLocation) } } 百度地图 /// 百度地图代理实现 class BMKMapViewDelegateImpl: NSObject, BMKMapViewDelegate { ...... func mapView(_ mapView: BMKMapView!, regionWillChangeAnimated animated: Bool) { delegate?.mapView(mapViewProtocol, regionWillChangeAnimated: animated) } func mapView(_ mapView: BMKMapView!, regionDidChangeAnimated animated: Bool, reason: BMKRegionChangeReason) { let wasUserAction = reason == BMKRegionChangeReasonGesture delegate?.mapView(mapViewProtocol, regionDidChangeAnimated: animated, wasUserAction: wasUserAction) } } "},"pages/map/08_New_zoom_level_added_to_the_map.html":{"url":"pages/map/08_New_zoom_level_added_to_the_map.html","title":"8.地图新增缩放等级","keywords":"","body":"8.地图新增缩放等级 十四、地图新增缩放等级 第1步：修改地图协议 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 缩放级别（默认3-19，有室内地图时为3-20） var zoomLevel: CGFloat { get set } /// 最小缩放级别 var minZoomLevel: CGFloat { get set } /// 最大缩放级别（有室内地图时最大为20，否则为19） var maxZoomLevel: CGFloat { get set } } 第2步：修改地图实现 高德地图 /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... /// 缩放级别（默认3-19，有室内地图时为3-20） var zoomLevel: CGFloat { get { mapView.zoomLevel } set { mapView.zoomLevel = newValue } } /// 最小缩放级别 var minZoomLevel: CGFloat { get { mapView.minZoomLevel } set { mapView.minZoomLevel = newValue } } /// 最大缩放级别（有室内地图时最大为20，否则为19） var maxZoomLevel: CGFloat { get { mapView.maxZoomLevel } set { mapView.maxZoomLevel = newValue } } } 百度地图 /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ...... /// 缩放级别（默认3-19，有室内地图时为3-20） var zoomLevel: CGFloat { get { CGFloat(mapView.zoomLevel) } set { mapView.zoomLevel = Float(newValue) } } /// 最小缩放级别 var minZoomLevel: CGFloat { get { CGFloat(mapView.minZoomLevel) } set { mapView.minZoomLevel = Float(newValue) } } /// 最大缩放级别（有室内地图时最大为20，否则为19） var maxZoomLevel: CGFloat { get { CGFloat(mapView.maxZoomLevel) } set { mapView.maxZoomLevel = Float(newValue) } } } "},"pages/map/09_Map_annotation_implementation.html":{"url":"pages/map/09_Map_annotation_implementation.html","title":"9.地图标注实现","keywords":"","body":"9.地图标注实现 十五、地图标注实现 第1步：新增标注协议、标注view协议 AnnotationProtocol（通用标注协议）、AnnotationViewProtocol（通用标注view协议）、PinAnnotationViewProtocol（大头针标注view协议） /// 标注协议 public protocol AnnotationProtocol: NSObjectProtocol { /// 标注view中心坐标 var coordinate: CLLocationCoordinate2D? { get set} /// annotation标题 var title: String? { get set } /// annotation副标题 var subtitle: String? { get set } /// 具体地图标注实现 var annotationImpl: Any? { get } /// 构造方法 /// - Parameter annotationImpl: 具体地图标注实现 init(annotationImpl: Any?) } /// 标注view协议 public protocol AnnotationViewProtocol: NSObjectProtocol { /// 是否允许弹出callout var canShowCallout: Bool { get set } /// 显示在默认弹出框右侧的view var rightCalloutAccessoryView: UIView? { get set } /// 是否支持拖动 var draggable: Bool { get set } /// 具体地图标注view实现 var annotationViewImpl: UIView? { get } /// 构造方法 /// - Parameter annotationViewImpl: 具体地图标注view实现 init(annotationViewImpl: UIView?) } /// 大头针标注view颜色 public enum PinAnnotationColor: Int { /// 第2步：新增标注实现、标注view实现 GaodePointAnnotation（高德标注）、GaodePinAnnotationView（高德大头针） BaiduPointAnnotation（百度标注）、BaiduPinAnnotationView（百度大头针） PointAnnotation（通用标注）、PinAnnotationView（通用大头针） /// 高德标注 public class GaodePointAnnotation: MAPointAnnotation { /// 聚合标注 weak var annotation: PointAnnotation? } /// 高德大头针标准view public class GaodePinAnnotationView: MAPinAnnotationView { } /// 百度标注 public class BaiduPointAnnotation: BMKPointAnnotation { /// 聚合标注 weak var annotation: PointAnnotation? } /// 百度大头针标准view public class BaiduPinAnnotationView: BMKPinAnnotationView { } /// 标注实现 public class PointAnnotation: NSObject, AnnotationProtocol { /// 标注view中心坐标 public var coordinate: CLLocationCoordinate2D? { get { if let annotation = gaodeAnnotation { return annotation.coordinate } if let annotation = baiduAnnotation { return annotation.coordinate } return nil } set { if let coordinate = newValue, let annotation = gaodeAnnotation { annotation.coordinate = coordinate } if let coordinate = newValue, let annotation = baiduAnnotation { annotation.coordinate = coordinate } } } /// annotation标题 public var title: String? { get { if let annotation = gaodeAnnotation { return annotation.title } if let annotation = baiduAnnotation { return annotation.title } return nil } set { if let title = newValue, let annotation = gaodeAnnotation { annotation.title = title } if let title = newValue, let annotation = baiduAnnotation { annotation.title = title } } } /// annotation副标题 public var subtitle: String? { get { if let annotation = gaodeAnnotation { return annotation.subtitle } if let annotation = baiduAnnotation { return annotation.subtitle } return nil } set { if let subtitle = newValue, let annotation = gaodeAnnotation { annotation.subtitle = subtitle } if let subtitle = newValue, let annotation = baiduAnnotation { annotation.subtitle = subtitle } } } /// 具体地图标注实现 private(set) public var annotationImpl: Any? /// 高德标注 private var gaodeAnnotation: GaodePointAnnotation? { if let annotation = annotationImpl as? GaodePointAnnotation { return annotation } return nil } /// 百度标注 private var baiduAnnotation: BaiduPointAnnotation? { if let annotation = annotationImpl as? BaiduPointAnnotation { return annotation } return nil } /// 构造方法 /// - Parameter annotationImpl: 具体地图标注实现 public required init(annotationImpl: Any?) { super.init() self.annotationImpl = annotationImpl gaodeAnnotation?.annotation = self baiduAnnotation?.annotation = self } } /// 通用大头针标注view public class PinAnnotationView: NSObject, PinAnnotationViewProtocol { /// 是否允许弹出callout public var canShowCallout: Bool { get { if let annotationView = gaodeAnnotationView { return annotationView.canShowCallout } if let annotationView = baiduAnnotationView { return annotationView.canShowCallout } return false } set { if let annotationView = gaodeAnnotationView { annotationView.canShowCallout = newValue } if let annotationView = baiduAnnotationView { annotationView.canShowCallout = newValue } } } /// 显示在默认弹出框右侧的view public var rightCalloutAccessoryView: UIView? { get { if let annotationView = gaodeAnnotationView { return annotationView.rightCalloutAccessoryView } if let annotationView = baiduAnnotationView { return annotationView.rightCalloutAccessoryView } return nil } set { if let annotationView = gaodeAnnotationView { annotationView.rightCalloutAccessoryView = newValue } if let annotationView = baiduAnnotationView { annotationView.rightCalloutAccessoryView = newValue } } } /// 是否支持拖动 public var draggable: Bool { get { if let annotationView = gaodeAnnotationView { return annotationView.isDraggable } if let annotationView = baiduAnnotationView { return annotationView.isDraggable } return false } set { if let annotationView = gaodeAnnotationView { annotationView.isDraggable = newValue } if let annotationView = baiduAnnotationView { annotationView.isDraggable = newValue } } } /// 大头针的颜色 public var pinColor: PinAnnotationColor { get { if let annotationView = gaodeAnnotationView { return PinAnnotationColor(rawValue: annotationView.pinColor.rawValue) ?? .red } if let annotationView = baiduAnnotationView { return PinAnnotationColor(rawValue: Int(annotationView.pinColor)) ?? .red } return .red } set { if let annotationView = gaodeAnnotationView, let pinColor = MAPinAnnotationColor(rawValue: newValue.rawValue) { annotationView.pinColor = pinColor } if let annotationView = baiduAnnotationView { annotationView.pinColor = BMKPinAnnotationColor(newValue.rawValue) } } } /// 添加到地图时是否使用下落动画效果 public var animatesDrop: Bool { get { if let annotationView = gaodeAnnotationView { return annotationView.animatesDrop } if let annotationView = baiduAnnotationView { return annotationView.animatesDrop } return false } set { if let annotationView = gaodeAnnotationView { annotationView.animatesDrop = newValue } if let annotationView = baiduAnnotationView { annotationView.animatesDrop = newValue } } } /// 具体地图标注view实现 private(set) public var annotationViewImpl: UIView? /// 高德地图标注view private var gaodeAnnotationView: MAPinAnnotationView? { return annotationViewImpl as? MAPinAnnotationView } /// 百度地图标注view private var baiduAnnotationView: BMKPinAnnotationView? { return annotationViewImpl as? BMKPinAnnotationView } /// 构造方法 /// - Parameter annotationViewImpl: 具体地图标注view实现 public required init(annotationViewImpl: UIView?) { self.annotationViewImpl = annotationViewImpl } } 第3步：地图工厂标准修改及实现 /// 地图工厂标准 public protocol MapFactoryProtocol: NSObjectProtocol { ...... /// 获取地图标注 func getPointAnnotation() -> PointAnnotation /// 获取标注view /// - Parameters: /// - annotation: 标注 /// - reuseIdentifier: 重用ID func getPinAnnotationView(annotation: AnnotationProtocol, reuseIdentifier: String) -> PinAnnotationViewProtocol } /// 百度地图工厂 class BaiduMapFactory: NSObject, MapFactoryProtocol { ...... /// 获取地图标注 func getPointAnnotation() -> PointAnnotation { let annotationImpl = BaiduPointAnnotation() return PointAnnotation(annotationImpl: annotationImpl) } /// 获取标注view /// - Parameters: /// - annotation: 标注 /// - reuseIdentifier: 重用ID func getPinAnnotationView(annotation: AnnotationProtocol, reuseIdentifier: String) -> PinAnnotationViewProtocol { var annotationImpl = BaiduPointAnnotation() if let annotationImplParam = annotation.annotationImpl as? BaiduPointAnnotation { annotationImpl = annotationImplParam } let annotationViewImpl = BaiduPinAnnotationView(annotation: annotationImpl, reuseIdentifier: reuseIdentifier) return PinAnnotationView(annotationViewImpl: annotationViewImpl) } } /// 高德地图工厂 class GaodeMapFactory: NSObject, MapFactoryProtocol { ...... /// 获取地图标注 func getPointAnnotation() -> PointAnnotation { let annotationImpl = GaodePointAnnotation() return PointAnnotation(annotationImpl: annotationImpl) } /// 获取标注view /// - Parameters: /// - annotation: 标注 /// - reuseIdentifier: 重用ID func getPinAnnotationView(annotation: AnnotationProtocol, reuseIdentifier: String) -> PinAnnotationViewProtocol { var annotationImpl = GaodePointAnnotation() if let annotationImplParam = annotation.annotationImpl as? GaodePointAnnotation { annotationImpl = annotationImplParam } let annotationViewImpl = GaodePinAnnotationView(annotation: annotationImpl, reuseIdentifier: reuseIdentifier) return PinAnnotationView(annotationViewImpl: annotationViewImpl) } } 第4步：地图协议修改及实现 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 向地图窗口添加标注 /// - Parameter annotation: 要添加的标注 func addAnnotation(_ annotation: AnnotationProtocol) /// 向地图窗口添加一组标注，需要实现MAMapViewDelegate的-mapView:viewForAnnotation:函数来生成标注对应的View /// - Parameter annotations: 要添加的标注数组 func addAnnotations(_ annotations: [AnnotationProtocol]) /// 设置地图使其可以显示数组中所有的annotation, 如果数组中只有一个则直接设置地图中心为annotation的位置。 /// - Parameters: /// - annotations: 需要显示的annotation /// - animated: 是否执行动画 func showAnnotations(_ annotations: [AnnotationProtocol], animated: Bool) /// 从复用内存池中获取制定复用标识的annotation view /// - Parameter withIdentifier: 复用标识 /// - Returns: annotation view func dequeueReusableAnnotationView(withIdentifier: String) -> PinAnnotationViewProtocol? } /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... /// 向地图窗口添加标注 /// - Parameter annotation: 要添加的标注 func addAnnotation(_ annotation: AnnotationProtocol) { if let annotationImpl = annotation.annotationImpl as? MAAnnotation { mapView.addAnnotation(annotationImpl) } } /// 向地图窗口添加一组标注，需要实现MAMapViewDelegate的-mapView:viewForAnnotation:函数来生成标注对应的View /// - Parameter annotations: 要添加的标注数组 func addAnnotations(_ annotations: [AnnotationProtocol]) { var annotationImpls: [Any] = [] for annotation in annotations { if let annotationImpl = annotation.annotationImpl { annotationImpls.append(annotationImpl) } } mapView.addAnnotations(annotationImpls) } /// 设置地图使其可以显示数组中所有的annotation, 如果数组中只有一个则直接设置地图中心为annotation的位置。 /// - Parameters: /// - annotations: 需要显示的annotation /// - animated: 是否执行动画 func showAnnotations(_ annotations: [AnnotationProtocol], animated: Bool) { var annotationImpls: [Any] = [] for annotation in annotations { if let annotationImpl = annotation.annotationImpl { annotationImpls.append(annotationImpl) } } mapView.showAnnotations(annotationImpls, animated: animated) } /// 从复用内存池中获取制定复用标识的annotation view /// - Parameter withIdentifier: 复用标识 /// - Returns: annotation view func dequeueReusableAnnotationView(withIdentifier: String) -> PinAnnotationViewProtocol? { if let annotationViewImpl = mapView.dequeueReusableAnnotationView(withIdentifier: withIdentifier) as? GaodePinAnnotationView { return PinAnnotationView(annotationViewImpl: annotationViewImpl) } return nil } } /// 高德地图代理实现 class MAMapViewDelegateImpl: NSObject, MAMapViewDelegate { ...... func mapView(_ mapView: MAMapView!, viewFor annotation: MAAnnotation!) -> MAAnnotationView! { if let pointAnnotation = annotation as? GaodePointAnnotation, let anno = pointAnnotation.annotation { if let pinAnnotationView = delegate?.mapView(mapViewProtocol, viewFor: anno)?.annotationViewImpl as? MAAnnotationView { return pinAnnotationView } return nil } return nil } } /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ...... /// 向地图窗口添加标注 /// - Parameter annotation: 要添加的标注 func addAnnotation(_ annotation: AnnotationProtocol) { if let annotationImpl = annotation.annotationImpl as? BMKAnnotation { mapView.addAnnotation(annotationImpl) } } /// 向地图窗口添加一组标注，需要实现MAMapViewDelegate的-mapView:viewForAnnotation:函数来生成标注对应的View /// - Parameter annotations: 要添加的标注数组 func addAnnotations(_ annotations: [AnnotationProtocol]) { var annotationImpls: [BMKAnnotation] = [] for annotation in annotations { if let annotationImpl = annotation.annotationImpl as? BMKAnnotation { annotationImpls.append(annotationImpl) } } mapView.addAnnotations(annotationImpls) } /// 设置地图使其可以显示数组中所有的annotation, 如果数组中只有一个则直接设置地图中心为annotation的位置。 /// - Parameters: /// - annotations: 需要显示的annotation /// - animated: 是否执行动画 func showAnnotations(_ annotations: [AnnotationProtocol], animated: Bool) { var annotationImpls: [BMKAnnotation] = [] for annotation in annotations { if let annotationImpl = annotation.annotationImpl as? BMKAnnotation { annotationImpls.append(annotationImpl) } } mapView.showAnnotations(annotationImpls, animated: animated) } /// 从复用内存池中获取制定复用标识的annotation view /// - Parameter withIdentifier: 复用标识 /// - Returns: annotation view func dequeueReusableAnnotationView(withIdentifier: String) -> PinAnnotationViewProtocol? { if let annotationViewImpl = mapView.dequeueReusableAnnotationView(withIdentifier: withIdentifier) as? BaiduPinAnnotationView { return PinAnnotationView(annotationViewImpl: annotationViewImpl) } return nil } } /// 百度地图代理实现 class BMKMapViewDelegateImpl: NSObject, BMKMapViewDelegate { ...... func mapView(_ mapView: BMKMapView!, viewFor annotation: BMKAnnotation!) -> BMKAnnotationView! { if let pointAnnotation = annotation as? BaiduPointAnnotation, let anno = pointAnnotation.annotation { if let pinAnnotationView = delegate?.mapView(mapViewProtocol, viewFor: anno)?.annotationViewImpl as? BMKAnnotationView{ return pinAnnotationView } } return nil } } 第5步：测试页面 class PointAnnotationViewController: UIViewController { /// 地图工厂 private lazy var factory: MapFactoryProtocol = { let factory = MapEngine.shared.getFactory()! return factory }() /// 地图实例 private lazy var mapView: MapViewProtocol = { // 显示地图 let mapView = factory.getMapView(frame: view.bounds) // 设置代理 mapView.delegate = self // 显示用户定位(放在设置代理之后，确保可以调用locationManager.requestAlwaysAuthorization()) mapView.showsUserLocation = true // 跟踪模式 mapView.userTrackingMode = .followWithHeading // 设置缩放 mapView.zoomLevel = 16 return mapView }() /// 标注数组 private lazy var annotations: Array = { var annotations = Array() let coordinates: [CLLocationCoordinate2D] = [ CLLocationCoordinate2D(latitude: 39.992520, longitude: 116.336170), CLLocationCoordinate2D(latitude: 39.978234, longitude: 116.352343), CLLocationCoordinate2D(latitude: 39.998293, longitude: 116.348904), CLLocationCoordinate2D(latitude: 40.004087, longitude: 116.353915), CLLocationCoordinate2D(latitude: 40.001442, longitude: 116.353915), CLLocationCoordinate2D(latitude: 39.989105, longitude: 116.360200), CLLocationCoordinate2D(latitude: 39.989098, longitude: 116.360201), CLLocationCoordinate2D(latitude: 39.998439, longitude: 116.324219), CLLocationCoordinate2D(latitude: 39.979590, longitude: 116.352792)] for (idx, coor) in coordinates.enumerated() { var anno = factory.getPointAnnotation() anno.coordinate = coor anno.title = String(idx) annotations.append(anno) } return annotations }() override func viewDidLoad() { super.viewDidLoad() view.backgroundColor = .white view.addSubview(mapView.getView()) } override func viewDidAppear(_ animated: Bool) { super.viewDidAppear(animated) mapView.addAnnotations(annotations) mapView.showAnnotations(annotations, animated: false) } } extension PointAnnotationViewController: MapViewDelegateProtocol { func mapView(_ mapView: MapViewProtocol, viewFor annotation: AnnotationProtocol) -> PinAnnotationViewProtocol? { if annotation.isKind(of: PointAnnotation.self) { let pointReuseIndetifier = \"pointReuseIndetifier\" var annotationView = mapView.dequeueReusableAnnotationView(withIdentifier: pointReuseIndetifier) if annotationView == nil { annotationView = factory.getPinAnnotationView(annotation: annotation, reuseIdentifier: pointReuseIndetifier) } annotationView!.canShowCallout = true annotationView!.animatesDrop = true annotationView!.draggable = true annotationView!.rightCalloutAccessoryView = UIButton(type: .detailDisclosure) let idx = annotations.index(of: annotation as! PointAnnotation) annotationView!.pinColor = PinAnnotationColor(rawValue: (idx ?? 3) % 3)! return annotationView } return nil } } "},"pages/map/10_Map_polygon_implementation.html":{"url":"pages/map/10_Map_polygon_implementation.html","title":"10.地图多边形实现","keywords":"","body":"10.地图多边形实现 十六、地图多边形实现 第1步：新增地图覆盖物协议、地图覆盖物Renderer协议 OverlayProtocol（地图覆盖物协议）、OverlayRendererProtocol（地图覆盖物Renderer协议） /// 地图覆盖物协议，所有地图的覆盖物需要实现 public protocol OverlayProtocol: NSObjectProtocol { /// 具体地图覆盖物实现 var overlayImpl: Any? { get } /// 构造方法 /// - Parameter annotationImpl: 具体地图覆盖物实现 init(overlayImpl: Any?) } /// 地图覆盖物Renderer协议 public protocol OverlayRendererProtocol: NSObjectProtocol { /// 填充颜色,默认是kMAOverlayRendererDefaultFillColor var fillColor: UIColor { get set } /// 笔触颜色,默认是kMAOverlayRendererDefaultStrokeColor var strokeColor: UIColor { get set } ///笔触宽度, 单位屏幕点坐标，默认是0 var lineWidth: CGFloat { get set } /// 具体地图标注view实现 var overRendererImpl: Any? { get } /// 构造方法 /// - Parameter overRendererImpl: 具体地图覆盖物Renderer实现 init(overRendererImpl: Any?) } 第2步：新增地图多边形实现、地图多边形Renderer实现 GaodePolygon（高德地图多边形）、GaodePolygonRenderer（高德地图多边形Renderer） BaiduPolygon（百度地图多边形）、BaiduPolygonRenderer（百度地图多边形Renderer） Polygon（通用多边形）、PolygonRenderer（通用多边形Renderer） /// 高德地图多边形 public class GaodePolygon: MAPolygon { /// 聚合多边形区域l weak var polygon: Polygon? } /// 高德地图多边形Renderer public class GaodePolygonRenderer: MAPolygonRenderer { } /// 百度多边形 public class BaiduPolygon: BMKPolygon { /// 聚合多边形区域l weak var polygon: Polygon? } /// 百度多边形Renderer public class BaiduPolygonRenderer: BMKPolygonView { } /// 通用多边形 public class Polygon: NSObject, OverlayProtocol { /// 具体地图覆盖物实现 private(set) public var overlayImpl: Any? /// 高德覆盖物 private var gaodePolygon: GaodePolygon? { if let overlay = overlayImpl as? GaodePolygon { return overlay } return nil } /// 百度覆盖物 private var baiduPolygon: BaiduPolygon? { if let overlay = overlayImpl as? BaiduPolygon { return overlay } return nil } /// 构造方法 /// - Parameter overlayImpl: 具体地图覆盖物实现 public required init(overlayImpl: Any?) { super.init() self.overlayImpl = overlayImpl gaodePolygon?.polygon = self baiduPolygon?.polygon = self } } /// 通用多边形Renderer public class PolygonRenderer: NSObject, OverlayRendererProtocol { /// 填充颜色,默认是kMAOverlayRendererDefaultFillColor public var fillColor: UIColor { get { if let overRenderer = gaodeOverRenderer { return overRenderer.fillColor } if let overRenderer = baiduOverRenderer { return overRenderer.fillColor } return .red } set { if let overRenderer = gaodeOverRenderer { overRenderer.fillColor = newValue } if let overRenderer = baiduOverRenderer { overRenderer.fillColor = newValue } } } /// 笔触颜色,默认是kMAOverlayRendererDefaultStrokeColor public var strokeColor: UIColor { get { if let overRenderer = gaodeOverRenderer { return overRenderer.strokeColor } if let overRenderer = baiduOverRenderer { return overRenderer.strokeColor } return .red } set { if let overRenderer = gaodeOverRenderer { overRenderer.strokeColor = newValue } if let overRenderer = baiduOverRenderer { overRenderer.strokeColor = newValue } } } ///笔触宽度, 单位屏幕点坐标，默认是0 public var lineWidth: CGFloat { get { if let overRenderer = gaodeOverRenderer { return overRenderer.lineWidth } if let overRenderer = baiduOverRenderer { return overRenderer.lineWidth } return 0 } set { if let overRenderer = gaodeOverRenderer { overRenderer.lineWidth = newValue } if let overRenderer = baiduOverRenderer { overRenderer.lineWidth = newValue } } } /// 具体地图覆盖物实现 public private(set) var overRendererImpl: Any? /// 高德地图覆盖物 private var gaodeOverRenderer: GaodePolygonRenderer? { return overRendererImpl as? GaodePolygonRenderer } /// 百度地图覆盖物 private var baiduOverRenderer: BaiduPolygonRenderer? { return overRendererImpl as? BaiduPolygonRenderer } /// 构造方法 /// - Parameter overRendererImpl: 具体地图覆盖物Renderer实现 public required init(overRendererImpl: Any?) { self.overRendererImpl = overRendererImpl } } 第3步：地图工厂标准修改及实现 /// 地图工厂标准 public protocol MapFactoryProtocol: NSObjectProtocol { ...... /// 根据经纬度坐标数据生成闭合多边形 /// - Parameters: /// - coordinates: 经纬度坐标点数据,coords对应的内存会拷贝,调用者负责该内存的释放 /// - count: 经纬度坐标点数组个数 /// - Returns: 新生成的多边形 func getPolygon(coordinates: UnsafeMutablePointer, count: UInt) -> Polygon /// 根据指定的多边形生成一个多边形Renderer /// - Parameter overlay: 指定的多边形数据对象 /// - Returns: 新生成的多边形Renderer func getPolygonRenderer(overlay: OverlayProtocol) -> OverlayRendererProtocol } /// 高德地图工厂 class GaodeMapFactory: NSObject, MapFactoryProtocol { ...... /// 根据经纬度坐标数据生成闭合多边形 /// - Parameters: /// - coordinates: 经纬度坐标点数据,coords对应的内存会拷贝,调用者负责该内存的释放 /// - count: 经纬度坐标点数组个数 /// - Returns: 新生成的多边形 func getPolygon(coordinates: UnsafeMutablePointer, count: UInt) -> Polygon { let polygonImpl = GaodePolygon(coordinates: coordinates, count: count) return Polygon(overlayImpl: polygonImpl) } /// 根据指定的多边形生成一个多边形Renderer /// - Parameter overlay: 指定的多边形数据对象 /// - Returns: 新生成的多边形Renderer func getPolygonRenderer(overlay: OverlayProtocol) -> OverlayRendererProtocol { var polygonImpl = GaodePolygon() if let overlayImplParam = overlay.overlayImpl as? GaodePolygon { polygonImpl = overlayImplParam } let polygonRendererImpl = GaodePolygonRenderer(overlay: polygonImpl) return PolygonRenderer(overRendererImpl: polygonRendererImpl) } } /// 百度地图工厂 class BaiduMapFactory: NSObject, MapFactoryProtocol { ...... /// 根据经纬度坐标数据生成闭合多边形 /// - Parameters: /// - coordinates: 经纬度坐标点数据,coords对应的内存会拷贝,调用者负责该内存的释放 /// - count: 经纬度坐标点数组个数 /// - Returns: 新生成的多边形 func getPolygon(coordinates: UnsafeMutablePointer, count: UInt) -> Polygon { let polygonImpl = BaiduPolygon(coordinates: coordinates, count: count) return Polygon(overlayImpl: polygonImpl) } /// 根据指定的多边形生成一个多边形Renderer /// - Parameter overlay: 指定的多边形数据对象 /// - Returns: 新生成的多边形Renderer func getPolygonRenderer(overlay: OverlayProtocol) -> OverlayRendererProtocol { var polygonImpl = BaiduPolygon() if let overlayImplParam = overlay.overlayImpl as? BaiduPolygon { polygonImpl = overlayImplParam } let polygonRendererImpl = BaiduPolygonRenderer(overlay: polygonImpl) return PolygonRenderer(overRendererImpl: polygonRendererImpl) } } 第4步：地图协议修改及实现 /// 地图协议 public protocol MapViewProtocol: NSObjectProtocol { ...... /// 向地图窗口添加一组Overlay，需要实现MAMapViewDelegate的-mapView:rendererForOverlay:函数来生成标注对应的Renderer /// - Parameter overlays: 要添加的overlay数组 func addOverlays(overlays: [OverlayProtocol]) /// 设置地图使其可以显示数组中所有的overlay, 如果数组中只有一个则直接设置地图中心为overlay的位置。 /// - Parameters: /// - overlays: 需要显示的overlays /// - animated: 是否执行动画 func showOverlays(overlays: [OverlayProtocol], animated: Bool) } /// 高德地图 class GaodeMapView: NSObject, MapViewProtocol { ...... /// 向地图窗口添加一组Overlay，需要实现MAMapViewDelegate的-mapView:rendererForOverlay:函数来生成标注对应的Renderer /// - Parameter overlays: 要添加的overlay数组 func addOverlays(overlays: [OverlayProtocol]) { var overlayImpls: [Any] = [] for overlay in overlays { if let overlayImpl = overlay.overlayImpl { overlayImpls.append(overlayImpl) } } mapView.addOverlays(overlayImpls) } /// 设置地图使其可以显示数组中所有的overlay, 如果数组中只有一个则直接设置地图中心为overlay的位置。 /// - Parameters: /// - overlays: 需要显示的overlays /// - animated: 是否执行动画 func showOverlays(overlays: [OverlayProtocol], animated: Bool) { var overlayImpls: [Any] = [] for overlay in overlays { if let overlayImpl = overlay.overlayImpl { overlayImpls.append(overlayImpl) } } mapView.showOverlays(overlayImpls, animated: animated) } } /// 高德地图代理实现 class MAMapViewDelegateImpl: NSObject, MAMapViewDelegate { ...... func mapView(_ mapView: MAMapView!, rendererFor overlay: MAOverlay!) -> MAOverlayRenderer! { if let polygonOverlay = overlay as? GaodePolygon, let poly = polygonOverlay.polygon { if let polygonRenderer = delegate?.mapView(mapViewProtocol, rendererFor: poly)?.overRendererImpl as? MAPolygonRenderer { return polygonRenderer } } return nil } } /// 百度地图 class BaiduMapView: NSObject, MapViewProtocol { ...... /// 向地图窗口添加一组Overlay，需要实现MAMapViewDelegate的-mapView:rendererForOverlay:函数来生成标注对应的Renderer /// - Parameter overlays: 要添加的overlay数组 func addOverlays(overlays: [OverlayProtocol]) { var overlayImpls: [BMKOverlay] = [] for overlay in overlays { if let overlayImpl = overlay.overlayImpl as? BMKOverlay { overlayImpls.append(overlayImpl) } } mapView.addOverlays(overlayImpls) } /// 设置地图使其可以显示数组中所有的overlay, 如果数组中只有一个则直接设置地图中心为overlay的位置。 /// - Parameters: /// - overlays: 需要显示的overlays /// - animated: 是否执行动画 func showOverlays(overlays: [OverlayProtocol], animated: Bool) { var overlayImpls: [BMKOverlay] = [] for overlay in overlays { if let overlayImpl = overlay.overlayImpl as? BMKOverlay { overlayImpls.append(overlayImpl) } } } } /// 百度地图代理实现 class BMKMapViewDelegateImpl: NSObject, BMKMapViewDelegate { ...... func mapView(_ mapView: BMKMapView!, viewFor overlay: BMKOverlay!) -> BMKOverlayView! { if let polygonOverlay = overlay as? BaiduPolygon, let poly = polygonOverlay.polygon { if let polygonRenderer = delegate?.mapView(mapViewProtocol, rendererFor: poly)?.overRendererImpl as? BMKPolygonView { return polygonRenderer } } return nil } } 第5步：测试页面 class PolygonViewController: UIViewController { /// 地图工厂 private lazy var factory: MapFactoryProtocol = { let factory = MapEngine.shared.getFactory()! return factory }() /// 地图实例 private lazy var mapView: MapViewProtocol = { // 显示地图 let mapView = factory.getMapView(frame: view.bounds) // 设置代理 mapView.delegate = self // 显示用户定位(放在设置代理之后，确保可以调用locationManager.requestAlwaysAuthorization()) mapView.showsUserLocation = true // 跟踪模式 mapView.userTrackingMode = .followWithHeading // 设置缩放 mapView.zoomLevel = 16 return mapView }() /// 标注数组 private lazy var polygons: Array = { var polygons = Array() var polygonCoordinates: [CLLocationCoordinate2D] = [ CLLocationCoordinate2D(latitude: 39.781892, longitude: 116.283413), CLLocationCoordinate2D(latitude: 39.787600, longitude: 116.391842), CLLocationCoordinate2D(latitude: 39.733187, longitude: 116.417932), CLLocationCoordinate2D(latitude: 39.704653, longitude: 116.338255)] let polygon = factory.getPolygon(coordinates: &polygonCoordinates, count: UInt(polygonCoordinates.count)) polygons.append(polygon) return polygons }() override func viewDidLoad() { super.viewDidLoad() view.backgroundColor = .white view.addSubview(mapView.getView()) } override func viewDidAppear(_ animated: Bool) { super.viewDidAppear(animated) mapView.addOverlays(overlays: polygons) mapView.showOverlays(overlays: polygons, animated: false) } } extension PolygonViewController: MapViewDelegateProtocol { func mapView(_ mapView: MapViewProtocol, rendererFor overlay: OverlayProtocol) -> OverlayRendererProtocol? { if overlay.isKind(of: Polygon.self) { let renderer = factory.getPolygonRenderer(overlay: overlay) renderer.lineWidth = 8.0 renderer.strokeColor = UIColor.magenta renderer.fillColor = UIColor.yellow.withAlphaComponent(0.4) return renderer } return nil } } "},"pages/share/wechat.html":{"url":"pages/share/wechat.html","title":"微信分享","keywords":"","body":"微信分享 微信开发平台 https://open.weixin.qq.com/ 官方Demo运行 1.在资源下载页面下载好最新的iOS开发工具包和范例代码。 2.将下载好的“OpenSDK1.8.9”拷贝至“ReleaseSample”下。 3.打开SDKSample，删除显示红色的“SDKExport”，右键工程文件通过“Add Files ...”将“OpenSDK1.8.9”导入工程。 4.点击SDKSample，选中Target,在Frameworks下增加WebKit.framework。 5.更新可用的bundleID及证书，真机就可以运行了。 当时最新的是（1.8.9版本，包含支付功能），这里选择包含支付版本的，因为官方Demo使用的是包含支付功能的。 “范例代码”不包含SDK，所以需要手动导入；“范例代码”没有依赖WebKit.framework,也需要手动导入；真机运行，需要更新可用的bundleID及证书。 新建工程 1.点击File，选择New->Project->App，一直Next直到完成。 2.删除SceneDelegate，直接Move To Trash。 3.删除AppDelegate中关于Scene的报错代码。 4.删除Info.plist的UIApplicationSceneManifest及Value。 5.在AppDelegate中增加如下代码。 var window: UIWindow? 没有第5步，工程运行会是黑屏，控制台会显示“The app delegate must implement the window property if it wants to use a main storyboard file.”。 集成OpenSDK（Swift） 官方接入点指南 1.首先新建Swift工程，工程名wechatshare，步骤如上。 2.右键“wechatshare”group，通过“Add Files ...”将“OpenSDK1.8.9”导入工程。 注意勾选“Copy items if needed”。 3.新建Swift桥接OC的文件，有一下两种方式： 3.1. 新建一个OC文件自动创建XXX-Bridging-Header.h。 3.2. 右键“wechatshare”group，New File...->Header File->wechatshare-Bridging-Header.h， 然后点击Target->Build Settings->搜索Objective-C Bridging Header->填写wechatshare/wecha tshare-Bridging-Header.h，这个是项目的相对路径。 4.测试sdk的使用 4.1.配置真机证书，设置iOS Deployment Target为真机支持的系统版本，新建工程的时候这里是最新的iOS版本，可能手机不是最新的，这里修改下就好。 4.2.wechatshare-Bridging-Header.h增加如下代码 #import \"WXApi.h\" 4.3.AppDelegate.swift增加如下代码 class AppDelegate: UIResponder, UIApplicationDelegate, WXApiDelegate { var window: UIWindow? func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool { // Override point for customization after application launch. WXApi.registerApp(\"\", universalLink: \"\") return true } } 4.4.运行工程，报错误： Undefined symbols for architecture arm64: \"operator delete[](void*)\", referenced from: +[WeChatApiUtil EncodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil NsDataEncodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil DecodeWithBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil DecodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) \"operator new[](unsigned long)\", referenced from: +[WeChatApiUtil EncodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil NsDataEncodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil DecodeWithBase64:] in libWeChatSDK.a(WeChatApiUtil.o) +[WeChatApiUtil DecodeBase64:] in libWeChatSDK.a(WeChatApiUtil.o) \"_OBJC_CLASS_$_WKWebView\", referenced from: objc-class-ref in libWeChatSDK.a(WapAuthHandler.o) \"_OBJC_CLASS_$_WKWebViewConfiguration\", referenced from: objc-class-ref in libWeChatSDK.a(WapAuthHandler.o) ld: symbol(s) not found for architecture arm64 clang: error: linker command failed with exit code 1 (use -v to see invocation) 4.5.修复报错： 开发者需要在工程中链接上:Security.framework，CoreGraphics.framework，WebKit.framework，libc++.tbd。 官方文档是上没有写需要接入libc++.tbd，但是实际是需要的，要不然编译会遇到错误 5.准备appid及universalLink 5.1.登录开发者者中心，确保app开启了Associated Domains的功能。 5.2.更新证书描述文件，确保包含了Associated Domains的功能。 5.4.在项目中配置applinks:xxxx。 5.5.去微信开发平台注册用户获得appid。 5.6.更新AppDelegate.swift class AppDelegate: UIResponder, UIApplicationDelegate, WXApiDelegate { var window: UIWindow? func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool { // Override point for customization after application launch. WXApi.registerApp(\"wx536bce0bc6a71ffd\", universalLink: \"https://huya.gq/demo.html\") return true } } // 处理universallink extension AppDelegate { func application(_ application: UIApplication, handleOpen url: URL) -> Bool { return WXApi.handleOpen(url, delegate: self) } func application(_ application: UIApplication, open url: URL, sourceApplication: String?, annotation: Any) -> Bool { return WXApi.handleOpen(url, delegate: self) } func application(_ application: UIApplication, continue userActivity: NSUserActivity, restorationHandler: @escaping ([UIUserActivityRestoring]?) -> Void) -> Bool { return WXApi.handleOpenUniversalLink(userActivity, delegate: self) } } 6.分享超链接 6.1.在ViewController.swift增加如下代码，点击按钮触发分享： /// 发送Link消息给微信 @IBAction @objc private func sendLinkContent() { let webpageObject = WXWebpageObject() webpageObject.webpageUrl = Constant.kLinkURL let message = WXMediaMessage() message.title = Constant.kLinkTitle; message.description = Constant.kLinkDescription; message.setThumbImage(UIImage(named: \"res2.png\")!) //分享后展示图片，没有就显示大大的问号图片。 message.mediaObject = webpageObject; let req = SendMessageToWXReq(); req.bText = false; req.message = message; req.scene = Int32(WXSceneSession.rawValue);//WXSceneSession; WXApi.send(req) { (result) in} } 6.2.发现报错： -canOpenURL: failed for URL: \"weixinULAPI://\" - error: \"This app is not allowed to query for scheme weixinulapi\" 6.3.解决报错 打开info.plist,增加如下代码： LSApplicationQueriesSchemes weixin weixinULAPI CFBundleURLTypes CFBundleURLName weixin CFBundleURLSchemes wx536bce0bc6a7**** 6.4.接着报错： *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '+[WXApi genExtraUrlByReq:withAppData:]: unrecognized selector sent to class 0x100286398' 6.5.解决报错 在你的工程文件中选择 Build Setting，在\"Other Linker Flags\"中加入\"-ObjC -all_load\" 代码 "},"pages/share/messenger.html":{"url":"pages/share/messenger.html","title":"Messenger分享","keywords":"","body":"Messenger分享 分享到 iOS 和 Android 版 Messenger iOS分享到国外社交APP调研 检查用户是否安装了iOS 9的Facebook Messenger 应用的 iOS9 兼容准备 遇到个问题：我之前以为Facebook和Messe是一个App，导致在没有装Messenger的情况一下一直分享报错：“说弹窗没有意义”，后来才发现我没有装Messenger，问题解决！ "},"pages/share/linkedin.html":{"url":"pages/share/linkedin.html","title":"LinkedIn分享","keywords":"","body":"LinkedIn分享 iOS LinkedIn登录及信息获取 如何使用 OAuth 2.0 将 LinkedIn 集成入 iOS 应用 linkedin开发者中心 通过iOS应用在LinkedIn上共享URL 如何使用 OAuth 2.0 将 LinkedIn 集成入 iOS 应用 "},"pages/ioscomponentization/Reference_link.html":{"url":"pages/ioscomponentization/Reference_link.html","title":"参考链接","keywords":"","body":"参考链接 iOS组件化之架构通用设计 写iOS SDK注意事项 BeeHive CocoaPods 创建私有仓库（ObjC） iOS组件化（下篇）-加载XIB、图片资源 iOS开发——组件化及去Mode化方案 CTModule Github搜索BundleName发现了不少好的仓库 基于CTMediator的组件化中间件 LocalizedStringKit UCPlanKit YXResources.h ZJBundleRes.h ImageCachePool.m LBResourceBundleManager NSBundle+DFBundle.h ZUXBundle.h NSBundle+DFBundle.h NSBundle+Resource.h NSBundle+Extension.h UIImage+QIBundle.m CCCommentsHeader.h UCUIMacro.h "},"pages/ioscomponentization/Create_private_library.html":{"url":"pages/ioscomponentization/Create_private_library.html","title":"1.创建私有库","keywords":"","body":"创建私有库 一、创建存放podsepc的私有仓库 在自己的git服务器中创建一个保存podspec的仓库：[repo name]。 创建完仓库之后获取仓库地址 在terminal中执行pod的指令将仓库添加到本地的pod repo中。 pod repo add [repo name] [url] 添加完成之后，在~/.cocoapods/repos中就可以看到名称为[repo name]的文件夹，这就是我们的私有pod仓库。 当然也可以使用pod repo remove [repo name]移除repo。 二、创建存放源码的仓库并推送到私有仓库 在git服务器上再创建一个仓库用于存放源代码。 在terminal中执行pod lib create [lib name]创建一个cocoapods的demo工程。 执行之后会从git克隆一个模板，并会问几个问题，依次按照需求选择即可，完成之后会打开一个Xcode project。 三、编辑podspec文件 Pod::Spec.new do |s| s.name = '名字' s.version = '版本号 需要和git tag保持一致' s.summary = '简述' # This description is used to generate tags and improve search results. # * Think: What does it do? Why did you write it? What is the focus? # * Try to keep it short, snappy and to the point. # * Write the description between the DESC delimiters below. # * Finally, don't worry about the indent, CocoaPods strips it! s.description = 'MIT', :file => 'LICENSE' } s.author = { 'changqing.chen' => 'chenchangqing198@126.com' } s.source = { :git => 'git仓库地址', :tag => s.version.to_s } # s.social_media_url = 'https://twitter.com/' s.ios.deployment_target = '9.0' s.source_files = 'utlogin/Classes/**/*' # s.resource_bundles = { # 'utlogin' => ['utlogin/Assets/*.png'] # } # s.public_header_files = 'Pod/Classes/**/*.h' # s.frameworks = 'UIKit', 'MapKit' # s.dependency 'AFNetworking', '~> 2.3' end 四、校验podspec文件 编辑完成之后，使用pod lib lint来验证podspec填写的准确性，可以选择参数: --verbose查看整个过程 --allow-warnings允许一些警告通过验证，如果验证出错，而project build success可以尝试添加这个参数 --source如果依赖的库是一个私有仓库创建的库，可以使用这个参数指定私有仓库的podspec仓库，除此之外最好将cocoapods公有库的source也指定一下 指定source pod lib lint --sources='[私有podsepec仓库地址],https://github.com/CocoaPods/Specs' --verbose --allow-warnings --no-clean 不指定source pod lib lint --verbose --allow-warnings --no-clean 执行问pod lib lint，看到[lib name] passed validation后就算通过校验。 五、推送至私有仓库 编辑.gitignore，将# Pods/修改为Pods/，忽略Pods文件夹，第三方代码就不会上传。 通过验证后，将源码推送至git仓库： git init git add . git commit -am 'desc' git remote add origin 'url' git push origin master git tag 'tag' git push --tags 将podsepc添加到私有repo中使用命令： pod repo push [repo name] [name.podspec] --verbose --allow-warnings "},"pages/ioscomponentization/Load_image_resources.html":{"url":"pages/ioscomponentization/Load_image_resources.html","title":"2.使用中间件","keywords":"","body":"3.加载图片资源 NSBundle+Resource.h #import NS_ASSUME_NONNULL_BEGIN @interface NSBundle (Resource) /** 获取资源的bundle @param className framework中的类名 @param bundleName 资源bundle的名字 */ + (instancetype)resourceBundleWithClassName:(NSString *)className bundleName:(NSString *)bundleName; /** 获取资源的bundle,默认bundle为当前类所在的bundle @param bundleName 资源bundle的名字 */ + (instancetype)resourceBundleWithBundleName:(NSString *)bundleName; /** 根据path来加载bundle 获取资源的bundle @param frameworkName 为资源所在的framework的名字 @param bundleName 资源bundle的名字 */ + (instancetype)resourceBundleWithFramework:(NSString *)frameworkName bundleName:(NSString *)bundleName; /** 这个方法会直接给出 framework的bundle @param className framework中的任意类名 */ + (instancetype)getFrameworkBundleWithClassName:(NSString *)className; /** 这个方法会直接给出 framework的bundle @param frameworkName framework的名称 */ + (instancetype)getFrameworkBundleWithFramework:(NSString *)frameworkName; /** 加载main bundle下 资源bundle @param bundleName 资源bundle名字 */ + (instancetype)getMainBundleWithResourceBundle:(NSString *)bundleName; @end NS_ASSUME_NONNULL_END NSBundle+Resource.m #import \"NSBundle+Resource.h\" @implementation NSBundle (Resource) + (instancetype)resourceBundleWithClassName:(NSString *)className bundleName:(NSString *)bundleName{ if (!className) { NSAssert(!className, @\"获取资源路径bundle时，类名为空\"); return nil; } NSBundle *frameworkBundle = [NSBundle bundleForClass:NSClassFromString(className)]; if (!frameworkBundle) { NSAssert(!className, @\"获取资源路径bundle时，获取framework的bundle为空\"); return nil; } NSURL *frameworkBundleUrl = [frameworkBundle URLForResource:bundleName withExtension:@\"bundle\"]; if (!frameworkBundleUrl) { NSAssert(!className, @\"获取资源路径bundle时，获取frameworkbundleURL为空\"); return nil; } return [self bundleWithURL:frameworkBundleUrl]; } + (instancetype)resourceBundleWithBundleName:(NSString *)bundleName{ return [self resourceBundleWithClassName:NSStringFromClass(self) bundleName:bundleName]; } + (instancetype)resourceBundleWithFramework:(NSString *)frameworkName bundleName:(NSString *)bundleName{ NSURL *frameworksURL = [[NSBundle mainBundle] URLForResource:@\"Frameworks\" withExtension:nil]; if (!frameworkName) { NSAssert(!frameworkName, @\"获取资源路径bundle时，frameworkName为空\"); } NSURL* libFrameworkURL = [frameworksURL URLByAppendingPathComponent:frameworkName]; libFrameworkURL = [libFrameworkURL URLByAppendingPathExtension:@\"framework\"]; NSBundle *libFrameworkBundle = [NSBundle bundleWithURL:libFrameworkURL]; if (!libFrameworkBundle) { NSAssert(!libFrameworkBundle, @\"获取资源路径bundle时，获取framework的bundle为空\"); } NSURL* libResourceBundleUrl = [libFrameworkBundle URLForResource:bundleName withExtension:@\"bundle\"]; return [self bundleWithURL:libResourceBundleUrl]; } + (instancetype)getFrameworkBundleWithClassName:(NSString *)className{ if (!className) { NSAssert(!className, @\"获取资源路径bundle时，类名为空\"); return nil; } NSBundle *frameworkBundle = [self bundleForClass:NSClassFromString(className)]; return frameworkBundle; } + (instancetype)getFrameworkBundleWithFramework:(NSString *)frameworkName{ NSURL *frameworksURL = [[NSBundle mainBundle] URLForResource:@\"Frameworks\" withExtension:nil]; if (!frameworkName) { NSAssert(!frameworkName, @\"获取资源路径bundle时，frameworkName为空\"); } NSURL* libFrameworkURL = [frameworksURL URLByAppendingPathComponent:frameworkName]; libFrameworkURL = [libFrameworkURL URLByAppendingPathExtension:@\"framework\"]; NSBundle *libFrameworkBundle = [self bundleWithURL:libFrameworkURL]; return libFrameworkBundle; } + (instancetype)getMainBundleWithResourceBundle:(NSString *)bundleName{ NSBundle *mainBundle = [NSBundle mainBundle]; NSURL *resourceUrl = [mainBundle URLForResource:bundleName withExtension:@\"bundle\"]; return [self bundleWithURL:resourceUrl]; } @end UIImage+Resource.h #import NS_ASSUME_NONNULL_BEGIN @interface UIImage (Resource) /** 通过图片名称，图片bundle所在framework的类名和图片所在bundle 来加载图片 @param imageName 图片名称 @param className 图片所在framework中任意一个类的名称 @param bundleName 图片所在 bundle */ + (UIImage *)imageNamed:(NSString *)imageName className:(NSString *)className bundleName:(NSString *)bundleName; /** 通过图片名称，图片所在framework的类名 此时图片直接位于framework bundle下 @param imageName 图片名称 @param className 图片所在framework中任意一个类的名称 */ + (UIImage *)imageNamed:(NSString *)imageName className:(NSString *)className; /** 通过图片名称，图片所在framework的类名 此时图片直接位于framework bundle下 @param imageName 图片名称 @param frameworkName 图片所在framework中任意一个类的名称 */ + (UIImage *)imageNamed:(NSString *)imageName frameworkName:(NSString *)frameworkName; /** 通过图片名称，图片bundle所在framework 和 图片所在的bundle 来加载图片 @param imageName 图片名称 @param frameworkName 框架名称 @param bundleName bundle 名字 */ + (UIImage *)imageNamed:(NSString *)imageName framework:(NSString *)frameworkName bundleName:(NSString *)bundleName; /** 加载main bundle下的图片 @param imageName 图片名称 @param bundleName bundle名称 */ + (UIImage *)imageNamed:(NSString *)imageName bundleName:(NSString *)bundleName; @end NS_ASSUME_NONNULL_END UIImage+Resource.m #import \"UIImage+Resource.h\" #import \"NSBundle+Resource.h\" @implementation UIImage (Resource) + (UIImage *)imageNamed:(NSString *)imageName className:(NSString *)className bundleName:(NSString *)bundleName{ NSBundle *bundle = [NSBundle resourceBundleWithClassName:className bundleName:bundleName]; UIImage *image = [UIImage imageNamed:imageName inBundle:bundle compatibleWithTraitCollection:nil]; return image; } + (UIImage *)imageNamed:(NSString *)imageName framework:(NSString *)frameworkName bundleName:(NSString *)bundleName{ NSBundle *bundle = [NSBundle resourceBundleWithFramework:frameworkName bundleName:bundleName]; UIImage *image = [UIImage imageNamed:imageName inBundle:bundle compatibleWithTraitCollection:nil]; return image; } + (UIImage *)imageNamed:(NSString *)imageName className:(NSString *)className{ NSBundle *bundle = [NSBundle getFrameworkBundleWithClassName:className]; UIImage *image = [UIImage imageNamed:imageName inBundle:bundle compatibleWithTraitCollection:nil]; return image; } + (UIImage *)imageNamed:(NSString *)imageName frameworkName:(NSString *)frameworkName{ NSBundle *bundle = [NSBundle getFrameworkBundleWithFramework:frameworkName]; UIImage *image = [UIImage imageNamed:imageName inBundle:bundle compatibleWithTraitCollection:nil]; return image; } + (UIImage *)imageNamed:(NSString *)imageName bundleName:(NSString *)bundleName{ if (!imageName || !bundleName) { NSAssert(!imageName || !bundleName, @\"image name 或者 bundle name 为空\"); return nil; } NSBundle *bundle = [NSBundle getMainBundleWithResourceBundle:bundleName]; UIImage *image = [UIImage imageNamed:imageName inBundle:bundle compatibleWithTraitCollection:nil]; return image; } @end "},"pages/ioscomponentization/Use_middleware.html":{"url":"pages/ioscomponentization/Use_middleware.html","title":"3.加载图片资源","keywords":"","body":"2.使用中间件 CTMediator 本文只介绍ObjC如何使用CTMediator。 一、新建业务组件Target 通过下载CTMediator的Demo，每个组件都会有一个Target的类，用于提供对外方法的实现。 Target类的命名规则：“Target_[lib name]”，例如：Target_A，A是组件名称，Target是固定不变的。 Target方法命名规则：“Action_[method name]”，例如：Action_ViewController，ViewController是方法名称，Action是固定不变的。 @implementation Target_testlib - (UIViewController *)Action_OneKeyLoginVC:(NSDictionary *)params { typedef void (^CallbackType)(NSString *); CallbackType callback = params[@\"callback\"]; if (callback) { callback(@\"success\"); } UTOneKeyLoginVC *viewController = [[UTOneKeyLoginVC alloc] init]; return viewController; } @end 二、新建业务组件对外接口 CTMediator是通过Target-Action的方式实现，所以我们只需要给CTMediator增加Category，在Category写组件对外的方法和实现即可。 在git服务器创建中间件仓库，通过pod lib create [中间件名称]创建中间件工程。 编辑podspec文件，增加s.dependency 'CTMediator'，然后执行pod install，完成CTMediator的依赖。 通过新增CTMediator的分类，实现组件对外的接口。 分类的命名：CTMediator+[lib name]，例如：CTMediator+testlib.h。 方法的命名：[lib name]_[method name]，例如：utlogin_OneKeyLoginVCWithCallback。 为CTMediator增加分类，编写业务组件的接口方法： @implementation CTMediator (utlogin) /// 一键登录页面 /// @param callback 回调 - (UIViewController *)utlogin_OneKeyLoginVCWithCallback:(void(^)(NSString *result))callback; { NSMutableDictionary *params = [[NSMutableDictionary alloc] init]; params[@\"callback\"] = callback; return [self performTarget:@\"utlogin\" action:@\"OneKeyLoginVC\" params:params shouldCacheTarget:NO]; } @end 这样就完成了组件对外的接口了，宿主工程可以通过依赖中间件直接调用到组件的Target_testlib的实现。 三、新建宿主工程 使用xcode创建宿主工程，在通过pod init创建Podfile文件。 using-cocoapods 编辑Podfile文件： # Uncomment the next line to define a global platform for your project # platform :ios, '9.0' source '私有仓库地址' source 'https://github.com/CocoaPods/Specs.git' target 'utopia' do # Comment the next line if you don't want to use dynamic frameworks use_frameworks! use_modular_headers! # Pods for utopia pod 'testlib'#业务组件 pod 'testmediatro'#中间件 end 执行pod install。 "},"pages/rxswift/01_Observable_observable_sequence.html":{"url":"pages/rxswift/01_Observable_observable_sequence.html","title":"1.Observable - 可监听序列","keywords":"","body":"1.Observable - 可监听序列 一、Observable 介绍 Observable 作为 Rx 的根基，我们首先对它要有一些基本的了解。 iOS-RxSwift-Tutorials 1.1 Observable Observable 这个类就是 Rx 框架的基础，我们可以称它为可观察序列。它的作用就是可以异步地产生一系列的 Event（事件），即一个 Observable 对象会随着时间推移不定期地发出 event(element : T) 这样一个东西。 而且这些 Event 还可以携带数据，它的泛型 就是用来指定这个 Event 携带的数据的类型。 有了可观察序列，我们还需要有一个 Observer（订阅者）来订阅它，这样这个订阅者才能收到 Observable 不时发出的 Event。 1.2 Event 查看 RxSwift 源码可以发现，事件 Event 的定义如下： public enum Event { /// Next element is produced. case next(Element) /// Sequence terminated with an error. case error(Swift.Error) /// Sequence completed successfully. case completed } 可以看到 Event 就是一个枚举，也就是说一个 Observable 是可以发出 3 种不同类型的 Event 事件： next：next 事件就是那个可以携带数据 的事件，可以说它就是一个“最正常”的事件。 参考文章 原文出自：www.hangge.com 转载请保留原文链接：https://www.hangge.com/blog/cache/detail_1922.html RxSwift中文文档 "},"pages/rxswift/02_Observable_Subscription_Event_Monitoring_Subscription_Destruction.html":{"url":"pages/rxswift/02_Observable_Subscription_Event_Monitoring_Subscription_Destruction.html","title":"2.Observable - 订阅、事件监听、订阅销毁","keywords":"","body":"2.Observable - 订阅、事件监听、订阅销毁 参考文章 Swift - RxSwift的使用详解1（基本介绍、安装配置） RxSwift中文文档 "},"pages/rxswift/03_Observer_Observer.html":{"url":"pages/rxswift/03_Observer_Observer.html","title":"3.Observer - 观察者","keywords":"","body":"3.Observer - 观察者 参考文章 Swift - RxSwift的使用详解1（基本介绍、安装配置） RxSwift中文文档 "},"pages/rxswift/06_Transformation_operator_buffer_map_flatMap_scan_etc.html":{"url":"pages/rxswift/06_Transformation_operator_buffer_map_flatMap_scan_etc.html","title":"6.变换操作符：buffer、map、flatMap、scan等","keywords":"","body":"6.变换操作符：buffer、map、flatMap、scan等 参考文章 为什么RxSwift也需要flatMap "},"pages/rxswift/07_Filter_operator_filter_take_skip_etc.html":{"url":"pages/rxswift/07_Filter_operator_filter_take_skip_etc.html","title":"7.过滤操作符：filter、take、skip等","keywords":"","body":"7.过滤操作符：filter、take、skip等 参考文章 RxSwift 操作符 "},"pages/push/Reference_link.html":{"url":"pages/push/Reference_link.html","title":"相关链接","keywords":"","body":"相关链接 客户端技术：一文带你了解 iOS 消息推送机制 iOS开发——iOS静默推送介绍及使用场景 iOS10 推送extension之 Service Extension你玩过了吗？ 极光 "},"pages/other/Understanding_the_MVVM_architectural_pattern.html":{"url":"pages/other/Understanding_the_MVVM_architectural_pattern.html","title":"理解MVVM架构模式","keywords":"","body":"理解MVVM架构模式 后续补充自己的理解与实战 参考文章 iOS 关于MVC和MVVM设计模式的那些事 iOS 关于MVVM Without ReactiveCocoa设计模式的那些事 iOS 关于MVVM With ReactiveCocoa设计模式的那些事 iOS 搭建App框架（MVVM+RAC+路由） 说说MVVM "},"pages/other/Client_network_logic_organization_and_suggestions.html":{"url":"pages/other/Client_network_logic_organization_and_suggestions.html","title":"客户端网络逻辑整理及建议","keywords":"","body":"客户端网络逻辑整理及建议 第一次进入页面显示占位 下面是已有逻辑，以后保持目前逻辑，统一处理。 无网络（没有Wifi，没有4G），期望提示语：No network。 服务端报错，期望提示语：Loading failed。 服务端返回空，期望提示语：No data。 第一次进入页面loading显示 loading显示场景不明确，导致开发自己想象，比如进入“消息中心”有loading，进入“首页”没有。 loading样式根据场景做不同的区别，比如第一次进入页面与点击事件的loading可以有不同的视觉效果。 loading开始显示与结束的逻辑需要明确，比如一个页面加载有2个请求，是等一个请求完成后消失还是两个都完成后消失，比如：首页。 点击事件及反馈 明确是否需要增加loading等待，比如目前逻辑：点赞不显示loading，发布动态显示loading。 涉及网络请求的点击，反馈提示不明确，应该如何提示，个人认为如果产品没有定义，客户端不应该存在提示，因为客户端也不知道如何提示。 允许用户最大的等待时间不明确，目前客户端统一的超时时间是30s，是否合理，有待确定。 统一错误提示需要明确，比如：网络超时，没有网络，服务端报错，根据不同错误码给出指定提示。 重复点击处理，用户可能重复发出多个事件及请求。 网络监听处理 用户第一次进入页面，没有网络，页面显示了空占位，是否通过网络监听自动重新加载数据。 用户看视频的时候，是否提示用户正在使用4g。 上拉下拉刷新 当页面已经有数据的情况，这个时候下拉刷新，服务端报错，这个时候是显示空还是继续显示数据，是否给出错误提示。 当页面已经有数据的情况，这个时候上拉加载更多，服务端报错，是否给出错误提示。 Toast重叠 当用点击了两个有提示的按钮，可能导致Toast重叠。 App弹窗显示逻辑处理，当App首页存在多个窗口的时候，那么需要处理。 日志完善 确保每个请求都在控制台有日志打印，目前有些网络请求没有。 调试日志完善，GWLog增加上报或者支持沙盒。 "},"pages/other/Problem_record.html":{"url":"pages/other/Problem_record.html","title":"问题记录","keywords":"","body":"问题记录 iOS 13.1.3下，修复UITextView富文本点击产生多次回调 创建富文本 // 删除 let result = NSMutableAttributedString(string: \" \\(GWLocalized.delete)\") let range = NSRange(location: 0, length: result.string.count) result.addAttribute(NSAttributedString.Key.link, value: \"Delete://\", range: range) result.addAttribute(NSAttributedString.Key.font, value: UIFont(regularFontWithSize: level == 1 ? 16 : 14)!, range: range) textView?.tintColor = UIColor(hexString: \"#848484\") 点击富文本 // 点击事件 func textView(_ textView: UITextView, shouldInteractWith URL: URL, in characterRange: NSRange, interaction: UITextItemInteraction) -> Bool { // fix:iphone 7 plus 13.1.3活动的评论，点击删除，弹出三个关闭框 var recognizedTapGesture = false for ges in textView.gestureRecognizers ?? [] { if let tapGes = ges as? UITapGestureRecognizer, tapGes.state == .ended { recognizedTapGesture = true } } if !recognizedTapGesture { return true } if URL.scheme == \"Delete\" { let commentId = reactor?.currentState.comment.commentId let level = reactor?.currentState.level reactor?.action.onNext(.deleteComment(commentId: commentId, level: level)) } return true } "},"pages/other/Exception_placeholder_image_package.html":{"url":"pages/other/Exception_placeholder_image_package.html","title":"异常占位图的封装","keywords":"","body":"解决问题 统一的异常占位处理 代码复用性 一、占位图封装 import Foundation import GWI18n import MJRefresh extension UIView { private enum RuntimeKey { static let emptyView = UnsafeRawPointer(bitPattern: \"emptyView\".hashValue) } } public extension UIView { enum EmptyType { case custom(image: UIImage?, title: String?, top: CGFloat = 0) // 自定义 case empty(Bool = true) // 无数据 case netError // 无网络 case serverError // 服务器错误 } func showEmptyData(type: EmptyType = .empty(), reloadBlock: ((UIButton) -> Void)? = nil) { var emptyView: EmptyDataView! switch type { case let .empty(showReloadBt): emptyView = EmptyDataView(frame: CGRect(x: 0, y: 0, width: self.width, height: self.height)) emptyView.logo_ImgV.image = UIImage(inUtilCore: \"no_content\") emptyView.desc_Lb.text = GWI18n.R.string.localizable.base_date_empty() if !showReloadBt { emptyView.reload_Btn.isHidden = true } else { emptyView.reload_Btn.isHidden = false } case .netError: emptyView = EmptyDataView(frame: CGRect(x: 0, y: 0, width: self.width, height: self.height)) emptyView.logo_ImgV.image = UIImage(inUtilCore: \"no_network\") emptyView.desc_Lb.text = GWI18n.R.string.localizable.base_net_lost() case .serverError: emptyView = EmptyDataView(frame: CGRect(x: 0, y: 0, width: self.width, height: self.height)) emptyView.logo_ImgV.image = UIImage(inUtilCore: \"load_failure\") emptyView.desc_Lb.text = GWI18n.R.string.localizable.base_loading_failed() case let .custom(image, title, top): emptyView = EmptyDataView(frame: CGRect(x: 0, y: top, width: self.width, height: self.height - top)) emptyView.logo_ImgV.image = image emptyView.desc_Lb.text = title } emptyView.backgroundColor = .white emptyView.reload_block = reloadBlock emptyView.reload_Btn.setTitle(GWI18n.R.string.localizable.base_re_load(), for: .normal) if let scrollView = self as? UIScrollView { scrollView.mj_footer?.isHidden = true } self.emptyDataView = emptyView } func hideEmptyData() { if let scrollView = self as? UIScrollView { scrollView.mj_footer?.isHidden = false } self.emptyDataView = nil } private var emptyDataView: EmptyDataView? { set(newValue) { self.emptyDataView?.removeFromSuperview() objc_setAssociatedObject(self, RuntimeKey.emptyView!, newValue, .OBJC_ASSOCIATION_RETAIN_NONATOMIC) guard let newView = newValue else { return } self.addSubview(newView) self.bringSubviewToFront(newView) } get { return objc_getAssociatedObject(self, RuntimeKey.emptyView!) as? EmptyDataView } } } 二、使用方法 /// 请求失败时显示默认占位图 self.tableView.showEmptyData(type: .netError) { [weak self] _ in /// 重新请求 DispatchQueue.main.asyncAfter(deadline: DispatchTime.now() + 1) { /// 隐藏占位图 self?.tableView.hideEmptyData() } } 三、使用方法（新增） 基于GWNetwork的GWError重构，当网络请求时，如果没有网络会立马返回noNetwork的错误，所有增加传入error显示异常占位的方法: // 根据错误显示占位 func showEmptyData(error: Swift.Error, reloadBlock: ((UIButton) -> Void)? = nil) { if let gwError = error as? GWError { // 服务器报错 var emptyType: UIView.EmptyType = .serverError switch gwError { case let .client(cError): // 无网络 if cError == .noNetwork { emptyType = .netError } break default: break } self.showEmptyData(type: emptyType, reloadBlock: reloadBlock) } else { self.showEmptyData(type: .serverError, reloadBlock: reloadBlock) } } 如何使用： }).catchError {[weak tableView, weak self] (error) -> Observable in GWSwiftSpinner.hide() tableView?.endLoadMore() GWToast(error.localizedDescription) // 空视图显示 tableView?.showEmptyData(error: error, reloadBlock: { (_) in self?.action.onNext(.setup(messageTypeId: messageTypeId, page: page, tableView: tableView)) }) return PublishSubject() } "},"pages/other/RxSwift_s_Timer.html":{"url":"pages/other/RxSwift_s_Timer.html","title":"RxSwift的Timer","keywords":"","body":"RxSwift的Timer // MARK: - 消息功能 private let queryUnreadMsgTimerStopped = BehaviorRelay(value: false) // 开启计时器 private func startQueryUnreadMsgTimer() { guard let reactor = self.reactor else { return } // 定时请求未读消息 queryUnreadMsgTimerStopped.accept(false) Observable.interval(DispatchTimeInterval.seconds(60), scheduler: MainScheduler.instance).startWith(0).takeWhile{_ in UserManager.isLogin }.map{_ in GWHHomeReactor.Action.queryUnreadCount }.takeUntil(queryUnreadMsgTimerStopped.asObservable().filter{ $0 }).bind(to: reactor.action).disposed(by: disposeBag) } // 关闭计时器 private func stopQueryUnreadMsgTimer() { queryUnreadMsgTimerStopped.accept(true) } "},"pages/other/Use_of_AppTimer.html":{"url":"pages/other/Use_of_AppTimer.html","title":"AppTimer的使用","keywords":"","body":"AppTimer的使用 技术：RxSwift实现Timer。 场景：为了解决App多个组件（爱车、社区）分别使用Timer，造成资源浪费的问题，在GWCommonComponent写了一个AppTimer。 功能 在App启动时调用，跟随App生命周期，目前会发布1s,10s,60s的通知。 当App进入后台停止计时，当App将要进入前台重新开始计时。 如何使用 务必在每个国家的宿主工程AppLaunch（壳工程调用）。 func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool { ...... window?.rootViewController = tabBarController window?.makeKeyAndVisible() AppTimer.shared.start() return true } 源码 import UIKit import RxSwift import GWUtilCore /** 跟随App生命周期的Timer 业务：消息中心，爱车 */ public class AppTimer { public static let shared = AppTimer() private let timer = Observable.interval(.seconds(1), scheduler: MainScheduler.instance) private var disposable: Disposable? /// AppTimer到1s的序列 public let reach1sObservable = PublishSubject() /// AppTimer到10s的序列 public let reach10sObservable = PublishSubject() /// AppTimer到60s的序列 public let reach60sObservable = PublishSubject() init() { _ = NotificationCenter.default.rx.notification(UIApplication.willEnterForegroundNotification).subscribe(onNext: {[weak self] _ in GWLog(\"----- App进入前台 ------\") self?.start() }) _ = NotificationCenter.default.rx.notification(UIApplication.didEnterBackgroundNotification).subscribe(onNext: {[weak self] _ in GWLog(\"----- App进入后台 ------\") self?.stop() }) } /// 开启计时器 public func start() { stop() GWLog(\"----- 开始AppTimer订阅 ------\") disposable = timer.subscribe(onNext: {[weak self] count in GWLog(\"----- 当前时间\\(count+1)s ------\") // 1s通知 NotificationCenter.default.post(name: NSNotification.Name.appTimerReach1s, object: nil) self?.reach1sObservable.onNext(()) // 10s通知 if (count + 1) % 10 == 0 { NotificationCenter.default.post(name: NSNotification.Name.appTimerReach10s, object: nil) self?.reach10sObservable.onNext(()) } // 60s通知 if (count + 1) % 60 == 0 { NotificationCenter.default.post(name: NSNotification.Name.appTimerReach60s, object: nil) self?.reach60sObservable.onNext(()) } }, onDisposed: { GWLog(\"----- 释放AppTimer订阅 ------\") }) } /// 停止计时器 public func stop() { disposable?.dispose() } } "},"pages/other/Rx_extension_for_UIViewController.html":{"url":"pages/other/Rx_extension_for_UIViewController.html","title":"UIViewController的常用Rx扩展","keywords":"","body":"UIViewController的常用Rx扩展 https://www.hangge.com/blog/cache/detail_1943.html "},"pages/other/Info_plist_internationalization.html":{"url":"pages/other/Info_plist_internationalization.html","title":"Info.plist国际化","keywords":"","body":"Info.plist国际化 新建一个InfoPlist.stings文件，配置不同的语言。 如何新建 New File -> Resource下的Strings File。 参考链接 3分钟实现iOS语言本地化/国际化（图文详解） how to localise the ios-info-plist-file iOS调用系统相册\\相机 显示中文标题 iOS设置默认语言 解决iOS9/10简体中文,InfoPlist.string权限描述国际化显示问题 "},"pages/other/Unify_the_page_title_font_style.html":{"url":"pages/other/Unify_the_page_title_font_style.html","title":"统一设置页面title字体样式","keywords":"","body":"统一设置页面title字体样式 方案实现 给UINavigationBar增加setTitleFont方法的扩展。 extension UINavigationBar { /// Set Navigation Bar title, title color and font. /// /// - Parameters: /// - font: title font /// - color: title text color (default is .black). public func setTitleFont(_ font: UIFont, color: UIColor = .black) { var attrs = [NSAttributedString.Key: Any]() attrs[.font] = font attrs[.foregroundColor] = color self.titleTextAttributes = attrs } } 如何使用 在创建UINavigationController的时候，手动设置title字体样式。 navVc.navigationBar.setTitleFont(.appMediuFont(16)) 统一设置导航栏背景颜色、标题颜色和大小、状态栏文本颜色 "},"pages/other/Business_components_are_sub_library_by_country.html":{"url":"pages/other/Business_components_are_sub_library_by_country.html","title":"业务组件按国家子库化","keywords":"","body":"业务组件按国家子库化 如何解决多个国家并行开发？ 一、解决方案 方案一：不同国家不同分支 一个国家对应一个分支，对应国家的宿主工程依赖国家分支。 方案二：多个国家多个子库 按国家新建不同的子库，并且新建不同国家的target分别依赖子库。 对比两个方案，个人倾向方案二 方案一修改代码比较麻烦，不利于代码复用。 二、新建子库 下面以GWHModuleHome为例： 2.1 整理项目文件夹，按国家划分 在Classes下分别新建Core/Thailand/Russia文件夹。 将Classes原来所有文件及文件夹放入Core Thailand和Russia随便放一个swift文件，为了pod install可以显示这两个子库。 2.2 修改GWHModuleHome.podspec 设置默认子库Core。 新建Core/Thailand/Russia子库，源文件及资源文件路径一定要正确。 执行pod install，这样就完成子库新建。 Pod::Spec.new do |s| ..... s.default_subspec = 'Core' s.subspec 'Core' do |ss| ss.source_files = 'GWHModuleHome/Classes/Core/**/*' ss.resource_bundles = { 'GWHModuleHome' => [ 'GWHModuleHome/Assets/**/{*}', 'GWHModuleHome/Classes/**/{*.storyboard,*.xib}' ] } ss.dependency 'GWHModuleMine' ss.dependency 'GWNetWork' ss.dependency 'GWCommonComponent' ss.dependency 'GWUserCenterBase' ss.dependency 'Aquaman' ss.dependency 'Trident' end s.subspec 'Russia' do |ss| ss.source_files = 'GWHModuleHome/Classes/Russia/**/*' ss.dependency 'GWHModuleHome/Core' end s.subspec 'Thailand' do |ss| ss.source_files = 'GWHModuleHome/Classes/Thailand/**/*' ss.dependency 'GWHModuleHome/Core' end end 三、新建target 分别参考下面的链接： Xcode 一个项目下创建多个Target 配置多个target及多个Target的podfile文件配置 Xcode中Active Compilation Conditions和Preprocessor Macros的区别 "},"pages/other/App_permission_management.html":{"url":"pages/other/App_permission_management.html","title":"App权限管理","keywords":"","body":"App权限管理 GWUtilCore组件增加PrivacyManager，统一App内权限获取。 自定义隐私类型 目前App只涉及3种权限,定义如下： /* * 隐私权限类型 1. 在Info.plist文件中配置应用所需权限； 2. 在项目的Targets->Capabilities中开启相应开关，目前Siri、Health、NFC、HomeKit需要开启； 3. 引入相关库； 4. 使用代码获取对应的隐私权限。 */ public enum PrivacyType { // 相机 Authorized Denied Unsupported Unkonw case camera // 相册 NotDetermined Authorized Denied Restricted Unkonw case photos // 定位 NotDetermined Unkonw Denied Authorized:AuthorizedAlways||AuthorizedWhenInUse case location public var title: String { ...... } public var subTitle: String { ...... } public static var all :[PrivacyType] { return [.location , .camera , .photos] } } 自定义权限类型 将权限结果统一包装为自定义的类型，定义如下： /// 获取隐私权限结果 public enum AuthorizationStatus{ // 未知 case unknow // 授权 case authorized // 拒绝 case denied // 不支持 case unsupported // 未决定（相册，地理位置） case notDetermined public var title: String { ...... } public var titleColor: UIColor? { ...... } } 隐私权限管理 PrivacyManager提供两个请求隐私权限的类方法,定义如下： synRequestAccess方法为同步方法，可以同步获取指定隐私的权限。 /// 同步请求隐私权限 /// - Parameter type: 隐私权限类型 /// - Returns: 隐私权限 public static func synRequestAccess(_ type: PrivacyType) -> AuthorizationStatus { switch type { case .photos: return shared.requestAccessPhotos(nil) case .location: return shared.requestAccessLocation(nil) case .camera: return shared.requestAccessCamera(nil) } } asynRequestAccess方法为异步方法，通过传入completionHandle异步获取隐私的权限，如果是第一次会触发询问窗口，当用户决定后，completionHandle会将决定后的结果返回。 /// 异步请求隐私权限 /// - Parameters: /// - type: 隐私权限类型 /// - completionHandle: 隐私权限结果代码块 public static func asynRequestAccess(_ type: PrivacyType , completionHandle: ((AuthorizationStatus) -> Void)?) { switch type { case .photos: _ = shared.requestAccessPhotos(completionHandle) break case .location: _ = shared.requestAccessLocation(completionHandle) break case .camera: _ = shared.requestAccessCamera(completionHandle) break } 请求权限实现 extension PrivacyManager: CLLocationManagerDelegate { /// 在主线程回调隐私权限结果 /// - Parameters: /// - status: 隐私权限结果 /// - completionHandle: 隐私权限结果代码块 private func response(status: AuthorizationStatus, completionHandle: ((AuthorizationStatus) -> Void)?) { DispatchQueue.main.async { completionHandle?(status) } } // MARK: 请求相机访问权限 func requestAccessCamera(_ completionHandle: ((AuthorizationStatus) -> Void)?) -> AuthorizationStatus { if UIImagePickerController.isSourceTypeAvailable(.camera) { let status = AVCaptureDevice.authorizationStatus(for: .video) if status == .notDetermined { if completionHandle != nil { AVCaptureDevice.requestAccess(for: .video) { [weak self] (granted) in if granted{ self?.response(status: .authorized, completionHandle: completionHandle) } else { self?.response(status: .denied, completionHandle: completionHandle) } } } return .notDetermined } else if status == .authorized{ response(status: .authorized, completionHandle: completionHandle) return .authorized } else if status == .denied || status == .restricted{ response(status: .denied, completionHandle: completionHandle) return .denied } else { response(status: .unknow, completionHandle: completionHandle) return .unknow } } else{ response(status: .unsupported, completionHandle: completionHandle) return .unsupported } } // MARK: 请求相册权限 func requestAccessPhotos(_ completionHandle: ((AuthorizationStatus) -> Void)?) -> AuthorizationStatus { let status = PHPhotoLibrary.authorizationStatus() if status == .notDetermined { if completionHandle != nil { PHPhotoLibrary.requestAuthorization { [weak self] (status) in if status == .notDetermined{ self?.response(status: .notDetermined, completionHandle: completionHandle) } else if status == .denied || status == .restricted { self?.response(status: .denied, completionHandle: completionHandle) } else{ self?.response(status: .authorized, completionHandle: completionHandle) } } } return .notDetermined } else if status == .authorized { response(status: .authorized, completionHandle: completionHandle) return .authorized } else if status == .denied || status == .restricted { response(status: .denied, completionHandle: completionHandle) return .denied } else{ response(status: .unknow, completionHandle: completionHandle) return .unknow } } // MARK: 请求定位权限 func requestAccessLocation(_ completionHandle: ((AuthorizationStatus) -> Void)?) -> AuthorizationStatus { let status = CLLocationManager.authorizationStatus() let serviceEnabled = CLLocationManager.locationServicesEnabled() if !serviceEnabled { return .unsupported } else if status == .notDetermined { if completionHandle != nil { PrivacyManager.shared.locationCompletionBlock = completionHandle let loc = CLLocationManager() loc.delegate = PrivacyManager.shared loc.requestWhenInUseAuthorization() loc.startUpdatingLocation() PrivacyManager.shared.locationManager = loc } return .notDetermined } else if status == .denied || status == .restricted { response(status: .denied, completionHandle: completionHandle) return .denied } else if status == .authorizedAlways || status == .authorizedWhenInUse{ response(status: .authorized, completionHandle: completionHandle) return .authorized } else{ response(status: .unknow, completionHandle: completionHandle) return .unknow } } public func locationManager(_ manager: CLLocationManager, didChangeAuthorization status: CLAuthorizationStatus) { switch status { case .notDetermined: response(status: .notDetermined, completionHandle: PrivacyManager.shared.locationCompletionBlock) break case .denied, .restricted: response(status: .denied, completionHandle: PrivacyManager.shared.locationCompletionBlock) PrivacyManager.shared.locationCompletionBlock = nil break case .authorizedAlways, .authorizedWhenInUse: response(status: .authorized, completionHandle: PrivacyManager.shared.locationCompletionBlock) PrivacyManager.shared.locationCompletionBlock = nil break @unknown default: response(status: .unknow, completionHandle: PrivacyManager.shared.locationCompletionBlock) PrivacyManager.shared.locationCompletionBlock = nil break } } } "},"pages/other/App_positioning_management.html":{"url":"pages/other/App_positioning_management.html","title":"App定位管理","keywords":"","body":"App定位管理 项目中商城及爱车分别使用到了系统定位，故将定位管理类从地图中抽离出来。 定位管理作为单例，提供“开始定位”、“停止定位”、“定位是否可用”、“检索地址”等方法。 细节：开始定位的时候，需要使用到PrivacyManager的异步请求权限的方法，第一次使用的时候，会有询问弹窗，用户决定授权后，会继续定位，否则定位失败。 2021.1.21:新增func startLocating(subLocManager: SubCLLocationManager),支持同时调用多次定位,定位完成subLocManager的locatingCompletion会被调用。 import UIKit import CoreLocation /// 系统定位的子类 public class SubCLLocationManager: CLLocationManager { /// 定位完成 public var locatingCompletion: ((CLLocation?) -> Void)? public override init() { super.init() desiredAccuracy = kCLLocationAccuracyBest } } /// 自定义定位代理 public protocol LocationManagerDelegate: NSObjectProtocol { /// 更新定位 /// - Parameter location: 系统定位 func onLocationUpdated(location: CLLocation) /// 定位失败 func onLocationFailure() } public extension LocationManagerDelegate { func onLocationUpdated(location: CLLocation) { } func onLocationFailure() { } } /// 自定义定位管理 public class LocationManager: NSObject { /// 代理 public weak var delegate: LocationManagerDelegate? /// 系统用户定位 public var userLocation: CLLocation? /// 系统定位管理 private let locationManager = SubCLLocationManager() private var tempManagers = [SubCLLocationManager]() /// 单例 public static let shared = LocationManager() /// 开始定位 public func startLocating() { startLocating(subLocManager: locationManager) } /// 开始定位 /// - Parameter subLocManager: 指定定位管理 public func startLocating(subLocManager: SubCLLocationManager) { tempManagers.append(subLocManager) PrivacyManager.asynRequestAccess(.location) {[weak self] (status) in switch status { case .authorized: subLocManager.delegate = self subLocManager.startUpdatingLocation() break default: subLocManager.delegate = self subLocManager.locatingCompletion?(nil) self?.delegate?.onLocationFailure() self?.remove(manager: subLocManager) break } } } /// 停止定位 public func stopLocating() { stopLocating(subLocManager: locationManager) } /// 停止定位 /// - Parameter subLocManager: 指定定位管理 public func stopLocating(subLocManager: SubCLLocationManager) { subLocManager.stopUpdatingLocation() remove(manager: subLocManager) } /// 移除指定定位管理 /// - Parameter manager: 指定定位管理 private func remove(manager: SubCLLocationManager) { if let index = tempManagers.firstIndex(of: manager) { tempManagers.remove(at: index) } } /// 定位是否可用 /// - Returns: 是否可用 public func locationServiceEnable() -> Bool { return PrivacyManager.synRequestAccess(.location) == .authorized } /// 检索地址 /// - Parameter location: 定位 /// - Returns: 地址 public func reverseGeocodeLocation(_ location: CLLocation, completionHandler: ((String?) -> Void)?) { CLGeocoder().reverseGeocodeLocation(location) { (placemarks, error) -> Void in if error != nil { GWLog(\"----- 检索地址失败 -----\") completionHandler?(nil) } else { let pm = CLPlacemark(placemark: placemarks![0] as CLPlacemark) var address: String = \"\" func addSymbol() -> String { if !address.isEmpty { address += \",\" } return address } if let subThoroughtare = pm.subThoroughfare // 门牌号 { address = addSymbol() + subThoroughtare } if let thoroughfare = pm.thoroughfare // 街道 { address = addSymbol() + thoroughfare } if let subLocality = pm.subLocality // 区 { address = addSymbol() + subLocality } if let locality = pm.locality // 市 { address = addSymbol() + locality } if let administrativeArea = pm.administrativeArea // 省（州） { address = addSymbol() + administrativeArea } if let country = pm.country // 国家 { address = addSymbol() + country } GWLog(\"----- 检索地址：\\(address) -----\") completionHandler?(address) } } } } extension LocationManager: CLLocationManagerDelegate { /// 定位回调 public func locationManager(_ manager: CLLocationManager, didUpdateLocations locations: [CLLocation]) { guard let lastLocation = locations.last else { GWLog(\"----- 定位失败:没有找到最后一个位置 -----\") delegate?.onLocationFailure() if let manager = manager as? SubCLLocationManager { manager.locatingCompletion?(nil) } return } GWLog(\"----- 定位成功:latitude-\\(lastLocation.coordinate.latitude),longitude-\\(lastLocation.coordinate.longitude) -----\") userLocation = lastLocation delegate?.onLocationUpdated(location: lastLocation) if let manager = manager as? SubCLLocationManager { manager.locatingCompletion?(lastLocation) } } /// 定位失败 public func locationManager(_ manager: CLLocationManager, didFailWithError error: Error) { if let error = error as? CLError, error.code == .denied { GWLog(\"----- 用户拒绝定位 -----\") if let manager = manager as? SubCLLocationManager { stopLocating(subLocManager: manager) } } GWLog(\"----- 定位失败:\\(error) -----\") delegate?.onLocationFailure() if let manager = manager as? SubCLLocationManager { manager.locatingCompletion?(nil) } } } 附： placemark "},"pages/other/Resource_packaging_class.html":{"url":"pages/other/Resource_packaging_class.html","title":"资源包装类","keywords":"","body":"资源包装类 资源：项目中使用的图片、颜色、字体、常量、多语言等。 为了方便管理项目资源，UtilCore组件新增了几个资源包装类，后续请确保项目统一使用包装类调用资源。 如果只在业务组件中使用，请为以下几个包装类分别做扩展，确保资源调用的高度统一。 解决的问题 明确项目中所有使用的字体、颜色、图片等，资源统一管理，为后续主题做铺垫。 如果项目全部替换为通过包装类使用资源，那么一旦某个颜色、图片不用了，可以直接删除包装类的定义，再删除具体的资源，减少垃圾资源的存在。 代码风格统一，调用简单清晰。 GWColor.swift public extension UIColor { /// 根据模式（正常/暗黑）渲染颜色 /// - Parameters: /// - normal: 正常Hex /// - dark: 暗黑Hex /// - Returns: 颜色 static func renderColor(_ color: GWDynamicColor) -> UIColor { var result: UIColor? if #available(iOS 13, *) { result = UIColor(dynamicProvider: { (traitCollection) -> UIColor in (traitCollection.userInterfaceStyle == .dark ? UIColor(hexString: color.dark) ?? UIColor.white : UIColor(hexString: color.normal)) ?? UIColor.white }) } else { result = UIColor(hexString: color.normal) } return result ?? UIColor.white } } /** 颜色包装类 - UtilCore及其它业务组件通用的颜色在这里定义 - 通过使用包装类方便管理颜色 - 业务组件可以通过Extensions去扩展自己的颜色 - 使用：GWConstant.xC7C3C3 */ /// 动态颜色类型 public typealias GWDynamicColor = (normal: String, dark: String) public enum GWColor { public static let xC7C3C3 = UIColor.renderColor(GWDynamicColor(\"#C7C3C3\", \"#C7C3C3\")) public static let x2F2F2F = UIColor.renderColor(GWDynamicColor(\"#2F2F2F\", \"#2F2F2F\")) public static let xF6F6F6 = UIColor.renderColor(GWDynamicColor(\"#F6F6F6\", \"#F6F6F6\")) public static let x2D78FF = UIColor.renderColor(GWDynamicColor(\"#2D78FF\", \"#2D78FF\")) // ... } GWConstant.swift /** 常量包装类 - UtilCore及其它业务组件通用的常量在这里定义 - 通过使用包装类方便管理常量 - 业务组件可以通过Extensions去扩展自己的常量 - 使用：GWConstant.xxx */ public struct GWConstant { } GWFont.swift /** 字体包装类 - UtilCore及其它业务组件通用的字体在这里定义 - 通过使用包装类方便管理字体 - 业务组件可以通过Extensions去扩展自己的字体 - 使用：GWLocalized.r11 */ public struct GWFont { public static let r11 = UIFont.appRegularFontWith(size: 11) public static let r12 = UIFont.appRegularFontWith(size: 12) public static let r14 = UIFont.appRegularFontWith(size: 14) public static let r16 = UIFont.appRegularFontWith(size: 16) public static let r20 = UIFont.appRegularFontWith(size: 20) public static let m20 = UIFont(mediumFontWithSize: 20) } GWImage.swift /** 图片包装类 - UtilCore及其它业务组件通用的图片在这里定义 - 通过使用包装类方便管理图片 - 业务组件可以通过Extensions去扩展自己的图片 - 使用：GWImage.ruNoContent */ public struct GWImage { // MARK: - UIView+Empty /// 俄罗斯·没数据 static var ruNoContent: UIImage? { UIImage(inUtilCore: \"img_russia_common_no_content\") } /// 俄罗斯·无网络 static var ruNoNetwork: UIImage? { UIImage(inUtilCore: \"img_russia_common_no_network\") } /// 泰国·没数据 static var thNoContent: UIImage? { UIImage(inUtilCore: \"no_content\") } /// 泰国·无网络 static var thNoNetwork: UIImage? { UIImage(inUtilCore: \"no_network\") } /// 泰国·服务器错误 static var thServerError: UIImage? { UIImage(inUtilCore: \"load_failure\") } } GWLocalized.swift /** 多语言包装类 - UtilCore及其它业务组件通用的多语言在这里定义 - 通过使用包装类方便管理多语言 - 业务组件可以通过Extensions去扩展自己的多语言 - 使用：GWLocalized.loading */ public struct GWLocalized { /// 提交中 public static var loading: String { GWI18n.R.string.localizable.base_loading() } /// Yes public static var yes: String { GWI18n.R.string.localizable.base_yes() } /// No public static var no: String { GWI18n.R.string.localizable.base_no() } /// 无网络 public static var noNetwork: String { GWI18n.R.string.localizable.base_net_lost() } /// 发布 public static var publish: String { GWI18n.R.string.localizable.re_publish() } /// 确认 public static var confirm: String { GWI18n.R.string.localizable.base_sure() } /// 取消 public static var cancel: String { GWI18n.R.string.localizable.base_cancel() } } "},"pages/other/Sync_unread_messages.html":{"url":"pages/other/Sync_unread_messages.html","title":"同步未读消息","keywords":"","body":"同步未读消息 需求 只有用户已经登录才同步未读消息。 当收到消息推送，触发同步。 当App停留在前台，每个60s，并且停留在指定页面（首页、我的），触发同步。 当指定页面（首页、我的）第一次显示，触发同步。 当离开指定页面（首页、我的）超过60s再回来，触发同步。 一、先实现需求4、5 为vc增加rx的扩展序列visibleDetail，当vc显示或者离开时会发出“是否显示”、“是否第一次显示”、“离开时间”的元组信号。通过对visibleDetail的订阅，当发出信号时，去判断是否第一次，如果是就触发一次同步，如果不是，再判断离开时间是否超过60s，如果超过也需要触发一次同步。 1.1 如何为vc增加扩展序列visibleDetail 首先增加vc生命周期的基本扩展，代码来源。 public extension Reactive where Base: UIViewController { public var viewDidAppear: ControlEvent { let source = self.methodInvoked(#selector(Base.viewDidAppear)) .map { $0.first as? Bool ?? false } return ControlEvent(events: source) } public var viewWillDisappear: ControlEvent { let source = self.methodInvoked(#selector(Base.viewWillDisappear)) .map { $0.first as? Bool ?? false } return ControlEvent(events: source) } // 表示视图是否显示的可观察序列，当VC显示状态改变时会触发 public var isVisible: Observable { let viewDidAppearObservable = self.base.rx.viewDidAppear.map { _ in true } let viewWillDisappearObservable = self.base.rx.viewWillDisappear .map { _ in false } return Observable.merge(viewDidAppearObservable, viewWillDisappearObservable) } ...... } 继续给vc增加进入/离开时间的属性扩展，用于记录页面的进入/离开的时间撮。 private var enterStampKey: Void? private var leaveStampKey: Void? public extension UIViewController { /// 离开时间撮 var leaveStamp: TimeInterval? { set(newValue) { objc_setAssociatedObject(self, &leaveStampKey, newValue, .OBJC_ASSOCIATION_RETAIN) } get { return objc_getAssociatedObject(self, &leaveStampKey) as? TimeInterval } } /// 进入时间撮 var enterStamp: TimeInterval? { set(newValue) { objc_setAssociatedObject(self, &enterStampKey, newValue, .OBJC_ASSOCIATION_RETAIN) } get { return objc_getAssociatedObject(self, &enterStampKey) as? TimeInterval } } } 有了以上两个扩展，这个时候就可以为vc增加rx扩展序列visibleDetail了,可以看下面的实现，visibleDetail依赖于isVisible,每当页面进入/离开会记录时间撮，同时该序列返回的类型是元组，第一个元素是“是否显示”，第一个元素是“是否第一次显示”，第三个元素是“离开时间”。 public extension Reactive where Base: UIViewController { /// 显示详情 /// - Parameters: 参数 /// - visible：是否显示 /// - isFirst：是否第一次显示 /// - leaveTime：离开时间，单位s，默认返回0 typealias VisibleDetail = (visible: Bool,isFirst: Bool, leaveTime: Int) var visibleDetail: Observable { return self.base.rx.isVisible.map { (isVisible) -> VisibleDetail in if isVisible { let isFirst = self.base.enterStamp == nil// 是否第一次显示 var leaveTime: Int = 0// 离开时间 self.base.enterStamp = Date().timeIntervalSince1970 if let enterStamp = self.base.enterStamp, let leaveStamp = self.base.leaveStamp, enterStamp > leaveStamp { leaveTime = Int(enterStamp - leaveStamp) } return (true, isFirst, leaveTime) } else { self.base.leaveStamp = Date().timeIntervalSince1970 return (false, false, 0) } } } } 1.2 增加SynMessageTool单例 SynMessageTool是一个单例，提供了一个start、stop方法，start方法需要传入一个vc，意思是消息同步是需要根据vc的生命周期来控制的，当vc第一次显示或者离开vc超过60s再回来就触发同步。 /// 同步未读消息，发出通知 public class SynMessageTool { public static let shared = SynMessageTool() /// SynMessage订阅取消 private var disposable: Disposable? /// 同步消息（数字） /// - Parameters: /// - messageVc: 同步消息关联vc public func start(messageVc: UIViewController?) { // 传vc才同步消息 guard let vc = messageVc else { return } // 停止订阅 stop() GWLog(\"----- vc is \\(vc.classForCoder) -----\") /// 页面触发同步序列 let synMessageObservable = vc.rx.visibleDetail.flatMap { (detail) -> Observable in if detail.isFirst { GWLog(\"----- 第一次显示 -----\") return HomeViewModel.getUnreadCount } else if detail.leaveTime > 60 { GWLog(\"----- 非第一次显示，但是离开时间超过60s -----\") return HomeViewModel.getUnreadCount } return Observable.of() } GWLog(\"----- 开始SynMessage订阅 ------\") disposable = synMessageObservable.subscribe(onNext: { count in GWLog(\"----- 当前消息数数\\(count) ------\") // 发通知 NotificationCenter.default.post(name: .synMessageCount, object: count, userInfo: [\"count\":count]) }, onDisposed: { GWLog(\"----- 释放SynMessage订阅 ------\") }) } public func stop() { disposable?.dispose() } } 二、实现需求3 需求：当App停留在前台，每个60s，并且停留在指定页面（首页、我的），触发同步。 这里需要使用到AppTimer及vc的rx扩展序列isVisible,将AppTimer的到达60s的序列与isVisible通过Observable.combineLatest组合成一个新的序列，再通过flatMap转换成我们需要的序列，当然这里需要判断“页面是否正在显示”，如果显示才会触发同步。start方法修改如下： /// 同步消息（数字） /// - Parameters: /// - messageVc: 同步消息关联vc public func start(messageVc: UIViewController?) { // 传vc才同步消息 guard let vc = messageVc else { return } // 停止订阅 stop() // 同步消息序列 var synMessageObservable: Observable! GWLog(\"----- vc is \\(vc.classForCoder) -----\") /// 页面触发同步序列 let synMessageObservableByVc = vc.rx.visibleDetail.flatMap { (detail) -> Observable in if detail.isFirst { GWLog(\"----- 第一次显示 -----\") return HomeViewModel.getUnreadCount } else if detail.leaveTime > 60 { GWLog(\"----- 非第一次显示，但是离开时间超过60s -----\") return HomeViewModel.getUnreadCount } return Observable.of() } /// 计时器序列 let synMessageObservableByTimer = Observable.combineLatest(AppTimer.shared.reach60sObservable,vc.rx.isVisible).flatMap { (tuple) -> Observable in GWLog(\"----- AppTimer 触发同步 -----\") if tuple.1 { return HomeViewModel.getUnreadCount } return Observable.of() } synMessageObservable = Observable.merge(synMessageObservableByVc,synMessageObservableByTimer) GWLog(\"----- 开始SynMessage订阅 ------\") disposable = synMessageObservable.subscribe(onNext: { count in GWLog(\"----- 当前消息数数\\(count) ------\") // 发通知 NotificationCenter.default.post(name: .synMessageCount, object: count, userInfo: [\"count\":count]) }, onDisposed: { GWLog(\"----- 释放SynMessage订阅 ------\") }) } 三、实现需求2 需求：当收到消息推送，触发同步。 SynMessageTool只需要增加一个立即同步的方法，在收到通知的地方调用即可。 /// 立即同步 public func immediatelyStart() { immediatelySynDisposable = HomeViewModel.getUnreadCount.subscribe(onNext: {[weak self] count in GWLog(\"----- immediately:当前消息数数\\(count) ------\") // 发通知 NotificationCenter.default.post(name: .synMessageCount, object: count, userInfo: [\"count\":count]) // 释放订阅 self?.immediatelyStop() }, onDisposed: { GWLog(\"----- immediately:释放SynMessage订阅 ------\") }) } 四、实现需求1 需求：只有用户已经登录才同步未读消息。 只需要在start和immediatelyStart方法中加入如下登录判断即可。 // 登录后才同步消息 guard UserManager.isLogin else { return } 五、完整代码 /// 同步未读消息，发出通知 public class SynMessageTool { public static let shared = SynMessageTool() /// SynMessage订阅取消 private var disposable: Disposable? /// 确保同步一次后取消 private var immediatelySynDisposable: Disposable? /// 同步消息（数字） /// - Parameters: /// - messageVc: 同步消息关联vc public func start(messageVc: UIViewController?) { // 登录后才同步消息 guard UserManager.isLogin else { return } // 传vc才同步消息 guard let vc = messageVc else { return } // 停止订阅 stop() // 同步消息序列 var synMessageObservable: Observable! GWLog(\"----- vc is \\(vc.classForCoder) -----\") /// 页面触发同步序列 let synMessageObservableByVc = vc.rx.visibleDetail.flatMap { (detail) -> Observable in if detail.isFirst { GWLog(\"----- 第一次显示 -----\") return HomeViewModel.getUnreadCount } else if detail.leaveTime > 60 { GWLog(\"----- 非第一次显示，但是离开时间超过60s -----\") return HomeViewModel.getUnreadCount } return Observable.of() } /// 计时器序列 let synMessageObservableByTimer = Observable.combineLatest(AppTimer.shared.reach60sObservable,vc.rx.isVisible).flatMap { (tuple) -> Observable in GWLog(\"----- AppTimer 触发同步 -----\") if tuple.1 { return HomeViewModel.getUnreadCount } return Observable.of() } synMessageObservable = Observable.merge(synMessageObservableByVc,synMessageObservableByTimer) GWLog(\"----- 开始SynMessage订阅 ------\") disposable = synMessageObservable.subscribe(onNext: { count in GWLog(\"----- 当前消息数数\\(count) ------\") // 发通知 NotificationCenter.default.post(name: .synMessageCount, object: count, userInfo: [\"count\":count]) }, onDisposed: { GWLog(\"----- 释放SynMessage订阅 ------\") }) } public func stop() { disposable?.dispose() } /// 立即同步 public func immediatelyStart() { // 登录后才同步消息 guard UserManager.isLogin else { return } immediatelySynDisposable = HomeViewModel.getUnreadCount.subscribe(onNext: {[weak self] count in GWLog(\"----- immediately:当前消息数数\\(count) ------\") // 发通知 NotificationCenter.default.post(name: .synMessageCount, object: count, userInfo: [\"count\":count]) // 释放订阅 self?.immediatelyStop() }, onDisposed: { GWLog(\"----- immediately:释放SynMessage订阅 ------\") }) } /// 立即停止 public func immediatelyStop() { immediatelySynDisposable?.dispose() } } "},"pages/other/custom_error.html":{"url":"pages/other/custom_error.html","title":"自定义错误","keywords":"","body":"自定义错误 Swift中的Error 官方解释 A type representing an error value that can be thrown. Any type that declares conformance to the Error protocol can be used to represent an error in Swift’s error handling system. Because the Error protocol has no requirements of its own, you can declare conformance on any custom type you create. protocol Error Error在Swift中就是是个协议，而且不需要实现任何属性或者方法，因此任何声明符合Error协议的类型都可以用来表示Swift的错误。 使用枚举作为错误 通常错误都是有很多种，因此枚举类型是作为错误最适合不过，下面是官方给的例子： enum IntParsingError: Error { case overflow case invalidInput(Character) } 使用结构体作为错误 官方解释 Sometimes you may want different error states to include the same common data, such as the position in a file or some of your application’s state. When you do, use a structure to represent errors. 意思就是当不同错误会包含通用的属性，例如文件中的某些位置或应用的状态，这个时候就可以使用结构体作为错误了，下面还是官方的例子： struct XMLParsingError: Error { enum ErrorKind { case invalidCharacter case mismatchedTag case internalError } let line: Int let column: Int let kind: ErrorKind } func parse(_ source: String) throws -> XMLDoc { // ... throw XMLParsingError(line: 19, column: 5, kind: .mismatchedTag) // ... } 与NSError的关系 我们在Swift中自定义的Error是可以轻松通过as转为NSError的，但是我们支持NSError使用拥有localizedDescription、code、domain、UserInfo等属性的，只有实现了以下两个协议，转换后NSError的这些属性就可以生效。 LocalizedError 用来桥接原来NSError中的localizedDescription。 public protocol LocalizedError : Error { var errorDescription: String? { get } // ... } CustomNSError 用来桥接原来NSError中的code、domain、UserInfo。 public protocol CustomNSError : Error { /// The domain of the error. public static var errorDomain: String { get } /// The error code within the given domain. public var errorCode: Int { get } /// The user-info dictionary. public var errorUserInfo: [String : Any] { get } } 附： 苹果文档 Swift：Corelocation处理didFailWithError中的NSError Swift Error 的介绍和使用 Swift 3必看：Error与NSError的关系 fatalError "},"pages/other/GWError_usage.html":{"url":"pages/other/GWError_usage.html","title":"GWError的使用","keywords":"","body":"GWError的使用 根据业务，在项目中可以对GWClientError或GWSeverError增加case，自定义错误。 解决问题 客户端请求接口，我们需要弹出指定的提示，例如：“no network（没有网络）”、“network error（服务器错误）”。 客户端请求接口，对指定的错误码返回自定的错误，例如：动态不存在的时候，服务器会返回712200，我们需要按需求自定义提示。 编写校验逻辑的时候，我们可以根据不同的情况自定义不同的Error抛出，这样我们可以通过catch对错误统一处理。 GWError GWError是对GWClientError、GWSeverError、MoyaError的包装，也就是引发错误的来源是被包装的三种自定义错误： public enum GWError: Swift.Error { // Moya错误 case moya(MoyaError) // 客户端错误 case client(GWClientError) // 服务端错误 case server(GWSeverError) } GWError分别实现了LocalizedError、CustomNSError： extension GWError: LocalizedError { // 错误描述 public var errorDescription: String? { switch self { case let .moya(error): return error.gwErrorDescription ?? GWI18n.R.string.localizable.base_net_erroe() case let .client(error): return error.errorDescription case let .server(error): return error.errorDescription } } } extension GWError: CustomNSError { /// The domain of the error. public static var errorDomain: String = \"com.gwm.error\" // 错误码 public var errorCode: Int { switch self { case let .moya(error): return error.errorCode case let .client(error): return error.errorCode case let .server(error): return error.errorCode } } // 错误信息 public var errorUserInfo: [String: Any] { var userInfo: [String: Any] = [:] userInfo[NSLocalizedDescriptionKey] = errorDescription userInfo[NSUnderlyingErrorKey] = underlyingError return userInfo } // 内部错误 internal var underlyingError: Swift.Error? { switch self { case let .moya(error): return error case let .client(error): return error case let .server(error): return error } } } 为了兼容老代码，保留了statusCode、message、code三个属性： // 状态码 public var statusCode: Int? { switch self { case let .moya(error): return error.response?.statusCode case let .client(error): return error.statusCode case let .server(error): return error.statusCode } } // 错误提示 public var message: String { errorDescription ?? \"\" } // 错误码 public var code: String { switch self { case let .moya(error): return String(error.errorCode) case let .client(error): return String(error.code) case let .server(error): return String(error.code) } } GWClientError GWClientError是客户端错误，可以是校验逻辑中的任何一种异常的封装，同样分别实现了LocalizedError、CustomNSError，目前有如下定义：。 public enum GWClientError: Swift.Error { // 没有网络 case noNetwork // 网络异常 case networkException // 禁言错误 case muted // 提交评论失败 case postComment // 状态码 public var statusCode: Int? { // ... } // 错误提示 public var message: String { // ... } // 错误码 public var code: String { // ... } } extension GWClientError: LocalizedError { // 错误描述 public var errorDescription: String? { // ... } } extension GWClientError: CustomNSError {/// The domain of the error. public static var errorDomain: String = \"com.gwm.error.client\" // 错误码 public var errorCode: Int { // ... } // 错误信息 public var errorUserInfo: [String: Any] { // ... } } GWSeverError GWSeverError是服务端错误，是服务端异常的封装，同样分别实现了LocalizedError、CustomNSError，目前定义如下： public enum GWSeverError: Swift.Error { // 资讯评论不存在(回复评论) case newsCommentNotExist // 动态评论不存在(回复评论) case dynamicCommentNotExist // 动态评论不存在(查询评论) case dynamicCommentNotExistOnQuery // 动态不存在 case dynamicNotExist // 结果异常 case resultException(code: Int, msg: String?, statusCode: Int?) // 业务异常 static let businessErrors: [GWSeverError] = [ // ... ] // 根据code匹配业务异常 static func getError(code: Int, msg: String?, statusCode: Int?) -> GWSeverError { // ... } // 状态码 public var statusCode: Int? { // ... } // 错误提示 public var message: String { // ... } // 错误码 public var code: String { // ... } } 实现了CustomNSError的errorCode,errorCode应该和服务端错误code保持一致： extension GWSeverError: LocalizedError { // 错误描述 public var errorDescription: String? { // ... } } extension GWSeverError: CustomNSError {/// The domain of the error. public static var errorDomain: String = \"com.gwm.error.server\" // 错误码 public var errorCode: Int { switch self { // 资讯评论不存在(回复评论) case .newsCommentNotExist: return 710009 // 动态评论不存在(回复评论) case .dynamicCommentNotExist: return 712123 // 动态评论不存在(查询评论) case .dynamicCommentNotExistOnQuery: return 712197 // 动态不存在 case .dynamicNotExist: return 712200 case .resultException(let code, msg: _, statusCode: _): return code } } // 错误信息 public var errorUserInfo: [String: Any] { // ... } } 服务端通用错误 当服务端返回的错误码不匹配我们约定的errorCode时，我们统一使用resultException这个服务器错误，resultException有code、msg、statusCode三个关联属性。 // 业务异常 static let businessErrors: [GWSeverError] = [ .newsCommentNotExist, .dynamicCommentNotExist, .dynamicNotExist, .dynamicCommentNotExistOnQuery ] // 根据code匹配业务异常 static func getError(code: Int, msg: String?, statusCode: Int?) -> GWSeverError { if let customError = businessErrors.filter({ $0.errorCode == code }).first { return customError } return .resultException(code: code, msg: msg, statusCode: statusCode) } 如何使用 客户端错误的使用，下面是没有网络的时候返回了noNetwork异常： if NetworkReachabilityManager()?.isReachable == false { failedCallBack?(.client(.noNetwork)) return nil } 服务端错误的使用，下面是判断评论不存在返回了newsCommentNotExist异常： }, onError: { (error) in // 匹配自定义错误 if let gwError = error as? GWError { if gwError.code == GWSeverError.newsCommentNotExist.code { observer.onError(GWError.server(.newsCommentNotExist)) observer.onCompleted() return } } observer.onError(GWError.client(.postComment)) observer.onCompleted() }) "},"pages/other/Universal_Links.html":{"url":"pages/other/Universal_Links.html","title":"Universal Links","keywords":"","body":"Universal Links https://www.soga.ga/apple-app-site-association 参考文章 2020 iOS 微信分享/Universal Links 参考配置 参考配置2 测试链接(applinks:soga.ga) 官方文档 UniversalLinks和Web Credentials配置 iOS通用链接（UniversalLink）配置详细流程 iOS H5打开App(通用链接) https://cdn-h5-html.gwmcloud.com/.well-known/apple-app-site-association Universal Link 用企业证书build包，可以通过链接拉起绑定的App(客户端) iOS JMLink SDK 常见问题 iOS 9 通用链接（Universal Links） http://cdn-th-h5-html.gwmcloud.com/.well-known/outlink.html http://cdn-th-h5-html.gwmcloud.com/.well-known/apple-app-site-association "},"pages/other/Configurations.html":{"url":"pages/other/Configurations.html","title":"Configurations","keywords":"","body":"Configurations 用xcconfig文件配置iOS app环境变量 "},"pages/other/iOS_integration_Flutter.html":{"url":"pages/other/iOS_integration_Flutter.html","title":"iOS集成Flutter","keywords":"","body":"iOS集成Flutter flutter开源地址 flutter中文文档 flutter英文文档 一、flutter环境配置 参考 第1步：下载最新的SDK 下载地址 我这里SDK文件放在用户目录下，解压后可以看到在用户目录下多了一个flutter文件夹。 第2步：配置flutter命令 在用户目录下找到.bash_profile，可以直接用文本编辑器打开，或则使用vi命令： vi ~/.bash_profile 在.bash_profile中写入以下命令，完成配置 export PATH=~/flutter/bin:$PATH 第3步：检测flutter命令 帮助 flutter -h flutter安装地址 which flutter dart安装地址 which flutter dart 第4步：运行 flutter doctor 命令 flutter doctor 1.问题1 🎉  ~  flutter doctor Doctor summary (to see all details, run flutter doctor -v): [✓] Flutter (Channel stable, 2.2.1, on Mac OS X 10.15.7 19H114 darwin-x64, locale zh-Hans-CN) [✗] Android toolchain - develop for Android devices ✗ Unable to locate Android SDK. Install Android Studio from: https://developer.android.com/studio/index.html On first launch it will assist you in installing the Android SDK components. (or visit https://flutter.dev/docs/get-started/install/macos#android-setup for detailed instructions). If the Android SDK has been installed to a custom location, please use `flutter config --android-sdk` to update to that location. [✓] Xcode - develop for iOS and macOS [✓] Chrome - develop for the web [!] Android Studio (not installed) [✓] Connected device (1 available) ! Error: 陈长青的 iPhone is not connected. Xcode will continue when 陈长青的 iPhone is connected. (code -13) 将iPhone连接到电脑，并且解锁，解决问题： 🎉  ~  flutter doctor Doctor summary (to see all details, run flutter doctor -v): [✓] Flutter (Channel stable, 2.2.1, on Mac OS X 10.15.7 19H114 darwin-x64, locale zh-Hans-CN) [✗] Android toolchain - develop for Android devices ✗ Unable to locate Android SDK. Install Android Studio from: https://developer.android.com/studio/index.html On first launch it will assist you in installing the Android SDK components. (or visit https://flutter.dev/docs/get-started/install/macos#android-setup for detailed instructions). If the Android SDK has been installed to a custom location, please use `flutter config --android-sdk` to update to that location. [✓] Xcode - develop for iOS and macOS [✓] Chrome - develop for the web [!] Android Studio (not installed) [✓] Connected device (2 available) 1.问题2 [✗] Android toolchain - develop for Android devices ✗ Unable to locate Android SDK. Install Android Studio from: https://developer.android.com/studio/index.html On first launch it will assist you in installing the Android SDK components. (or visit https://flutter.dev/docs/get-started/install/macos#android-setup for detailed instructions). If the Android SDK has been installed to a custom location, please use `flutter config --android-sdk` to update to that location. 二、设置 iOS 开发环境 第1步：安装 Xcode 1.通过 直接下载 或者通过 Mac App Store 来安装最新稳定版 Xcode 2.通过在命令行中运行以下命令来配置 Xcode command-line tools: sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer sudo xcodebuild -runFirstLaunch 当你安装了最新版本的 Xcode，大部分情况下，上面的路径都是一样的。但如果你安装了不同版本的 Xcode，你可能要更改一下上述命令中的路径。 3.运行一次 Xcode 或者通过输入命令 sudo xcodebuild -license 来确保已经同意 Xcode 的许可协议。 第2步：配置 iOS 模拟器 open -a Simulator 三、创建并运行一个简单的 Flutter 应用 通过以下步骤来创建你的第一个 Flutter 应用并进行测试： 1.通过运行以下命令来创建一个新的 Flutter 应用： flutter create my_app 2.上述命令创建了一个 my_app 的目录，包含了 Flutter 初始的应用模版，切换路径到这个目录内： cd my_app 3.确保模拟器已经处于运行状态，输入以下命令来启动应用： flutter run 问题1: 🎉  ~/Desktop/代码/flutter/my_app  flutter run Multiple devices found: 陈长青的 iPhone (mobile) • ac82c56391c359c3313eb0f84cfe5f9c6fb45ca5 • ios • iOS 14.4.2 iPhone 12 Pro Max (mobile) • 4594BEED-D4F8-4CE4-A566-BF483F2A7565 • ios • com.apple.CoreSimulator.SimRuntime.iOS-14-4 (simulator) Chrome (web) • chrome • web-javascript • Google Chrome 90.0.4430.212 [1]: 陈长青的 iPhone (ac82c56391c359c3313eb0f84cfe5f9c6fb45ca5) [2]: iPhone 12 Pro Max (4594BEED-D4F8-4CE4-A566-BF483F2A7565) [3]: Chrome (chrome) Please choose one (To quit, press \"q/Q\"): 这里直接选择2，2是我刚才打开的模拟器，接下来会看到： Please choose one (To quit, press \"q/Q\"): 2 Launching lib/main.dart on iPhone 12 Pro Max in debug mode... Running Xcode build... └─Compiling, linking and signing... 10.8s Xcode build done. 39.8s Syncing files to device iPhone 12 Pro Max... 89ms Flutter run key commands. r Hot reload. 🔥🔥🔥 R Hot restart. h Repeat this help message. d Detach (terminate \"flutter run\" but leave application running). c Clear the screen q Quit (terminate the application on the device). 💪 Running with sound null safety 💪 An Observatory debugger and profiler on iPhone 12 Pro Max is available at: http://127.0.0.1:54602/2Xt-Ae85-qk=/ Activating Dart DevTools... 24.5s The Flutter DevTools debugger and profiler on iPhone 12 Pro Max is available at: http://127.0.0.1:9100?uri=http%3A%2F%2F127.0.0.1%3A54602%2F2Xt-Ae85-qk%3D%2F 这个时候，切换至模拟器，就可以看到应用的界面了！ 注意：如果选择1真机的话，需要打开Runner.xcworkspace，在xcode中配置好证书描述文件。 四、解决现有flutter项目问题 问题1 Invalid Podfile file: cannot load such file -- ../my_flutter/.ios/Flutter/podhelper.rb. 参考 问题2 Version solving failed. #18937 问题3 pub get failed (server unavailable) 问题4 Include of non-modular header inside framework module error "},"pages/other/Automatic_release.html":{"url":"pages/other/Automatic_release.html","title":"自动发版","keywords":"","body":" 命令行输入 gem which cocoapods 会得到类似 /Users/user/.rvm/rubies/ruby-2.6.3/lib/ruby/gems/2.6.0/gems/cocoapods-1.9.3/lib/cocoapods.rb 前往文件夹（command+shift+g） 会看到一个 cocoapods文件夹 cocoapods.rb文件 进入cocoapods文件夹，找到文件validator.rb拷贝出来一份作为备份。再拷贝一份进行修改如下： 搜索： when 找到如下代码 when :ios command += %w(CODE_SIGN_IDENTITY=- -sdk iphonesimulator) command += Fourflusher::SimControl.new.destination(:oldest, 'iOS', deployment_target) xcconfig = consumer.pod_target_xcconfig if xcconfig archs = xcconfig['VALID_ARCHS'] if archs && (archs.include? 'armv7') && !(archs.include? 'i386') && (archs.include? 'x86_64') # Prevent Xcodebuild from testing the non-existent i386 simulator if armv7 is specified without i386 command += %w(ARCHS=x86_64) end end when :watchos 修改为： when :ios # command += %w(CODE_SIGN_IDENTITY=- -sdk iphonesimulator) # command += Fourflusher::SimControl.new.destination(:oldest, 'iOS', deployment_target) # xcconfig = consumer.pod_target_xcconfig # if xcconfig # archs = xcconfig['VALID_ARCHS'] # if archs && (archs.include? 'armv7') && !(archs.include? 'i386') && (archs.include? 'x86_64') # # Prevent Xcodebuild from testing the non-existent i386 simulator if armv7 is specified without i386 # command += %w(ARCHS=x86_64) # end # end command += %w(--help) when :watchos 意思就是， 注释掉这部分代码，用一个help命令代替。 然后替换原validator.rb文件即可。 参考链接： https://www.jianshu.com/p/88180b4d2ab7 "},"pages/fultter/Build_Flutter_development_environment_on_MacOS.html":{"url":"pages/fultter/Build_Flutter_development_environment_on_MacOS.html","title":"在MacOS上搭建Flutter开发环境","keywords":"","body":"在MacOS上搭建Flutter开发环境 问题记录 下载地址错误 在下载fultter的时候，需要下载macOS下的x64，而不是arm64。如果使用了错误的CPU架构类型，运行flutter命令，flutter会提示CPU不支持的信息。 命令行没有翻墙 如果没有翻墙，会导致以下两个错误： HTTP Host availability check is taking a long time...[!] HTTP Host Availability ✗ HTTP host \"https://maven.google.com/\" is not reachable. Reason: An error occurred while checking the HTTP host: Operation timed out ✗ HTTP host \"https://cloud.google.com/\" is not reachable. Reason: An error occurred while checking the HTTP host: Operation timed out Android sdkmanager not found. https://stackoverflow.com/questions/70719767/android-sdkmanager-not-found-update-to-the-latest-android-sdk-and-ensure-that-t ! NO_PROXY is not set https://zhuanlan.zhihu.com/p/474652737 "},"pages/fultter/Create_Flutter_application.html":{"url":"pages/fultter/Create_Flutter_application.html","title":"创建Flutter应用","keywords":"","body":"创建Flutter应用 1）安装Flutter和Dart插件 2）创建Flutter应用 3）Organization命名不规范错误 发现一个错误，Organization命名不规范，不能出现纯数字，修改如下： 4）Android真机运行 5）iOS模拟器运行 6）在Android Studio上选择多设备运行 效果： 《Flutter实战·第二版》1.3.2 ～ 1.3.3 "},"pages/fultter/The_first_Flutter_application.html":{"url":"pages/fultter/The_first_Flutter_application.html","title":"第一个Flutter应用","keywords":"","body":"第一个Flutter应用 模板代码分析 1. 导入包 import 'package:flutter/material.dart'; 此行代码作用是导入了 Material UI 组件库。 2. 应用入口 void main() => runApp(MyApp()); 启动Flutter应用：main调用runApp，runApp接受MyApp对象参数，MyApp()是 Flutter应用根组件。 3. 应用结构 class MyApp extends StatelessWidget { @override Widget build(BuildContext context) { return MaterialApp( //应用名称 title: 'Flutter Demo', theme: ThemeData( //蓝色主题 primarySwatch: Colors.blue, ), //应用首页路由 home: MyHomePage(title: 'Flutter Demo Home Page'), ); } } MyApp类代表Flutter应用，继承了StatelessWidget类，是一个widget。 Flutter通过widget提供的build方法来构建UI界面。 MaterialApp是Flutter APP框架，可以设置应用的名称、主题、语言、首页及路由列表等。 home指定了App的首页，是一个widget。 首页 1. MyHomePage类 class MyHomePage extends StatefulWidget { MyHomePage({Key? key, required this.title}) : super(key: key); final String title; @override _MyHomePageState createState() => _MyHomePageState(); } class _MyHomePageState extends State { ... } MyHomePage是应用首页，它继承自StatefulWidget类，表示它是一个有状态的组件（Stateful widget）。 MyHomePage没有提供build方法，但是它有_MyHomePageState状态类，bulid方法被挪到了这个状态类， 2. State类 组件的状态。int _counter = 0; //用于记录按钮点击的总次数 设置状态的自增函数。 void _incrementCounter() { setState(() { _counter++; }); } 点击按钮+时会调用，通过setState通知Flutter状态修改了，然后重新执行build方法重新构建UI。 构建UI界面的build方法 Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: Text(widget.title), ), body: Center( child: Column( mainAxisAlignment: MainAxisAlignment.center, children: [ Text('You have pushed the button this many times:'), Text( '$_counter', style: Theme.of(context).textTheme.headline4, ), ], ), ), floatingActionButton: FloatingActionButton( onPressed: _incrementCounter, tooltip: 'Increment', child: Icon(Icons.add), ), ); } Scaffold是 Material 库中提供的页面脚手架。 body是具体的组件树。 floatingActionButton右下角的加号按钮。 现在，我们将整个计数器执行流程串起来：当右下角的floatingActionButton按钮被点击之后，会调用_incrementCounter方法。在_incrementCounter方法中，首先会自增_counter计数器（状态），然后setState会通知 Flutter 框架状态发生变化，接着，Flutter 框架会调用build方法以新的状态重新构建UI，最终显示在设备屏幕上。 3. build方法放在State的原因 为什么要将build方法放在State中，而不是放在StatefulWidget中？ 状态访问不便。 将build方法放在widget中，由于构建UI需要访问State的属性，例如上面的_counter，也就是说build方法需要依赖State ，并且公开_counter，这就会导致对状态的修改将会变的不可控。反之，build放在State中，可以直接访问状态，并且拿到_counter，这会非常方便。 继承StatefulWidget不便。 子类在调用父类build方法时，需要依赖父类State类，这是不合理的，因为父类的状态是父类内部的实现细节，不应该暴露给子类。 模板代码分析 "},"pages/fultter/Widget_introduction.html":{"url":"pages/fultter/Widget_introduction.html","title":"Widget简介","keywords":"","body":"Widget简介 Widget概念 描述一个UI元素的配置信息，比如对于Text来讲，文本的内容、对齐方式、文本样式都是它的配置信息。 Widget类的声明： @immutable // 不可变的 abstract class`Widget`extends DiagnosticableTree { const Widget({ this.key }); final Key? key; @protected @factory Element createElement(); @override String toStringShort() { final String type = objectRuntimeType(this, 'Widget'); return key == null ? type : '$type-$key'; } @override void debugFillProperties(DiagnosticPropertiesBuilder properties) { super.debugFillProperties(properties); properties.defaultDiagnosticsTreeStyle = DiagnosticsTreeStyle.dense; } @override @nonVirtual bool operator ==(Object other) => super == other; @override @nonVirtual int get hashCode => super.hashCode; static bool canUpdate(Widget oldWidget,`Widget`newWidget) { return oldWidget.runtimeType == newWidget.runtimeType && oldWidget.key == newWidget.key; } ... } @immutable代表Widget是不可变的，限制Widget中的属性必须使用final修饰。 为啥不允许属性修改呢？属性修改会重新创建新的Widget实例，这没有意义，还有就是可变属性应该交给State管理。 key的作用是决定下一次build是否复用旧的widget，决定的条件在canUpdate()方法。 createElement构建UI树时，生成对应节点的Element对象。一个widget可以对应多个Element。 canUpdate(...)：是否用新的widget对象去更新旧UI树上所对应的Element对象的配置，返回false的话，会重新创建新的Element。 Widget类是一个抽象类，其中最核心的就是定义了createElement()接口。 Flutter中的四棵树 Widget 只是描述一个UI元素的配置信息，那么真正的布局、绘制是由谁来完成的呢？Flutter 框架的的处理流程是这样的： 根据Widget树生成一个 Element 树，Element 树中的节点都继承自 Element 类。 根据 Element 树生成Render树（渲染树），渲染树中的节点都继承自RenderObject 类。 根据渲染树生成 Layer 树，然后上屏显示，Layer 树中的节点都继承自 Layer 类。 真正的布局和渲染逻辑在Render树中，Element 是Widget和 RenderObject 的粘合剂，可以理解为一个中间代理。 Widget类 1. StatelessWidget 继承Widget类，重写了createElement()方法，对应StatelessElement类。 用于不需要维护状态的场景，有一个build方法用来构建UI。 2. BuildContext build方法的context参数，表示当前widget在widget树中的上下文，每个widget对应一个context。 context是当前widget在widget树中位置中执行”相关操作“的一个句柄。 3. StatefulWidget 对应StatefulElement，多了一个createState()方法。 StatefulElement中可能会多次调用createState()来创建状态（State）对象。 createState()用于创建和StatefulWidget相关的状态，它在StatefulWidget的生命周期中可能会被多次调用。 在StatefulWidget中，State对象和StatefulElement具有一一对应的关系。 State 1. 简介 一个StatefulWidget类会对应一个State类，State表示与其对应的StatefulWidget要维护的状态。State 中的保存的状态信息可以： 在widget构建时可以被同步读取。 可以修改，然后手动调用其setState()方法，重新调用其build方法达到更新UI的目的。 State 中有两个常用属性： widget，重新构建时可能会变化，但State实例只会在第一次插入到树中时被创建。 context，StatefulWidget对应的BuildContext。 2. State生命周期 initState：当widget第一次插入到widget树时会被调用。 didChangeDependencies：当State对象的依赖发生变化时会被调用。 build()会在如下场景被调用： 在调用initState()之后。 在调用didUpdateWidget()之后。 在调用setState()之后。 在调用didChangeDependencies()之后。 在State对象从树中一个位置移除后（会调用deactivate）又重新插入到树的其他位置之后。 reassemble：热重载(hot reload)时会被调用，此回调在Release模式下永远不会被调用。 didUpdateWidget：在widget重新构建时，Flutter 框架会调用widget.canUpdate来检测widget树中同一位置的新旧节点，然后决定是否需要更新，如果widget.canUpdate返回true则会调用此回调。 deactivate：当 State 对象从树中被移除时，会调用此回调。 dispose：当 State 对象从树中被永久移除时调用。 在widget树中获取State对象 1. 通过Context获取 findAncestorStateOfType方法： // 查找父级最近的Scaffold对应的ScaffoldState对象 ScaffoldState _state = context.findAncestorStateOfType()!; // 打开抽屉菜单 _state.openDrawer(); StatefulWidget中提供一个of静态方法来获取其State对象。// 直接通过of静态方法来获取ScaffoldState ScaffoldState _state=Scaffold.of(context); // 打开抽屉菜单 _state.openDrawer(); 2. 通过GlobalKey 给目标StatefulWidget添加GlobalKey。 //定义一个globalKey, 由于GlobalKey要保持全局唯一性，我们使用静态变量存储 static GlobalKey _globalKey= GlobalKey(); ... Scaffold( key: _globalKey , //设置key ... ) 通过GlobalKey来获取State对象 _globalKey.currentState.openDrawer() 通过 RenderObject 自定义 Widget 通过RenderObject定义组件的方式： class CustomWidget extends LeafRenderObjectWidget{ @override RenderObject createRenderObject(BuildContext context) { // 创建 RenderObject return RenderCustomObject(); } @override void updateRenderObject(BuildContext context, RenderCustomObject renderObject) { // 更新 RenderObject super.updateRenderObject(context, renderObject); } } class RenderCustomObject extends RenderBox{ @override void performLayout() { // 实现布局逻辑 } @override void paint(PaintingContext context, Offset offset) { // 实现绘制 } } 如果组件不会包含子组件，则我们可以直接继承自 LeafRenderObjectWidget。 abstract class LeafRenderObjectWidget extends RenderObjectWidget { const LeafRenderObjectWidget({ Key? key }) : super(key: key); @override LeafRenderObjectElement createElement() => LeafRenderObjectElement(this); } 重写了 createRenderObject 方法。该方法被组件对应的 Element 调用（构建渲染树时）用于生成渲染对象。 updateRenderObject 方法是用于在组件树状态发生变化但不需要重新创建 RenderObject 时用于更新组件渲染对象的回调。 RenderCustomObject 类是继承自 RenderBox，而 RenderBox 继承自 RenderObject，我们需要在 RenderCustomObject 中实现布局、绘制、事件响应等逻辑 Flutter SDK内置组件库介绍 1. 基础组件 要使用基础组件库，需要先导入： import 'package:flutter/widgets.dart'; Text：文本。 Row、Column：在水平（Row）和垂直（Column）方向上创建灵活的布局。 Stack：取代线性布局。 Container：矩形视觉元素。 2. Material组件 Android系UI。 要使用 Material 组件，需要先引入它： import 'package:flutter/material.dart'; Material 应用程序以MaterialApp (opens new window) 组件开始。 Material 组件有Scaffold、AppBar、TextButton等。 3. Cupertino组件 iOS系UI。 MaterialPageRoute：在路由切换时，如果是 Android 系统，它将会使用 Android 系统默认的页面切换动画(从底向上)；如果是 iOS 系统，它会使用 iOS 系统默认的页面切换动画（从右向左）。 Cupertino 组件风格的页面： //导入cupertino `widget`库 import 'package:flutter/cupertino.dart'; class CupertinoTestRoute extends StatelessWidget { @override `widget`build(BuildContext context) { return CupertinoPageScaffold( navigationBar: CupertinoNavigationBar( middle: Text(\"Cupertino Demo\"), ), child: Center( child: CupertinoButton( color: CupertinoColors.activeBlue, child: Text(\"Press\"), onPressed: () {} ), ), ); } } 总结 Flutter 的widget类型分为StatefulWidget 和 StatelessWidget 两种。 引入过多组件库会让你的应用安装包变大。 由于 Material 和 Cupertino 都是在基础组件库之上的，所以如果我们的应用中引入了这两者之一，则不需要再引入flutter/ widgets.dart了，因为它们内部已经引入过了。 Widget 简介Flutter中Widget 、Element、RenderObject角色深入分析Flutter State生命周期 "},"pages/fultter/status_management.html":{"url":"pages/fultter/status_management.html","title":"状态管理","keywords":"","body":"状态管理 Widget管理自身状态 _TapboxAState 类: 管理TapboxA的状态。 定义_active：确定盒子的当前颜色的布尔值。 定义_handleTap()函数，该函数在点击该盒子时更新_active，并调用setState()更新UI。 实现widget的所有交互式行为。 // TapboxA 管理自身状态. //------------------------- TapboxA ---------------------------------- class TapboxA extends StatefulWidget { TapboxA({Key? key}) : super(key: key); @override _TapboxAState createState() => _TapboxAState(); } class _TapboxAState extends State { bool _active = false; void _handleTap() { setState(() { _active = !_active; }); } @override Widget build(BuildContext context) { return GestureDetector( onTap: _handleTap, child: Container( width: 200.0, height: 200.0, decoration: BoxDecoration( color: _active ? Colors.lightGreen[700] : Colors.grey[600], ), child: Center( child: Text( _active ? 'Active' : 'Inactive', style: const TextStyle(fontSize: 32.0, color: Colors.white), ), ), ), ); } } 父Widget管理子Widget的状态 ParentWidgetState 类: 为TapboxB 管理_active状态。 实现_handleTapboxChanged()，当盒子被点击时调用的方法。 当状态改变时，调用setState()更新UI。 TapboxB 类: 继承StatelessWidget类，因为所有状态都由其父组件处理。 当检测到点击时，它会通知父组件。 // ParentWidget 为 TapboxB 管理状态. //------------------------ ParentWidget -------------------------------- class ParentWidget extends StatefulWidget { @override _ParentWidgetState createState() => _ParentWidgetState(); } class _ParentWidgetState extends State { bool _active = false; void _handleTapboxChanged(bool newValue) { setState(() { _active = newValue; }); } @override Widget build(BuildContext context) { return Container( child: TapboxB( active: _active, onChanged: _handleTapboxChanged, ), ); } } //------------------------- TapboxB ---------------------------------- class TapboxB extends StatelessWidget { TapboxB({Key? key, this.active: false, required this.onChanged}) : super(key: key); final bool active; final ValueChanged onChanged; void _handleTap() { onChanged(!active); } Widget build(BuildContext context) { return GestureDetector( onTap: _handleTap, child: Container( child: Center( child: Text( active ? 'Active' : 'Inactive', style: TextStyle(fontSize: 32.0, color: Colors.white), ), ), width: 200.0, height: 200.0, decoration: BoxDecoration( color: active ? Colors.lightGreen[700] : Colors.grey[600], ), ), ); } } 混合状态管理 _ParentWidgetStateC类: 管理_active 状态。 实现 _handleTapboxChanged() ，当盒子被点击时调用。 当点击盒子并且_active状态改变时调用setState()更新UI。 _TapboxCState 对象: 管理_highlight 状态。 GestureDetector监听所有tap事件。当用户点下时，它添加高亮（深绿色边框）；当用户释放时，会移除高亮。 当按下、抬起、或者取消点击时更新_highlight状态，调用setState()更新UI。 当点击时，将状态的改变传递给父组件。 //---------------------------- ParentWidget ---------------------------- class ParentWidgetC extends StatefulWidget { @override _ParentWidgetCState createState() => _ParentWidgetCState(); } class _ParentWidgetCState extends State { bool _active = false; void _handleTapboxChanged(bool newValue) { setState(() { _active = newValue; }); } @override Widget build(BuildContext context) { return Container( child: TapboxC( active: _active, onChanged: _handleTapboxChanged, ), ); } } //----------------------------- TapboxC ------------------------------ class TapboxC extends StatefulWidget { TapboxC({Key? key, this.active: false, required this.onChanged}) : super(key: key); final bool active; final ValueChanged onChanged; @override _TapboxCState createState() => _TapboxCState(); } class _TapboxCState extends State { bool _highlight = false; void _handleTapDown(TapDownDetails details) { setState(() { _highlight = true; }); } void _handleTapUp(TapUpDetails details) { setState(() { _highlight = false; }); } void _handleTapCancel() { setState(() { _highlight = false; }); } void _handleTap() { widget.onChanged(!widget.active); } @override Widget build(BuildContext context) { // 在按下时添加绿色边框，当抬起时，取消高亮 return GestureDetector( onTapDown: _handleTapDown, // 处理按下事件 onTapUp: _handleTapUp, // 处理抬起事件 onTap: _handleTap, onTapCancel: _handleTapCancel, child: Container( child: Center( child: Text( widget.active ? 'Active' : 'Inactive', style: TextStyle(fontSize: 32.0, color: Colors.white), ), ), width: 200.0, height: 200.0, decoration: BoxDecoration( color: widget.active ? Colors.lightGreen[700] : Colors.grey[600], border: _highlight ? Border.all( color: Colors.black38, width: 10.0, ) : null, ), ), ); } } 全局状态管理 实现一个全局的事件总线，将语言状态改变对应为一个事件，然后在APP中依赖应用语言的组件的initState 方法中订阅语言改变的事件。当用户在设置页切换语言后，我们发布语言改变事件，而订阅了此事件的组件就会收到通知，收到通知后调用setState(...)方法重新build一下自身即可。 这种类似iOS的通知。 使用一些专门用于状态管理的包，如 Provider、Redux，读者可以在 pub 上查看其详细信息。 状态管理 "},"pages/fultter/routing_management.html":{"url":"pages/fultter/routing_management.html","title":"路由管理","keywords":"","body":"路由管理 一个简单示例 创建一个新路由，命名“NewRoute”。 class NewRoute extends StatelessWidget { @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: Text(\"New route\"), ), body: Center( child: Text(\"This is new route\"), ), ); } } 我们添加了一个打开新路由的按钮，点击该按钮后就会打开新的路由页面。 Column( mainAxisAlignment: MainAxisAlignment.center, children: [ ... //省略无关代码 TextButton( child: Text(\"open new route\"), onPressed: () { //导航到新路由 Navigator.push( context, MaterialPageRoute(builder: (context) { return NewRoute(); }), ); }, ), ], ) MaterialPageRoute MaterialPageRoute继承自PageRoute类，PageRoute类是一个抽象类，表示占有整个屏幕空间的一个模态路由页面。 MaterialPageRoute 是 Material组件库提供的组件，它可以针对不同平台，实现与平台页面切换动画风格一致的路由切换动画。 MaterialPageRoute 构造函数的各个参数的意义： MaterialPageRoute({ WidgetBuilder builder, RouteSettings settings, bool maintainState = true, bool fullscreenDialog = false, }) builder：是一个回调函数，返回值是一个widget，也就是我们跳转的页面。 settings：包含路由的配置信息，如路由名称、是否初始路由（首页）。 maintainState：一个已经不可见(被上面的盖住完全看不到啦~)的组件，是否还需要保存状态。 fullscreenDialog：表示新的路由页面是否是一个全屏的模态对话框。 Navigator Navigator是一个路由管理的组件，它提供了打开和退出路由页方法。 Future push(BuildContext context, Route route) 将给定的路由入栈（即打开新的页面），返回值是一个Future对象，用以接收新路由出栈（即关闭）时的返回数据。 bool pop(BuildContext context, [ result ]) 将栈顶路由出栈，result 为页面关闭时返回给上一个页面的数据。 实例方法 Navigator.push(BuildContext context, Route route)等价于Navigator.of(context).push(Route route)。 路由传值 传值： Navigator.pop(context, \"我是返回值\") 获取值： () async { // 打开`TipRoute`，并等待返回结果 var result = await Navigator.push( context, MaterialPageRoute( builder: (context) { return TipRoute( // 路由参数 text: \"我是提示xxxx\", ); }, ), ); //输出`TipRoute`路由返回结果 print(\"路由返回值: $result\"); } 命名路由 1. 路由表 它是一个Map，key为路由的名字，是个字符串；value是个builder回调函数，用于生成相应的路由widget。 Map routes; 2. 注册路由表 MaterialApp添加routes属性: MaterialApp( title: 'Flutter Demo', theme: ThemeData( primarySwatch: Colors.blue, ), //注册路由表 routes:{ \"new_page\":(context) => NewRoute(), ... // 省略其他路由注册信息 } , home: MyHomePage(title: 'Flutter Demo Home Page'), ); 将home注册为命名路由: MaterialApp( title: 'Flutter Demo', initialRoute:\"/\", //名为\"/\"的路由作为应用的home(首页) theme: ThemeData( primarySwatch: Colors.blue, ), //注册路由表 routes:{ \"new_page\":(context) => NewRoute(), \"/\":(context) => MyHomePage(title: 'Flutter Demo Home Page'), //注册首页路由 } ); 3. 打开新路由页 可以使用Navigator 的pushNamed方法： Future pushNamed(BuildContext context, String routeName,{Object arguments}) 调用： onPressed: () { Navigator.pushNamed(context, \"new_page\"); //Navigator.push(context, // MaterialPageRoute(builder: (context) { // return NewRoute(); //})); }, 4. 命名路由参数传递 传递参数: Navigator.of(context).pushNamed(\"new_page\", arguments: \"hi\"); 获取路由参数: @override Widget build(BuildContext context) { //获取路由参数 var args=ModalRoute.of(context).settings.arguments; //...省略无关代码 } 5. 带参数的路由 MaterialApp( ... //省略无关代码 routes: { \"tip2\": (context){ return TipRoute(text: ModalRoute.of(context)!.settings.arguments); }, }, ); 路由生成钩子 MaterialApp有一个onGenerateRoute属性，它在打开命名路由时可能会被调用，之所以说可能，是因为当调用Navigator.pushNamed(...)打开命名路由时，如果指定的路由名在路由表中已注册，则会调用路由表中的builder函数来生成路由组件；如果路由表中没有注册，才会调用onGenerateRoute来生成路由。onGenerateRoute回调签名如下： Route Function(RouteSettings settings) 有了onGenerateRoute回调，要实现上面控制页面权限的功能就非常容易：我们放弃使用路由表，取而代之的是提供一个onGenerateRoute回调，然后在该回调中进行统一的权限控制，如： MaterialApp( ... //省略无关代码 onGenerateRoute:(RouteSettings settings){ return MaterialPageRoute(builder: (context){ String routeName = settings.name; // 如果访问的路由页需要登录，但当前未登录，则直接返回登录页路由， // 引导用户登录；其他情况则正常打开路由。 } ); } ); onGenerateRoute onUnknownRoute区别：onGenerateRoute 无法生成路由，会触发OnUnknownRoute 属性来处理该场景。 注意，onGenerateRoute 只会对命名路由生效。也就是需要调用Navigator.pushNamed。 路由管理Flutter 路由原理解析Flutter 中的 onUnknownRoute 是什么 "},"pages/fultter/package_management.html":{"url":"pages/fultter/package_management.html","title":"包管理","keywords":"","body":"包管理 YAML name: flutter_in_action description: First Flutter Application. version: 1.0.0+1 dependencies: flutter: sdk: flutter cupertino_icons: ^0.1.2 dev_dependencies: flutter_test: sdk: flutter flutter: uses-material-design: true Pub仓库 Pub（https://pub.dev/ ）是 Google 官方的 Dart Packages 仓库，类似于 node 中的 npm仓库、Android中的 jcenter。我们可以在 Pub 上面查找我们需要的包和插件，也可以向 Pub 发布我们的包和插件。 示例 1.将“english_words” 添加到依赖项列表，如下： dependencies: flutter: sdk: flutter # 新添加的依赖 english_words: ^4.0.0 2.下载包。在Android Studio的编辑器视图中查看pubspec.yaml时（图2-13），单击右上角的 Pub get 。3.引入english_words包。 import 'package:english_words/english_words.dart'; 4.使用english_words包来生成随机字符串。 // 生成随机字符串 final wordPair = WordPair.random(); 其他依赖方式 1.依赖本地包 dependencies: pkg1: path: ../../code/pkg1 2.依赖Git dependencies: pkg1: git: url: git://github.com/xxx/pkg1.git 3.使用path参数指定相对位置 dependencies: package1: git: url: git://github.com/flutter/packages.git path: packages/package1 依赖方式:https://www.dartlang.org/tools/pub/dependencies 包管理 "},"pages/fultter/resource_management.html":{"url":"pages/fultter/resource_management.html","title":"资源管理","keywords":"","body":"资源管理 常见类型的 assets 包括静态数据（例如JSON文件）、配置文件、图标和图片等。 指定 assets 在pubspec.yaml中配置： flutter: assets: - assets/my_icon.png - assets/background.png assets指定应包含在应用程序中的文件， 每个 asset 都通过相对于pubspec.yaml文件所在的文件系统路径来标识自身的路径。 Asset 变体（variant） 例如，如果应用程序目录中有以下文件: …/pubspec.yaml …/graphics/my_icon.png …/graphics/background.png …/graphics/dark/background.png …etc. 然后pubspec.yaml文件中只需包含: flutter: assets: - graphics/background.png 那么这两个graphics/background.png和graphics/dark/background.png 都将包含在您的 asset bundle中。前者被认为是main asset （主资源），后者被认为是一种变体（variant）。 在选择匹配当前设备分辨率的图片时，Flutter会使用到 asset 变体（见下文）。 加载 assets 1. 加载文本assets rootBundle：全局静态对象。 Future loadAsset() async { return await rootBundle.loadString('assets/config.json'); } DefaultAssetBundle: DefaultAssetBundle.of(context) 2. 加载图片 1) 声明分辨率相关的图片 assets 2) 加载图片 Widget build(BuildContext context) { return DecoratedBox( decoration: BoxDecoration( image: DecorationImage( image: AssetImage('graphics/background.png'), ), ), ); } AssetImage 并非是一个widget， 它实际上是一个ImageProvider，有些时候你可能期望直接得到一个显示图片的widget，那么你可以使用Image.asset()方法，如： Widget build(BuildContext context) { return Image.asset('graphics/background.png'); } 3) 依赖包中的资源图片 AssetImage('icons/heart.png', package: 'my_icons' 或 Image.asset('icons/heart.png', package: 'my_icons') 4) 打包包中的 assets 3. 特定平台 assets 1）设置APP图标 2）更新启动页 平台共享 assets Flutter 提供了一种Flutter和原生之间共享资源的方式。 资源管理 "},"pages/fultter/Text_and_style.html":{"url":"pages/fultter/Text_and_style.html","title":"文本及样式","keywords":"","body":"文本及样式 源码 Flutter 中使用自定义字体 文本及样式 Text Text(\"Hello world\", textAlign: TextAlign.left, ); Text(\"Hello world! I'm Jack. \"*4, maxLines: 1, overflow: TextOverflow.ellipsis, ); Text(\"Hello world\", textScaleFactor: 1.5, ); TextStyle Text(\"Hello world\", style: TextStyle( color: Colors.blue, fontSize: 18.0, height: 1.2, fontFamily: \"Courier\", background: Paint()..color=Colors.yellow, decoration:TextDecoration.underline, decorationStyle: TextDecorationStyle.dashed ), ); TextSpan Text.rich(TextSpan( children: [ TextSpan( text: \"Home: \" ), TextSpan( text: \"https://flutterchina.club\", style: TextStyle( color: Colors.blue ), recognizer: _tapRecognizer ), ] )) DefaultTextStyle DefaultTextStyle( //1.设置文本默认样式 style: TextStyle( color:Colors.red, fontSize: 20.0, ), textAlign: TextAlign.start, child: Column( crossAxisAlignment: CrossAxisAlignment.start, children: [ Text(\"hello world\"), Text(\"I am Jack\"), Text(\"I am Jack\", style: TextStyle( inherit: false, //2.不继承默认样式 color: Colors.grey ), ), ], ), ); 字体 "},"pages/fultter/Button_style.html":{"url":"pages/fultter/Button_style.html","title":"按钮样式","keywords":"","body":"按钮样式 源码 按钮 flutter TextButton样式 ElevatedButton ElevatedButton( child: Text(\"normal\"), onPressed: () {}, ); TextButton TextButton( child: Text(\"normal\"), onPressed: () {}, ) OutlinedButton OutlineButton( child: Text(\"normal\"), onPressed: () {}, ) IconButton IconButton( icon: Icon(Icons.thumb_up), onPressed: () {}, ) 带图标的按钮 ElevatedButton.icon( icon: Icon(Icons.send), label: Text(\"发送\"), onPressed: _onPressed, ), OutlinedButton.icon( icon: Icon(Icons.add), label: Text(\"添加\"), onPressed: _onPressed, ), TextButton.icon( icon: Icon(Icons.info), label: Text(\"详情\"), onPressed: _onPressed, ), "},"pages/fultter/Picture_style.html":{"url":"pages/fultter/Picture_style.html","title":"图片样式","keywords":"","body":"图片样式 源码 图片及ICON 图片资源 使用flutter加载本地图片报错 图片 Flutter 中，我们可以通过Image组件来加载并显示图片，Image的数据源可以是asset、文件、内存以及网络。 1. ImageProvider ImageProvider 是一个抽象类，主要定义了图片数据获取的接口load()，从不同的数据源获取图片需要实现不同的ImageProvider ，如AssetImage是实现了从Asset中加载图片的 ImageProvider，而NetworkImage 实现了从网络加载图片的 ImageProvider。 2. Image Image widget 有一个必选的image参数，它对应一个 ImageProvider。下面我们分别演示一下如何从 asset 和网络加载图片。 1）先配置.yaml，再从asset中加载图片： Image( image: AssetImage(\"images/avatar.png\"), width: 100.0 ); 或 Image.asset(\"images/avatar.png\", width: 100.0, ) 2) 从网络加载图片： Image( image: NetworkImage( \"https://avatars2.githubusercontent.com/u/20411648?s=460&v=4\"), width: 100.0, ) 或 Image.network( \"https://avatars2.githubusercontent.com/u/20411648?s=460&v=4\", width: 100.0, ) 3）参数 ICON "},"pages/fultter/Radio_switches_and_checkboxes.html":{"url":"pages/fultter/Radio_switches_and_checkboxes.html","title":"单选开关和复选框","keywords":"","body":"单选开关和复选框 源码 单选开关和复选框 class SwitchAndCheckBoxTestRoute extends StatefulWidget { @override _SwitchAndCheckBoxTestRouteState createState() => _SwitchAndCheckBoxTestRouteState(); } class _SwitchAndCheckBoxTestRouteState extends State { bool _switchSelected=true; //维护单选开关状态 bool _checkboxSelected=true;//维护复选框状态 @override Widget build(BuildContext context) { return Column( children: [ Switch( value: _switchSelected,//当前状态 onChanged:(value){ //重新构建页面 setState(() { _switchSelected=value; }); }, ), Checkbox( value: _checkboxSelected, activeColor: Colors.red, //选中时的颜色 onChanged:(value){ setState(() { _checkboxSelected=value; }); } , ) ], ); } } "},"pages/fultter/Input_box_and_form.html":{"url":"pages/fultter/Input_box_and_form.html","title":"输入框及表单","keywords":"","body":"输入框及表单 源码 输入框及表单 TextField参数 controller：编辑框的控制器，通过它可以设置/获取编辑框的内容、选择编辑内容、监听编辑文本改变事件。大多数情况下我们都需要显式提供一个controller来与文本框交互。如果没有提供controller，则TextField内部会自动创建一个。 focusNode：用于控制TextField是否占有当前键盘的输入焦点。它是我们和键盘交互的一个句柄（handle）。 InputDecoration：用于控制TextField的外观显示，如提示文本、背景颜色、边框等。 keyboardType：用于设置该输入框默认的键盘输入类型。 textInputAction：键盘动作按钮图标(即回车键位图标)，它是一个枚举值，有多个可选值。 style：正在编辑的文本样式。 textAlign: 输入框内编辑文本在水平方向的对齐方式。 autofocus: 是否自动获取焦点。 obscureText：是否隐藏正在编辑的文本，如用于输入密码的场景等，文本内容会用“•”替换。 maxLines：输入框的最大行数，默认为1；如果为null，则无行数限制。 maxLength和maxLengthEnforcement ：maxLength代表输入框文本的最大长度，设置后输入框右下角会显示输入的文本计数。maxLengthEnforcement决定当输入文本长度超过maxLength时如何处理，如截断、超出等。 toolbarOptions：长按或鼠标右击时出现的菜单，包括 copy、cut、paste 以及 selectAll。 onChange：输入框内容改变时的回调函数；注：内容改变事件也可以通过controller来监听。 onEditingComplete和onSubmitted：这两个回调都是在输入框输入完成时触发，比如按了键盘的完成键（对号图标）或搜索键（🔍图标）。不同的是两个回调签名不同，onSubmitted回调是ValueChanged类型，它接收当前输入内容做为参数，而onEditingComplete不接收参数。 inputFormatters：用于指定输入格式；当用户输入内容改变时，会根据指定的格式来校验。 enable：如果为false，则输入框会被禁用，禁用状态不接收输入和事件，同时显示禁用态样式（在其decoration中定义）。 cursorWidth、cursorRadius和cursorColor：这三个属性是用于自定义输入框光标宽度、圆角和颜色的。 示例 1. 布局 Column( children: [ TextField( autofocus: true, decoration: InputDecoration( labelText: \"用户名\", hintText: \"用户名或邮箱\", prefixIcon: Icon(Icons.person) ), ), TextField( decoration: InputDecoration( labelText: \"密码\", hintText: \"您的登录密码\", prefixIcon: Icon(Icons.lock) ), obscureText: true, ), ], ); 2. 获取输入内容 //定义一个controller TextEditingController _unameController = TextEditingController(); TextField( autofocus: true, controller: _unameController, //设置controller ... ) print(_unameController.text) 3. 监听文本变化 TextField( autofocus: true, onChanged: (v) { print(\"onChange: $v\"); } ) 或 @override void initState() { //监听输入改变 _unameController.addListener((){ print(_unameController.text); }); } 4. 控制焦点 5. 监听焦点状态改变事件 6. 自定义样式 表单Form 1. Form 2. FormField 3. FormState "},"pages/fultter/progress_indicator.html":{"url":"pages/fultter/progress_indicator.html","title":"进度指示器","keywords":"","body":"进度指示器 源码 进度指示器 LinearProgressIndicator LinearProgressIndicator是一个线性、条状的进度条，定义如下： LinearProgressIndicator({ double value, Color backgroundColor, Animation valueColor, ... }) value：value表示当前的进度，取值范围为[0,1]；如果value为null时则指示器会执行一个循环动画（模糊进度）；当value不为null时，指示器为一个具体进度的进度条。 backgroundColor：指示器的背景色。 valueColor: 指示器的进度条颜色；值得注意的是，该值类型是Animation，这允许我们对进度条的颜色也可以指定动画。如果我们不需要对进度条颜色执行动画，换言之，我们想对进度条应用一种固定的颜色，此时我们可以通过AlwaysStoppedAnimation来指定。 示例： // 模糊进度条(会执行一个动画) LinearProgressIndicator( backgroundColor: Colors.grey[200], valueColor: AlwaysStoppedAnimation(Colors.blue), ), //进度条显示50% LinearProgressIndicator( backgroundColor: Colors.grey[200], valueColor: AlwaysStoppedAnimation(Colors.blue), value: .5, ) CircularProgressIndicator CircularProgressIndicator是一个圆形进度条，定义如下： CircularProgressIndicator({ double value, Color backgroundColor, Animation valueColor, this.strokeWidth = 4.0, ... }) 前三个参数和LinearProgressIndicator相同，不再赘述。strokeWidth 表示圆形进度条的粗细。示例如下： // 模糊进度条(会执行一个旋转动画) CircularProgressIndicator( backgroundColor: Colors.grey[200], valueColor: AlwaysStoppedAnimation(Colors.blue), ), //进度条显示50%，会显示一个半圆 CircularProgressIndicator( backgroundColor: Colors.grey[200], valueColor: AlwaysStoppedAnimation(Colors.blue), value: .5, ), "},"pages/fultter/Introduction_to_layout_components.html":{"url":"pages/fultter/Introduction_to_layout_components.html","title":"布局类组件介绍","keywords":"","body":"布局类组件介绍 布局类组件介绍 布局类组件都会包含一个或多个子组件，不同的布局类组件对子组件排列（layout）方式不同。 布局类 1. LeafRenderObjectWidget 非容器类组件基类，Widget树的叶子节点，用于没有子节点的widget，通常基础组件都属于这一类，如Image。 2. SingleChildRenderObjectWidget 单子组件基类，包含一个子Widget，如：ConstrainedBox、DecoratedBox等。 3. MultiChildRenderObjectWidget 多子组件基类，包含多个子Widget，一般都有一个children参数，接受一个Widget数组。如Row、Column、Stack等。 4. 继承关系 Widget > RenderObjectWidget > (Leaf/SingleChild/MultiChild)RenderObjectWidget 。 RenderObjectWidget 子类必须实现创建、更新RenderObject的方法。RenderObject是最终布局、渲染UI界面的对象，实现布局算法。Stack（层叠布局）对应的RenderObject对象就是RenderStack，而层叠布局的实现就在RenderStack中。 "},"pages/fultter/Layout_principles_and_constraints.html":{"url":"pages/fultter/Layout_principles_and_constraints.html","title":"布局原理与约束","keywords":"","body":"布局原理与约束 源码 布局原理与约束 尺寸限制类容器用于限制容器大小，Flutter中提供了多种这样的容器，如ConstrainedBox、SizedBox、UnconstrainedBox、AspectRatio 等，本节将介绍一些常用的。 任何时候子组件都必须先遵守父组件的约束。 Flutter布局模型 1. 两种布局模型 基于 RenderBox 的盒模型布局。 基于 Sliver ( RenderSliver ) 按需加载列表布局。 2. 布局流程 上层组件向下层组件传递约束（constraints）条件。 下层组件确定自己的大小，然后告诉上层组件。注意下层组件的大小必须符合父组件的约束。 上层组件确定下层组件相对于自身的偏移和确定自身的大小（大多数情况下会根据子组件的大小来确定自身的大小）。 BoxConstraints BoxConstraints 是盒模型布局过程中父渲染对象传递给子渲染对象的约束信息。BoxConstraints.tight(Size size)：固定宽高，BoxConstraints.expand()：尽可能大。 const BoxConstraints({ this.minWidth = 0.0, //最小宽度 this.maxWidth = double.infinity, //最大宽度 this.minHeight = 0.0, //最小高度 this.maxHeight = double.infinity //最大高度 }) ConstrainedBox 对子组件添加额外的约束，可以设置constraints。 ConstrainedBox( constraints: BoxConstraints( minWidth: double.infinity, //宽度尽可能大 minHeight: 50.0 //最小高度为50像素 ), child: Container( height: 5.0, child: redBox , ), ) SizedBox 用于给子元素指定固定的宽高。 SizedBox( width: 80.0, height: 80.0, child: redBox ) ConstrainedBox与SizedBox关系 ConstrainedBox和SizedBox都是通过RenderConstrainedBox来渲染的，我们可以看到ConstrainedBox和SizedBox的createRenderObject()方法都返回的是一个RenderConstrainedBox对象 @override RenderConstrainedBox createRenderObject(BuildContext context) { return RenderConstrainedBox( additionalConstraints: ..., ); } 多重限制 ConstrainedBox( constraints: BoxConstraints(minWidth: 60.0, minHeight: 60.0), //父 child: ConstrainedBox( constraints: BoxConstraints(minWidth: 90.0, minHeight: 20.0),//子 child: redBox, ), ) 或 ConstrainedBox( constraints: BoxConstraints(minWidth: 90.0, minHeight: 20.0), child: ConstrainedBox( constraints: BoxConstraints(minWidth: 60.0, minHeight: 60.0), child: redBox, ) ) 显示效果相同，90x60。我们发现有多重限制时，对于minWidth和minHeight来说，是取父子中相应数值较大的。实际上，只有这样才能保证父限制与子限制不冲突。 UnconstrainedBox ConstrainedBox( constraints: BoxConstraints(minWidth: 60.0, minHeight: 100.0), //父 child: UnconstrainedBox( //“去除”父级限制 child: ConstrainedBox( constraints: BoxConstraints(minWidth: 90.0, minHeight: 20.0),//子 child: redBox, ), ) ) 上面代码中，如果没有中间的UnconstrainedBox，那么根据上面所述的多重限制规则，那么最终将显示一个90×100的红色框。但是由于UnconstrainedBox “去除”了父ConstrainedBox的限制，则最终会按照子ConstrainedBox的限制来绘制redBox，即90×20。 但是，读者请注意，UnconstrainedBox对父组件限制的“去除”并非是真正的去除：上面例子中虽然红色区域大小是90×20，但上方仍然有80的空白空间。也就是说父限制的minHeight(100.0)仍然是生效的，只不过它不影响最终子元素redBox的大小，但仍然还是占有相应的空间，可以认为此时的父ConstrainedBox是作用于子UnconstrainedBox上，而redBox只受子ConstrainedBox限制。 "},"pages/fultter/Linear_layout.html":{"url":"pages/fultter/Linear_layout.html","title":"线性布局","keywords":"","body":"线性布局 源码 线性布局 主轴和纵轴 主轴：Row的主轴是水平方向，因为Row是沿水平方向布局；Column的主轴是垂直方向，同理； 纵轴：Row的纵轴是垂直方向，而Column的纵轴是水平方向。 Row Row可以沿水平方向排列其子widget。定义如下： Row({ ... TextDirection textDirection, MainAxisSize mainAxisSize = MainAxisSize.max, MainAxisAlignment mainAxisAlignment = MainAxisAlignment.start, VerticalDirection verticalDirection = VerticalDirection.down, CrossAxisAlignment crossAxisAlignment = CrossAxisAlignment.center, List children = const [], }) textDirection：Row是沿着水平方向布局，那么就存在，从左到右还是从右到左，textDirection就是决定这个布局方向。 mainAxisSize：水平方向上是否占用最大空间。 mainAxisAlignment：水平方向的对齐方式，与textDirection共同决定。 verticalDirection：垂直方向对齐方向。 crossAxisAlignment：垂直方向的对齐方式，与verticalDirection共同决定。 示例： Column( //测试Row对齐方式，排除Column默认居中对齐的干扰 crossAxisAlignment: CrossAxisAlignment.start, children: [ Row( mainAxisAlignment: MainAxisAlignment.center, children: [ Text(\" hello world \"), Text(\" I am Jack \"), ], ), Row( mainAxisSize: MainAxisSize.min, mainAxisAlignment: MainAxisAlignment.center, children: [ Text(\" hello world \"), Text(\" I am Jack \"), ], ), Row( mainAxisAlignment: MainAxisAlignment.end, textDirection: TextDirection.rtl, children: [ Text(\" hello world \"), Text(\" I am Jack \"), ], ), Row( crossAxisAlignment: CrossAxisAlignment.start, verticalDirection: VerticalDirection.up, children: [ Text(\" hello world \", style: TextStyle(fontSize: 30.0),), Text(\" I am Jack \"), ], ), ], ) 解释：第一个Row很简单，默认为居中对齐；第二个Row，由于mainAxisSize值为MainAxisSize.min，Row的宽度等于两个Text的宽度和，所以对齐是无意义的，所以会从左往右显示；第三个Row设置textDirection值为TextDirection.rtl，所以子组件会从右向左的顺序排列，而此时MainAxisAlignment.end表示左对齐，所以最终显示结果就是图中第三行的样子；第四个 Row 测试的是纵轴的对齐方式，由于两个子 Text 字体不一样，所以其高度也不同，我们指定了verticalDirection值为VerticalDirection.up，即从低向顶排列，而此时crossAxisAlignment值为CrossAxisAlignment.start表示底对齐。 Column Column可以在垂直方向排列其子组件。 "},"pages/fultter/flexible_layout.html":{"url":"pages/fultter/flexible_layout.html","title":"弹性布局","keywords":"","body":"弹性布局 源码 弹性布局 弹性布局允许子组件按照一定比例来分配父容器空间。 Flex Flex继承自MultiChildRenderObjectWidget，对应的RenderObject为RenderFlex，RenderFlex中实现了其布局算法。 Flex({ ... required this.direction, //弹性布局的方向, Row默认为水平方向，Column默认为垂直方向 List children = const [], }) Expanded Expanded 只能作为 Flex 的孩子（否则会报错），它可以按比例“扩伸”Flex子组件所占用的空间。因为 Row和Column 都继承自 Flex，所以 Expanded 也可以作为它们的孩子。 const Expanded({ int flex = 1, required Widget child, }) flex参数为弹性系数，如果为 0 或null，则child是没有弹性的，即不会被扩伸占用的空间。如果大于0，所有的Expanded按照其 flex 的比例来分割主轴的全部空闲空间。下面我们看一个例子： class FlexLayoutTestRoute extends StatelessWidget { @override Widget build(BuildContext context) { return Column( children: [ //Flex的两个子widget按1：2来占据水平空间 Flex( direction: Axis.horizontal, children: [ Expanded( flex: 1, child: Container( height: 30.0, color: Colors.red, ), ), Expanded( flex: 2, child: Container( height: 30.0, color: Colors.green, ), ), ], ), Padding( padding: const EdgeInsets.only(top: 20.0), child: SizedBox( height: 100.0, //Flex的三个子widget，在垂直方向按2：1：1来占用100像素的空间 child: Flex( direction: Axis.vertical, children: [ Expanded( flex: 2, child: Container( height: 30.0, color: Colors.red, ), ), Spacer( flex: 1, ), Expanded( flex: 1, child: Container( height: 30.0, color: Colors.green, ), ), ], ), ), ), ], ); } } 示例中的Spacer的功能是占用指定比例的空间，实际上它只是Expanded的一个包装类，Spacer的源码如下： class Spacer extends StatelessWidget { const Spacer({Key? key, this.flex = 1}) : assert(flex != null), assert(flex > 0), super(key: key); final int flex; @override Widget build(BuildContext context) { return Expanded( flex: flex, child: const SizedBox.shrink(), ); } } "},"pages/fultter/flow_layout.html":{"url":"pages/fultter/flow_layout.html","title":"流式布局","keywords":"","body":"流式布局 源码 流式布局 超出屏幕显示范围会自动折行的布局称为流式布局。 Wrap 下面是Wrap的定义: Wrap({ ... this.direction = Axis.horizontal, this.alignment = WrapAlignment.start, this.spacing = 0.0, this.runAlignment = WrapAlignment.start, this.runSpacing = 0.0, this.crossAxisAlignment = WrapCrossAlignment.start, this.textDirection, this.verticalDirection = VerticalDirection.down, List children = const [], }) spacing：主轴方向子widget的间距 runSpacing：纵轴方向的间距 runAlignment：纵轴方向的对齐方式 下面看一个示例子： Wrap( spacing: 8.0, // 主轴(水平)方向间距 runSpacing: 4.0, // 纵轴（垂直）方向间距 alignment: WrapAlignment.center, //沿主轴方向居中 children: [ Chip( avatar: CircleAvatar(backgroundColor: Colors.blue, child: Text('A')), label: Text('Hamilton'), ), Chip( avatar: CircleAvatar(backgroundColor: Colors.blue, child: Text('M')), label: Text('Lafayette'), ), Chip( avatar: CircleAvatar(backgroundColor: Colors.blue, child: Text('H')), label: Text('Mulligan'), ), Chip( avatar: CircleAvatar(backgroundColor: Colors.blue, child: Text('J')), label: Text('Laurens'), ), ], ) Flow 主要用于一些需要自定义布局策略或性能要求较高(如动画中)的场景。 "},"pages/fultter/cascading_layout.html":{"url":"pages/fultter/cascading_layout.html","title":"层叠布局","keywords":"","body":"层叠布局 源码 层叠布局 子组件可以根据距父容器四个角的位置来确定自身的位置。层叠布局允许子组件按照代码中声明的顺序堆叠起来。 Stack Stack({ this.alignment = AlignmentDirectional.topStart, this.textDirection, this.fit = StackFit.loose, this.clipBehavior = Clip.hardEdge, List children = const [], }) alignment：决定如何去对齐没有定位（没有使用Positioned）或部分定位的子组件。 textDirection：和Row、Wrap的textDirection功能一样，都用于确定alignment对齐的参考系。 fit：此参数用于确定没有定位的子组件如何去适应Stack的大小。 clipBehavior：此属性决定对超出Stack显示空间的部分如何剪裁。 Positioned const Positioned({ Key? key, this.left, this.top, this.right, this.bottom, this.width, this.height, required Widget child, }) left、top 、right、 bottom分别代表离Stack左、上、右、底四边的距离。width和height用于指定需要定位元素的宽度和高度。注意，Positioned的width、height 和其他地方的意义稍微有点区别，此处用于配合left、top 、right、 bottom来定位组件，举个例子，在水平方向时，你只能指定left、right、width三个属性中的两个，如指定left和width后，right会自动算出(left+width)，如果同时指定三个属性则会报错，垂直方向同理。 示例 //通过ConstrainedBox来确保Stack占满屏幕 ConstrainedBox( constraints: BoxConstraints.expand(), child: Stack( alignment:Alignment.center , //指定未定位或部分定位widget的对齐方式 children: [ Container( child: Text(\"Hello world\",style: TextStyle(color: Colors.white)), color: Colors.red, ), Positioned( left: 18.0, child: Text(\"I am Jack\"), ), Positioned( top: 18.0, child: Text(\"Your friend\"), ) ], ), ); 由于第一个子文本组件Text(\"Hello world\")没有指定定位，并且alignment值为Alignment.center，所以它会居中显示。第二个子文本组件Text(\"I am Jack\")只指定了水平方向的定位(left)，所以属于部分定位，即垂直方向上没有定位，那么它在垂直方向的对齐方式则会按照alignment指定的对齐方式对齐，即垂直方向居中。对于第三个子文本组件Text(\"Your friend\")，和第二个Text原理一样，只不过是水平方向没有定位，则水平方向居中。 我们给上例中的Stack指定一个fit属性，然后将三个子文本组件的顺序调整一下： Stack( alignment:Alignment.center , fit: StackFit.expand, //未定位widget占满Stack整个空间 children: [ Positioned( left: 18.0, child: Text(\"I am Jack\"), ), Container(child: Text(\"Hello world\",style: TextStyle(color: Colors.white)), color: Colors.red, ), Positioned( top: 18.0, child: Text(\"Your friend\"), ) ], ), 可以看到，由于第二个子文本组件没有定位，所以fit属性会对它起作用，就会占满Stack。由于Stack子元素是堆叠的，所以第一个子文本组件被第二个遮住了，而第三个在最上层，所以可以正常显示。 "},"pages/fultter/Alignment_and_relative_positioning.html":{"url":"pages/fultter/Alignment_and_relative_positioning.html","title":"对齐与相对定位","keywords":"","body":"对齐与相对定位 源码 对齐与相对定位 Align Align 组件可以调整子组件的位置，定义如下： Align({ Key key, this.alignment = Alignment.center, this.widthFactor, this.heightFactor, Widget child, }) alignment：需要一个AlignmentGeometry类型的值，表示子组件在父组件中的起始位置。 AlignmentGeometry 是一个抽象类，它有两个常用的子类：Alignment和 FractionalOffset。 widthFactor和heightFactor是用于确定Align 组件本身宽高的属性；它们是两个缩放因子，会分别乘以子元素的宽、高，最终的结果就是Align 组件的宽高。如果值为null，则组件的宽高将会占用尽可能多的空间。 示例 Container( height: 120.0, width: 120.0, color: Colors.blue.shade50, child: Align( alignment: Alignment.topRight, child: FlutterLogo( size: 60, ), ), ) 或 Align( widthFactor: 2, heightFactor: 2, alignment: Alignment.topRight, child: FlutterLogo( size: 60, ), ), 效果一样，FlutterLogo的宽高为 60，则Align的最终宽高都为2*60=120。 Alignment.topRight： //右上角 static const Alignment topRight = Alignment(1.0, -1.0); 可以看到它只是Alignment的一个实例。 Alignment Alignment继承自AlignmentGeometry，表示矩形内的一个点，他有两个属性x、y，分别表示在水平和垂直方向的偏移，Alignment定义如下： Alignment(this.x, this.y) Alignment Widget会以矩形的中心点作为坐标原点。 Alignment可以通过其坐标转换公式将其坐标转为子元素的具体偏移坐标： (Alignment.x*childWidth/2+childWidth/2, Alignment.y*childHeight/2+childHeight/2) FractionalOffset FractionalOffset 继承自 Alignment，它和 Alignment唯一的区别就是坐标原点不同！FractionalOffset 的坐标原点为矩形的左侧顶点，这和布局系统的一致。FractionalOffset的坐标转换公式为： 实际偏移 = (FractionalOffse.x * childWidth, FractionalOffse.y * childHeight) Center Center组件其实是对齐方式确定（Alignment.center）了的Align。 class Center extends Align { const Center({ Key? key, double widthFactor, double heightFactor, Widget? child }) : super(key: key, widthFactor: widthFactor, heightFactor: heightFactor, child: child); } "},"pages/fultter/LayoutBuilder.html":{"url":"pages/fultter/LayoutBuilder.html","title":"LayoutBuilder","keywords":"","body":"LayoutBuilder 源码 LayoutBuilder 通过 LayoutBuilder，我们可以在布局过程中拿到父组件传递的约束信息，然后我们可以根据约束信息动态的构建不同的布局。 示例 比如我们实现一个响应式的 Column 组件 ResponsiveColumn，它的功能是当当前可用的宽度小于 200 时，将子组件显示为一列，否则显示为两列。简单来实现一下： class ResponsiveColumn extends StatelessWidget { const ResponsiveColumn({Key? key, required this.children}) : super(key: key); final List children; @override Widget build(BuildContext context) { // 通过 LayoutBuilder 拿到父组件传递的约束，然后判断 maxWidth 是否小于200 return LayoutBuilder( builder: (BuildContext context, BoxConstraints constraints) { if (constraints.maxWidth []; for (var i = 0; i 可以发现 LayoutBuilder 的使用很简单，但是不要小看它，因为它非常实用且重要，它主要有两个使用场景： 可以使用 LayoutBuilder 来根据设备的尺寸来实现响应式布局。 LayoutBuilder 可以帮我们高效排查问题。比如我们在遇到布局问题或者想调试组件树中某一个节点布局的约束时 LayoutBuilder 就很有用。 打印布局时的约束信息 class LayoutLogPrint extends StatelessWidget { const LayoutLogPrint({ Key? key, this.tag, required this.child, }) : super(key: key); final Widget child; final T? tag; //指定日志tag @override Widget build(BuildContext context) { return LayoutBuilder(builder: (_, constraints) { // assert在编译release版本时会被去除 assert(() { print('${tag ?? key ?? child}: $constraints'); return true; }()); return child; }); } } 控制台输出： flutter: Text(\"xfsdjlfsx\"): BoxConstraints(0.0 注意！我们的大前提是盒模型布局，如果是Sliver 布局，可以使用 SliverLayoutBuiler 来打印。 AfterLayout "},"pages/fultter/Padding.html":{"url":"pages/fultter/Padding.html","title":"Padding","keywords":"","body":"Padding 源码 填充（Padding） Padding Padding可以给其子节点添加填充（留白），和边距效果类似。我们在前面很多示例中都已经使用过它了，现在来看看它的定义： Padding({ ... EdgeInsetsGeometry padding, Widget child, }) EdgeInsetsGeometry是一个抽象类，开发中，我们一般都使用EdgeInsets类，它是EdgeInsetsGeometry的一个子类，定义了一些设置填充的便捷方法。 EdgeInsets 我们看看EdgeInsets提供的便捷方法： fromLTRB(double left, double top, double right, double bottom)：分别指定四个方向的填充。 all(double value) : 所有方向均使用相同数值的填充。 only({left, top, right ,bottom })：可以设置具体某个方向的填充(可以同时指定多个方向)。 symmetric({ vertical, horizontal })：用于设置对称方向的填充，vertical指top和bottom，horizontal指left和right。 示例 下面的示例主要展示了EdgeInsets的不同用法，比较简单，源码如下： class PaddingTestRoute extends StatelessWidget { const PaddingTestRoute({Key? key}) : super(key: key); @override Widget build(BuildContext context) { return Padding( //上下左右各添加16像素补白 padding: const EdgeInsets.all(16), child: Column( //显式指定对齐方式为左对齐，排除对齐干扰 crossAxisAlignment: CrossAxisAlignment.start, mainAxisSize: MainAxisSize.min, children: const [ Padding( //左边添加8像素补白 padding: EdgeInsets.only(left: 8), child: Text(\"Hello world\"), ), Padding( //上下各添加8像素补白 padding: EdgeInsets.symmetric(vertical: 8), child: Text(\"I am Jack\"), ), Padding( // 分别指定四个方向的补白 padding: EdgeInsets.fromLTRB(20, 0, 20, 20), child: Text(\"Your friend\"), ) ], ), ); } } "},"pages/fultter/DecoratedBox.html":{"url":"pages/fultter/DecoratedBox.html","title":"DecoratedBox","keywords":"","body":"DecoratedBox 源码 装饰容器（DecoratedBox） DecoratedBox DecoratedBox可以在其子组件绘制前(或后)绘制一些装饰（Decoration），如背景、边框、渐变等。DecoratedBox定义如下： const DecoratedBox({ Decoration decoration, DecorationPosition position = DecorationPosition.background, Widget? child }) decoration：代表将要绘制的装饰，它的类型为Decoration。Decoration是一个抽象类，它定义了一个接口 createBoxPainter()，子类的主要职责是需要通过实现它来创建一个画笔，该画笔用于绘制装饰。 position：此属性决定在哪里绘制Decoration，它接收DecorationPosition的枚举类型，该枚举类有两个值： background：在子组件之后绘制，即背景装饰。 foreground：在子组件之上绘制，即前景。BoxDecoration 我们通常会直接使用BoxDecoration类，它是一个Decoration的子类，实现了常用的装饰元素的绘制。 BoxDecoration({ Color color, //颜色 DecorationImage image,//图片 BoxBorder border, //边框 BorderRadiusGeometry borderRadius, //圆角 List boxShadow, //阴影,可以指定多个 Gradient gradient, //渐变 BlendMode backgroundBlendMode, //背景混合模式 BoxShape shape = BoxShape.rectangle, //形状 }) 示例 DecoratedBox( decoration: BoxDecoration( gradient: LinearGradient(colors:[Colors.red,Colors.orange.shade700]), //背景渐变 borderRadius: BorderRadius.circular(3.0), //3像素圆角 boxShadow: [ //阴影 BoxShadow( color:Colors.black54, offset: Offset(2.0,2.0), blurRadius: 4.0 ) ] ), child: Padding( padding: EdgeInsets.symmetric(horizontal: 80.0, vertical: 18.0), child: Text(\"Login\", style: TextStyle(color: Colors.white),), ) ) LinearGradient、RadialGradient、SweepGradient "},"pages/fultter/Transform.html":{"url":"pages/fultter/Transform.html","title":"Transform","keywords":"","body":"Transform 源码 变换（Transform） Transform可以在其子组件绘制时对其应用一些矩阵变换来实现一些特效。Matrix4是一个4D矩阵，通过它我们可以实现各种矩阵操作，下面是一个例子： Container( color: Colors.black, child: Transform( alignment: Alignment.topRight, //相对于坐标系原点的对齐方式 transform: Matrix4.skewY(0.3), //沿Y轴倾斜0.3弧度 child: Container( padding: const EdgeInsets.all(8.0), color: Colors.deepOrange, child: const Text('Apartment for rent!'), ), ), ) 平移 Transform.translate接收一个offset参数，可以在绘制时沿x、y轴对子组件平移指定的距离。 DecoratedBox( decoration:BoxDecoration(color: Colors.red), //默认原点为左上角，左移20像素，向上平移5像素 child: Transform.translate( offset: Offset(-20.0, -5.0), child: Text(\"Hello world\"), ), ) 旋转 DecoratedBox( decoration:BoxDecoration(color: Colors.red), child: Transform.rotate( //旋转90度 angle:math.pi/2 , child: Text(\"Hello world\"), ), ) 注意：要使用math.pi需先进行如下导包。 import 'dart:math' as math; 缩放 Transform.scale可以对子组件进行缩小或放大，如： DecoratedBox( decoration:BoxDecoration(color: Colors.red), child: Transform.scale( scale: 1.5, //放大到1.5倍 child: Text(\"Hello world\") ) ); Transform 注意事项 Transform的变换是应用在绘制阶段，而并不是应用在布局(layout)阶段，所以无论对子组件应用何种变化，其占用空间的大小和在屏幕上的位置都是固定不变的，因为这些是在布局阶段就确定的。下面我们具体说明： Row( mainAxisAlignment: MainAxisAlignment.center, children: [ DecoratedBox( decoration:BoxDecoration(color: Colors.red), child: Transform.scale(scale: 1.5, child: Text(\"Hello world\") ) ), Text(\"你好\", style: TextStyle(color: Colors.green, fontSize: 18.0),) ], ) RotatedBox RotatedBox和Transform.rotate功能相似，它们都可以对子组件进行旋转变换，但是有一点不同：RotatedBox的变换是在layout阶段，会影响在子组件的位置和大小。我们将上面介绍Transform.rotate时的示例改一下： Row( mainAxisAlignment: MainAxisAlignment.center, children: [ DecoratedBox( decoration: BoxDecoration(color: Colors.red), //将Transform.rotate换成RotatedBox child: RotatedBox( quarterTurns: 1, //旋转90度(1/4圈) child: Text(\"Hello world\"), ), ), Text(\"你好\", style: TextStyle(color: Colors.green, fontSize: 18.0),) ], ), "},"pages/fultter/Container.html":{"url":"pages/fultter/Container.html","title":"Container","keywords":"","body":"Container 源码 容器组件（Container） Container是一个组合类容器，它本身不对应具体的RenderObject，它是DecoratedBox、ConstrainedBox、Transform、Padding、Align等组件组合的一个多功能容器，所以我们只需通过一个Container组件可以实现同时需要装饰、变换、限制的场景。下面是Container的定义： Container({ this.alignment, this.padding, //容器内补白，属于decoration的装饰范围 Color color, // 背景色 Decoration decoration, // 背景装饰 Decoration foregroundDecoration, //前景装饰 double width,//容器的宽度 double height, //容器的高度 BoxConstraints constraints, //容器大小的限制条件 this.margin,//容器外补白，不属于decoration的装饰范围 this.transform, //变换 this.child, ... }) 容器的大小可以通过width、height属性来指定，也可以通过constraints来指定；如果它们同时存在时，width、height优先。实际上Container内部会根据width、height来生成一个constraints。 color和decoration是互斥的，如果同时设置它们则会报错！实际上，当指定color时，Container内会自动创建一个decoration。 示例 Container( margin: EdgeInsets.only(top: 50.0, left: 120.0), constraints: BoxConstraints.tightFor(width: 200.0, height: 150.0),//卡片大小 decoration: BoxDecoration( //背景装饰 gradient: RadialGradient( //背景径向渐变 colors: [Colors.red, Colors.orange], center: Alignment.topLeft, radius: .98, ), boxShadow: [ //卡片阴影 BoxShadow( color: Colors.black54, offset: Offset(2.0, 2.0), blurRadius: 4.0, ) ], ), transform: Matrix4.rotationZ(.2),//卡片倾斜变换 alignment: Alignment.center, //卡片内文字居中 child: Text( //卡片文字 \"5.20\", style: TextStyle(color: Colors.white, fontSize: 40.0), ), ) Padding和Margin ... Container( margin: EdgeInsets.all(20.0), //容器外补白 color: Colors.orange, child: Text(\"Hello world!\"), ), Container( padding: EdgeInsets.all(20.0), //容器内补白 color: Colors.orange, child: Text(\"Hello world!\"), ), ... 等价于： ... Padding( padding: EdgeInsets.all(20.0), child: DecoratedBox( decoration: BoxDecoration(color: Colors.orange), child: Text(\"Hello world!\"), ), ), DecoratedBox( decoration: BoxDecoration(color: Colors.orange), child: Padding( padding: const EdgeInsets.all(20.0), child: Text(\"Hello world!\"), ), ), ... "},"pages/fultter/Clip.html":{"url":"pages/fultter/Clip.html","title":"Clip","keywords":"","body":"Clip 源码 剪裁（Clip） 剪裁类组件 剪裁Widget 默认行为 ClipOval 子组件为正方形时剪裁成内贴圆形；为矩形时，剪裁成内贴椭圆 ClipRRect 将子组件剪裁为圆角矩形 ClipRect 默认剪裁掉子组件布局空间之外的绘制内容（溢出部分剪裁） ClipPath 按照自定义的路径剪裁 示例 import 'package:flutter/material.dart'; class ClipTestRoute extends StatelessWidget { @override Widget build(BuildContext context) { // 头像 Widget avatar = Image.asset(\"imgs/avatar.png\", width: 60.0); return Center( child: Column( children: [ avatar, //不剪裁 ClipOval(child: avatar), //剪裁为圆形 ClipRRect( //剪裁为圆角矩形 borderRadius: BorderRadius.circular(5.0), child: avatar, ), Row( mainAxisAlignment: MainAxisAlignment.center, children: [ Align( alignment: Alignment.topLeft, widthFactor: .5,//宽度设为原来宽度一半，另一半会溢出 child: avatar, ), Text(\"你好世界\", style: TextStyle(color: Colors.green),) ], ), Row( mainAxisAlignment: MainAxisAlignment.center, children: [ ClipRect(//将溢出部分剪裁 child: Align( alignment: Alignment.topLeft, widthFactor: .5,//宽度设为原来宽度一半 child: avatar, ), ), Text(\"你好世界\",style: TextStyle(color: Colors.green)) ], ), ], ), ); } } 自定义裁剪（CustomClipper） 首先，自定义一个CustomClipper： class MyClipper extends CustomClipper { @override Rect getClip(Size size) => Rect.fromLTWH(10.0, 15.0, 40.0, 30.0); @override bool shouldReclip(CustomClipper oldClipper) => false; } getClip()是用于获取剪裁区域的接口，由于图片大小是60×60，我们返回剪裁区域为Rect.fromLTWH(10.0, 15.0, 40.0, 30.0)，即图片中部40×30像素的范围。 shouldReclip() 接口决定是否重新剪裁。如果在应用中，剪裁区域始终不会发生变化时应该返回false，这样就不会触发重新剪裁，避免不必要的性能开销。如果剪裁区域会发生变化（比如在对剪裁区域执行一个动画），那么变化后应该返回true来重新执行剪裁。DecoratedBox( decoration: BoxDecoration( color: Colors.red ), child: ClipRect( clipper: MyClipper(), //使用自定义的clipper child: avatar ), ) ClipPath 可以按照自定义的路径实现剪裁，它需要自定义一个CustomClipper 类型的 Clipper，定义方式和 MyClipper 类似，只不过 getClip 需要返回一个 Path。 "},"pages/fultter/FittedBox.html":{"url":"pages/fultter/FittedBox.html","title":"FittedBox","keywords":"","body":"FittedBox 源码 空间适配（FittedBox） FittedBox const FittedBox({ Key? key, this.fit = BoxFit.contain, // 适配方式 this.alignment = Alignment.center, //对齐方式 this.clipBehavior = Clip.none, //是否剪裁 Widget? child, }) 适配原理 FittedBox允许子组件无限大(0 FittedBox 对子组件布局结束后就可以获得子组件真实的大小。 FittedBox 知道子组件的真实大小也知道他父组件的约束，那么FittedBox 就可以通过指定的适配方式（BoxFit 枚举中指定），让起子组件在 FittedBox 父组件的约束范围内按照指定的方式显示。 示例 Widget build(BuildContext context) { return Center( child: Column( children: [ wContainer(BoxFit.none), Text('Wendux'), wContainer(BoxFit.contain), Text('Flutter中国'), ], ), ); } Widget wContainer(BoxFit boxFit) { return Container( width: 50, height: 50, color: Colors.red, child: FittedBox( fit: boxFit, // 子容器超过父容器大小 child: Container(width: 60, height: 70, color: Colors.blue), ), ); } 单行缩放布局 @override Widget build(BuildContext context) { return Center( child: Column( children: [ wRow(' 90000000000000000 '), FittedBox(child: wRow(' 90000000000000000 ')), wRow(' 800 '), FittedBox(child: wRow(' 800 ')), ] .map((e) => Padding( padding: EdgeInsets.symmetric(vertical: 20), child: e, )) .toList();, ), ); } // 直接使用Row Widget wRow(String text) { Widget child = Text(text); child = Row( mainAxisAlignment: MainAxisAlignment.spaceEvenly, children: [child, child, child], ); return child; } "},"pages/fultter/Scaffold.html":{"url":"pages/fultter/Scaffold.html","title":"Scaffold","keywords":"","body":"Scaffold 源码 页面骨架（Scaffold） Scaffold 是一个路由页的骨架，我们使用它可以很容易地拼装出一个完整的页面。 示例 我们实现一个页面，它包含： 一个导航栏 导航栏右边有一个分享按钮 有一个抽屉菜单 有一个底部导航 右下角有一个悬浮的动作按钮 class ScaffoldRoute extends StatefulWidget { @override _ScaffoldRouteState createState() => _ScaffoldRouteState(); } class _ScaffoldRouteState extends State { int _selectedIndex = 1; @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( //导航栏 title: Text(\"App Name\"), actions: [ //导航栏右侧菜单 IconButton(icon: Icon(Icons.share), onPressed: () {}), ], ), drawer: MyDrawer(), //抽屉 bottomNavigationBar: BottomNavigationBar( // 底部导航 items: [ BottomNavigationBarItem(icon: Icon(Icons.home), title: Text('Home')), BottomNavigationBarItem(icon: Icon(Icons.business), title: Text('Business')), BottomNavigationBarItem(icon: Icon(Icons.school), title: Text('School')), ], currentIndex: _selectedIndex, fixedColor: Colors.blue, onTap: _onItemTapped, ), floatingActionButton: FloatingActionButton( //悬浮按钮 child: Icon(Icons.add), onPressed:_onAdd ), ); } void _onItemTapped(int index) { setState(() { _selectedIndex = index; }); } void _onAdd(){ } } 组件名称 解释 AppBar 一个导航栏骨架 MyDrawer 抽屉菜单 BottomNavigationBar 底部导航栏 FloatingActionButton 漂浮按钮 AppBar AppBar是一个Material风格的导航栏，通过它可以设置导航栏标题、导航栏菜单、导航栏底部的Tab标题等。下面我们看看AppBar的定义： AppBar({ Key? key, this.leading, //导航栏最左侧Widget，常见为抽屉菜单按钮或返回按钮。 this.automaticallyImplyLeading = true, //如果leading为null，是否自动实现默认的leading按钮 this.title,// 页面标题 this.actions, // 导航栏右侧菜单 this.bottom, // 导航栏底部菜单，通常为Tab按钮组 this.elevation = 4.0, // 导航栏阴影 this.centerTitle, //标题是否居中 this.backgroundColor, ... //其他属性见源码注释 }) 抽屉菜单Drawer class MyDrawer extends StatelessWidget { const MyDrawer({ Key? key, }) : super(key: key); @override Widget build(BuildContext context) { return Drawer( child: MediaQuery.removePadding( context: context, //移除抽屉菜单顶部默认留白 removeTop: true, child: Column( crossAxisAlignment: CrossAxisAlignment.start, children: [ Padding( padding: const EdgeInsets.only(top: 38.0), child: Row( children: [ Padding( padding: const EdgeInsets.symmetric(horizontal: 16.0), child: ClipOval( child: Image.asset( \"imgs/avatar.png\", width: 80, ), ), ), Text( \"Wendux\", style: TextStyle(fontWeight: FontWeight.bold), ) ], ), ), Expanded( child: ListView( children: [ ListTile( leading: const Icon(Icons.add), title: const Text('Add account'), ), ListTile( leading: const Icon(Icons.settings), title: const Text('Manage accounts'), ), ], ), ), ], ), ), ); } } "},"pages/fultter/Introduction_to_scrollable_components.html":{"url":"pages/fultter/Introduction_to_scrollable_components.html","title":"可滚动组件简介","keywords":"","body":"可滚动组件简介 可滚动组件简介CustomScrollView和NestedScrollView的详细介绍在Flutter中创建有意思的滚动效果 - Sliver系列 Sliver布局模型 1. 布局模型 基于 RenderBox 的盒模型布局。 基于 Sliver ( RenderSliver ) 按需加载列表布局。 2. 主要作用 Sliver 可以包含一个或多个子组件。加载子组件并确定每一个子组件的布局和绘制信息，实现按需加载模型。 3. 三个角色 Scrollable ：用于处理滑动手势，确定滑动偏移，滑动偏移变化时构建 Viewport 。 Viewport：显示的视窗，即列表的可视区域； Sliver：视窗里显示的元素。 4. 角色关系 三者所占用的空间重合。 Sliver 父组件为 Viewport，Viewport的 父组件为 Scrollable。 5. 布局过程 Scrollable 监听到用户滑动行为后，根据最新的滑动偏移构建 Viewport 。 Viewport 将当前视口信息和配置信息通过 SliverConstraints 传递给 Sliver。 Sliver 中对子组件（RenderBox）按需进行构建和布局，然后确认自身的位置、绘制等信息，保存在 geometry 中（一个 SliverGeometry 类型的对象）。 6. cacheExtent 预渲染的高度，在可视区域之外，如果 RenderBox 进入这个区域内，即使它还未显示在屏幕上，也是要先进行构建，默认值是 250。 Scrollable 用于处理滑动手势，确定滑动偏移，滑动偏移变化时构建 Viewport，我们看一下其关键的属性： Scrollable({ ... this.axisDirection = AxisDirection.down,//滚动方向。 this.controller, this.physics, required this.viewportBuilder, //后面介绍 }) 1. physics ScrollPhysics类型的对象，响应用户操作： 用户滑动完抬起手指后，继续执行动画。 滑动到边界时，如何显示。 ScrollPhysics的子类： ClampingScrollPhysics：列表滑动到边界时将不能继续滑动，通常在Android 中 配合 GlowingOverscrollIndicator（实现微光效果的组件） 使用。 BouncingScrollPhysics：iOS 下弹性效果。 2. controller ScrollController对象，默认的PrimaryScrollController，控制滚动位置和监听滚动事件。 3. viewportBuilder 构建 Viewport 的回调。当用户滑动时，Scrollable 会调用此回调构建新的 Viewport，同时传递一个 ViewportOffset 类型的 offset 参数，该参数描述 Viewport 应该显示那一部分内容。 Viewport 显示 Sliver。 Viewport({ Key? key, this.axisDirection = AxisDirection.down, this.crossAxisDirection, this.anchor = 0.0, required ViewportOffset offset, // 用户的滚动偏移 // 类型为Key，表示从什么地方开始绘制，默认是第一个元素 this.center, this.cacheExtent, // 预渲染区域 //该参数用于配合解释cacheExtent的含义，也可以为主轴长度的乘数 this.cacheExtentStyle = CacheExtentStyle.pixel, this.clipBehavior = Clip.hardEdge, List slivers = const [], // 需要显示的 Sliver 列表 }) 1. offset 该参数为Scrollable 构建 Viewport 时传入，它描述了 Viewport 应该显示那一部分内容。 2. cacheExtent 和 cacheExtentStyle CacheExtentStyle 是一个枚举，有 pixel 和 viewport 两个取值。 当 cacheExtentStyle 值为 pixel 时，cacheExtent 的值为预渲染区域的具体像素长度； 当值为 viewport 时，cacheExtent 的值是一个乘数，表示有几个 viewport 的长度，最终的预渲染区域的像素长度为：cacheExtent * viewport 的积。 Sliver 1. 主要作用 对子组件进行构建和布局，比如 ListView 的 Sliver 需要实现子组件（列表项）按需加载功能，只有当列表项进入预渲染区域时才会去对它进行构建和布局、渲染。 2. RenderSliver Sliver 对应的渲染对象类型是 RenderSliver。 RenderSliver 和 RenderBox 都继承自 RenderObject 类。 RenderSliver 和 RenderBox 约束信息分别是 BoxConstraints 和 SliverConstraints。 通用配置 scrollDirection（滑动的主轴）、reverse（滑动方向是否反向）、controller、physics 、cacheExtent ，这些属性最终会透传给对应的 Scrollable 和 Viewport，这些属性我们可以认为是可滚动组件的通用属性。 reverse表示是否按照阅读方向相反的方向滑动，如：scrollDirection值为Axis.horizontal 时，即滑动发现为水平，如果阅读方向是从左到右（取决于语言环境，阿拉伯语就是从右到左）。reverse为true时，那么滑动方向就是从右往左。 "},"pages/fultter/SingleChildScrollView.html":{"url":"pages/fultter/SingleChildScrollView.html","title":"SingleChildScrollView","keywords":"","body":"SingleChildScrollView 源码 SingleChildScrollView 简介 SingleChildScrollView类似于Android中的ScrollView，它只能接收一个子组件，定义如下： SingleChildScrollView({ this.scrollDirection = Axis.vertical, //滚动方向，默认是垂直方向 this.reverse = false, this.padding, bool primary, this.physics, this.controller, this.child, }) 示例 下面是一个将大写字母 A-Z 沿垂直方向显示的例子，由于垂直方向空间会超过屏幕视口高度，所以我们使用SingleChildScrollView： class SingleChildScrollViewTestRoute extends StatelessWidget { @override Widget build(BuildContext context) { String str = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"; return Scrollbar( // 显示进度条 child: SingleChildScrollView( padding: EdgeInsets.all(16.0), child: Center( child: Column( //动态创建一个List children: str.split(\"\") //每一个字母都用一个Text显示,字体为原来的两倍 .map((c) => Text(c, textScaleFactor: 2.0,)) .toList(), ), ), ), ); } } "},"pages/fultter/ListView.html":{"url":"pages/fultter/ListView.html","title":"ListView","keywords":"","body":"ListView 源码 ListView ListView是最常用的可滚动组件之一，它可以沿一个方向线性排布所有子组件，并且它也支持列表项懒加载（在需要时才会创建）。 默认构造函数 ListView({ ... //可滚动widget公共参数 Axis scrollDirection = Axis.vertical, bool reverse = false, ScrollController? controller, bool? primary, ScrollPhysics? physics, EdgeInsetsGeometry? padding, //ListView各个构造函数的共同参数 double? itemExtent, Widget? prototypeItem, //列表项原型，后面解释 bool shrinkWrap = false, bool addAutomaticKeepAlives = true, bool addRepaintBoundaries = true, double? cacheExtent, // 预渲染区域长度 //子widget列表 List children = const [], }) itemExtent：列表项高度。 prototypeItem：指定一个列表项，用于计算列表项高度。 shrinkWrap：列表高度是否是子组件总长度。 示例： ListView( shrinkWrap: true, padding: const EdgeInsets.all(20.0), children: [ const Text('I\\'m dedicating every day to you'), const Text('Domestic life was never quite my style'), const Text('When you smile, you knock me out, I fall apart'), const Text('And I thought I was so smart'), ], ); ListView.builder ListView.builder适合列表项比较多或者列表项不确定的情况，下面看一下ListView.builder的核心参数列表： ListView.builder({ // ListView公共参数已省略 ... required IndexedWidgetBuilder itemBuilder, int itemCount, ... }) itemBuilder：根据index参数动态构建列表项。 itemCount：列表项的数量，如果为null，则为无限列表。 下面看一个例子： ListView.builder( itemCount: 100, itemExtent: 50.0, //强制高度为50.0 itemBuilder: (BuildContext context, int index) { return ListTile(title: Text(\"$index\")); } ); ListView.separated ListView.separated可以在生成的列表项之间添加一个分割组件，它比ListView.builder多了一个separatorBuilder参数，该参数是一个分割组件生成器。 下面我们看一个例子：奇数行添加一条蓝色下划线，偶数行添加一条绿色下划线。 class ListView3 extends StatelessWidget { @override Widget build(BuildContext context) { //下划线widget预定义以供复用。 Widget divider1=Divider(color: Colors.blue,); Widget divider2=Divider(color: Colors.green); return ListView.separated( itemCount: 100, //列表项构造器 itemBuilder: (BuildContext context, int index) { return ListTile(title: Text(\"$index\")); }, //分割器构造器 separatorBuilder: (BuildContext context, int index) { return index%2==0?divider1:divider2; }, ); } } "},"pages/fultter/Scroll_monitoring_and_control.html":{"url":"pages/fultter/Scroll_monitoring_and_control.html","title":"滚动监听及控制","keywords":"","body":"滚动监听及控制 源码 滚动监听及控制 ScrollController ScrollController构造函数如下： ScrollController({ double initialScrollOffset = 0.0, //初始滚动位置 this.keepScrollOffset = true,//是否保存滚动位置 ... }) offset：可滚动组件当前的滚动位置。 jumpTo(double offset)、animateTo(double offset,...)：这两个方法用于跳转到指定的位置，它们不同之处在于，后者在跳转时会执行一个动画，而前者不会。 1. 滚动监听 ScrollController间接继承自Listenable，我们可以根据ScrollController来监听滚动事件，如： controller.addListener(()=>print(controller.offset)) 2. 实例 我们创建一个ListView，当滚动位置发生变化时，我们先打印出当前滚动位置，然后判断当前位置是否超过1000像素，如果超过则在屏幕右下角显示一个“返回顶部”的按钮，该按钮点击后可以使ListView恢复到初始位置；如果没有超过1000像素，则隐藏“返回顶部”按钮。代码如下： class ScrollControllerTestRoute extends StatefulWidget { @override ScrollControllerTestRouteState createState() { return ScrollControllerTestRouteState(); } } class ScrollControllerTestRouteState extends State { ScrollController _controller = ScrollController(); bool showToTopBtn = false; //是否显示“返回到顶部”按钮 @override void initState() { super.initState(); //监听滚动事件，打印滚动位置 _controller.addListener(() { print(_controller.offset); //打印滚动位置 if (_controller.offset = 1000 && showToTopBtn == false) { setState(() { showToTopBtn = true; }); } }); } @override void dispose() { //为了避免内存泄露，需要调用_controller.dispose _controller.dispose(); super.dispose(); } @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar(title: Text(\"滚动控制\")), body: Scrollbar( child: ListView.builder( itemCount: 100, itemExtent: 50.0, //列表项高度固定时，显式指定高度是一个好习惯(性能消耗小) controller: _controller, itemBuilder: (context, index) { return ListTile(title: Text(\"$index\"),); } ), ), floatingActionButton: !showToTopBtn ? null : FloatingActionButton( child: Icon(Icons.arrow_upward), onPressed: () { //返回到顶部时执行动画 _controller.animateTo( .0, duration: Duration(milliseconds: 200), curve: Curves.ease, ); } ), ); } } 3. 滚动位置恢复 PageStorage是一个用于保存页面(路由)相关数据的组件，它并不会影响子树的UI外观，其实，PageStorage是一个功能型组件，它拥有一个存储桶（bucket），子树中的Widget可以通过指定不同的PageStorageKey来存储各自的数据或状态。 使用PageStorage在页面切换时保存状态 4. ScrollPosition ScrollPosition是用来保存可滚动组件的滚动位置的。一个ScrollController对象可以同时被多个可滚动组件使用，ScrollController会为每一个可滚动组件创建一个ScrollPosition对象，这些ScrollPosition保存在ScrollController的positions属性中（List）。ScrollPosition是真正保存滑动位置信息的对象，offset只是一个便捷属性： double get offset => position.pixels; 5. ScrollController控制原理 滚动监听 1. 滚动通知 Flutter Widget树中子Widget可以通过发送通知（Notification）与父(包括祖先)Widget通信。 2. 实例 import 'package:flutter/material.dart'; class ScrollNotificationTestRoute extends StatefulWidget { @override _ScrollNotificationTestRouteState createState() => _ScrollNotificationTestRouteState(); } class _ScrollNotificationTestRouteState extends State { String _progress = \"0%\"; //保存进度百分比 @override Widget build(BuildContext context) { return Scrollbar( //进度条 // 监听滚动通知 child: NotificationListener( onNotification: (ScrollNotification notification) { double progress = notification.metrics.pixels / notification.metrics.maxScrollExtent; //重新构建 setState(() { _progress = \"${(progress * 100).toInt()}%\"; }); print(\"BottomEdge: ${notification.metrics.extentAfter == 0}\"); return false; //return true; //放开此行注释后，进度条将失效 }, child: Stack( alignment: Alignment.center, children: [ ListView.builder( itemCount: 100, itemExtent: 50.0, itemBuilder: (context, index) => ListTile(title: Text(\"$index\")), ), CircleAvatar( //显示进度百分比 radius: 30.0, child: Text(_progress), backgroundColor: Colors.black54, ) ], ), ), ); } } "},"pages/fultter/AnimatedList.html":{"url":"pages/fultter/AnimatedList.html","title":"AnimatedList","keywords":"","body":"AnimatedList 源码 AnimatedList AnimatedList 和 ListView 的功能大体相似，不同的是， AnimatedList 可以在列表中插入或删除节点时执行一个动画，在需要添加或删除列表项的场景中会提高用户体验。 AnimatedList 是一个 StatefulWidget，它对应的 State 类型为 AnimatedListState，添加和删除元素的方法位于 AnimatedListState 中： void insertItem(int index, { Duration duration = _kDuration }); void removeItem(int index, AnimatedListRemovedItemBuilder builder, { Duration duration = _kDuration }) ; 下面我们看一个示例：实现下面这样的一个列表，点击底部 + 按钮时向列表追加一个列表项；点击每个列表项后面的删除按钮时，删除该列表项，添加和删除时分别执行指定的动画： 初始的时候有5个列表项，先点击了 + 号按钮，会添加一个 6，添加过程执行渐显动画。然后点击了 4 后面的删除按钮，删除的时候执行了一个渐隐+收缩的合成动画。 下面是实现代码： class AnimatedListRoute extends StatefulWidget { const AnimatedListRoute({Key? key}) : super(key: key); @override _AnimatedListRouteState createState() => _AnimatedListRouteState(); } class _AnimatedListRouteState extends State { var data = []; int counter = 5; final globalKey = GlobalKey(); @override void initState() { for (var i = 0; i animation, ) { //添加列表项时会执行渐显动画 return FadeTransition( opacity: animation, child: buildItem(context, index), ); }, ), buildAddBtn(), ], ); } // 创建一个 “+” 按钮，点击后会向列表中插入一项 Widget buildAddBtn() { return Positioned( child: FloatingActionButton( child: Icon(Icons.add), onPressed: () { // 添加一个列表项 data.add('${++counter}'); // 告诉列表项有新添加的列表项 globalKey.currentState!.insertItem(data.length - 1); print('添加 $counter'); }, ), bottom: 30, left: 0, right: 0, ); } // 构建列表项 Widget buildItem(context, index) { String char = data[index]; return ListTile( //数字不会重复，所以作为Key key: ValueKey(char), title: Text(char), trailing: IconButton( icon: Icon(Icons.delete), // 点击时删除 onPressed: () => onDelete(context, index), ), ); } void onDelete(context, index) { // 待实现 } } 删除的时候需要我们通过AnimatedListState 的 removeItem 方法来应用删除动画，具体逻辑在 onDelete 中： setState(() { globalKey.currentState!.removeItem( index, (context, animation) { // 删除过程执行的是反向动画，animation.value 会从1变为0 var item = buildItem(context, index); print('删除 ${data[index]}'); data.removeAt(index); // 删除动画是一个合成动画：渐隐 + 缩小列表项告诉 return FadeTransition( opacity: CurvedAnimation( parent: animation, //让透明度变化的更快一些 curve: const Interval(0.5, 1.0), ), // 不断缩小列表项的高度 child: SizeTransition( sizeFactor: animation, axisAlignment: 0.0, child: item, ), ); }, duration: Duration(milliseconds: 200), // 动画时间为 200 ms ); }); 代码很简单，但我们需要注意，我们的数据是单独在 data 中维护的，调用 AnimatedListState 的插入和移除方法知识相当于一个通知：在什么位置执行插入或移除动画，仍然是数据驱动的（响应式并非命令式）。 "},"pages/fultter/GridView.html":{"url":"pages/fultter/GridView.html","title":"GridView","keywords":"","body":"GridView 源码 GridView SliverGridDelegate SliverGridDelegate是一个抽象类，定义了GridView Layout相关接口，子类需要通过实现它们来实现具体的布局算法。Flutter中提供了两个SliverGridDelegate的子类SliverGridDelegateWithFixedCrossAxisCount和SliverGridDelegateWithMaxCrossAxisExtent。 1. SliverGridDelegateWithFixedCrossAxisCount 通过设置横轴子元素的数量和比例的layout算法。 SliverGridDelegateWithFixedCrossAxisCount({ @required double crossAxisCount, double mainAxisSpacing = 0.0, double crossAxisSpacing = 0.0, double childAspectRatio = 1.0, }) 2. SliverGridDelegateWithMaxCrossAxisExtent 设置子元素最大长度的layout算法。 SliverGridDelegateWithMaxCrossAxisExtent({ double maxCrossAxisExtent, double mainAxisSpacing = 0.0, double crossAxisSpacing = 0.0, double childAspectRatio = 1.0, }) GridView.count GridView.count构造函数内部使用了SliverGridDelegateWithFixedCrossAxisCount。 GridView.count( crossAxisCount: 3, childAspectRatio: 1.0, children: [ Icon(Icons.ac_unit), Icon(Icons.airport_shuttle), Icon(Icons.all_inclusive), Icon(Icons.beach_access), Icon(Icons.cake), Icon(Icons.free_breakfast), ], ); GridView.extent GridView.extent构造函数内部使用了SliverGridDelegateWithMaxCrossAxisExtent。 GridView.extent( maxCrossAxisExtent: 120.0, childAspectRatio: 2.0, children: [ Icon(Icons.ac_unit), Icon(Icons.airport_shuttle), Icon(Icons.all_inclusive), Icon(Icons.beach_access), Icon(Icons.cake), Icon(Icons.free_breakfast), ], ); GridView.builder 当子widget比较多时，我们可以通过GridView.builder来动态创建子widget。GridView.builder 必须指定的参数有两个： GridView.builder( ... required SliverGridDelegate gridDelegate, required IndexedWidgetBuilder itemBuilder, ) 示例 假设我们需要从一个异步数据源（如网络）分批获取一些Icon，然后用GridView来展示： class InfiniteGridView extends StatefulWidget { @override _InfiniteGridViewState createState() => _InfiniteGridViewState(); } class _InfiniteGridViewState extends State { List _icons = []; //保存Icon数据 @override void initState() { super.initState(); // 初始化数据 _retrieveIcons(); } @override Widget build(BuildContext context) { return GridView.builder( gridDelegate: SliverGridDelegateWithFixedCrossAxisCount( crossAxisCount: 3, //每行三列 childAspectRatio: 1.0, //显示区域宽高相等 ), itemCount: _icons.length, itemBuilder: (context, index) { //如果显示到最后一个并且Icon总数小于200时继续获取数据 if (index == _icons.length - 1 && _icons.length "},"pages/fultter/PageView.html":{"url":"pages/fultter/PageView.html","title":"PageView","keywords":"","body":"PageView 源码 PageView与页面缓存 构造方法 PageView({ Key? key, this.scrollDirection = Axis.horizontal, // 滑动方向 this.reverse = false, PageController? controller, this.physics, List children = const [], this.onPageChanged, //每次滑动是否强制切换整个页面，如果为false，则会根据实际的滑动距离显示页面 this.pageSnapping = true, //主要是配合辅助功能用的，后面解释 this.allowImplicitScrolling = false, //后面解释 this.padEnds = true, }) 示例 我们看一个 Tab 切换的实例，为了突出重点，我们让每个 Tab 页都只显示一个数字。 // Tab 页面 class Page extends StatefulWidget { const Page({ Key? key, required this.text }) : super(key: key); final String text; @override _PageState createState() => _PageState(); } class _PageState extends State { @override Widget build(BuildContext context) { print(\"build ${widget.text}\"); return Center(child: Text(\"${widget.text}\", textScaleFactor: 5)); } } 我们创建一个 PageView： @override Widget build(BuildContext context) { var children = []; // 生成 6 个 Tab 页 for (int i = 0; i 页面缓存 allowImplicitScrolling设置为true可以缓存前后各一页数据。 "},"pages/fultter/KeepAlive.html":{"url":"pages/fultter/KeepAlive.html","title":"KeepAlive","keywords":"","body":"KeepAlive 源码 KeepAlive AutomaticKeepAlive详解 在列表项Widget的State中混入AutomaticKeepAliveClientMixin。 覆写wantKeepAlive返回true。 在build()内调用super.build(context)。 "},"pages/fultter/TabbarView.html":{"url":"pages/fultter/TabbarView.html","title":"TabbarView","keywords":"","body":"TabbarView 源码 TabbarView TabBarView TabBarView 封装了 PageView，它的构造方法很简单 TabBarView({ Key? key, required this.children, // tab 页 this.controller, // TabController this.physics, this.dragStartBehavior = DragStartBehavior.start, }) TabController 用于监听和控制 TabBarView 的页面切换，通常和 TabBar 联动。如果没有指定，则会在组件树中向上查找并使用最近的一个 DefaultTabController 。 TabBar TabBar 有很多配置参数，通过这些参数我们可以定义 TabBar 的样式，很多属性都是在配置 indicator 和 label，拿上图来举例，Label 是每个Tab 的文本，indicator 指 “历史” 下面的白色下划线。 const TabBar({ Key? key, required this.tabs, // 具体的 Tabs，需要我们创建 this.controller, this.isScrollable = false, // 是否可以滑动 this.padding, this.indicatorColor,// 指示器颜色，默认是高度为2的一条下划线 this.automaticIndicatorColorAdjustment = true, this.indicatorWeight = 2.0,// 指示器高度 this.indicatorPadding = EdgeInsets.zero, //指示器padding this.indicator, // 指示器 this.indicatorSize, // 指示器长度，有两个可选值，一个tab的长度，一个是label长度 this.labelColor, this.labelStyle, this.labelPadding, this.unselectedLabelColor, this.unselectedLabelStyle, this.mouseCursor, this.onTap, ... }) TabBar 通常位于 AppBar 的底部，它也可以接收一个 TabController ，如果需要和 TabBarView 联动， TabBar 和 TabBarView 使用同一个 TabController 即可，注意，联动时 TabBar 和 TabBarView 的孩子数量需要一致。如果没有指定 controller，则会在组件树中向上查找并使用最近的一个 DefaultTabController 。另外我们需要创建需要的 tab 并通过 tabs 传给 TabBar， tab 可以是任何 Widget，不过Material 组件库中已经实现了一个 Tab 组件，我们一般都会直接使用它： const Tab({ Key? key, this.text, //文本 this.icon, // 图标 this.iconMargin = const EdgeInsets.only(bottom: 10.0), this.height, this.child, // 自定义 widget }) 注意，text 和 child 是互斥的，不能同时制定。 实例 class TabViewRoute1 extends StatefulWidget { @override _TabViewRoute1State createState() => _TabViewRoute1State(); } class _TabViewRoute1State extends State with SingleTickerProviderStateMixin { late TabController _tabController; List tabs = [\"新闻\", \"历史\", \"图片\"]; @override void initState() { super.initState(); _tabController = TabController(length: tabs.length, vsync: this); } @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: Text(\"App Name\"), bottom: TabBar( controller: _tabController, tabs: tabs.map((e) => Tab(text: e)).toList(), ), ), body: TabBarView( //构建 controller: _tabController, children: tabs.map((e) { return KeepAliveWrapper( child: Container( alignment: Alignment.center, child: Text(e, textScaleFactor: 5), ), ); }).toList(), ), ); } @override void dispose() { // 释放资源 _tabController.dispose(); super.dispose(); } } 滑动页面时顶部的 Tab 也会跟着动，点击顶部 Tab 时页面也会跟着切换。为了实现 TabBar 和 TabBarView 的联动，我们显式创建了一个 TabController，由于 TabController 又需要一个 TickerProvider （vsync 参数）， 我们又混入了 SingleTickerProviderStateMixin；由于 TabController 中会执行动画，持有一些资源，所以我们在页面销毁时必须得释放资源（dispose）。综上，我们发现创建 TabController 的过程还是比较复杂，实战中，如果需要 TabBar 和 TabBarView 联动，通常会创建一个 DefaultTabController 作为它们共同的父级组件，这样它们在执行时就会从组件树向上查找，都会使用我们指定的这个 DefaultTabController。我们修改后的实现如下： class TabViewRoute2 extends StatelessWidget { @override Widget build(BuildContext context) { List tabs = [\"新闻\", \"历史\", \"图片\"]; return DefaultTabController( length: tabs.length, child: Scaffold( appBar: AppBar( title: Text(\"App Name\"), bottom: TabBar( tabs: tabs.map((e) => Tab(text: e)).toList(), ), ), body: TabBarView( //构建 children: tabs.map((e) { return KeepAliveWrapper( child: Container( alignment: Alignment.center, child: Text(e, textScaleFactor: 5), ), ); }).toList(), ), ), ); } } 可以看到我们无需去手动管理 Controller 的生命周期，也不需要提供 SingleTickerProviderStateMixin，同时也没有其他的状态需要管理，也就不需要用 StatefulWidget 了，这样简单很多。 "},"pages/fultter/CustomScrollView.html":{"url":"pages/fultter/CustomScrollView.html","title":"CustomScrollView","keywords":"","body":"CustomScrollView 源码 CustomScrollView CustomScrollView 前面介绍的 ListView、GridView、PageView 都是一个完整的可滚动组件，所谓完整是指它们都包括Scrollable 、 Viewport 和 Sliver。假如我们想要在一个页面中，同时包含多个可滚动组件，且使它们的滑动效果能统一起来，比如：我们想将已有的两个沿垂直方向滚动的 ListView 成一个 ListView ，这样在第一ListView 滑动到底部时能自动接上第二 ListView，如果尝试写一个 demo： Widget buildTwoListView() { var listView = ListView.builder( itemCount: 20, itemBuilder: (_, index) => ListTile(title: Text('$index')), ); return Column( children: [ Expanded(child: listView), Divider(color: Colors.grey), Expanded(child: listView), ], ); } } 页面中有两个 ListView，各占可视区域一半高度，虽然能够显式出来，但每一个 ListView 只会响应自己可视区域中滑动，实现不了我们想要的效果。之所以会这样的原因是两个 ListView 都有自己独立的 Scrollable 、 Viewport 和 Sliver，既然如此，我们自己创建一个共用的 Scrollable 和 Viewport 对象，然后再将两个 ListView 对应的 Sliver 添加到这个共用的 Viewport 对象中就可以实现我们想要的效果了。如果这个工作让开发者自己来做无疑是比较麻烦的，因此 Flutter 提供了一个 CustomScrollView 组件来帮助我们创建一个公共的 Scrollable 和 Viewport ，然后它的 slivers 参数接受一个 Sliver 数组，这样我们就可以使用CustomScrollView 方面的实现我们期望的功能了： Widget buildTwoSliverList() { // SliverFixedExtentList 是一个 Sliver，它可以生成高度相同的列表项。 // 再次提醒，如果列表项高度相同，我们应该优先使用SliverFixedExtentList // 和 SliverPrototypeExtentList，如果不同，使用 SliverList. var listView = SliverFixedExtentList( itemExtent: 56, //列表项高度固定 delegate: SliverChildBuilderDelegate( (_, index) => ListTile(title: Text('$index')), childCount: 10, ), ); // 使用 return CustomScrollView( slivers: [ listView, listView, ], ); } Sliver Sliver名称 功能 对应的可滚动组件 SliverList 列表 ListView SliverFixedExtentList 高度固定的列表 ListView，指定itemExtent时 SliverAnimatedList 添加/删除列表项可以执行动画 AnimatedList SliverGrid 网格 GridView SliverPrototypeExtentList 根据原型生成高度固定的列表 ListView，指定prototypeItem 时 SliverFillViewport 包含多个子组件，每个都可以填满屏幕 PageView 除了和列表对应的 Sliver 之外还有一些用于对 Sliver 进行布局、装饰的组件，它们的子组件必须是 Sliver，我们列举几个常用的： |Sliver名称|对应RenderBox| |----|----| |SliverPadding|Padding| |SliverVisibility、SliverOpacity |Visibility、Opacity| |SliverFadeTransition|FadeTransition| |SliverLayoutBuilder|LayoutBuilder| 还有一些其他常用的 Sliver： |Sliver名称|说明| |----|----| |SliverAppBar|对应 AppBar，主要是为了在 CustomScrollView 中使用。| |SliverToBoxAdapter|一个适配器，可以将 RenderBox 适配为 Sliver，后面介绍。| |SliverPersistentHeader|滑动到顶部时可以固定住，后面介绍。| 示例 // 因为本路由没有使用 Scaffold，为了让子级Widget(如Text)使用 // Material Design 默认的样式风格,我们使用 Material 作为本路由的根。 Material( child: CustomScrollView( slivers: [ // AppBar，包含一个导航栏. SliverAppBar( pinned: true, // 滑动到顶端时会固定住 expandedHeight: 250.0, flexibleSpace: FlexibleSpaceBar( title: const Text('Demo'), background: Image.asset( \"./imgs/sea.png\", fit: BoxFit.cover, ), ), ), SliverPadding( padding: const EdgeInsets.all(8.0), sliver: SliverGrid( //Grid gridDelegate: SliverGridDelegateWithFixedCrossAxisCount( crossAxisCount: 2, //Grid按两列显示 mainAxisSpacing: 10.0, crossAxisSpacing: 10.0, childAspectRatio: 4.0, ), delegate: SliverChildBuilderDelegate( (BuildContext context, int index) { //创建子widget return Container( alignment: Alignment.center, color: Colors.cyan[100 * (index % 9)], child: Text('grid item $index'), ); }, childCount: 20, ), ), ), SliverFixedExtentList( itemExtent: 50.0, delegate: SliverChildBuilderDelegate( (BuildContext context, int index) { //创建列表项 return Container( alignment: Alignment.center, color: Colors.lightBlue[100 * (index % 9)], child: Text('list item $index'), ); }, childCount: 20, ), ), ], ), ); 头部SliverAppBar：SliverAppBar对应AppBar，两者不同之处在于SliverAppBar可以集成到CustomScrollView。SliverAppBar可以结合FlexibleSpaceBar实现Material Design中头部伸缩的模型，具体效果，读者可以运行该示例查看。 中间的SliverGrid：它用SliverPadding包裹以给SliverGrid添加补白。SliverGrid是一个两列，宽高比为4的网格，它有20个子组件。 底部SliverFixedExtentList：它是一个所有子元素高度都为50像素的列表。 SliverToBoxAdapter 在实际布局中，我们通常需要往 CustomScrollView 中添加一些自定义的组件，而这些组件并非都有 Sliver 版本，为此 Flutter 提供了一个 SliverToBoxAdapter 组件，它是一个适配器：可以将 RenderBox 适配为 Sliver。比如我们想在列表顶部添加一个可以横向滑动的 PageView，可以使用 SliverToBoxAdapter 来配置： CustomScrollView( slivers: [ SliverToBoxAdapter( child: SizedBox( height: 300, child: PageView( children: [Text(\"1\"), Text(\"2\")], ), ), ), buildSliverFixedList(), ], ); 注意，上面的代码是可以正常运行的，但是如果将 PageView 换成一个滑动方向和 CustomScrollView 一致的 ListView 则不会正常工作！原因是：CustomScrollView 组合 Sliver 的原理是为所有子 Sliver 提供一个共享的 Scrollable，然后统一处理指定滑动方向的滑动事件，如果 Sliver 中引入了其他的 Scrollable，则滑动事件便会冲突。上例中 PageView 之所以能正常工作，是因为 PageView 的 Scrollable 只处理水平方向的滑动，而 CustomScrollView 是处理垂直方向的，两者并未冲突，所以不会有问题，但是换一个也是垂直方向的 ListView 时则不能正常工作，最终的效果是，在ListView内滑动时只会对ListView 起作用，原因是滑动事件被 ListView 的 Scrollable 优先消费，CustomScrollView 的 Scrollable 便接收不到滑动事件了。 SliverPersistentHeader SliverPersistentHeader 的功能是当滑动到 CustomScrollView 的顶部时，可以将组件固定在顶部。 "},"pages/fultter/CustomSliver.html":{"url":"pages/fultter/CustomSliver.html","title":"CustomSliver","keywords":"","body":"CustomSliver "},"pages/fultter/NestedScrollView.html":{"url":"pages/fultter/NestedScrollView.html","title":"NestedScrollView","keywords":"","body":"NestedScrollView "},"pages/fultter/Flutter_learning_record.html":{"url":"pages/fultter/Flutter_learning_record.html","title":"Flutter学习记录","keywords":"","body":"Flutter学习记录 问题记录 \" Error: Member not found: 'packageRoot' \" in Flutter [duplicate] 使用 Flutter 将文本基线与列内的文本对齐 flutter中密码输入如何切换隐藏/显示？ Do not use BuildContexts across async gaps Bad state: add Fetch Article was called without a registered event handler [Discussion] Should Cubit expose Stream API? #1429 Flutter TextField输入框如何优雅的禁止弹出软键盘 入门 Flutter 开发文档 Flutter Gallery 推荐几个优质Flutter 开源项目 Flutter 快速上手 - 4.2 assets导入资源 | 猫哥 Flutter学习记录——28.Flutter 调试及 Android 和 iOS 打包 FLUTTER开发之DART线程与异步 点击空白区域无反应解决办法 Dart语言教程 如何在 Flutter 中添加 ListTile 源码分析系列之InheritedWidget Flutter-状态管理 Flutter学习笔记：Flutter状态管理（使用Provider进行状态管理） Flutter中文网 rxDart RxDart - 使用 map、flatMap、concatMap、switchMap、asyncMap、exhaustMap text截断方式 text截断方式 Text 以字符的方式截断 dart dart.cn Dart | 浅析dart中库的导入与拆分 【-Flutter/Dart 语法补遗-】 sync 和 async 、yield 和yield* 、async 和 await Dart中两个点..和三个点...的用法 Flutter 事件机制 - Future 和 MicroTask 全解析 Dart中的Future及其then、catchError方法 Dart:factory工厂构造函数的使用场景 Dart String转数值int或double GSYGithubApp 「 Flutter 项目实战 」设计企业级项目入口 main.dart 设计与实现 ( GSYGithubApp 源码解读·二 ) GSYGithubApp 源码分析视频 Flutter使用 json_serializable 解析 JSON 最佳方案 环境配置 flutter 配置不同的开发环境(qa/dev/prod) flutter环境配置 Redux Flutter Redux 中的 combineReducers redux_epics 国际化 flutter-Intl国际化、格式化日期、数字 Flutter 多语言国际化配置 -- 使用Intl插件 FunFlutter系列之国际化Intl方案 知识 systemOverlayStyle可以设置状态栏颜色 Flutter ThemeData详解 BaseAppBar Complete Guide To Flutter Drawer Flutter下拉刷新组件CupertinoSliverRefreshControl苹果风格刷新效果 Flutter 基于NestedScrollView+RefreshIndicator完美解决滑动冲突 GraphQL 入门看这篇就够了 获取屏幕尺寸 Flutter-获取屏幕高度、密度、安全区域等 Flutter控制组件显示和隐藏三种方式详解 工具 Flutter中计算文字的宽度/高度 Flutter Utils 全网最齐全的工具类 Flutter-出生日期计算年龄工具类 主题 Flutter 处理主题 Theme 的一些建议 用抽象工厂方法构建 Flutter 主题 Flutter之textTheme FlutterComponent最佳实践之色彩管理 Flutter 小技巧之 Flutter 3 下的 ThemeExtensions 和 Material3 Flutter TextStyle参数解析 如何在FLutter ThemeData中使用ColorScheme？ Flutter 深色模式分析&实践 Flutter: 如何扩展ThemeData 如何在 Flutter 中更改颜色的色调、饱和度或值？ flutter Icons全部图标 iconfont Flutter3.3对Material3设计风格的支持 实战项目 Flutter 手把手写出超漂亮的登录注册 UI - 饮料食谱App - Speed Code flutter-demo 教程 Flutter App Development Tutorial Series' Articles Flutter基础教程 fluttervideo 快捷键 105--Flutter编辑器Android Studio快捷操作 Flutter-开发中常用的快捷键Android Studio（Mac） 按钮 Flutter的button的按钮ElevatedButton 单元测试 使用 Mockito 对 Flutter 代码进行单元测试 Flutter 单元测试 使用 Mockito 模拟依赖关系 状态管理 flutter_bloc [Flutter] Cubit介紹＆Bloc優缺比較 Flutter 入门与实战（八十九）：使用BlocListener处理状态变化 Flutter 入门与实战（九十）：使用 BlocConsumer 同时构建响应式组件和监听状态 Flutter 入门与实战（九十一）：使用 RepositoryProvider简化父子组件的传值 flutter_bloc使用解析---骚年，你还在手搭bloc吗！ slaw-getx Flutter GetX使用---简洁的魅力！ get-cli getx_pattern flutter_ducafecat_news_getx getx-docs 使用组件 Flutter 仿iOS左滑删除，长按拖动 "},"pages/harmony/dev_eco_studio.html":{"url":"pages/harmony/dev_eco_studio.html","title":"DevEco Studio","keywords":"","body":"DevEco Studio Mac下安装 先注册华为账号 下载zip文件 解压devecostudio-mac-5.0.3.404.zip 双击dmg 拖拽至applications 不设置安装文件夹，直接点击ok 点击同意 界面预览 双向预览 多设备预览 关闭实时预览 @Preview：组件预览 预览模拟数据 参考链接 安装DevEco Studio HarmonyOS应用开发者基础认证 论坛 指南 考试答案 吃多少发多少 【HarmonyOS第一课】从简单的页面开始答案 【HarmonyOS第一课】从简单的页面开始答案2 codelabs 从简单的页面开始 问题记录 设置文件夹类型 文件夹中间有个点？ 点击如下选项 "},"pages/librtmp/librtmp_debug.html":{"url":"pages/librtmp/librtmp_debug.html","title":"librtmp源码之调试","keywords":"","body":"librtmp源码之调试 "},"pages/librtmp/librtmp_struct.html":{"url":"pages/librtmp/librtmp_struct.html","title":"librtmp源码之相关结构体","keywords":"","body":"librttmp源码之相关结构体 分析RTMP_Connect之前，有必要了解下一下定义。 Ip Protocol /* Standard well-defined IP protocols. */ enum { IPPROTO_IP = 0, /* Dummy protocol for TCP. */ IPPROTO_ICMP = 1, /* Internet Control Message Protocol. */ IPPROTO_IGMP = 2, /* Internet Group Management Protocol. */ IPPROTO_IPIP = 4, /* IPIP tunnels (older KA9Q tunnels use 94). */ IPPROTO_TCP = 6, /* Transmission Control Protocol. */ IPPROTO_EGP = 8, /* Exterior Gateway Protocol. */ IPPROTO_PUP = 12, /* PUP protocol. */ IPPROTO_UDP = 17, /* User Datagram Protocol. */ IPPROTO_IDP = 22, /* XNS IDP protocol. */ IPPROTO_TP = 29, /* SO Transport Protocol Class 4. */ IPPROTO_DCCP = 33, /* Datagram Congestion Control Protocol. */ IPPROTO_IPV6 = 41, /* IPv6 header. */ IPPROTO_RSVP = 46, /* Reservation Protocol. */ IPPROTO_GRE = 47, /* General Routing Encapsulation. */ IPPROTO_ESP = 50, /* encapsulating security payload. */ IPPROTO_AH = 51, /* authentication header. */ IPPROTO_MTP = 92, /* Multicast Transport Protocol. */ IPPROTO_BEETPH = 94, /* IP option pseudo header for BEET. */ IPPROTO_ENCAP = 98, /* Encapsulation Header. */ IPPROTO_PIM = 103, /* Protocol Independent Multicast. */ IPPROTO_COMP = 108, /* Compression Header Protocol. */ IPPROTO_SCTP = 132, /* Stream Control Transmission Protocol. */ IPPROTO_UDPLITE = 136, /* UDP-Lite protocol. */ IPPROTO_RAW = 255, /* Raw IP packets. */ IPPROTO_MAX }; struct sockaddr 内容来源于：https://www.cnblogs.com/cyx-b/p/12450811.html struct sockaddr { 　　unsigned short sa_family;// 2字节，地址族，AF_xxx 　　char sa_data[14]; // 14字节，包含套接字中的目标地址和端口信息 }; struct addrinfo 内容来源于：https://www.cnblogs.com/LubinLew/p/POSIX-DataStructure.html The header shall define the addrinfo structure, which shall include at least the following members: #include #include #include /* ======================Types of sockets====================== */ enum __socket_type { SOCK_STREAM = 1, /* Sequenced, reliable, connection-based byte streams. */ SOCK_DGRAM = 2, /* Connectionless, unreliable datagrams of fixed maximum length. */ SOCK_RAW = 3, /* Raw protocol interface. */ SOCK_RDM = 4, /* Reliably-delivered messages. */ SOCK_SEQPACKET = 5, /* Sequenced, reliable, connection-based,datagrams of fixed maximum length. */ SOCK_DCCP = 6, /* Datagram Congestion Control Protocol. */ SOCK_PACKET = 10, /* Linux specific way of getting packets at the dev level. For writing rarp and other similar things on the user level. */ /* Flags to be ORed into the type parameter of socket and socketpair and used for the flags parameter of paccept. */ SOCK_CLOEXEC = 02000000, /* Atomically set close-on-exec flag for the new descriptor(s). */ SOCK_NONBLOCK = 00004000 /* Atomically mark descriptor(s) as non-blocking. */ }; /* ============Protocol families(只列出常用几个)================= */ #define PF_UNSPEC 0 /* Unspecified. */ #define PF_LOCAL 1 /* Local to host (pipes and file-domain). */ #define PF_INET 2 /* IP protocol family. */ #define PF_IPX 4 /* Novell Internet Protocol. */ #define PF_APPLETALK 5 /* Appletalk DDP. */ #define PF_INET6 10 /* IP version 6. */ #define PF_TIPC 30 /* TIPC sockets. */ #define PF_BLUETOOTH 31 /* Bluetooth sockets. */ /* ==============Address families(只列出常用几个)================= */ #define AF_UNSPEC PF_UNSPEC #define AF_LOCAL PF_LOCAL #define AF_UNIX PF_UNIX #define AF_FILE PF_FILE #define AF_INET PF_INET #define AF_IPX PF_IPX #define AF_APPLETALK PF_APPLETALK #define AF_INET6 PF_INET6 #define AF_ROSE PF_ROSE #define AF_NETLINK PF_NETLINK #define AF_TIPC PF_TIPC #define AF_BLUETOOTH PF_BLUETOOTH /* ====Possible values for `ai_flags' field in `addrinfo' structure.===== */ #define AI_PASSIVE 0x0001 /* Socket address is intended for `bind'. */ #define AI_CANONNAME 0x0002 /* Request for canonical name. */ #define AI_NUMERICHOST 0x0004 /* Don't use name resolution. */ #define AI_V4MAPPED 0x0008 /* IPv4 mapped addresses are acceptable. */ #define AI_ALL 0x0010 /* Return IPv4 mapped and IPv6 addresses. */ #define AI_ADDRCONFIG 0x0020 /* Use configuration of this host to choose returned address type. */ #ifdef __USE_GNU #define AI_IDN 0x0040 /* IDN encode input (assuming it is encoded in the current locale's character set) before looking it up. */ #define AI_CANONIDN 0x0080 /* Translate canonical name from IDN format. */ #define AI_IDN_ALLOW_UNASSIGNED 0x0100 /* Don't reject unassigned Unicode code points. */ #define AI_IDN_USE_STD3_ASCII_RULES 0x0200 /* Validate strings according to STD3 rules. */ #endif #define AI_NUMERICSERV 0x0400 /* Don't use name resolution. */ /* =======================struct addrinfo======================= */ struct addrinfo { int ai_flags; /* 附加选项,多个选项可以使用或操作结合 */ int ai_family; /* 指定返回地址的协议簇,取值范围:AF_INET(IPv4)、AF_INET6(IPv6)、AF_UNSPEC(IPv4 and IPv6) */ int ai_socktype; /* enum __socket_type 类型，设置为0表示任意类型 */ int ai_protocol; /* 协议类型，设置为0表示任意类型,具体见上一节的 Ip Protocol */ socklen_t ai_addrlen; /* socket address 的长度 */ struct sockaddr *ai_addr; /* socket address 的地址 */ char *ai_canonname; /* Canonical name of service location. */ struct addrinfo *ai_next; /* 指向下一条信息,因为可能返回多个地址 */ }; RTMP 内容来源于：https://www.jianshu.com/p/05b1e5d70c06 RTMP定义在rtmp.h。 /* ** 远程调用方法 */ typedef struct RTMP_METHOD { AVal name; int num; } RTMP_METHOD; typedef struct RTMPSockBuf { int sb_socket; // 套接字 int sb_size; // 缓冲区可读大小 char *sb_start; // 缓冲区读取位置 char sb_buf[RTMP_BUFFER_CACHE_SIZE]; // 套接字读取缓冲区 int sb_timedout; // 超时标志 void *sb_ssl; // TLS上下文 } RTMPSockBuf; typedef struct RTMP { int m_inChunkSize; // 最大接收块大小 int m_outChunkSize; // 最大发送块大小 int m_nBWCheckCounter; // 带宽检测计数器 int m_nBytesIn; // 接收数据计数器 int m_nBytesInSent; // 当前数据已回应计数器 int m_nBufferMS; // 当前缓冲的时间长度，以MS为单位 int m_stream_id; // 当前连接的流ID int m_mediaChannel; // 当前连接媒体使用的块流ID uint32_t m_mediaStamp; // 当前连接媒体最新的时间戳 uint32_t m_pauseStamp; // 当前连接媒体暂停时的时间戳 int m_pausing; // 是否暂停状态 int m_nServerBW; // 服务器带宽 int m_nClientBW; // 客户端带宽 uint8_t m_nClientBW2; // 客户端带宽调节方式 uint8_t m_bPlaying; // 当前是否推流或连接中 uint8_t m_bSendEncoding; // 连接服务器时发送编码 uint8_t m_bSendCounter; // 设置是否向服务器发送接收字节应答 int m_numInvokes; // 0x14命令远程过程调用计数 int m_numCalls; // 0x14命令远程过程请求队列数量 RTMP_METHOD *m_methodCalls; // 远程过程调用请求队列 RTMPPacket *m_vecChannelsIn[RTMP_CHANNELS]; // 对应块流ID上一次接收的报文 RTMPPacket *m_vecChannelsOut[RTMP_CHANNELS]; // 对应块流ID上一次发送的报文 int m_channelTimestamp[RTMP_CHANNELS]; // 对应块流ID媒体的最新时间戳 double m_fAudioCodecs; // 音频编码器代码 double m_fVideoCodecs; // 视频编码器代码 double m_fEncoding; /* AMF0 or AMF3 */ double m_fDuration; // 当前媒体的时长 int m_msgCounter; // 使用HTTP协议发送请求的计数器 int m_polling; // 使用HTTP协议接收消息主体时的位置 int m_resplen; // 使用HTTP协议接收消息主体时的未读消息计数 int m_unackd; // 使用HTTP协议处理时无响应的计数 AVal m_clientID; // 使用HTTP协议处理时的身份ID RTMP_READ m_read; // RTMP_Read()操作的上下文 RTMPPacket m_write; // RTMP_Write()操作使用的可复用报文对象 RTMPSockBuf m_sb; // RTMP_ReadPacket()读包操作的上下文 RTMP_LNK Link; // RTMP连接上下文 } RTMP; RTMP_LINK 内容来源于：https://www.jianshu.com/p/05b1e5d70c06 typedef struct RTMP_LNK { AVal hostname; // 目标主机地址 AVal sockshost; // socks代理地址 // 连接和推拉流涉及的一些参数信息 AVal playpath0; /* parsed from URL */ AVal playpath; /* passed in explicitly */ AVal tcUrl; AVal swfUrl; AVal pageUrl; AVal app; AVal auth; AVal flashVer; AVal subscribepath; AVal token; AMFObject extras; int edepth; int seekTime; // 播放流的开始时间 int stopTime; // 播放流的停止时间 #define RTMP_LF_AUTH 0x0001 /* using auth param */ #define RTMP_LF_LIVE 0x0002 /* stream is live */ #define RTMP_LF_SWFV 0x0004 /* do SWF verification */ #define RTMP_LF_PLST 0x0008 /* send playlist before play */ #define RTMP_LF_BUFX 0x0010 /* toggle stream on BufferEmpty msg */ #define RTMP_LF_FTCU 0x0020 /* free tcUrl on close */ int lFlags; int swfAge; int protocol; // 连接使用的协议 int timeout; // 连接超时时间 unsigned short socksport; // socks代理端口 unsigned short port; // 目标主机端口 #ifdef CRYPTO #define RTMP_SWF_HASHLEN 32 void *dh; /* for encryption */ void *rc4keyIn; void *rc4keyOut; uint32_t SWFSize; uint8_t SWFHash[RTMP_SWF_HASHLEN]; char SWFVerificationResponse[RTMP_SWF_HASHLEN+10]; #endif } RTMP_LNK; RTMPPacket 内容来源于：https://blog.csdn.net/NB_vol_1/article/details/58660181 https://blog.csdn.net/bwangk/article/details/112802823 // 原始的rtmp消息块 typedef struct RTMPChunk { int c_headerSize; // 头部的长度 int c_chunkSize; // chunk的大小 char *c_chunk; // 数据 char c_header[RTMP_MAX_HEADER_SIZE]; // chunk头部 } RTMPChunk; // rtmp消息块 typedef struct RTMPPacket { // chunk basic header（大部分情况是一个字节） // #define RTMP_PACKET_SIZE_LARGE 0 onMetaData流开始的绝对时间戳控制消息（如connect） // #define RTMP_PACKET_SIZE_MEDIUM 1 大部分的rtmp header都是8字节的 // #define RTMP_PACKET_SIZE_SMALL 2 比较少见 // #define RTMP_PACKET_SIZE_MINIMUM 3 偶尔出现，低于8字节频率 // chunk type id (2bit)fmt 对应message head {0,3,7,11} + (6bit)chunk stream id // 块类型ID，消息头的第1个字节的前2位，决定消息头长度 uint8_t m_headerType; // Message type ID（1-7协议控制；8，9音视频；10以后为AMF编码消息） uint8_t m_packetType; // 是否含有Extend timeStamp字段 uint8_t m_hasAbsTimestamp; /* timestamp absolute or relative? */ // channel 即 stream id字段 // 第一个字节的低6位，命名为Chunk Stream ID，Chunk Stream ID用来表示消息的级别： // chunk stream id 级别 // 2 low level // 3 high level(像connect, create_stream一类消息) // 4 control stream // 5 video // 6 audio // 8 control stream int m_nChannel; // 时间戳 uint32_t m_nTimeStamp; /* timestamp */ // message stream id int32_t m_nInfoField2; /* last 4 bytes in a long header */ // chunk体的长度 uint32_t m_nBodySize; uint32_t m_nBytesRead; RTMPChunk *m_chunk; // 原始rtmp消息块 char *m_body; } RTMPPacket; typedef struct RTMPPacket { uint8_t m_headerType; //basic header 中的type头字节，值为(0,1,2,3)表示ChunkMsgHeader的类型（4种） uint8_t m_packetType; //Chunk Msg Header中msg type 1字节：消息类型id（8: audio；9:video；18:AMF0编码的元数据） uint8_t m_hasAbsTimestamp; //bool值，是否是绝对时间戳(类型1时为true) int m_nChannel; //块流ID ，通过设置ChannelID来设置Basic stream id的长度和值 uint32_t m_nTimeStamp; //时间戳，消息头前三字节 int32_t m_nInfoField2; //Chunk Msg Header中msg StreamID 4字节：消息流id uint32_t m_nBodySize; //Chunk Msg Header中msg length 4字节：消息长度 uint32_t m_nBytesRead; //已读取的数据 RTMPChunk *m_chunk; //raw chunk结构体指针，把RTMPPacket的真实头部和数据段拷贝进来 char *m_body; //数据段指针 } RTMPPacket; RTMP_READ 内容来源于：https://blog.csdn.net/NB_vol_1/article/details/58660181 /* ** AVal表示一个字符串 */ typedef struct AVal { char *av_val; int av_len; } AVal; /* state for read() wrapper */ // read函数的包装器，包括状态等等 typedef struct RTMP_READ { char *buf; char *bufpos; unsigned int buflen; uint32_t timestamp; uint8_t dataType; uint8_t flags; #define RTMP_READ_HEADER 0x01 #define RTMP_READ_RESUME 0x02 #define RTMP_READ_NO_IGNORE 0x04 #define RTMP_READ_GOTKF 0x08 #define RTMP_READ_GOTFLVK 0x10 #define RTMP_READ_SEEKING 0x20 int8_t status; #define RTMP_READ_COMPLETE -3 #define RTMP_READ_ERROR -2 #define RTMP_READ_EOF -1 #define RTMP_READ_IGNORE 0 /* if bResume == TRUE */ uint8_t initialFrameType; uint32_t nResumeTS; char *metaHeader; char *initialFrame; uint32_t nMetaHeaderSize; uint32_t nInitialFrameSize; uint32_t nIgnoredFrameCounter; uint32_t nIgnoredFlvFrameCounter; } RTMP_READ; "},"pages/librtmp/librtmp_RTMP_Connect.html":{"url":"pages/librtmp/librtmp_RTMP_Connect.html","title":"librtmp源码之RTMP_Connect","keywords":"","body":"librttmp源码之RTMP_Connect RTMP_Connect int RTMP_Connect(RTMP *r, RTMPPacket *cp) { struct addrinfo *service; if (!r->Link.hostname.av_len) return FALSE; // 设置直接连接的服务器地址 if (r->Link.socksport) { /* Connect via SOCKS */ if (!add_addr_info(&service, &r->Link.sockshost, r->Link.socksport)) return FALSE; } else { /* Connect directly */ if (!add_addr_info(&service, &r->Link.hostname, r->Link.port)) return FALSE; } RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_flags:%d\", service->ai_flags); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_family:%d\", service->ai_family); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_socktype:%d\", service->ai_socktype); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_protocol:%d\", service->ai_protocol); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addrlen:%d\", service->ai_addrlen); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addr->sa_family:%d\", service->ai_addr->sa_family); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addr->sa_data:%s\", service->ai_addr->sa_data); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_canonname:%s\", service->ai_canonname); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_next:%s\", service->ai_next); if (!RTMP_Connect0(r, service)) { freeaddrinfo(service); return FALSE; } freeaddrinfo(service); r->m_bSendCounter = TRUE; return RTMP_Connect1(r, cp); } 代码片段分析1 // 设置直接连接的服务器地址 if (r->Link.socksport) { /* Connect via SOCKS */ if (!add_addr_info(&service, &r->Link.sockshost, r->Link.socksport)) return FALSE; } else { /* Connect directly */ if (!add_addr_info(&service, &r->Link.hostname, r->Link.port)) return FALSE; } RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_flags:%d\", service->ai_flags); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_family:%d\", service->ai_family); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_socktype:%d\", service->ai_socktype); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_protocol:%d\", service->ai_protocol); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addrlen:%d\", service->ai_addrlen); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addr->sa_family:%d\", service->ai_addr->sa_family); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_addr->sa_data:%s\", service->ai_addr->sa_data); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_canonname:%s\", service->ai_canonname); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: addrinfo->ai_next:%s\", service->ai_next); // DEBUG: CCQ: addrinfo->ai_flags:0 /* 0代表没有设值，如果设值，取值范围见上`AI_PASSIVE`... */ // DEBUG: CCQ: addrinfo->ai_family:2 /* 2代表`AF_INET`，也就是`PF_INET`，取之范围见上`AF_UNSPEC`... */ // DEBUG: CCQ: addrinfo->ai_socktype:1 /* 1代表`SOCK_STREAM`，取值范围见上`__socket_type` */ // DEBUG: CCQ: addrinfo->ai_protocol:6 /* 6代表`IPPROTO_TCP`，取之范围见上`Ip Protocol` */ // DEBUG: CCQ: addrinfo->ai_addrlen:16 /* socket address 的长度 */ // DEBUG: CCQ: addrinfo->ai_addr->sa_family:2 /* 2代表`AF_INET`，也就是`PF_INET`，取之范围见上`AF_UNSPEC`... */ // DEBUG: CCQ: addrinfo->ai_addr->sa_data:\u0007\\217\\300\\250 // DEBUG: CCQ: addrinfo->ai_canonname:(null) /* Canonical name of service location. */ // DEBUG: CCQ: addrinfo->ai_next:(null) 代码片段分析2 if (!RTMP_Connect0(r, service)) { freeaddrinfo(service); return FALSE; } freeaddrinfo(service); r->m_bSendCounter = TRUE;// 设置是否向服务器发送接收字节应答 return RTMP_Connect1(r, cp); 内容来源于：https://www.jianshu.com/p/05b1e5d70c06 开始调用RTMP_Connect1，继续执行SSL或HTTP协商，以及RTMP握手。 add_addr_info 内容来源于：https://blog.csdn.net/weixin_37921201/article/details/90111641 填充struct addrinfo结构体用于之后的socket通信。 /** 填充struct addrinfo结构体用于之后的socket通信。 service: addrinfo指针的指针 host: 192.168.0.12:1935/zbcs/room port: 1935 */ static int add_addr_info(struct addrinfo **service, AVal *host, int port) { struct addrinfo hints; char *hostname, portNo[32]; int ret = TRUE; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: host->av_val：%s\", host->av_val); if (host->av_val[host->av_len]) { hostname = malloc(host->av_len+1); memcpy(hostname, host->av_val, host->av_len); hostname[host->av_len] = '\\0'; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: hostname：%s\", hostname); } else { hostname = host->av_val; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: hostname2：%s\", hostname); } sprintf(portNo, \"%d\", port); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: portNo：%s\", portNo); memset(&hints, 0, sizeof(struct addrinfo)); hints.ai_socktype = SOCK_STREAM; hints.ai_family = AF_UNSPEC; if(getaddrinfo(hostname, portNo, &hints, service) != 0) { RTMP_Log(RTMP_LOGERROR, \"Problem accessing the DNS. (addr: %s)\", hostname); ret = FALSE; } finish: if (hostname != host->av_val) free(hostname); return ret; } 代码片段分析1 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: host->av_val：%s\", host->av_val); if (host->av_val[host->av_len]) { hostname = malloc(host->av_len+1); memcpy(hostname, host->av_val, host->av_len); hostname[host->av_len] = '\\0'; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: hostname：%s\", hostname); } else { hostname = host->av_val; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: hostname2：%s\", hostname); } // 输出结果： // DEBUG: host->av_val：192.168.0.12:1935/zbcs/room // DEBUG: hostname：192.168.0.12 通过打印结果分析，这段代码就是给hostname赋值，得到IP地址段。 代码片段分析2 sprintf(portNo, \"%d\", port); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: portNo：%d\", portNo); // DEBUG: CCQ: portNo：1935 sprintf：https://baike.baidu.com/item/sprintf/9703430?fr=aladdin 这里就是给portNo赋值，将int转char *。 代码片段分析3 memset(&hints, 0, sizeof(struct addrinfo));// 给hints分配内存 hints.ai_socktype = SOCK_STREAM;// 设置sock类型，设置范围：__socket_type hints.ai_family = AF_UNSPEC;// 指定返回地址的协议簇 内容来源于： https://blog.csdn.net/u011003120/article/details/78277133 https://www.cnblogs.com/LubinLew/p/POSIX-getaddrinfo.html ai_family解释：指定返回地址的协议簇，取值范围:AF_INET(IPv4)、AF_INET6(IPv6)、AF_UNSPEC(IPv4 and IPv6)。 代码片段分析4 // hostname：IP地址，例如：192.168.0.12 // portNo：端口号，例如：1935 // hints：struct addrinfo地址 // service：传进来的struct addrinfo，用于获取信息结果 if(getaddrinfo(hostname, portNo, &hints, service) != 0) { RTMP_Log(RTMP_LOGERROR, \"Problem accessing the DNS. (addr: %s)\", hostname); ret = FALSE; } 内容来源于： https://www.cnblogs.com/LubinLew/p/POSIX-getaddrinfo.html 函数注释： int getaddrinfo(const char *restrict nodename, /* host 或者IP地址 */ const char *restrict servname, /* 十进制端口号 或者常用服务名称如\"ftp\"、\"http\"等 */ const struct addrinfo *restrict hints, /* 获取信息要求设置 */ struct addrinfo **restrict res); /* 获取信息结果 */ IPv4中使用gethostbyname()函数完成主机名到地址解析，这个函数仅仅支持IPv4，且不允许调用者指定所需地址类型的任何信息，返回的结构只包含了用于存储IPv4地址的空间。IPv6中引入了新的API getaddrinfo()，它是协议无关的，既可用于IPv4也可用于IPv6。getaddrinfo() 函数能够处理名字到地址以及服务到端口这两种转换，返回的是一个 struct addrinfo 的结构体(列表)指针而不是一个地址清单。这些 struct addrinfo 结构体随后可由套接口函数直接使用。如此以来，getaddrinfo()函数把协议相关性安全隐藏在这个库函数内部。应用程序只要处理由getaddrinfo()函数填写的套接口地址结构。 代码片段分析5 finish: if (hostname != host->av_val) free(hostname); 释放hostname内存，至此add_addr_info函数分析完毕。 RTMP_Connect0 int RTMP_Connect0(RTMP *r, struct addrinfo * service) { int on = 1; r->m_sb.sb_timedout = FALSE; r->m_pausing = 0; r->m_fDuration = 0.0; // 创建套接字 r->m_sb.sb_socket = socket(service->ai_family, service->ai_socktype, service->ai_protocol); if (r->m_sb.sb_socket != -1) {// 连接对端 if (connect(r->m_sb.sb_socket, service->ai_addr, service->ai_addrlen) Link.socksport) { RTMP_Log(RTMP_LOGDEBUG, \"%s ... SOCKS negotiation\", __FUNCTION__); if (!SocksNegotiate(r)) { RTMP_Log(RTMP_LOGERROR, \"%s, SOCKS negotiation failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } } } else { RTMP_Log(RTMP_LOGERROR, \"%s, failed to create socket. Error: %d\", __FUNCTION__, GetSockError()); return FALSE; } /* set timeout */ { SET_RCVTIMEO(tv, r->Link.timeout); if (setsockopt (r->m_sb.sb_socket, SOL_SOCKET, SO_RCVTIMEO, (char *)&tv, sizeof(tv))) { RTMP_Log(RTMP_LOGERROR, \"%s, Setting socket timeout to %ds failed!\", __FUNCTION__, r->Link.timeout); } } setsockopt(r->m_sb.sb_socket, SOL_SOCKET, SO_NOSIGPIPE, (char *) &on, sizeof(on)); setsockopt(r->m_sb.sb_socket, IPPROTO_TCP, TCP_NODELAY, (char *) &on, sizeof(on)); return TRUE; } 代码片段分析1 int on = 1;// setsockopt函数使用 r->m_sb.sb_timedout = FALSE;// 超时标志 r->m_pausing = 0;// 是否暂停状态 r->m_fDuration = 0.0;// 当前媒体的时长 代码片段分析2 // 创建套接字 // DEBUG: CCQ: addrinfo->ai_family:2 /* 2代表`AF_INET`，也就是`PF_INET`，取之范围见上`AF_UNSPEC`... */ // DEBUG: CCQ: addrinfo->ai_socktype:1 /* 1代表`SOCK_STREAM`，取值范围见上`__socket_type` */ // DEBUG: CCQ: addrinfo->ai_protocol:6 /* 6代表`IPPROTO_TCP`，取之范围见上`Ip Protocol` */ r->m_sb.sb_socket = socket(service->ai_family, service->ai_socktype, service->ai_protocol); if (r->m_sb.sb_socket != -1) { // 连接对端 // DEBUG: CCQ: addrinfo->ai_addrlen:16 /* socket address 的长度 */ // DEBUG: CCQ: addrinfo->ai_addr->sa_family:2 /* 2代表`AF_INET`，也就是`PF_INET`，取之范围见上`AF_UNSPEC`... */ // DEBUG: CCQ: addrinfo->ai_addr->sa_data:\u0007\\217\\300\\250 if (connect(r->m_sb.sb_socket, service->ai_addr, service->ai_addrlen) 内容来源于：http://c.biancheng.net/view/2131.html 在 Linux 下使用 头文件中 socket() 函数来创建套接字，原型为： int socket(int af, int type, int protocol); 1) af 为地址族（Address Family），也就是 IP 地址类型，常用的有 AF_INET 和 AF_INET6。AF 是“Address Family”的简写，INET是“Inetnet”的简写。AF_INET 表示 IPv4 地址，例如 127.0.0.1；AF_INET6 表示 IPv6 地址，例如 1030::C9B4:FF12:48AA:1A2B。 大家需要记住127.0.0.1，它是一个特殊IP地址，表示本机地址，后面的教程会经常用到。 你也可以使用 PF 前缀，PF 是“Protocol Family”的简写，它和 AF 是一样的。例如，PF_INET 等价于 AF_INET，PF_INET6 等价于 AF_INET6。 2) type 为数据传输方式/套接字类型，常用的有 SOCK_STREAM（流格式套接字/面向连接的套接字） 和 SOCK_DGRAM（数据报套接字/无连接的套接字），我们已经在《套接字有哪些类型》一节中进行了介绍。 3) protocol 表示传输协议，常用的有 IPPROTO_TCP 和 IPPTOTO_UDP，分别表示 TCP 传输协议和 UDP 传输协议。 connect(r->m_sb.sb_socket, service->ai_addr, service->ai_addrlen)抓包结果： 过滤条件：ip.addr eq 81.68.250.191 9833 2261.079011 192.168.1.3 81.68.250.191 TCP 78 51965 → 1935 [SYN] Seq=0 Win=65535 Len=0 MSS=1460 WS=64 TSval=45457028 TSecr=0 SACK_PERM=1 9834 2261.090254 81.68.250.191 192.168.1.3 TCP 74 1935 → 51965 [SYN, ACK] Seq=0 Ack=1 Win=28960 Len=0 MSS=1400 SACK_PERM=1 TSval=2657772015 TSecr=45457028 WS=128 9835 2261.090325 192.168.1.3 81.68.250.191 TCP 66 51965 → 1935 [ACK] Seq=1 Ack=1 Win=131840 Len=0 TSval=45457039 TSecr=2657772015 代码片段分析3 // 执行Socks协商 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: r->Link.socksport：%d\", r->Link.socksport); // DEBUG: CCQ: r->Link.socksport：0 if (r->Link.socksport) { RTMP_Log(RTMP_LOGDEBUG, \"%s ... SOCKS negotiation\", __FUNCTION__); if (!SocksNegotiate(r)) { RTMP_Log(RTMP_LOGERROR, \"%s, SOCKS negotiation failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } } r->Link.socksport：0，首次不会执行SocksNegotiate。 代码片段分析4 /* set timeout */ { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: r->Link.timeout：%d\", r->Link.timeout); // DEBUG: CCQ: r->Link.timeout：30 // #define SET_RCVTIMEO(tv,s) int tv = s*1000 SET_RCVTIMEO(tv, r->Link.timeout); // r->m_sb.sb_socket：标识一个套接口的描述字（RTMP->RTMPSockBuf->sb_socket） // SOL_SOCKET：选项定义的层次；目前仅支持SOL_SOCKET和IPPROTO_TCP层次。 // SO_RCVTIMEO：接收超时。 // tv：超时时间 if (setsockopt (r->m_sb.sb_socket, SOL_SOCKET, SO_RCVTIMEO, (char *)&tv, sizeof(tv))) { RTMP_Log(RTMP_LOGERROR, \"%s, Setting socket timeout to %ds failed!\", __FUNCTION__, r->Link.timeout); } } 内容来源于：https://www.cnblogs.com/cthon/p/9270778.html SET_RCVTIMEO是宏定义，给tv赋值，这里tv为30*1000。 代码片段分析5 setsockopt(r->m_sb.sb_socket, SOL_SOCKET, SO_NOSIGPIPE, (char *) &on, sizeof(on)); setsockopt(r->m_sb.sb_socket, IPPROTO_TCP, TCP_NODELAY, (char *) &on, sizeof(on)); 内容来源于：https://www.cnblogs.com/cthon/p/9270778.html TCP_NODELAY选项禁止Nagle算法。Nagle算法通过将未确认的数据存入缓冲区直到蓄足一个包一起发送的方法，来减少主机发送的零碎小数据包的数目。但对于某些应用来说，这种算法将降低系统性能。所以TCP_NODELAY可用来将此算法关闭。应用程序编写者只有在确切了解它的效果并确实需要的情况下，才设置TCP_NODELAY选项，因为设置后对网络性能有明显的负面影响。TCP_NODELAY是唯一使用IPPROTO_TCP层的选项，其他所有选项都使用SOL_SOCKET层。 内容来源于：http://www.sinohandset.com/mac-osx%E4%B8%8Bso_nosigpipe%E7%9A%84%E6%80%AA%E5%BC%82%E8%A1%A8%E7%8E%B0 在linux下为了避免网络出错引起程序退出，我们一般采用MSG_NOSIGNAL来避免系统发送singal。这种错误一般发送在网络断开，但是程序仍然发送数据时，在接收时，没有必要使用。但是在linux下，使用此参数，也不会引起不好的结果。 RTMP_Connect1 int RTMP_Connect1(RTMP *r, RTMPPacket *cp) { if (r->Link.protocol & RTMP_FEATURE_SSL) { #if defined(CRYPTO) && !defined(NO_SSL) TLS_client(RTMP_TLS_ctx, r->m_sb.sb_ssl); TLS_setfd(r->m_sb.sb_ssl, r->m_sb.sb_socket); if (TLS_connect(r->m_sb.sb_ssl) Link.protocol & RTMP_FEATURE_HTTP) { r->m_msgCounter = 1; r->m_clientID.av_val = NULL; r->m_clientID.av_len = 0; HTTP_Post(r, RTMPT_OPEN, \"\", 1); if (HTTP_read(r, 1) != 0) { r->m_msgCounter = 0; RTMP_Log(RTMP_LOGDEBUG, \"%s, Could not connect for handshake\", __FUNCTION__); RTMP_Close(r); return 0; } r->m_msgCounter = 0; } RTMP_Log(RTMP_LOGDEBUG, \"%s, ... connected, handshaking\", __FUNCTION__); if (!HandShake(r, TRUE)) { RTMP_Log(RTMP_LOGERROR, \"%s, handshake failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } RTMP_Log(RTMP_LOGDEBUG, \"%s, handshaked\", __FUNCTION__); if (!SendConnectPacket(r, cp)) { RTMP_Log(RTMP_LOGERROR, \"%s, RTMP connect failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } return TRUE; } 代码片段分析1 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, r->Link.protocol:%d\", __FUNCTION__, r->Link.protocol); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, RTMP_FEATURE_SSL:%d\", __FUNCTION__, RTMP_FEATURE_SSL); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, r->Link.protocol & RTMP_FEATURE_SSL:%d\", __FUNCTION__, r->Link.protocol & RTMP_FEATURE_SSL); // DEBUG: CCQ: RTMP_Connect1, r->Link.protocol:16 // DEBUG: CCQ: RTMP_Connect1, RTMP_FEATURE_SSL:4 // #define RTMP_FEATURE_SSL 0x04 // DEBUG: CCQ: RTMP_Connect1, r->Link.protocol & RTMP_FEATURE_SSL:0 if (r->Link.protocol & RTMP_FEATURE_SSL) { #if defined(CRYPTO) && !defined(NO_SSL) TLS_client(RTMP_TLS_ctx, r->m_sb.sb_ssl); TLS_setfd(r->m_sb.sb_ssl, r->m_sb.sb_socket); if (TLS_connect(r->m_sb.sb_ssl) 1.为什么r->Link.protocol=16？ #define RTMP_FEATURE_WRITE 0x10 /* publish, not play */ void RTMP_EnableWrite(RTMP *r) { r->Link.protocol |= RTMP_FEATURE_WRITE; } RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, r->Link.protocol1:%d\", __FUNCTION__, _rtmp->Link.protocol); // DEBUG: CCQ: -[RTMPPusher connectWithURL:], r->Link.protocol1:0 RTMP_EnableWrite(_rtmp); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, r->Link.protocol2:%d\", __FUNCTION__, _rtmp->Link.protocol); // DEBUG: CCQ: -[RTMPPusher connectWithURL:], r->Link.protocol2:16 2.r->Link.protocol & RTMP_FEATURE_SSL为0，不会执行if后的代码。 代码片段分析2 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s, r->Link.protocol & RTMP_FEATURE_HTTP:%d\", __FUNCTION__, r->Link.protocol & RTMP_FEATURE_HTTP); // DEBUG: CCQ: RTMP_Connect1, r->Link.protocol & RTMP_FEATURE_HTTP:0 if (r->Link.protocol & RTMP_FEATURE_HTTP) { r->m_msgCounter = 1; r->m_clientID.av_val = NULL; r->m_clientID.av_len = 0; HTTP_Post(r, RTMPT_OPEN, \"\", 1); if (HTTP_read(r, 1) != 0) { r->m_msgCounter = 0; RTMP_Log(RTMP_LOGDEBUG, \"%s, Could not connect for handshake\", __FUNCTION__); RTMP_Close(r); return 0; } r->m_msgCounter = 0; } RTMP_Log(RTMP_LOGDEBUG, \"%s, ... connected, handshaking\", __FUNCTION__); r->Link.protocol & RTMP_FEATURE_HTTP为0，不会执行if后的代码。连接成功，开始执行handshaking。 代码片段3 // 进行HandShake if (!HandShake(r, TRUE)) { RTMP_Log(RTMP_LOGERROR, \"%s, handshake failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } RTMP_Log(RTMP_LOGDEBUG, \"%s, handshaked\", __FUNCTION__); connect(r->m_sb.sb_socket, service->ai_addr, service->ai_addrlen)： 14 6.159961 192.168.1.3 81.68.250.191 TCP 78 52049 → 1935 [SYN] Seq=0 Win=65535 Len=0 MSS=1460 WS=64 TSval=1695460720 TSecr=0 SACK_PERM=1 15 6.177631 81.68.250.191 192.168.1.3 TCP 74 1935 → 52049 [SYN, ACK] Seq=0 Ack=1 Win=28960 Len=0 MSS=1400 SACK_PERM=1 TSval=2658240513 TSecr=1695460720 WS=128 16 6.177717 192.168.1.3 81.68.250.191 TCP 66 52049 → 1935 [ACK] Seq=1 Ack=1 Win=131840 Len=0 TSval=1695460737 TSecr=2658240513 HandShake： 17 6.177792 192.168.1.3 81.68.250.191 TCP 1454 52049 → 1935 [ACK] Seq=1 Ack=1 Win=131840 Len=1388 TSval=1695460737 TSecr=2658240513 18 6.177793 192.168.1.3 81.68.250.191 RTMP 215 Handshake C0+C1 19 6.186903 81.68.250.191 192.168.1.3 TCP 66 1935 → 52049 [ACK] Seq=1 Ack=1538 Win=32128 Len=0 TSval=2658240524 TSecr=1695460737 20 6.187494 81.68.250.191 192.168.1.3 TCP 1454 1935 → 52049 [ACK] Seq=1 Ack=1538 Win=32128 Len=1388 TSval=2658240524 TSecr=1695460737 21 6.187498 81.68.250.191 192.168.1.3 TCP 1454 1935 → 52049 [ACK] Seq=1389 Ack=1538 Win=32128 Len=1388 TSval=2658240524 TSecr=1695460737 22 6.187499 81.68.250.191 192.168.1.3 RTMP 363 Handshake S0+S1+S2 23 6.187559 192.168.1.3 81.68.250.191 TCP 66 52049 → 1935 [ACK] Seq=1538 Ack=3074 Win=128768 Len=0 TSval=1695460746 TSecr=2658240524 24 6.187636 192.168.1.3 81.68.250.191 TCP 1454 52049 → 1935 [ACK] Seq=1538 Ack=3074 Win=131072 Len=1388 TSval=1695460746 TSecr=2658240524 25 6.187636 192.168.1.3 81.68.250.191 RTMP 214 Handshake C2 26 6.195197 81.68.250.191 192.168.1.3 TCP 66 1935 → 52049 [ACK] Seq=3074 Ack=3074 Win=35200 Len=0 TSval=2658240532 TSecr=1695460746 代码片段4 // /*握手成功之后，发送Connect Packet*/ if (!SendConnectPacket(r, cp)) { RTMP_Log(RTMP_LOGERROR, \"%s, RTMP connect failed.\", __FUNCTION__); RTMP_Close(r); return FALSE; } "},"pages/librtmp/librtmp_HandShake.html":{"url":"pages/librtmp/librtmp_HandShake.html","title":"librtmp源码之HandShake","keywords":"","body":"librttmp源码之HandShake HandShake static int HandShake(RTMP * r, int FP9HandShake) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s start\", __FUNCTION__); int i, offalg = 0; int dhposClient = 0; int digestPosClient = 0; int encrypted = r->Link.protocol & RTMP_FEATURE_ENC; RC4_handle keyIn = 0; RC4_handle keyOut = 0; int32_t *ip; uint32_t uptime; uint8_t clientbuf[RTMP_SIG_SIZE + 4], *clientsig=clientbuf+4; uint8_t serversig[RTMP_SIG_SIZE], client2[RTMP_SIG_SIZE], *reply; uint8_t type; getoff *getdh = NULL, *getdig = NULL; if (encrypted || r->Link.SWFSize) FP9HandShake = TRUE; else FP9HandShake = FALSE; r->Link.rc4keyIn = r->Link.rc4keyOut = 0; if (encrypted) { clientsig[-1] = 0x06; /* 0x08 is RTMPE as well */ offalg = 1; } else clientsig[-1] = 0x03; uptime = htonl(RTMP_GetTime()); memcpy(clientsig, &uptime, 4); if (FP9HandShake) { /* set version to at least 9.0.115.0 */ if (encrypted) { clientsig[4] = 128; clientsig[6] = 3; } else { clientsig[4] = 10; clientsig[6] = 45; } clientsig[5] = 0; clientsig[7] = 2; RTMP_Log(RTMP_LOGDEBUG, \"%s: Client type: %02X\", __FUNCTION__, clientsig[-1]); getdig = digoff[offalg]; getdh = dhoff[offalg]; } else { memset(&clientsig[4], 0, 4); } /* generate random data */ #ifdef _DEBUG memset(clientsig+8, 0, RTMP_SIG_SIZE-8); #else ip = (int32_t *)(clientsig+8); for (i = 2; i Link.dh = DHInit(1024); if (!r->Link.dh) { RTMP_Log(RTMP_LOGERROR, \"%s: Couldn't initialize Diffie-Hellmann!\", __FUNCTION__); return FALSE; } dhposClient = getdh(clientsig, RTMP_SIG_SIZE); RTMP_Log(RTMP_LOGDEBUG, \"%s: DH pubkey position: %d\", __FUNCTION__, dhposClient); if (!DHGenerateKey(r->Link.dh)) { RTMP_Log(RTMP_LOGERROR, \"%s: Couldn't generate Diffie-Hellmann public key!\", __FUNCTION__); return FALSE; } if (!DHGetPublicKey(r->Link.dh, &clientsig[dhposClient], 128)) { RTMP_Log(RTMP_LOGERROR, \"%s: Couldn't write public key!\", __FUNCTION__); return FALSE; } } digestPosClient = getdig(clientsig, RTMP_SIG_SIZE); /* reuse this value in verification */ RTMP_Log(RTMP_LOGDEBUG, \"%s: Client digest offset: %d\", __FUNCTION__, digestPosClient); CalculateDigest(digestPosClient, clientsig, GenuineFPKey, 30, &clientsig[digestPosClient]); RTMP_Log(RTMP_LOGDEBUG, \"%s: Initial client digest: \", __FUNCTION__); RTMP_LogHex(RTMP_LOGDEBUG, clientsig + digestPosClient, SHA256_DIGEST_LENGTH); } #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"Clientsig: \"); RTMP_LogHex(RTMP_LOGDEBUG, clientsig, RTMP_SIG_SIZE); #endif if (!WriteN(r, (char *)clientsig-1, RTMP_SIG_SIZE + 1)) return FALSE; if (ReadN(r, (char *)&type, 1) != 1) /* 0x03 or 0x06 */ return FALSE; RTMP_Log(RTMP_LOGDEBUG, \"%s: Type Answer : %02X\", __FUNCTION__, type); if (type != clientsig[-1]) RTMP_Log(RTMP_LOGWARNING, \"%s: Type mismatch: client sent %d, server answered %d\", __FUNCTION__, clientsig[-1], type); if (ReadN(r, (char *)serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE) return FALSE; /* decode server response */ memcpy(&uptime, serversig, 4); uptime = ntohl(uptime); RTMP_Log(RTMP_LOGDEBUG, \"%s: Server Uptime : %d\", __FUNCTION__, uptime); RTMP_Log(RTMP_LOGDEBUG, \"%s: FMS Version : %d.%d.%d.%d\", __FUNCTION__, serversig[4], serversig[5], serversig[6], serversig[7]); if (FP9HandShake && type == 3 && !serversig[4]) FP9HandShake = FALSE; #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"Server signature:\"); RTMP_LogHex(RTMP_LOGDEBUG, serversig, RTMP_SIG_SIZE); #endif if (FP9HandShake) { uint8_t digestResp[SHA256_DIGEST_LENGTH]; uint8_t *signatureResp = NULL; /* we have to use this signature now to find the correct algorithms for getting the digest and DH positions */ int digestPosServer = getdig(serversig, RTMP_SIG_SIZE); if (!VerifyDigest(digestPosServer, serversig, GenuineFMSKey, 36)) { RTMP_Log(RTMP_LOGWARNING, \"Trying different position for server digest!\"); offalg ^= 1; getdig = digoff[offalg]; getdh = dhoff[offalg]; digestPosServer = getdig(serversig, RTMP_SIG_SIZE); if (!VerifyDigest(digestPosServer, serversig, GenuineFMSKey, 36)) { RTMP_Log(RTMP_LOGERROR, \"Couldn't verify the server digest\"); /* continuing anyway will probably fail */ return FALSE; } } /* generate SWFVerification token (SHA256 HMAC hash of decompressed SWF, key are the last 32 bytes of the server handshake) */ if (r->Link.SWFSize) { const char swfVerify[] = { 0x01, 0x01 }; char *vend = r->Link.SWFVerificationResponse+sizeof(r->Link.SWFVerificationResponse); memcpy(r->Link.SWFVerificationResponse, swfVerify, 2); AMF_EncodeInt32(&r->Link.SWFVerificationResponse[2], vend, r->Link.SWFSize); AMF_EncodeInt32(&r->Link.SWFVerificationResponse[6], vend, r->Link.SWFSize); HMACsha256(r->Link.SWFHash, SHA256_DIGEST_LENGTH, &serversig[RTMP_SIG_SIZE - SHA256_DIGEST_LENGTH], SHA256_DIGEST_LENGTH, (uint8_t *)&r->Link.SWFVerificationResponse[10]); } /* do Diffie-Hellmann Key exchange for encrypted RTMP */ if (encrypted) { /* compute secret key */ uint8_t secretKey[128] = { 0 }; int len, dhposServer; dhposServer = getdh(serversig, RTMP_SIG_SIZE); RTMP_Log(RTMP_LOGDEBUG, \"%s: Server DH public key offset: %d\", __FUNCTION__, dhposServer); len = DHComputeSharedSecretKey(r->Link.dh, &serversig[dhposServer], 128, secretKey); if (len Link.rc4keyIn = keyIn; r->Link.rc4keyOut = keyOut; /* update the keystreams */ if (r->Link.rc4keyIn) { RC4_encrypt(r->Link.rc4keyIn, RTMP_SIG_SIZE, (uint8_t *) buff); } if (r->Link.rc4keyOut) { RC4_encrypt(r->Link.rc4keyOut, RTMP_SIG_SIZE, (uint8_t *) buff); } } } else { if (memcmp(serversig, clientsig, RTMP_SIG_SIZE) != 0) { RTMP_Log(RTMP_LOGWARNING, \"%s: client signature does not match!\", __FUNCTION__); } } RTMP_Log(RTMP_LOGDEBUG, \"%s: Handshaking finished....\", __FUNCTION__); return TRUE; } 代码片段分析1 int i, offalg = 0;// offalg加密才使用 int dhposClient = 0;// 加密才使用 int digestPosClient = 0;// 加密才使用 int encrypted = r->Link.protocol & RTMP_FEATURE_ENC; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s encrypted:%d\", __FUNCTION__, encrypted); // DEBUG: CCQ: HandShake encrypted:0 // 不加密 RC4_handle keyIn = 0;// 加密才使用 RC4_handle keyOut = 0;// 加密才使用 int32_t *ip;/* generate random data */ uint32_t uptime;// 当前时间，填充C1前4字节 代码片段分析2 RC4_handle keyIn = 0;// 加密才使用 RC4_handle keyOut = 0;// 加密才使用 int32_t *ip;/* generate random data */ uint32_t uptime;// 当前时间，填充C1前4字节 // #define RTMP_SIG_SIZE 1536:C1和S1消息有1536字节长 uint8_t clientbuf[RTMP_SIG_SIZE + 4], *clientsig=clientbuf+4; uint8_t serversig[RTMP_SIG_SIZE], client2[RTMP_SIG_SIZE], *reply;// reply,client2加密才使用 uint8_t type;// ReadN(r, (char *)&type, 1)之后获得 getoff *getdh = NULL, *getdig = NULL;// 加密才使用 代码片段分析3 if (encrypted || r->Link.SWFSize) FP9HandShake = TRUE; else //普通的 FP9HandShake = FALSE; r->Link.rc4keyIn = r->Link.rc4keyOut = 0;// 加密才使用 /*C0 字段已经写入clientsig*/ if (encrypted) { clientsig[-1] = 0x06; /* 0x08 is RTMPE as well */ offalg = 1; } else //0x03代表RTMP协议的版本（客户端要求的） //数组竟然能有“-1”下标,因为clientsig指向的是clientbuf+4,所以不存在非法地址 //C0中的字段(1B) clientsig[-1] = 0x03; uptime = htonl(RTMP_GetTime()); //void *memcpy(void *dest, const void *src, int n); //由src指向地址为起始地址的连续n个字节的数据复制到以dest指向地址为起始地址的空间内 //把uptime的前4字节（其实一共就4字节）数据拷贝到clientsig指向的地址中 //C1中的字段(4B) // ———————————————— // 版权声明：本文为CSDN博主「雷霄骅」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 // 原文链接：https://blog.csdn.net/leixiaohua1020/article/details/12954329 memcpy(clientsig, &uptime, 4); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: uptime: \"); RTMP_LogHexString(RTMP_LOGDEBUG, clientsig, 4); // DEBUG: Clientsig: // DEBUG: 0000: 0c b7 58 56 ..XV clientsig[-1] = 0x03;将RTMP协议的版本号写入，1字节。 memcpy(clientsig, &uptime, 4);将当前时间写入clientsig，4字节。 代码片段分析4 if (FP9HandShake)// 加密才处理 { /* set version to at least 9.0.115.0 */ if (encrypted) { clientsig[4] = 128; clientsig[6] = 3; } else { clientsig[4] = 10; clientsig[6] = 45; } clientsig[5] = 0; clientsig[7] = 2; RTMP_Log(RTMP_LOGDEBUG, \"%s: Client type: %02X\", __FUNCTION__, clientsig[-1]); getdig = digoff[offalg]; getdh = dhoff[offalg]; } else { memset(&clientsig[4], 0, 4); } memset(&clientsig[4], 0, 4);当前时间的后补0，占4字节。 代码片段分析5 /* generate random data */ #ifdef _DEBUG //将clientsig+8开始的1528个字节替换为0（这是一种简单的方法） //这是C1中的random字段 memset(clientsig+8, 0, RTMP_SIG_SIZE-8); #else //实际中使用rand()循环生成1528字节的伪随机数 ip = (int32_t *)(clientsig+8); for (i = 2; i 根据上面的分析，得知clientsig的-1位置存放这0x03（RTMP）的版本号，占1字节；0-4位置存放这当前时间uptime，占4字节；之后5-8位置补0，占4字节；而clientsig的总长度是1536个字节，已经占用了8字节，所以上面代码是在补全剩下的1528个字节，补全的方式是使用随机数。 代码片段分析5 #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"Clientsig: \"); RTMP_LogHex(RTMP_LOGDEBUG, clientsig, RTMP_SIG_SIZE); #endif RTMP_Log(RTMP_LOGDEBUG, \"CCQ: Clientsig: \"); RTMP_LogHexString(RTMP_LOGDEBUG, clientsig, RTMP_SIG_SIZE); //发送数据报C0+C1 //从clientsig-1开始发，长度1536+1，两个包合并 //握手---------------- RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。发送握手数据C0+C1\", __FUNCTION__); if (!WriteN(r, (char *)clientsig-1, RTMP_SIG_SIZE + 1)) return FALSE; //读取数据报，长度1，存入type //是服务器的S0，表示服务器使用的RTMP版本 if (ReadN(r, (char *)&type, 1) != 1) /* 0x03 or 0x06 */ return FALSE; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。接收握手数据S0\", __FUNCTION__); RTMP_Log(RTMP_LOGDEBUG, \"%s: Type Answer : %02X\", __FUNCTION__, type); //客户端要求的版本和服务器提供的版本不同 if (type != clientsig[-1]) RTMP_Log(RTMP_LOGWARNING, \"%s: Type mismatch: client sent %d, server answered %d\", __FUNCTION__, clientsig[-1], type); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。接收握手数据S1\", __FUNCTION__); //客户端和服务端随机序列长度是否相同 if (ReadN(r, (char *)serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE) return FALSE; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。接收握手数据S1\", __FUNCTION__); /* decode server response */ //把serversig的前四个字节赋值给uptime memcpy(&uptime, serversig, 4); uptime = ntohl(uptime);//大端转小端 RTMP_Log(RTMP_LOGDEBUG, \"%s: Server Uptime : %d\", __FUNCTION__, uptime); RTMP_Log(RTMP_LOGDEBUG, \"%s: FMS Version : %d.%d.%d.%d\", __FUNCTION__, serversig[4], serversig[5], serversig[6], serversig[7]); 内容来源于：https://blog.csdn.net/bwangk/article/details/112802823 C0 和 S0消息格式：C0和S0是单独的一个字节，表示版本信息。 在C0中这个字段表示客户端要求的RTMP版本 。在S0中这个字段表示服务器选择的RTMP版本。本规范所定义的版本是3；0-2是早期产品所用的，已被丢弃；4-31保留在未来使用 ；32-255不允许使用 （为了区分其他以某一字符开始的文本协议）。如果服务无法识别客户端请求的版本，应该返回3 。客户端可以选择减到版本3或选择取消握手 C1 和 S1消息格式：C1和S1消息有1536字节长。 时间：4字节：本字段包含时间戳。该时间戳应该是发送这个数据块的端点的后续块的时间起始点。可以是0，或其他的任何值。为了同步多个流，端点可能发送其块流的当前值。 零：4字节：本字段必须是全零。 随机数据：1528字节。本字段可以包含任何值。因为每个端点必须用自己初始化的握手和对端初始化的握手来区分身份，所以这个数据应有充分的随机性。但是并不需要加密安全的随机值，或者动态值。 C2 和 S2 消息格式：C2和S2消息有1536字节长，只是S1和C1的回复。 时间：4字节：本字段必须包含对等段发送的时间（对C2来说是S1，对S2来说是C1）。 时间2：4字节：本字段必须包含先前发送的并被对端读取的包的时间戳。 随机回复：1528字节：本字段必须包含对端发送的随机数据字段（对C2来说是S1，对S2来说是C1）。每个对等端可以用时间和时间2字段中的时间戳来快速地估计带宽和延迟。但这样做可能并不实用。 上面代码的功能简单说：发送C0+C1，解析S0、S1。 代码片段分析6 #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"Server signature:\"); RTMP_LogHex(RTMP_LOGDEBUG, serversig, RTMP_SIG_SIZE); #endif RTMP_Log(RTMP_LOGDEBUG, \"CCQ: Server signature:\"); RTMP_LogHexString(RTMP_LOGDEBUG, serversig, RTMP_SIG_SIZE); if (FP9HandShake) {} else { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s reply = serversig\", __FUNCTION__); //直接赋值 reply = serversig; #if 0 uptime = htonl(RTMP_GetTime()); memcpy(reply+4, &uptime, 4); #endif } 代码片段分析7 #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"%s: Sending handshake response: \", __FUNCTION__); RTMP_LogHex(RTMP_LOGDEBUG, reply, RTMP_SIG_SIZE); #endif //把reply中的1536字节数据发送出去 //对应C2 //握手---------------- RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。发送握手数据C2\", __FUNCTION__); if (!WriteN(r, (char *)reply, RTMP_SIG_SIZE)) return FALSE; /* 2nd part of handshake */ //读取1536字节数据到serversig //握手---------------- RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。读取握手数据S2\", __FUNCTION__); if (ReadN(r, (char *)serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE) return FALSE; #ifdef _DEBUG RTMP_Log(RTMP_LOGDEBUG, \"%s: 2nd handshake: \", __FUNCTION__); RTMP_LogHex(RTMP_LOGDEBUG, serversig, RTMP_SIG_SIZE); #endif RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s: 2nd handshake: \", __FUNCTION__); RTMP_LogHexString(RTMP_LOGDEBUG, serversig, RTMP_SIG_SIZE); if (FP9HandShake)// 加密才执行 {} else { //int memcmp(const void *buf1, const void *buf2, unsigned int count); 当buf1=buf2时，返回值=0 //比较serversig和clientsig是否相等 //握手---------------- RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。比较握手数据签名\", __FUNCTION__); if (memcmp(serversig, clientsig, RTMP_SIG_SIZE) != 0) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 建立连接：第1次连接。握手数据签名不匹配！\", __FUNCTION__); RTMP_Log(RTMP_LOGWARNING, \"%s: client signature does not match!\", __FUNCTION__); } } 上面代码简单说：发送C2，解析S2，客户端成功解析S2，服务端成功接收C2，第一次连接握手成功。 至此，HandShake的代码分析完毕。 "},"pages/librtmp/librtmp_SendConnectPacket.html":{"url":"pages/librtmp/librtmp_SendConnectPacket.html","title":"librtmp源码之SendConnectPacket","keywords":"","body":"librtmp源码之SendConnectPacket 代码片段分析1 RTMPPacket packet; // pend：AMF_EncodeNamedString函数参数 // pbuf: packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; char pbuf[4096], *pend = pbuf + sizeof(pbuf); char *enc; if (cp) // 不会走 return RTMP_SendPacket(r, cp, TRUE); 代码片段分析2 // chunk stream id(chunk basic header)字段 // 块流ID，消息头的第1个字节的后6位 packet.m_nChannel = 0x03; /* control channel (invoke) */ // #define RTMP_PACKET_SIZE_LARGE 0 onMetaData流开始的绝对时间戳控制消息（如connect） // #define RTMP_PACKET_SIZE_MEDIUM 1 大部分的rtmp header都是8字节的 // #define RTMP_PACKET_SIZE_SMALL 2 比较少见 // #define RTMP_PACKET_SIZE_MINIMUM 3 偶尔出现，低于8字节频率 // chunk type id (2bit)fmt 对应message head {0,3,7,11} + (6bit)chunk stream id // 块类型ID，消息头的第1个字节的前2位，决定消息头长度 packet.m_headerType = RTMP_PACKET_SIZE_LARGE; // 消息类型为20的用AMF0编码，这些消息用于在远端实现连接，创建流，发布，播放和暂停等操作 // #define RTMP_PACKET_TYPE_INVOKE 0x14 // Message type ID（1-7协议控制；8，9音视频；10以后为AMF编码消息） // 消息类型ID，消息头的第8个字节，0x14表示以AMF0编码。另外还有如0x04表示用户控制消息，0x05表示Window Acknowledgement Size，0x06表示 Set Peer Bandwith等等。 packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; // 时间搓，消息头的第2-4个字节 packet.m_nTimeStamp = 0; // Stream ID通常用以完成某些特定的工作，如使用ID为0的Stream来完成客户端和服务器的连接和控制，使用ID为1的Stream来完成视频流的控制和播放等工作。 // 流ID，消息头的第9-12个字节（末尾最后4个字节） packet.m_nInfoField2 = 0; // 是否含有Extend timeStamp字段 packet.m_hasAbsTimestamp = 0; // #define RTMP_MAX_HEADER_SIZE 18 // 设置chunk body(data)的起始指针，前18个字节用来存储消息头，之后就用来存消息体 packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; 消息头的第5-7个字节是放的是body的长度，后面会设置。 代码片段分析3 // 指针赋值，通过enc来设置消息体的内容 enc = packet.m_body; // 压入connect命令和操作流水号 // connect使用##进行字符串化连接，此处编码connect字符串 enc = AMF_EncodeString(enc, pend, &av_connect); // m_numInvokes：0x14命令远程过程调用计数 enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); // 压入对象 *enc++ = AMF_OBJECT; // 压入对象的“app”字符串，客户端连接到的服务器端应用的名字 enc = AMF_EncodeNamedString(enc, pend, &av_app, &r->Link.app); if (!enc) return FALSE; // 压入“type”，这里是“nonprivate” if (r->Link.protocol & RTMP_FEATURE_WRITE) { enc = AMF_EncodeNamedString(enc, pend, &av_type, &av_nonprivate); if (!enc) return FALSE; } // 压入“flashver”，Flash Player 版本号。和ApplicationScript getversion() 方法返回的是同一个字符串。FMSc/1.0 if (r->Link.flashVer.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_flashVer, &r->Link.flashVer); if (!enc) return FALSE; } // 压入“swfUrl”，进行当前连接的 SWF 文件源地址。file://C:/FlvPlayer.swf if (r->Link.swfUrl.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_swfUrl, &r->Link.swfUrl); if (!enc) return FALSE; } // 压入“tcUrl”，服务器 URL。具有以下格式：protocol://servername:port/appName/appInstance，rtmp://localhost:1935/testapp/instance1 if (r->Link.tcUrl.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_tcUrl, &r->Link.tcUrl); if (!enc) return FALSE; } if (!(r->Link.protocol & RTMP_FEATURE_WRITE)) { // 压入“fpad”，如果使用了代理就是 true。true 或者 false。 enc = AMF_EncodeNamedBoolean(enc, pend, &av_fpad, FALSE); if (!enc) return FALSE; enc = AMF_EncodeNamedNumber(enc, pend, &av_capabilities, 15.0); if (!enc) return FALSE; // 压入“audioCodecs“，表明客户端所支持的音频编码。SUPPORT_SND_MP3 enc = AMF_EncodeNamedNumber(enc, pend, &av_audioCodecs, r->m_fAudioCodecs); if (!enc) return FALSE; // 压入“videoCodecs”，表明支持的视频编码。SUPPORT_VID_SORENSON enc = AMF_EncodeNamedNumber(enc, pend, &av_videoCodecs, r->m_fVideoCodecs); if (!enc) return FALSE; // 压入“videoFunction”，表明所支持的特殊视频方法。SUPPORT_VID_CLIENT_SEEK enc = AMF_EncodeNamedNumber(enc, pend, &av_videoFunction, 1.0); if (!enc) return FALSE; if (r->Link.pageUrl.av_len) { // 压入“pageUrl“，SWF 文件所加载的网页 URL。http://somehost/sample.html enc = AMF_EncodeNamedString(enc, pend, &av_pageUrl, &r->Link.pageUrl); if (!enc) return FALSE; } } if (r->m_fEncoding != 0.0 || r->m_bSendEncoding) { /* AMF0, AMF3 not fully supported yet */ enc = AMF_EncodeNamedNumber(enc, pend, &av_objectEncoding, r->m_fEncoding); if (!enc) return FALSE; } // 判断是否溢出 if (enc + 3 >= pend) return FALSE; // 压入属性结束标记 *enc++ = 0; *enc++ = 0; /* end of object - 0x00 0x00 0x09 */ *enc++ = AMF_OBJECT_END; /* add auth string */ if (r->Link.auth.av_len) { enc = AMF_EncodeBoolean(enc, pend, r->Link.lFlags & RTMP_LF_AUTH); if (!enc) return FALSE; enc = AMF_EncodeString(enc, pend, &r->Link.auth); if (!enc) return FALSE; } if (r->Link.extras.o_num) { int i; for (i = 0; i Link.extras.o_num; i++) { enc = AMFProp_Encode(&r->Link.extras.o_props[i], enc, pend); if (!enc) return FALSE; } } 代码片段分析4 if (!enc) return FALSE; if (r->Link.protocol & RTMP_FEATURE_WRITE) { enc = AMF_EncodeNamedString(enc, pend, &av_type, &av_nonprivate); if (!enc) return FALSE; } if (r->Link.flashVer.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_flashVer, &r->Link.flashVer); if (!enc) return FALSE; } if (r->Link.swfUrl.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_swfUrl, &r->Link.swfUrl); if (!enc) return FALSE; } if (r->Link.tcUrl.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_tcUrl, &r->Link.tcUrl); if (!enc) return FALSE; } if (!(r->Link.protocol & RTMP_FEATURE_WRITE)) { enc = AMF_EncodeNamedBoolean(enc, pend, &av_fpad, FALSE); if (!enc) return FALSE; enc = AMF_EncodeNamedNumber(enc, pend, &av_capabilities, 15.0); if (!enc) return FALSE; enc = AMF_EncodeNamedNumber(enc, pend, &av_audioCodecs, r->m_fAudioCodecs); if (!enc) return FALSE; enc = AMF_EncodeNamedNumber(enc, pend, &av_videoCodecs, r->m_fVideoCodecs); if (!enc) return FALSE; enc = AMF_EncodeNamedNumber(enc, pend, &av_videoFunction, 1.0); if (!enc) return FALSE; if (r->Link.pageUrl.av_len) { enc = AMF_EncodeNamedString(enc, pend, &av_pageUrl, &r->Link.pageUrl); if (!enc) return FALSE; } } if (r->m_fEncoding != 0.0 || r->m_bSendEncoding) { /* AMF0, AMF3 not fully supported yet */ enc = AMF_EncodeNamedNumber(enc, pend, &av_objectEncoding, r->m_fEncoding); if (!enc) return FALSE; } if (enc + 3 >= pend) return FALSE; // 压入属性结束标记 *enc++ = 0; *enc++ = 0; /* end of object - 0x00 0x00 0x09 */ *enc++ = AMF_OBJECT_END; 代码片段分析5 /* add auth string */ if (r->Link.auth.av_len) { enc = AMF_EncodeBoolean(enc, pend, r->Link.lFlags & RTMP_LF_AUTH); if (!enc) return FALSE; enc = AMF_EncodeString(enc, pend, &r->Link.auth); if (!enc) return FALSE; } if (r->Link.extras.o_num) { int i; for (i = 0; i Link.extras.o_num; i++) { enc = AMFProp_Encode(&r->Link.extras.o_props[i], enc, pend); if (!enc) return FALSE; } } // 经过AMF编码组包后，message的大小 (如果是音视频数据 即FLV格式一个Tag中Tag Data 大小) // 消息体长度，消息头的第5-7个字节 packet.m_nBodySize = enc - packet.m_body; // 发送报文，并记入应答队列 return RTMP_SendPacket(r, &packet, TRUE); 报文数据 DEBUG2: 0000: 03 00 00 00 00 00 55 14 00 00 00 00 ......U..... DEBUG2: 0000: 02 00 07 63 6f 6e 6e 65 63 74 00 3f f0 00 00 00 ...connect.?.... DEBUG2: 0010: 00 00 00 03 00 03 61 70 70 02 00 04 6c 69 76 65 ......app...live DEBUG2: 0020: 00 04 74 79 70 65 02 00 0a 6e 6f 6e 70 72 69 76 ..type...nonpriv DEBUG2: 0030: 61 74 65 00 05 74 63 55 72 6c 02 00 15 72 74 6d ate..tcUrl...rtm DEBUG2: 0040: 70 3a 2f 2f 31 32 32 31 2e 73 69 74 65 2f 6c 69 p://1221.site/li DEBUG2: 0050: 76 65 00 00 09 ve... 参考： RTMP 协议规范(中文版) 流媒体-RTMP协议-librtmp库学习（二） librtmp源码分析之核心实现解读 rtmp源码分析之RTMP_Write 和 RTMP_SendPacket ASCII对照表 RTMP协议详解及实例分析 手撕rtmp协议细节（2）——rtmp Header "},"pages/librtmp/librtmp_RTMP_SendPacket.html":{"url":"pages/librtmp/librtmp_RTMP_SendPacket.html","title":"librtmp源码之RTMP_SendPacket","keywords":"","body":"librtmp源码之RTMP_SendPacket 代码片段分析1 // 取出对应块流ID上一次发送的报文 const RTMPPacket *prevPacket; // 上一次相对时间戳 uint32_t last = 0; // 表示块头初始大小 int nSize; // hSize表示块头大小 // 块基本头是1-3字节，因此用变量cSize来表示剩下的0-2字节 int hSize, cSize; // header头指针指向头部，hend块尾指针指向body头部 // hbuf表示头部最大18（3字节最大块基本头+11字节最大快消息头+4字节扩展时间戳）缓冲数组 // hptr指向header // c = packet->m_headerType m_nTimeStamp - last; // 时间戳增量 uint32_t t; // buffer 表示 data 的指针，tbuf 块初始指针，toff 块指针 char *buffer, *tbuf = NULL, *toff = NULL; // Chunk大小，默认是128字节 int nChunkSize; // tlen ? int tlen; // ? if (packet->m_nChannel >= r->m_channelsAllocatedOut) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if (packet->m_nChannel >= r->m_channelsAllocatedOut)\", __FUNCTION__); int n = packet->m_nChannel + 10; RTMPPacket **packets = realloc(r->m_vecChannelsOut, sizeof(RTMPPacket*) * n); if (!packets) { free(r->m_vecChannelsOut); r->m_vecChannelsOut = NULL; r->m_channelsAllocatedOut = 0; return FALSE; } r->m_vecChannelsOut = packets; memset(r->m_vecChannelsOut + r->m_channelsAllocatedOut, 0, sizeof(RTMPPacket*) * (n - r->m_channelsAllocatedOut)); r->m_channelsAllocatedOut = n; } 代码片段分析2 // 获取该通道，上一次的数据 prevPacket = r->m_vecChannelsOut[packet->m_nChannel]; // 尝试对非LARGE报文进行字段压缩 // 前一个packet存在且不是完整的ChunkMsgHeader，因此有可能需要调整块消息头的类型 // fmt字节 /* case 0:chunk msg header 长度为11 * case 1:chunk msg header 长度为7 * case 2:chunk msg header 长度为3 * case 3:chunk msg header 长度为0 */ if (prevPacket && packet->m_headerType != RTMP_PACKET_SIZE_LARGE) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if (prevPacket && packet->m_headerType != RTMP_PACKET_SIZE_LARGE)\", __FUNCTION__); /* compress a bit by using the prev packet's attributes */ // 获取ChunkMsgHeader类型，前一个Chunk与当前Chunk比较 // 如果前后两个块的大小、包类型及块头类型都相同，则将块头类型fmt设为2， // 即可省略消息长度、消息类型id、消息流id // 可以参考官方协议：流的分块 --- 6.1.2.3节 if (prevPacket->m_nBodySize == packet->m_nBodySize && prevPacket->m_packetType == packet->m_packetType && packet->m_headerType == RTMP_PACKET_SIZE_MEDIUM) packet->m_headerType = RTMP_PACKET_SIZE_SMALL; // 前后两个块的时间戳相同，且块头类型fmt为2，则相应的时间戳也可省略，因此将块头类型置为3 // 可以参考官方协议：流的分块 --- 6.1.2.4节 if (prevPacket->m_nTimeStamp == packet->m_nTimeStamp && packet->m_headerType == RTMP_PACKET_SIZE_SMALL) packet->m_headerType = RTMP_PACKET_SIZE_MINIMUM; // 上一次相对时间戳 last = prevPacket->m_nTimeStamp; } // 块头类型fmt取值0、1、2、3，超过3就表示出错(fmt占二个字节) if (packet->m_headerType > 3) /* sanity */ { RTMP_Log(RTMP_LOGERROR, \"sanity failed!! trying to send header of type: 0x%02x.\", (unsigned char)packet->m_headerType); return FALSE; } // 块头初始大小 = 基本头(1字节) + 块消息头大小(11/7/3/0) = [12, 8, 4, 1] // 块基本头是1-3字节，因此用变量cSize来表示剩下的0-2字节 // nSize 表示块头初始大小， hSize表示块头大小 nSize = packetSize[packet->m_headerType]; hSize = nSize; cSize = 0; // 时间戳增量 t = packet->m_nTimeStamp - last; 代码片段分析3 if (packet->m_body) { // 块头的首指针 向前平移了 基本头(1字节) + 块消息头大小(11/7/3/0) 字节 header = packet->m_body - nSize; // 块头的尾指针 hend = packet->m_body; } else { header = hbuf + 6; hend = hbuf + sizeof(hbuf); } // 计算基本头的扩充大小 // 块基本头是1-3字节，因此用变量cSize来表示剩下的0-2字节 if (packet->m_nChannel > 319) // 块流id(cs id)大于319，则块基本头占3个字节 cSize = 2; else if (packet->m_nChannel > 63) // 块流id(cs id)在64与319之间，则块基本头占2个字节 cSize = 1; if (cSize) { // 向前平移了 块基本头是1-3字节，因此用变量cSize来表示剩下的0-2字节 个字节 header -= cSize; hSize += cSize; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s cSize:%d\", __FUNCTION__, cSize); } // 根据时间戳计算是否需要扩充头大小 // 如果 t>0xffffff， 则需要使用 extended timestamp if (t >= 0xffffff) { // 向前平移了 extended timestamp header -= 4; hSize += 4; RTMP_Log(RTMP_LOGWARNING, \"Larger timestamp than 24-bit: 0x%x\", t); } // 确定好 Header 的位置后，就可以开始赋值了 // 向缓冲区压入基本头 hptr = header; // 把ChunkBasicHeader的Fmt类型左移6位 // cSize 表示块基本头剩下的0-2字节 // 设置basic header的第一个字节值，前两位为fmt. 可以参考官方协议：流的分块 --- 6.1.1节 c = packet->m_headerType m_nChannel;// chunk stream ID RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 把ChunkBasicHeader的低6位设置成ChunkStreamID\", __FUNCTION__); break; case 1:// 同理，但低6位设置成000000 break; case 2:// 同理，但低6位设置成000001 c |= 1; break; } // 可以拆分成两句*hptr=c; hptr++，此时hptr指向第2个字节 *hptr++ = c; // 设置basic header的第二(三)个字节值 if (cSize) { // 将要放到第2字节的内容tmp int tmp = packet->m_nChannel - 64; // 获取低位存储与第2字节 *hptr++ = tmp & 0xff; // ChunkBasicHeader是最大的3字节时,获取高位存储于最后1个字节（注意：排序使用大端序列，和主机相反） if (cSize == 2) *hptr++ = tmp >> 8; } // nSize 块头初始大小 = 基本头(1字节) + 块消息头大小(11/7/3/0) = [12, 8, 4, 1] // 向缓冲区压入时间戳 if (nSize > 1) { // 写入 timestamp 或 timestamp delta hptr = AMF_EncodeInt24(hptr, hend, t > 0xffffff ? 0xffffff : t); } // 向缓冲区压入负载大小和报文类型 if (nSize > 4) { // 写入 message length hptr = AMF_EncodeInt24(hptr, hend, packet->m_nBodySize); // 写入 message type id *hptr++ = packet->m_packetType; } // 向缓冲区压入流ID if (nSize > 8) // 写入 message stream id // 还原Chunk为Message的时候都是根据这个ID来辨识是否是同一个消息的Chunk的 hptr += EncodeInt32LE(hptr, packet->m_nInfoField2); // 向缓冲区压入扩展时间戳 if (t >= 0xffffff) // 写入 extended timestamp hptr = AMF_EncodeInt32(hptr, hend, t); // 到此为止，已经将块头填写好了 // 此时nSize表示负载数据的长度, buffer是指向负载数据区的指针 nSize = packet->m_nBodySize; buffer = packet->m_body; // Chunk大小，默认是128字节 nChunkSize = r->m_outChunkSize; RTMP_Log(RTMP_LOGDEBUG2, \"%s: fd=%d, size=%d\", __FUNCTION__, r->m_sb.sb_socket, nSize); 代码片段分析4 /* send all chunks in one HTTP request */ if (r->Link.protocol & RTMP_FEATURE_HTTP) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if (r->Link.protocol & RTMP_FEATURE_HTTP)\", __FUNCTION__); // nSize:Message负载长度；nChunkSize：Chunk长度； // 例nSize：307，nChunkSize:128； // 可分为（307 + 128 - 1）/128 = 3个 // 为什么加 nChunkSize - 1？因为除法会只取整数部分！ int chunks = (nSize+nChunkSize-1) / nChunkSize; if (chunks > 1) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s chunks > 1，chunks: %d\", __FUNCTION__, chunks); // 注意：ChunkBasicHeader的长度 = cSize + 1 // 消息分n块后总的开销： // n个ChunkBasicHeader，1个ChunkMsgHeader，1个Message负载 // 实际上只有第一个Chunk是完整的，剩下的只有ChunkBasicHeader tlen = chunks * (cSize + 1) + nSize + hSize; tbuf = malloc(tlen); if (!tbuf) return FALSE; toff = tbuf; } } // 消息的负载 + 头 while (nSize + hSize) { int wrote; // 消息负载大小 Link.protocol采用Http协议，则将RTMP包数据封装成多个Chunk，然后一次性发送。 // 否则每封装成一个块，就立即发送出去 if (tbuf) { // 将从Chunk头开始的nChunkSize + hSize个字节拷贝至toff中， // 这些拷贝的数据包括块头数据(hSize字节)和nChunkSize个负载数据 memcpy(toff, header, nChunkSize + hSize); toff += nChunkSize + hSize; RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if (tbuf)\", __FUNCTION__); } // 负载数据长度不超过设定的块大小，不需要分块，因此tbuf为NULL；或者r->Link.protocol不采用Http else { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if !(tbuf)\", __FUNCTION__); // 直接将负载数据和块头数据发送出去 wrote = WriteN(r, header, nChunkSize + hSize); if (!wrote) return FALSE; } // 消息负载长度 - Chunk负载长度 nSize -= nChunkSize; // buffer指针前移1个Chunk负载长度 buffer += nChunkSize; // 重置块头大小为0，后续的块只需要有基本头(或加上扩展时间戳)即可 hSize = 0; // 如果消息负载数据还没有发完，准备填充下一个块的块头数据 // 若只有部分负载发送成功，则需继续构造块再次发送 if (nSize > 0) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 继续构造块再次发送\", __FUNCTION__); // 只需要构造3号类型的块头 header = buffer - 1; // basic header 字节 hSize = 1; if (cSize) { header -= cSize; hSize += cSize; } if (t >= 0xffffff) { header -= 4; hSize += 4; } // c 为 basic header 第一个字节 *header = (0xc0 | c); if (cSize) { int tmp = packet->m_nChannel - 64; header[1] = tmp & 0xff; if (cSize == 2) header[2] = tmp >> 8; } if (t >= 0xffffff) { char* extendedTimestamp = header + 1 + cSize; AMF_EncodeInt32(extendedTimestamp, extendedTimestamp + 4, t); } } } 代码片段分析5 if (tbuf) { RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s if (tbuf)， write tbuf\", __FUNCTION__); int wrote = WriteN(r, tbuf, toff-tbuf); free(tbuf); tbuf = NULL; if (!wrote) return FALSE; } /* we invoked a remote method */ // 如果是0x14远程调用，则需要解出调用名称，加入等待响应的队列中 if (packet->m_packetType == RTMP_PACKET_TYPE_INVOKE) { AVal method; char *ptr; ptr = packet->m_body + 1; AMF_DecodeString(ptr, &method); RTMP_Log(RTMP_LOGDEBUG, \"Invoking %s\", method.av_val); /* keep it in call queue till result arrives */ if (queue) { int txn; ptr += 3 + method.av_len; txn = (int)AMF_DecodeNumber(ptr); AV_queue(&r->m_methodCalls, &r->m_numCalls, &method, txn); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 如果是0x14远程调用，则需要解出调用名称，加入等待响应的队列中\", __FUNCTION__); } } // 记录这个块流ID刚刚发送的报文，但是应忽略负载 if (!r->m_vecChannelsOut[packet->m_nChannel]) r->m_vecChannelsOut[packet->m_nChannel] = malloc(sizeof(RTMPPacket)); memcpy(r->m_vecChannelsOut[packet->m_nChannel], packet, sizeof(RTMPPacket)); return TRUE; 报文数据 DEBUG2: 0000: 03 00 00 00 00 00 55 14 00 00 00 00 ......U..... DEBUG2: 0000: 02 00 07 63 6f 6e 6e 65 63 74 00 3f f0 00 00 00 ...connect.?.... DEBUG2: 0010: 00 00 00 03 00 03 61 70 70 02 00 04 6c 69 76 65 ......app...live DEBUG2: 0020: 00 04 74 79 70 65 02 00 0a 6e 6f 6e 70 72 69 76 ..type...nonpriv DEBUG2: 0030: 61 74 65 00 05 74 63 55 72 6c 02 00 15 72 74 6d ate..tcUrl...rtm DEBUG2: 0040: 70 3a 2f 2f 31 32 32 31 2e 73 69 74 65 2f 6c 69 p://1221.site/li DEBUG2: 0050: 76 65 00 00 09 ve... 参考： librtmp源码分析之核心实现解读 rtmp源码分析之RTMP_Write 和 RTMP_SendPacket librtmp协议分析---RTMP_SendPacket函数 librtmp 源码分析笔记 RTMP_SendPacket "},"pages/librtmp/librtmp_WriteN.html":{"url":"pages/librtmp/librtmp_WriteN.html","title":"librtmp源码之WriteN","keywords":"","body":"librtmp源码之WriteN // 发送数据报的时候调用（连接，buffer，长度） static int WriteN(RTMP *r, const char *buffer, int n) { const char *ptr = buffer; #ifdef CRYPTO char *encrypted = 0; char buf[RTMP_BUFFER_CACHE_SIZE]; if (r->Link.rc4keyOut) { if (n > sizeof(buf)) encrypted = (char *)malloc(n); else encrypted = (char *)buf; ptr = encrypted; RC4_encrypt2(r->Link.rc4keyOut, n, buffer, ptr); } #endif while (n > 0) { int nBytes; // 因方式的不同而调用不同函数 // 如果使用的是HTTP协议进行连接 if (r->Link.protocol & RTMP_FEATURE_HTTP) nBytes = HTTP_Post(r, RTMPT_SEND, ptr, n); else nBytes = RTMPSockBuf_Send(&r->m_sb, ptr, n); /*RTMP_Log(RTMP_LOGDEBUG, \"%s: %d\\n\", __FUNCTION__, nBytes); */ // 成功发送字节数sb_ssl) { rc = TLS_write(sb->sb_ssl, buf, len); } else #endif { // 向一个已连接的套接口发送数据。 // int send( SOCKET s, const char * buf, int len, int flags); // s：一个用于标识已连接套接口的描述字。 // buf：包含待发送数据的缓冲区。 // len：缓冲区中数据的长度。 // flags：调用执行方式。 // rc:所发数据量。 rc = send(sb->sb_socket, buf, len, 0); } return rc; } 参考： RTMPdump（libRTMP） 源代码分析 8： 发送消息（Message） "},"pages/librtmp/librtmp_RTMP_ConnectStream.html":{"url":"pages/librtmp/librtmp_RTMP_ConnectStream.html","title":"librtmp源码之RTMP_ConnectStream","keywords":"","body":"librtmp源码之RTMP_ConnectStream // 建立流（NetStream） int RTMP_ConnectStream(RTMP *r, int seekTime) { RTMPPacket packet = { 0 }; /* seekTime was already set by SetupStream / SetupURL. * This is only needed by ReconnectStream. */ if (seekTime > 0) r->Link.seekTime = seekTime; // 当前连接媒体使用的块流ID r->m_mediaChannel = 0; // 接收到的实际上是块(Chunk)，而不是消息(Message)，因为消息在网上传输的时候要分割成块. while (!r->m_bPlaying && RTMP_IsConnected(r) && RTMP_ReadPacket(r, &packet)) { // 一个消息可能被封装成多个块(Chunk)，只有当所有块读取完才处理这个消息包 if (RTMPPacket_IsReady(&packet)) { if (!packet.m_nBodySize) continue; // 读取到flv数据包，则继续读取下一个包 if ((packet.m_packetType == RTMP_PACKET_TYPE_AUDIO) || (packet.m_packetType == RTMP_PACKET_TYPE_VIDEO) || (packet.m_packetType == RTMP_PACKET_TYPE_INFO)) { RTMP_Log(RTMP_LOGWARNING, \"Received FLV packet before play()! Ignoring.\"); RTMPPacket_Free(&packet); continue; } // 处理收到的数据包 RTMP_ClientPacket(r, &packet); // 处理完毕，清除数据 RTMPPacket_Free(&packet); } } // 当前是否推流或连接中 return r->m_bPlaying; } 简单的一个逻辑判断，重点在while循环里。首先，必须要满足三个条件。其次，进入循环以后只有出错或者建立流（NetStream）完成后，才能退出循环。 在RTMP_ConnectStream()处理交互准备的过程中，有两个重要函数：RTMP_ReadPacket()负责接收报文，RTMP_ClientPacket()负责逻辑的分派处理。 参考： RTMPdump（libRTMP） 源代码分析 6： 建立一个流媒体连接 （NetStream部分 1） RTMP推流及协议学习 librtmp源码分析之核心实现解读 【原】librtmp源码详解 "},"pages/librtmp/librtmp_RTMP_ReadPacket.html":{"url":"pages/librtmp/librtmp_RTMP_ReadPacket.html","title":"librtmp源码之RTMP_ReadPacket","keywords":"","body":"librtmp源码之RTMP_ReadPacket /** * @brief 读取接收到的消息块(Chunk)，存放在packet中. 对接收到的消息不做任何处理。 块的格式为： * * | basic header(1-3字节）| chunk msg header(0/3/7/11字节) | Extended Timestamp(0/4字节) | chunk data | * * 其中 basic header还可以分解为：| fmt(2位) | cs id (3 m_sb.sb_socket); // 收下来的数据存入hbuf if (ReadN(r, (char *)hbuf, 1) == 0) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read RTMP packet header\", __FUNCTION__); return FALSE; } // 块类型fmt packet->m_headerType = (hbuf[0] & 0xc0) >> 6; // 块流ID（2-63） packet->m_nChannel = (hbuf[0] & 0x3f); header++; // 块流ID第一个字节为0，表示块流ID占2个字节，表示ID的范围是64-319（第二个字节 + 64） if (packet->m_nChannel == 0) { // 读取接下来的1个字节存放在hbuf[1]中 if (ReadN(r, (char *)&hbuf[1], 1) != 1) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read RTMP packet header 2nd byte\", __FUNCTION__); return FALSE; } // 块流ID = 第二个字节 + 64 = hbuf[1] + 64 packet->m_nChannel = hbuf[1]; packet->m_nChannel += 64; header++; } // 块流ID第一个字节为1，表示块流ID占3个字节，表示ID范围是64 -- 65599（第三个字节*256 + 第二个字节 + 64） else if (packet->m_nChannel == 1) { int tmp; // 读取2个字节存放在hbuf[1]和hbuf[2]中 if (ReadN(r, (char *)&hbuf[1], 2) != 2) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read RTMP packet header 3nd byte\", __FUNCTION__); return FALSE; } // 块流ID = 第三个字节*256 + 第二个字节 + 64 tmp = (hbuf[2] m_nChannel = tmp + 64; RTMP_Log(RTMP_LOGDEBUG, \"%s, m_nChannel: %0x\", __FUNCTION__, packet->m_nChannel); header += 2; } // 块消息头(ChunkMsgHeader)有四种类型，大小分别为11、7、3、0,每个值加1 就得到该数组的值 // 块头 = BasicHeader(1-3字节) + ChunkMsgHeader + ExtendTimestamp(0或4字节) nSize = packetSize[packet->m_headerType]; if (packet->m_nChannel >= r->m_channelsAllocatedIn) { int n = packet->m_nChannel + 10; int *timestamp = realloc(r->m_channelTimestamp, sizeof(int) * n); RTMPPacket **packets = realloc(r->m_vecChannelsIn, sizeof(RTMPPacket*) * n); if (!timestamp) free(r->m_channelTimestamp); if (!packets) free(r->m_vecChannelsIn); r->m_channelTimestamp = timestamp; r->m_vecChannelsIn = packets; if (!timestamp || !packets) { r->m_channelsAllocatedIn = 0; return FALSE; } memset(r->m_channelTimestamp + r->m_channelsAllocatedIn, 0, sizeof(int) * (n - r->m_channelsAllocatedIn)); memset(r->m_vecChannelsIn + r->m_channelsAllocatedIn, 0, sizeof(RTMPPacket*) * (n - r->m_channelsAllocatedIn)); r->m_channelsAllocatedIn = n; } // 块类型fmt为0的块，在一个块流的开始和时间戳返回的时候必须有这种块 // 块类型fmt为1、2、3的块使用与先前块相同的数据 // 关于块类型的定义，可参考官方协议：流的分块 --- 6.1.2节 // 如果是标准大头，设置时间戳为绝对的 if (nSize == RTMP_LARGE_HEADER_SIZE) /* if we get a full header the timestamp is absolute */ // 11个字节的完整ChunkMsgHeader的TimeStamp是绝对时间戳 packet->m_hasAbsTimestamp = TRUE; // 如果非标准大头，首次尝试拷贝上一次的报头 else if (nSize m_vecChannelsIn[packet->m_nChannel]) memcpy(packet, r->m_vecChannelsIn[packet->m_nChannel], sizeof(RTMPPacket)); } // 真实的ChunkMsgHeader的大小，此处减1是因为前面获取包类型的时候多加了1 nSize--; // 读取nSize个字节存入header if (nSize > 0 && ReadN(r, header, nSize) != nSize) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read RTMP packet header. type: %x\", __FUNCTION__, (unsigned int)hbuf[0]); return FALSE; } // 目前已经读取的字节数 = chunk msg header + basic header // 计算基本块头+消息块头的大小 hSize = nSize + (header - (char *)hbuf); // chunk msg header为11、7、3字节，fmt类型值为0、1、2 if (nSize >= 3) { // TimeStamp(注意 BigEndian to SmallEndian)(11，7，3字节首部都有) // 首部前3个字节为timestamp packet->m_nTimeStamp = AMF_DecodeInt24(header); /*RTMP_Log(RTMP_LOGDEBUG, \"%s, reading RTMP packet chunk on channel %x, headersz %i, timestamp %i, abs timestamp %i\", __FUNCTION__, packet.m_nChannel, nSize, packet.m_nTimeStamp, packet.m_hasAbsTimestamp); */ // chunk msg header为11或7字节，fmt类型值为0或1 // 消息长度(11，7字节首部都有) if (nSize >= 6) { // 解析负载长度 packet->m_nBodySize = AMF_DecodeInt24(header + 3); packet->m_nBytesRead = 0; //(11，7字节首部都有) if (nSize > 6) { // Msg type ID // 解析包类型 packet->m_packetType = header[6]; // Msg Stream ID // 解析流ID if (nSize == 11) // msg stream id，小端字节序 packet->m_nInfoField2 = DecodeInt32LE(header + 7); } } } // 读取扩展时间戳并解析 // Extend Tiemstamp,占4个字节 extendedTimestamp = packet->m_nTimeStamp == 0xffffff; if (extendedTimestamp) { if (ReadN(r, header + nSize, 4) != 4) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read extended timestamp\", __FUNCTION__); return FALSE; } packet->m_nTimeStamp = AMF_DecodeInt32(header + nSize); hSize += 4; } RTMP_LogHexString(RTMP_LOGDEBUG2, (uint8_t *)hbuf, hSize); // 负载非0，需要分配内存，或第一个分块的初使化工作 if (packet->m_nBodySize > 0 && packet->m_body == NULL) { if (!RTMPPacket_Alloc(packet, packet->m_nBodySize)) { RTMP_Log(RTMP_LOGDEBUG, \"%s, failed to allocate packet\", __FUNCTION__); return FALSE; } didAlloc = TRUE; packet->m_headerType = (hbuf[0] & 0xc0) >> 6; } // 剩下的消息数据长度如果比块尺寸大，则需要分块,否则块尺寸就等于剩下的消息数据长度 // 准备读取的数据和块大小 nToRead = packet->m_nBodySize - packet->m_nBytesRead; nChunk = r->m_inChunkSize; if (nToRead m_chunk) { // 块头大小 packet->m_chunk->c_headerSize = hSize; // 填充块头数据 memcpy(packet->m_chunk->c_header, hbuf, hSize); // 块消息数据缓冲区指针 packet->m_chunk->c_chunk = packet->m_body + packet->m_nBytesRead; // 块大小 packet->m_chunk->c_chunkSize = nChunk; } // 读取负载到缓冲区中 // 读取一个块大小的数据存入块消息数据缓冲区 if (ReadN(r, packet->m_body + packet->m_nBytesRead, nChunk) != nChunk) { RTMP_Log(RTMP_LOGERROR, \"%s, failed to read RTMP packet body. len: %u\", __FUNCTION__, packet->m_nBodySize); return FALSE; } RTMP_LogHexString(RTMP_LOGDEBUG2, (uint8_t *)packet->m_body + packet->m_nBytesRead, nChunk); // 更新已读数据字节个数 packet->m_nBytesRead += nChunk; /* keep the packet as ref for other packets on this channel */ // 将这个包作为通道中其他包的参考 // 保存当前块流ID最新的报文，与RTMP_SendPacket()不同的是，负载部分也被保存了，以应对不完整的分块报文 if (!r->m_vecChannelsIn[packet->m_nChannel]) r->m_vecChannelsIn[packet->m_nChannel] = malloc(sizeof(RTMPPacket)); memcpy(r->m_vecChannelsIn[packet->m_nChannel], packet, sizeof(RTMPPacket)); // 设置扩展时间戳 if (extendedTimestamp) { r->m_vecChannelsIn[packet->m_nChannel]->m_nTimeStamp = 0xffffff; } // 若报文负载接收完整 if (RTMPPacket_IsReady(packet)) { /* make packet's timestamp absolute */ // 处理增量时间戳 // 绝对时间戳 = 上一次绝对时间戳 + 时间戳增量 if (!packet->m_hasAbsTimestamp) packet->m_nTimeStamp += r->m_channelTimestamp[packet->m_nChannel]; /* timestamps seem to be always relative!! */ // 保存当前块流ID的时间戳 // 当前绝对时间戳保存起来，供下一个包转换时间戳使用 r->m_channelTimestamp[packet->m_nChannel] = packet->m_nTimeStamp; /* reset the data from the stored packet. we keep the header since we may use it later if a new packet for this channel */ /* arrives and requests to re-use some info (small packet header) */ // 清理上下文中当前块流ID最新的报文的负载信息 // 重置保存的包。保留块头数据，因为通道中新到来的包(更短的块头)可能需要使用前面块头的信息. r->m_vecChannelsIn[packet->m_nChannel]->m_body = NULL; r->m_vecChannelsIn[packet->m_nChannel]->m_nBytesRead = 0; r->m_vecChannelsIn[packet->m_nChannel]->m_hasAbsTimestamp = FALSE; /* can only be false if we reuse header */ } else { packet->m_body = NULL; /* so it won't be erased on free */ } return TRUE; } 参考： RTMPdump（libRTMP） 源代码分析 6： 建立一个流媒体连接 （NetStream部分 1） RTMP推流及协议学习 librtmp源码分析之核心实现解读 【原】librtmp源码详解 "},"pages/librtmp/librtmp_RTMP_ClientPacket.html":{"url":"pages/librtmp/librtmp_RTMP_ClientPacket.html","title":"librtmp源码之RTMP_ClientPacket","keywords":"","body":"librtmp源码之RTMP_ClientPacket // 处理消息（Message），并做出响应 int RTMP_ClientPacket(RTMP *r, RTMPPacket *packet) { // 返回值为1表示推拉流正在正作中，为2表示已经停止 int bHasMediaPacket = 0; switch (packet->m_packetType) { // RTMP消息类型ID=1,设置块大小 case RTMP_PACKET_TYPE_CHUNK_SIZE: /* chunk size */ RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 处理消息 Set Chunk Size (typeID=1)\", __FUNCTION__); // 更新接收处理时的块限制 HandleChangeChunkSize(r, packet); break; // 对端反馈的已读大小 // RTMP消息类型ID=3 case RTMP_PACKET_TYPE_BYTES_READ_REPORT: /* bytes read report */ RTMP_Log(RTMP_LOGDEBUG, \"%s, received: bytes read report\", __FUNCTION__); break; // RTMP消息类型ID=4，用户控制 // 处理对端发送的控制报文 case RTMP_PACKET_TYPE_CONTROL: /* ctrl */ RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 处理消息 User Control (typeID=4)\", __FUNCTION__); HandleCtrl(r, packet); break; // RTMP消息类型ID=5 case RTMP_PACKET_TYPE_SERVER_BW: /* server bw */ // 处理对端发送的应答窗口大小，这里由服务器发送，即告之客户端收到对应大小的数据后应发送反馈 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 处理消息 Window Acknowledgement Size (typeID=5)\", __FUNCTION__); HandleServerBW(r, packet); break; // RTMP消息类型ID=6 case RTMP_PACKET_TYPE_CLIENT_BW: /* client bw */ // 处理对端发送的设置发送带宽大小，这里由服务器发送，即设置客户端的发送带宽 RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 处理消息 Set Peer Bandwidth (typeID=6)\", __FUNCTION__); HandleClientBW(r, packet); break; // RTMP消息类型ID=8，音频数据 case RTMP_PACKET_TYPE_AUDIO: /* audio data */ /*RTMP_Log(RTMP_LOGDEBUG, \"%s, received: audio %lu bytes\", __FUNCTION__, packet.m_nBodySize); */ HandleAudio(r, packet); bHasMediaPacket = 1; if (!r->m_mediaChannel) r->m_mediaChannel = packet->m_nChannel; if (!r->m_pausing) r->m_mediaStamp = packet->m_nTimeStamp; break; // RTMP消息类型ID=9，视频数据 case RTMP_PACKET_TYPE_VIDEO: /* video data */ /*RTMP_Log(RTMP_LOGDEBUG, \"%s, received: video %lu bytes\", __FUNCTION__, packet.m_nBodySize); */ HandleVideo(r, packet); bHasMediaPacket = 1; if (!r->m_mediaChannel) r->m_mediaChannel = packet->m_nChannel; if (!r->m_pausing) r->m_mediaStamp = packet->m_nTimeStamp; break; // RTMP消息类型ID=15，AMF3编码，忽略 case RTMP_PACKET_TYPE_FLEX_STREAM_SEND: /* flex stream send */ RTMP_Log(RTMP_LOGDEBUG, \"%s, flex stream send, size %u bytes, not supported, ignoring\", __FUNCTION__, packet->m_nBodySize); break; // RTMP消息类型ID=16，AMF3编码，忽略 case RTMP_PACKET_TYPE_FLEX_SHARED_OBJECT: /* flex shared object */ RTMP_Log(RTMP_LOGDEBUG, \"%s, flex shared object, size %u bytes, not supported, ignoring\", __FUNCTION__, packet->m_nBodySize); break; // RTMP消息类型ID=17，AMF3编码，忽略 case RTMP_PACKET_TYPE_FLEX_MESSAGE: /* flex message */ { RTMP_Log(RTMP_LOGDEBUG, \"%s, flex message, size %u bytes, not fully supported\", __FUNCTION__, packet->m_nBodySize); /*RTMP_LogHex(packet.m_body, packet.m_nBodySize); */ /* some DEBUG code */ #if 0 RTMP_LIB_AMFObject obj; int nRes = obj.Decode(packet.m_body+1, packet.m_nBodySize-1); if(nRes m_body + 1, packet->m_nBodySize - 1) == 1) bHasMediaPacket = 2; break; } // RTMP消息类型ID=18，AMF0编码，数据消息 case RTMP_PACKET_TYPE_INFO: /* metadata (notify) */ RTMP_Log(RTMP_LOGDEBUG, \"%s, received: notify %u bytes\", __FUNCTION__, packet->m_nBodySize); if (HandleMetadata(r, packet->m_body, packet->m_nBodySize)) bHasMediaPacket = 1; break; // RTMP消息类型ID=19，AMF0编码，忽略 case RTMP_PACKET_TYPE_SHARED_OBJECT: RTMP_Log(RTMP_LOGDEBUG, \"%s, shared object, not supported, ignoring\", __FUNCTION__); break; // RTMP消息类型ID=20，AMF0编码，命令消息 // 处理命令消息！ case RTMP_PACKET_TYPE_INVOKE: /* invoke */ RTMP_Log(RTMP_LOGDEBUG, \"%s, received: invoke %u bytes\", __FUNCTION__, packet->m_nBodySize); RTMP_Log(RTMP_LOGDEBUG, \"CCQ: %s 处理消息 (typeID=20，AMF0编码)\", __FUNCTION__); /*RTMP_LogHex(packet.m_body, packet.m_nBodySize); */ if (HandleInvoke(r, packet->m_body, packet->m_nBodySize) == 1) bHasMediaPacket = 2; break; // RTMP消息类型ID=22 case RTMP_PACKET_TYPE_FLASH_VIDEO: { /* go through FLV packets and handle metadata packets */ unsigned int pos = 0; uint32_t nTimeStamp = packet->m_nTimeStamp; while (pos + 11 m_nBodySize) { uint32_t dataSize = AMF_DecodeInt24(packet->m_body + pos + 1); /* size without header (11) and prevTagSize (4) */ if (pos + 11 + dataSize + 4 > packet->m_nBodySize) { RTMP_Log(RTMP_LOGWARNING, \"Stream corrupt?!\"); break; } if (packet->m_body[pos] == 0x12) { HandleMetadata(r, packet->m_body + pos + 11, dataSize); } else if (packet->m_body[pos] == 8 || packet->m_body[pos] == 9) { nTimeStamp = AMF_DecodeInt24(packet->m_body + pos + 4); nTimeStamp |= (packet->m_body[pos + 7] m_pausing) r->m_mediaStamp = nTimeStamp; /* FLV tag(s) */ /*RTMP_Log(RTMP_LOGDEBUG, \"%s, received: FLV tag(s) %lu bytes\", __FUNCTION__, packet.m_nBodySize); */ bHasMediaPacket = 1; break; } default: RTMP_Log(RTMP_LOGDEBUG, \"%s, unknown packet type received: 0x%02x\", __FUNCTION__, packet->m_packetType); #ifdef _DEBUG RTMP_LogHex(RTMP_LOGDEBUG, packet->m_body, packet->m_nBodySize); #endif } // 返回值为1表示推拉流正在正作中，为2表示已经停止 return bHasMediaPacket; } 参考： RTMPdump（libRTMP） 源代码分析 7： 建立一个流媒体连接 （NetStream部分 2） librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp_ReadN.html":{"url":"pages/librtmp/librtmp_ReadN.html","title":"librtmp源码之ReadN","keywords":"","body":"librtmp源码之ReadN // 从HTTP或SOCKET中读取n个数据存放在buffer中. static int ReadN(RTMP *r, char *buffer, int n) { int nOriginalSize = n; int avail; char *ptr; r->m_sb.sb_timedout = FALSE; #ifdef _DEBUG memset(buffer, 0, n); #endif ptr = buffer; while (n > 0) { int nBytes = 0, nRead; if (r->Link.protocol & RTMP_FEATURE_HTTP) { int refill = 0; while (!r->m_resplen) { int ret; if (r->m_sb.sb_size m_unackd) HTTP_Post(r, RTMPT_IDLE, \"\", 1); if (RTMPSockBuf_Fill(&r->m_sb) m_sb.sb_timedout) RTMP_Close(r); return 0; } } if ((ret = HTTP_read(r, 0)) == -1) { RTMP_Log(RTMP_LOGDEBUG, \"%s, No valid HTTP response found\", __FUNCTION__); RTMP_Close(r); return 0; } else if (ret == -2) { refill = 1; } else { refill = 0; } } if (r->m_resplen && !r->m_sb.sb_size) RTMPSockBuf_Fill(&r->m_sb); avail = r->m_sb.sb_size; if (avail > r->m_resplen) avail = r->m_resplen; } else { avail = r->m_sb.sb_size; if (avail == 0) { if (RTMPSockBuf_Fill(&r->m_sb) m_sb.sb_timedout) RTMP_Close(r); return 0; } avail = r->m_sb.sb_size; } } nRead = ((n 0) { memcpy(ptr, r->m_sb.sb_start, nRead); r->m_sb.sb_start += nRead; r->m_sb.sb_size -= nRead; nBytes = nRead; r->m_nBytesIn += nRead; if (r->m_bSendCounter && r->m_nBytesIn > ( r->m_nBytesInSent + r->m_nClientBW / 10)) if (!SendBytesReceived(r)) return FALSE; } /*RTMP_Log(RTMP_LOGDEBUG, \"%s: %d bytes\\n\", __FUNCTION__, nBytes); */ #ifdef _DEBUG fwrite(ptr, 1, nBytes, netstackdump_read); #endif if (nBytes == 0) { RTMP_Log(RTMP_LOGDEBUG, \"%s, RTMP socket closed by peer\", __FUNCTION__); /*goto again; */ RTMP_Close(r); break; } if (r->Link.protocol & RTMP_FEATURE_HTTP) r->m_resplen -= nBytes; #ifdef CRYPTO if (r->Link.rc4keyIn) { RC4_encrypt(r->Link.rc4keyIn, nBytes, ptr); } #endif n -= nBytes; ptr += nBytes; } return nOriginalSize - n; } // 调用Socket编程中的recv()函数，接收数据 int RTMPSockBuf_Fill(RTMPSockBuf *sb) { int nBytes; if (!sb->sb_size) sb->sb_start = sb->sb_buf; while (1) { // 缓冲区长度：总长-未处理字节-已处理字节 // |-----已处理--------|-----未处理--------|---------缓冲区----------| // sb_buf sb_start sb_size nBytes = sizeof(sb->sb_buf) - 1 - sb->sb_size - (sb->sb_start - sb->sb_buf); #if defined(CRYPTO) && !defined(NO_SSL) if (sb->sb_ssl) { nBytes = TLS_read(sb->sb_ssl, sb->sb_start + sb->sb_size, nBytes); } else #endif { // int recv( SOCKET s, char * buf, int len, int flags); // s ：一个标识已连接套接口的描述字。 // buf ：用于接收数据的缓冲区。 // len ：缓冲区长度。 // flags：指定调用方式。 // 从sb_start（待处理的下一字节） + sb_size（）还未处理的字节开始buffer为空，可以存储 nBytes = recv(sb->sb_socket, sb->sb_start + sb->sb_size, nBytes, 0); } if (nBytes != -1) { // 未处理的字节又多了 sb->sb_size += nBytes; } else { int sockerr = GetSockError(); RTMP_Log(RTMP_LOGDEBUG, \"%s, recv returned %d. GetSockError(): %d (%s)\", __FUNCTION__, nBytes, sockerr, strerror(sockerr)); if (sockerr == EINTR && !RTMP_ctrlC) continue; if (sockerr == EWOULDBLOCK || sockerr == EAGAIN) { sb->sb_timedout = TRUE; nBytes = 0; } } break; } return nBytes; } 参考： RTMP推流及协议学习 RTMPdump（libRTMP） 源代码分析 9： 接收消息（Message）（接收视音频数据） "},"pages/librtmp/librtmp_HandleInvoke.html":{"url":"pages/librtmp/librtmp_HandleInvoke.html","title":"librtmp源码之HandleInvoke","keywords":"","body":"librtmp源码之HandleInvoke // 处理服务器发来的AMF0编码的命令 /* Returns 0 for OK/Failed/error, 1 for 'Stop or Complete' */ static int HandleInvoke(RTMP *r, const char *body, unsigned int nBodySize){ AMFObject obj; AVal method; double txn; int ret = 0, nRes; // 确保响应报文是0x14的命令字 // 0x02:string if (body[0] != 0x02) /* make sure it is a string method name we start with */ { RTMP_Log(RTMP_LOGWARNING, \"%s, Sanity failed. no string method in invoke packet\", __FUNCTION__); return 0; } // 将各参数以无名称的对象属性方式进行解析 nRes = AMF_Decode(&obj, body, nBodySize, FALSE); if (nRes \", __FUNCTION__, method.av_val); // 接收到服务端返回的一个_result包，所以我们需要找到这个包对应的那条命令，从而处理这条命令的对应事件。 // 比如我们之前发送了个connect给服务端，服务端必然会返回_result，然后我们异步收到result后，会调用 // RTMP_SendServerBW,RTMP_SendCtrl,以及RTMP_SendCreateStream来创建一个stream // 过程名称为_result if (AVMATCH(&method, &av__result)) { AVal methodInvoked = {0}; int i; // 删除请求队列中的流水项 for (i=0; im_numCalls; i++) { // 找到这条指令对应的触发的方法 if (r->m_methodCalls[i].num == (int)txn) { methodInvoked = r->m_methodCalls[i].name; AV_erase(r->m_methodCalls, &r->m_numCalls, i, FALSE); break; } } if (!methodInvoked.av_val) { RTMP_Log(RTMP_LOGDEBUG, \"%s, received result id %f without matching request\", __FUNCTION__, txn); goto leave; } RTMP_Log(RTMP_LOGDEBUG, \"%s, received result for method call \", __FUNCTION__, methodInvoked.av_val); // 找到了连接请求，确认是连接响应 if (AVMATCH(&methodInvoked, &av_connect)) { if (r->Link.token.av_len) { AMFObjectProperty p; if (RTMP_FindFirstMatchingProperty(&obj, &av_secureToken, &p)) { DecodeTEA(&r->Link.token, &p.p_vu.p_aval); SendSecureTokenResponse(r, &p.p_vu.p_aval); } } // 客户端推流 if (r->Link.protocol & RTMP_FEATURE_WRITE) { // 通知服务器释放流通道和清理推流资源 SendReleaseStream(r); SendFCPublish(r); } // 客户端拉流 else { // 设置服务器的应答窗口大小 // 告诉服务端，我们的期望是什么，窗口大小，等 RTMP_SendServerBW(r); RTMP_SendCtrl(r, 3, 0, 300); } // 发送创建流通道请求 // 因为服务端同意了我们的connect，所以这里发送createStream创建一个流 // 创建完成后，会再次进如这个函数从而走到下面的av_createStream分支，从而发送play过去 RTMP_SendCreateStream(r); if (!(r->Link.protocol & RTMP_FEATURE_WRITE)) { /* Authenticate on Justin.tv legacy servers before sending FCSubscribe */ if (r->Link.usherToken.av_len) SendUsherToken(r, &r->Link.usherToken); /* Send the FCSubscribe if live stream or if subscribepath is set */ if (r->Link.subscribepath.av_len) SendFCSubscribe(r, &r->Link.subscribepath); else if (r->Link.lFlags & RTMP_LF_LIVE) SendFCSubscribe(r, &r->Link.playpath); } } // 找到了创建流请求，确认是创建流的响应 else if (AVMATCH(&methodInvoked, &av_createStream)) { // 从响应中取流ID r->m_stream_id = (int)AMFProp_GetNumber(AMF_GetProp(&obj, NULL, 3)); // 客户端推流 if (r->Link.protocol & RTMP_FEATURE_WRITE) { // 发送推流点 // 如果是要发送，那么高尚服务端，我们要发数据 SendPublish(r); } // 客户端拉流 else { // 否则告诉他我们要接受数据 if (r->Link.lFlags & RTMP_LF_PLST) SendPlaylist(r); // 发送play过去 // 发送拉流点 SendPlay(r); // 以及我们的buf大小 // 发送拉流缓冲时长 RTMP_SendCtrl(r, 3, r->m_stream_id, r->m_nBufferMS); } } // 找到了推流和拉流请求，确认是它们的响应 else if (AVMATCH(&methodInvoked, &av_play) || AVMATCH(&methodInvoked, &av_publish)) { // 接收到了play的回复，那么标记为play // 标识已经进入流状态 r->m_bPlaying = TRUE; } free(methodInvoked.av_val); } else if (AVMATCH(&method, &av_onBWDone)) { if (!r->m_nBWCheckCounter) SendCheckBW(r); } else if (AVMATCH(&method, &av_onFCSubscribe)) { /* SendOnFCSubscribe(); */ } else if (AVMATCH(&method, &av_onFCUnsubscribe)) { RTMP_Close(r); ret = 1; } // 过程名称为ping else if (AVMATCH(&method, &av_ping)) { // 发送pong响应 SendPong(r, txn); } else if (AVMATCH(&method, &av__onbwcheck)) { SendCheckBWResult(r, txn); } else if (AVMATCH(&method, &av__onbwdone)) { int i; for (i = 0; i m_numCalls; i++) if (AVMATCH(&r->m_methodCalls[i].name, &av__checkbw)) { AV_erase(r->m_methodCalls, &r->m_numCalls, i, TRUE); break; } } // 过程名称为_error else if (AVMATCH(&method, &av__error)) { #ifdef CRYPTO AVal methodInvoked = {0}; int i; if (r->Link.protocol & RTMP_FEATURE_WRITE) { for (i=0; im_numCalls; i++) { if (r->m_methodCalls[i].num == txn) { methodInvoked = r->m_methodCalls[i].name; AV_erase(r->m_methodCalls, &r->m_numCalls, i, FALSE); break; } } if (!methodInvoked.av_val) { RTMP_Log(RTMP_LOGDEBUG, \"%s, received result id %f without matching request\", __FUNCTION__, txn); goto leave; } RTMP_Log(RTMP_LOGDEBUG, \"%s, received error for method call \", __FUNCTION__, methodInvoked.av_val); if (AVMATCH(&methodInvoked, &av_connect)) { AMFObject obj2; AVal code, level, description; AMFProp_GetObject(AMF_GetProp(&obj, NULL, 3), &obj2); AMFProp_GetString(AMF_GetProp(&obj2, &av_code, -1), &code); AMFProp_GetString(AMF_GetProp(&obj2, &av_level, -1), &level); AMFProp_GetString(AMF_GetProp(&obj2, &av_description, -1), &description); RTMP_Log(RTMP_LOGDEBUG, \"%s, error description: %s\", __FUNCTION__, description.av_val); /* if PublisherAuth returns 1, then reconnect */ if (PublisherAuth(r, &description) == 1) { CloseInternal(r, 1); if (!RTMP_Connect(r, NULL) || !RTMP_ConnectStream(r, 0)) goto leave; } } } else { RTMP_Log(RTMP_LOGERROR, \"rtmp server sent error\"); } free(methodInvoked.av_val); #else RTMP_Log(RTMP_LOGERROR, \"rtmp server sent error\"); #endif } // 过程名称为close else if (AVMATCH(&method, &av_close)) { RTMP_Log(RTMP_LOGERROR, \"rtmp server requested close\"); RTMP_Close(r); } // 过程名称为onStatus else if (AVMATCH(&method, &av_onStatus)) { // 获取返回对象及其主要属性 AMFObject obj2; AVal code, level; AMFProp_GetObject(AMF_GetProp(&obj, NULL, 3), &obj2); AMFProp_GetString(AMF_GetProp(&obj2, &av_code, -1), &code); AMFProp_GetString(AMF_GetProp(&obj2, &av_level, -1), &level); RTMP_Log(RTMP_LOGDEBUG, \"%s, onStatus: %s\", __FUNCTION__, code.av_val); // 出错返回 if (AVMATCH(&code, &av_NetStream_Failed) || AVMATCH(&code, &av_NetStream_Play_Failed) || AVMATCH(&code, &av_NetStream_Play_StreamNotFound) || AVMATCH(&code, &av_NetConnection_Connect_InvalidApp)) { r->m_stream_id = -1; RTMP_Close(r); RTMP_Log(RTMP_LOGERROR, \"Closing connection: %s\", code.av_val); } // 启动拉流成功 else if (AVMATCH(&code, &av_NetStream_Play_Start) || AVMATCH(&code, &av_NetStream_Play_PublishNotify)) { int i; r->m_bPlaying = TRUE; for (i = 0; i m_numCalls; i++) { if (AVMATCH(&r->m_methodCalls[i].name, &av_play)) { AV_erase(r->m_methodCalls, &r->m_numCalls, i, TRUE); break; } } } // 启动推流成功 else if (AVMATCH(&code, &av_NetStream_Publish_Start)) { int i; r->m_bPlaying = TRUE; for (i = 0; i m_numCalls; i++) { if (AVMATCH(&r->m_methodCalls[i].name, &av_publish)) { AV_erase(r->m_methodCalls, &r->m_numCalls, i, TRUE); break; } } } // 通知流完成或结束 /* Return 1 if this is a Play.Complete or Play.Stop */ else if (AVMATCH(&code, &av_NetStream_Play_Complete) || AVMATCH(&code, &av_NetStream_Play_Stop) || AVMATCH(&code, &av_NetStream_Play_UnpublishNotify)) { RTMP_Close(r); ret = 1; } else if (AVMATCH(&code, &av_NetStream_Seek_Notify)) { r->m_read.flags &= ~RTMP_READ_SEEKING; } // 通知流暂停 else if (AVMATCH(&code, &av_NetStream_Pause_Notify)) { if (r->m_pausing == 1 || r->m_pausing == 2) { RTMP_SendPause(r, FALSE, r->m_pauseStamp); r->m_pausing = 3; } } } else if (AVMATCH(&method, &av_playlist_ready)) { int i; for (i = 0; i m_numCalls; i++) { if (AVMATCH(&r->m_methodCalls[i].name, &av_set_playlist)) { AV_erase(r->m_methodCalls, &r->m_numCalls, i, TRUE); break; } } } else { } leave: AMF_Reset(&obj); return ret; } 参考： librtmp实时消息传输协议(RTMP)库代码浅析 librtmp源码分析之核心实现解读 RTMPdump（libRTMP） 源代码分析 7： 建立一个流媒体连接 （NetStream部分 2） 手撕Rtmp协议细节（3）——Rtmp Body "},"pages/librtmp/librtmp_SendReleaseStream.html":{"url":"pages/librtmp/librtmp_SendReleaseStream.html","title":"librtmp源码之SendReleaseStream","keywords":"","body":"librtmp源码之SendReleaseStream // 发送RealeaseStream命令 // Real Time Messaging Protocol (AMF0 Command releaseStream()) // RTMP Header // 01.. .... = Format: 1 // ..00 0011 = Chunk Stream ID: 3 // Timestamp delta: 0 // Timestamp: 0 (calculated) // Body size: 39 // Type ID: AMF0 Command (0x14) // RTMP Body // String 'releaseStream' // Number 2 // Null // String 'livestream' static int SendReleaseStream(RTMP *r) { RTMPPacket packet; char pbuf[1024], *pend = pbuf + sizeof(pbuf); char *enc; // 块流ID，`..00 0011 = Chunk Stream ID: 3`中的11，第1个字节的低两位 packet.m_nChannel = 0x03; /* control channel (invoke) */ // 块类型ID，`01.. .... = Format: 1`中的01，第1个字节的高两位 packet.m_headerType = RTMP_PACKET_SIZE_MEDIUM; // 命令类型ID，`Type ID: AMF0 Command (0x14)`，第8个字节 packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; // 时间戳第2、3、4字节 packet.m_nTimeStamp = 0; // 消息流ID，这里块类型ID为1，只有8个字节，这里是没有消息流ID的 packet.m_nInfoField2 = 0; // 是否是绝对时间戳(类型1时为true) packet.m_hasAbsTimestamp = 0; // 块body指针，前面18（RTMP_MAX_HEADER_SIZE）字节用来放块头，后面放body packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; // 设置body开始位置 enc = packet.m_body; // 对“releaseStream”字符串进行AMF编码 enc = AMF_EncodeString(enc, pend, &av_releaseStream); // 对传输ID（0）进行AMF编码，0x14命令远程过程调用计数 enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); // 编码Null *enc++ = AMF_NULL; // 对播放路径字符串进行AMF编码，livestream enc = AMF_EncodeString(enc, pend, &r->Link.playpath); if (!enc) return FALSE; // 块body大小，第5、6、7个字节 packet.m_nBodySize = enc - packet.m_body; return RTMP_SendPacket(r, &packet, FALSE); } 参考： librtmp源码分析之核心实现解读 RTMPdump（libRTMP） 源代码分析 8： 发送消息（Message）） 手撕rtmp协议细节（2）——rtmp Header 手撕Rtmp协议细节（3）——Rtmp Body "},"pages/librtmp/librtmp_SendFCPublish.html":{"url":"pages/librtmp/librtmp_SendFCPublish.html","title":"librtmp源码之SendFCPublish","keywords":"","body":"librtmp源码之SendFCPublish // 发送准备推流点请求 // Real Time Messaging Protocol (AMF0 Command FCPublish()) // RTMP Header // 01.. .... = Format: 1 // ..00 0011 = Chunk Stream ID: 3 // Timestamp delta: 0 // Timestamp: 0 (calculated) // Body size: 35 // Type ID: AMF0 Command (0x14) // RTMP Body // String 'FCPublish' // AMF0 type: String (0x02) // String length: 9 // String: FCPublish // Number 3 // AMF0 type: Number (0x00) // Number: 3 // Null // AMF0 type: Null (0x05) // String 'livestream' // AMF0 type: String (0x02) // String length: 10 // String: livestream static int SendFCPublish(RTMP *r) { RTMPPacket packet; char pbuf[1024], *pend = pbuf + sizeof(pbuf); char *enc; packet.m_nChannel = 0x03; /* control channel (invoke) */ packet.m_headerType = RTMP_PACKET_SIZE_MEDIUM; packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; packet.m_nTimeStamp = 0; packet.m_nInfoField2 = 0; packet.m_hasAbsTimestamp = 0; packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; enc = packet.m_body; // 对“FCPublish”字符串进行AMF编码 enc = AMF_EncodeString(enc, pend, &av_FCPublish); enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); *enc++ = AMF_NULL; enc = AMF_EncodeString(enc, pend, &r->Link.playpath); if (!enc) return FALSE; packet.m_nBodySize = enc - packet.m_body; return RTMP_SendPacket(r, &packet, FALSE); } 和SendReleaseStream函数类似，区别在与body的第一个字段是FCPublish。 参考： librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp_RTMP_SendServerBW.html":{"url":"pages/librtmp/librtmp_RTMP_SendServerBW.html","title":"librtmp源码之RTMP_SendServerBW","keywords":"","body":"librtmp源码之RTMP_SendServerBW // 设置缓冲大小 int RTMP_SendServerBW(RTMP *r) { RTMPPacket packet; char pbuf[256], *pend = pbuf + sizeof(pbuf); // Chunk Stream ID用来表示消息的级别 // 2:low level // 块流ID，`..00 0010 = Chunk Stream ID: 2`中的11，第1个字节的低两位 packet.m_nChannel = 0x02; /* control channel (invoke) */ // Format为0的时候，RTMP Header的长度为12 // 块类型ID，`01.. .... = Format: 1`中的01，第1个字节的高两位 packet.m_headerType = RTMP_PACKET_SIZE_LARGE; // #define RTMP_PACKET_TYPE_SERVER_BW 0x05 // 命令类型ID，`Type ID: AMF0 Command (0x14)`，第8个字节 packet.m_packetType = RTMP_PACKET_TYPE_SERVER_BW; // 时间戳第2、3、4字节 packet.m_nTimeStamp = 0; // 消息流ID，这里块类型ID为1，有12个字节，这里是没有消息流ID的 packet.m_nInfoField2 = 0; // 是否是绝对时间戳(类型1时为true) packet.m_hasAbsTimestamp = 0; // 块body指针，前面18（RTMP_MAX_HEADER_SIZE）字节用来放块头，后面放body packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; // 块body大小，第5、6、7个字节 packet.m_nBodySize = 4; // 压入4字节带宽 AMF_EncodeInt32(packet.m_body, pend, r->m_nServerBW); return RTMP_SendPacket(r, &packet, FALSE); } 参考： librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp_RTMP_SendCtrl.html":{"url":"pages/librtmp/librtmp_RTMP_SendCtrl.html","title":"librtmp源码之RTMP_SendCtrl","keywords":"","body":"librtmp源码之RTMP_SendCtrl // 发送用戶控制消息 //Ping 是 RTMP 中最神秘的消息，直到现在我们还没有完全解释它。总之，Ping 消息用作客户端和服务器之间交换的特殊命令。此页面旨在记录所有已知的 Ping 消息。预计名单会增长。 // //Ping 数据包的类型为 0x4，包含两个强制参数和两个可选参数。第一个参数是 Ping 的类型，简称整数。第二个参数是 ping 的目标。由于 Ping 始终在 Channel 2（控制通道）中发送，并且 RTMP 标头中的目标对象始终为 0，这意味着 Connection 对象，因此有必要放置一个额外的参数来指示 Ping 发送到的确切目标对象。第二个参数承担这个责任。该值与 RTMP 头中的目标对象字段含义相同。 （第二个值也可以做其他用途，比如RTT Ping/Pong。它用作时间戳。）第三个和第四个参数是可选的，可以看作是Ping包的参数。以下是 Ping 消息的无穷无尽的列表。 // // * 类型 0：清除流。没有第三个和第四个参数。第二个参数可以是 0。建立连接后，服务器会向客户端发送 Ping 0,0。该消息还将在 Play 开始时发送给客户端，并响应 Seek 或 Pause/Resume 请求。此 Ping 告诉客户端使用下一个数据包服务器发送的时间戳重新校准时钟。 // * 类型 1：告诉流清除播放缓冲区。 // * 类型 3：客户端的缓冲时间。第三个参数是以毫秒为单位的缓冲时间。 // * 类型 4：重置流。在 VOD 的情况下与类型 0 一起使用。通常在类型 0 之前发送。 // * 类型 6：从服务器 Ping 客户端。第二个参数是当前时间。 // * 类型 7：来自客户的 Pong 回复。第二个参数是服务器发送他的 ping 请求的时间。 // * 类型 26：SWFVerification 请求 // * 类型 27：SWFVerification 响应 int RTMP_SendCtrl(RTMP *r, short nType, unsigned int nObject, unsigned int nTime) { RTMPPacket packet; char pbuf[256], *pend = pbuf + sizeof(pbuf); int nSize; char *buf; RTMP_Log(RTMP_LOGDEBUG, \"sending ctrl. type: 0x%04x\", (unsigned short)nType); packet.m_nChannel = 0x02; /* control channel (ping) */ packet.m_headerType = RTMP_PACKET_SIZE_MEDIUM; // #define RTMP_PACKET_TYPE_CONTROL 0x04 packet.m_packetType = RTMP_PACKET_TYPE_CONTROL; packet.m_nTimeStamp = 0; /* RTMP_GetTime(); */ packet.m_nInfoField2 = 0; packet.m_hasAbsTimestamp = 0; packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; switch(nType) { case 0x03: nSize = 10; break; /* buffer time */ case 0x1A: nSize = 3; break; /* SWF verify request */ case 0x1B: nSize = 44; break; /* SWF verify response */ default: nSize = 6; break; } packet.m_nBodySize = nSize; buf = packet.m_body; buf = AMF_EncodeInt16(buf, pend, nType); if (nType == 0x1B) { #ifdef CRYPTO memcpy(buf, r->Link.SWFVerificationResponse, 42); RTMP_Log(RTMP_LOGDEBUG, \"Sending SWFVerification response: \"); RTMP_LogHex(RTMP_LOGDEBUG, (uint8_t *)packet.m_body, packet.m_nBodySize); #endif } else if (nType == 0x1A) { *buf = nObject & 0xff; } else { if (nSize > 2) buf = AMF_EncodeInt32(buf, pend, nObject); if (nSize > 6) buf = AMF_EncodeInt32(buf, pend, nTime); } return RTMP_SendPacket(r, &packet, FALSE); } 参考： librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp_RTMP_SendCreateStream.html":{"url":"pages/librtmp/librtmp_RTMP_SendCreateStream.html","title":"librtmp源码之RTMP_SendCreateStream","keywords":"","body":"librtmp源码之RTMP_SendCreateStream // 发送创建流请求 int RTMP_SendCreateStream(RTMP *r) { RTMPPacket packet; char pbuf[256], *pend = pbuf + sizeof(pbuf); char *enc; packet.m_nChannel = 0x03; /* control channel (invoke) */ packet.m_headerType = RTMP_PACKET_SIZE_MEDIUM; packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; packet.m_nTimeStamp = 0; packet.m_nInfoField2 = 0; packet.m_hasAbsTimestamp = 0; packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; enc = packet.m_body; enc = AMF_EncodeString(enc, pend, &av_createStream); enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); *enc++ = AMF_NULL; /* NULL */ packet.m_nBodySize = enc - packet.m_body; return RTMP_SendPacket(r, &packet, TRUE); } 内容和SendReleaseStream函数差不多，只不过命令改用了av_createStream。 参考： librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp_SendPublish.html":{"url":"pages/librtmp/librtmp_SendPublish.html","title":"librtmp源码之SendPublish","keywords":"","body":"librtmp源码之SendPublish // 发送Publish命令 static int SendPublish(RTMP *r) { RTMPPacket packet; char pbuf[1024], *pend = pbuf + sizeof(pbuf); char *enc; // 块流ID，04: control stream packet.m_nChannel = 0x04; /* source channel (invoke) */ packet.m_headerType = RTMP_PACKET_SIZE_LARGE; packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; packet.m_nTimeStamp = 0; // 消息流ID，这个时候就有了流ID，因为这个方法是在收到服务器返回创建流的时候调用 packet.m_nInfoField2 = r->m_stream_id; packet.m_hasAbsTimestamp = 0; packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; enc = packet.m_body; // 压入publish enc = AMF_EncodeString(enc, pend, &av_publish); enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); *enc++ = AMF_NULL; enc = AMF_EncodeString(enc, pend, &r->Link.playpath); if (!enc) return FALSE; /* FIXME: should we choose live based on Link.lFlags & RTMP_LF_LIVE? */ enc = AMF_EncodeString(enc, pend, &av_live); if (!enc) return FALSE; packet.m_nBodySize = enc - packet.m_body; return RTMP_SendPacket(r, &packet, TRUE); } 参考： RTMPdump（libRTMP） 源代码分析 8： 发送消息（Message） librtmp源码分析之核心实现解读 "},"pages/librtmp/librtmp_SendPlay.html":{"url":"pages/librtmp/librtmp_SendPlay.html","title":"librtmp源码之SendPlay","keywords":"","body":"librtmp源码之SendPlay // 发送命令“播放” static int SendPlay(RTMP *r) { RTMPPacket packet; char pbuf[1024], *pend = pbuf + sizeof(pbuf); char *enc; // 8:control stream packet.m_nChannel = 0x08; /* we make 8 our stream channel */ packet.m_headerType = RTMP_PACKET_SIZE_LARGE; packet.m_packetType = RTMP_PACKET_TYPE_INVOKE; packet.m_nTimeStamp = 0; // 指定流ID packet.m_nInfoField2 = r->m_stream_id; /*0x01000000; */ packet.m_hasAbsTimestamp = 0; packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE; enc = packet.m_body; enc = AMF_EncodeString(enc, pend, &av_play); enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes); *enc++ = AMF_NULL; RTMP_Log(RTMP_LOGDEBUG, \"%s, seekTime=%d, stopTime=%d, sending play: %s\", __FUNCTION__, r->Link.seekTime, r->Link.stopTime, r->Link.playpath.av_val); enc = AMF_EncodeString(enc, pend, &r->Link.playpath); if (!enc) return FALSE; // 指定开始时间 /* Optional parameters start and len. * * start: -2, -1, 0, positive number * -2: looks for a live stream, then a recorded stream, * if not found any open a live stream * -1: plays a live stream * >=0: plays a recorded streams from 'start' milliseconds */ // #define RTMP_LF_LIVE 0x0002 /* stream is live */ if (r->Link.lFlags & RTMP_LF_LIVE) enc = AMF_EncodeNumber(enc, pend, -1000.0); else { if (r->Link.seekTime > 0.0) enc = AMF_EncodeNumber(enc, pend, r->Link.seekTime); /* resume from here */ else enc = AMF_EncodeNumber(enc, pend, 0.0); /*-2000.0);*/ /* recorded as default, -2000.0 is not reliable since that freezes the player if the stream is not found */ } if (!enc) return FALSE; // 指点播放时长 /* len: -1, 0, positive number * -1: plays live or recorded stream to the end (default) * 0: plays a frame 'start' ms away from the beginning * >0: plays a live or recoded stream for 'len' milliseconds */ /*enc += EncodeNumber(enc, -1.0); */ /* len */ if (r->Link.stopTime) { enc = AMF_EncodeNumber(enc, pend, r->Link.stopTime - r->Link.seekTime); if (!enc) return FALSE; } packet.m_nBodySize = enc - packet.m_body; return RTMP_SendPacket(r, &packet, TRUE); } 参考： RTMP学习（十一）rtmpdump源码阅读（6）请求播放 librtmp源码分析之核心实现解读 "},"pages/ffmpeg/Related_links.html":{"url":"pages/ffmpeg/Related_links.html","title":"相关链接","keywords":"","body":"相关链接 FFmpeg官网 程序员秘密:”ffmpeg编译ios“ 的搜索结果 1.编译FFmpeg库 FFmpeg使用遇到问题记录 实战FFmpeg－－编译iOS平台使用的FFmpeg库（支持arm64的FFmpeg2.6.2） gas-preprocessor.pl build-ffmpeg.sh FFMpeg下载编译iOS库 iOS audio and video - compiled FFmpeg 八.FFmepg--编译静态库(iOS) FFMpeg cross compiler library Android-SO ffmpeg从入门到实战 Android权限 java.io.IOException：对createNewFile（）的权限被拒绝 请求应用权限 视频解码 一文读懂 Android FFmpeg 视频解码过程与实战分析 视频编码 FFmpeg视频编码 YUV420P编码H264 Audio and video development (c) - encoded video Audio and video development-audio and video learning materials ffmpeg入门教程之YUV编码成h264 博客 音视频学习 雷霄骅 雷神的《基于 FFmpeg + SDL 的视频播放器的制作》课程的视频 断点实验室 如何写一个不到 1000 行的视频播放器 落影loyinglin 不朽的传奇 HBStream 12年专注直播点播、网络视频技术 wcf 课程 【免费】FFmpeg/WebRTC/RTMP/NDK/Android音视频流媒体高级开发 ffmpeg入門教程 李超 SDL·lazyfoo tongtong 视频下载 ultravideo yuv视频处理 media adb mac下安装adb环境的三种方式 adb命令读取Android手机内存卡文件 adb基本命令操作(四) 推拉流 Nginx学习之配置RTMP模块搭建推流服务 FFmpeg问题 《FFmpeg精讲与实战》常见问题与解答 音频编码 ffmpeg导入x264、libfdk_aac外部库 播放器 vlc 一个小玩具：NDK编译SDL的例子 交叉编译sdl2成android的.so库 利用ffmpeg和SDL实现一个跨android版本的音视频播放器 SDL播放YUV——循环 SDL播放yuv 使用SDL显示YUV ffmpeg实战教程（二）用SDL播放YUV，并结合ffmpeg实现简易播放器 最简iOS播放器（二） kxmovie iOS 基于ffmpeg的音视频编、解码以及播放器的制作 iOS-FFmpeg实现简单播放器(编译fak-aac+x264+sdl) ffmpeg-video-player ijkplayer ijkplayer ijkplayer#topics ffplayer ffplayer ffplayer 原理、架构及代码分析——播放器功能原理 ffplay for mfc 代码备忘 ffplay基本使用 ffplayer 调试 使用XCode debug ffmpeg/ffplay ffplay调试环境搭建 Windows平台上编译ffmpeg源码，调试ffplay Xcode调试ffmpeg源码(十五) ffplayer 源码解析 ffplay.c源码阅读之音频、字幕、视频渲染原理(一) ffplay源码分析1-概述 FFPLAY的原理（一） Ffplay源码read_thread解读（一） 零基础读懂视频播放器控制原理： ffplay 播放器源代码分析 ffplaymfc-ffplaycore.cpp ffplay 播放器源代码分析 Fplay源代码分析：整体流程图 调试分析FFmpeg 常用结构体 FFMPEG 安装 教程（支持mp3） Ffplay源码read_thread解读（一） ffplay frame queue分析 ffmpeg源码分析之ffplay主流程 开源 ffmpeg_develop_doc 权限处理 Android permission denied原因归纳和解决办法 Android11缺少权限导致无法修改原文件，获取所有文件访问权限的方法 sdl SDL中文教程 SDL论坛 Android平台上使用SDL官方demo播放视频（使用ffmpeg解码） ant NDK NDK下载 Unsupported-Downloads android-ndk-r10e-darwin-x86_64 视频编辑 YXYVideoEditor SRS 手把手带你实现srs流媒体推流和拉流操作 Chmod 777 to a folder and all contents [duplicate] 腾讯云轻量服务器开放端口方法教程 v4_CN_Home SRS 4.0开发环境搭建:包括推流、服务器配置、拉流测试 ffmpeg -re -i ./doc/source.flv -c copy -f flv rtmp://www.1221.site/live/livestream ffmpeg -re -i /Users/you5yi/Downloads/蓝莓之夜/蓝莓之夜.1080p.国英双语.BD中英双字[66影视www.66Ys.Co].mkv -c copy -f flv rtmp://www.1221.site/live/livestream Welcome to SRS wiki! LFLiveKit LFLiveKit源码分析 LFLiveKit-Review.md ios基于LFLiveKit的直播项目 点播 HLS点播实现（H.264和AAC码流） RTMP 利用librtmp库在iOS上进行推流 【原】librtmp源码详解 调试libRTMP代码来分析RTMP协议 srs-librtmp-for-ios 从动手到放弃之我不要自己编译ffmpeg了 【踩坑+草稿篇】 简述RTMPDump与编译移植 RTMPdump 源代码分析 1： main()函数 rtmp源码分析之RTMP_Write 和 RTMP_SendPacket librtmp源码分析之核心实现解读 流媒体-RTMP协议-librtmp库学习（二） RTMP推流及协议学习 RTMP协议详解及实例分析 RTMP 协议规范(中文版) 手撕Rtmp协议细节（3）——Rtmp Body Socket socket()函数用法详解：创建套接字 "},"pages/ffmpeg/FFmpeg_compile.html":{"url":"pages/ffmpeg/FFmpeg_compile.html","title":"FFmpeg编译","keywords":"","body":"编译FFmpeg库 一、下载音视频框架 1. 上网下载 下载网址：http://www.ffmpeg.org/download.html 2. Shell脚本下载 1) 下载命令curl 它可以通过http\\ftp等等这样的网络方式下载和上传文件（它是一个强大网络工具）。 2）解压命令tar 表示解压和压缩（打包），基本语法：tar options，例如：tar xj，options选项分为很多中类型： -x 表示：解压文件选项， -j 表示：是否需要解压bz2压缩包（压缩包格式类型有很多：zip、bz2等等…）。 3）Shell脚本：download-ffmpeg.sh。 二、编译配置选项 1. 查看选项 进入FFmpeg框架包中，执行configure命令： cd ffmpeg-3.4 ./configure --help 2. 解释选项 1）Help options：帮助选项 --list-decoders：显示所有的解码器 --list-encoders：显示所有的编码器 --list-hwaccels：显示所有可用硬件加速 2）Standard options：标准选项 --logfile=FILE：日志输入文件 --disable-logging：不要打印debug日志 --prefix=PREFIX：安装目录 3）Licensing options：许可证选项 --enable-gpl：允许使用gpl代码，由此生成你的库或者二进制文件 GPL（许可证）：开源、免费、公用、修改、扩展。 4）Configuration options：配置备选选项 --disable-static：不能编译静态库 --enable-shared：构建动态库 5）Component options：组件选项 --disable-avdevice disable libavdevice build --disable-avcodec disable libavcodec build --disable-avformat disable libavformat build --disable-swresample disable libswresample build --disable-swscale disable libswscale build --disable-postproc disable libpostproc build --disable-avfilter disable libavfilter build --enable-avresample enable libavresample build [no] 6）External library support：外部库支持 Using any of the following switches will allow FFmpeg to link to the corresponding external library. All the components depending on that library will become enabled, if all their other dependencies are met and they are not explicitly disabled. E.g. --enable-libwavpack will enable linking to libwavpack and allow the libwavpack encoder to be built, unless it is specifically disabled with --disable-encoder=libwavpack. Note that only the system libraries are auto-detected. All the other external libraries must be explicitly enabled. Also note that the following help text describes the purpose of the libraries themselves, not all their features will necessarily be usable by FFmpeg. --enable-libfdk-aac：启用acc编码 7）Toolchain options：工具链选项 --arch=ARCH：指定我么需要编译平台CPU架构类型，例如：arm64、x86等等… --target-os=OS：指定操作系统 8）Advanced options：高级选项（暂时用不到） 9）Optimization options (experts only)：优化选项 10）Developer options：开发者模式 --disable-debug：禁用调试 --enable-debug=LEVEL：调试级别 四、Android平台编译 1. 下载源码 download-ffmpeg.sh是下载脚本，执行sh download-ffmpeg.sh下载源码，这里以ffmpeg-3.4为例。 2. 下载ndk https://developer.android.google.cn/ndk/downloads/older_releases.html 我这里使用的是ndkr10e。 在ffmpeg源码的同目录下新建ndk文件交，将下载好的ndk放入ndk文件夹。 3. 修改configure # SLIBNAME_WITH_MAJOR='$(SLIBNAME).$(LIBMAJOR)' # LIB_INSTALL_EXTRA_CMD='$$(RANLIB) \"$(LIBDIR)/$(LIBNAME)\"' # SLIB_INSTALL_NAME='$(SLIBNAME_WITH_VERSION)' # SLIB_INSTALL_LINKS='$(SLIBNAME_WITH_MAJOR) $(SLIBNAME)' The above code is modified as the following SLIBNAME_WITH_MAJOR='$(SLIBPREF)$(FULLNAME)-$(LIBMAJOR)$(SLIBSUF)' LIB_INSTALL_EXTRA_CMD='$$(RANLIB) \"$(LIBDIR)/$(LIBNAME)\"' SLIB_INSTALL_NAME='$(SLIBNAME_WITH_MAJOR)' SLIB_INSTALL_LINKS='$(SLIBNAME)' 4. 执行编译 android-build-ffmpeg.sh是编译脚本，将编译脚本放在和源码的同一目录，执行： sh android-build-ffmpeg.sh "},"pages/ffmpeg/FFmpeg_integration.html":{"url":"pages/ffmpeg/FFmpeg_integration.html","title":"FFmpeg集成","keywords":"","body":"FFmpeg集成 一、iOS集成FFmpeg 代码工程 1. 集成FFmpeg 测试我们自己编译库FFmpeg。 (1) 第一步：新建工程 删除Scenedelegate，参考：Xcode 11新建项目多了Scenedelegate。 (2) 第二步：导入库文件。 在工程目录新建FFmpeg-3.4，拷贝已经编译好的arm64静态库文件夹，删除不需要的share、lib/pkgconfig文件夹，最后将FFmpeg-3.4Add进入工程。 (3）第三步：添加依赖库 CoreMedia.framework CoreGraphics.framework VideoToolbox.framework AudioToolbox.framework libiconv.tbd libz.tbd libbz2.tbd (4) 配置头文件 1) 复制头文件路径 选中Target>Build Setting>搜索Library Search>双击Library Search Paths复制FFmpeg lib路径>修改lib为include就是FFmpeg头文件路径： $(PROJECT_DIR)/FFmpegCompiled/FFmpeg-3.4/arm64/include。 2) 配置头文件路径 选中Target>Build Setting>搜索Header Search>选中Header Search Paths>增加上面复制好头文件路径。 (5) 编译成功 2. 测试案例 (1) 打印配置信息 新建FFmpegTest测试类，增加类方法： // 引入头文件 // 核心库->音视频编解码库 #import /// 测试FFmpeg配置 + (void)ffmpegTestConfig { const char *configuration = avcodec_configuration(); NSLog(@\"配置信息: %s\", configuration); } 在ViewController中测试： #import \"FFmpegTest.h\" - (void)viewDidLoad { [super viewDidLoad]; // Do any additional setup after loading the view. //测试一 [FFmpegTest ffmpegTestConfig]; } console输出： 配置信息: --target-os=darwin --arch=arm64 --cc='xcrun -sdk iphoneos clang' --as='gas-preprocessor.pl -arch aarch64 -- xcrun -sdk iphoneos clang' --enable-cross-compile --disable-debug --disable-programs --disable-doc --enable-pic --extra-cflags='-arch arm64 -mios-version-min=7.0 -fembed-bitcode' --extra-ldflags='-arch arm64 -mios-version-min=7.0 -fembed-bitcode' --prefix=/Users/chenchangqing/Documents/FFmpeg/ffmpeg-3.4-target-iOS/arm64 (2) 打开视频文件 FFmpegTest增加类方法： // 导入封装格式库 #import /// 打开视频文件 + (void)ffmpegVideoOpenfile:(NSString*)filePath { // 第一步：注册组件 av_register_all(); // 第二步：打开封装格式文件 // 参数一：封装格式上下文 AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：打开视频地址->path const char *url = [filePath UTF8String]; // 参数三：指定输入封装格式->默认格式 // 参数四：指定默认配置信息->默认配置 int avformat_open_input_reuslt = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_reuslt != 0){ // 失败了 // 获取错误信息 // char* error_info = NULL; // av_strerror(avformat_open_input_reuslt, error_info, 1024); NSLog(@\"打开文件失败\"); return; } NSLog(@\"打开文件成功\"); } 项目新增视频Test.mov，在ViewController中测试： // 测试二 NSString *path = [[NSBundle mainBundle] pathForResource:@\"Test\" ofType:@\".mov\"]; [FFmpegTest ffmpegVideoOpenfile:path]; console输出： 打开文件成功 二、Android集成FFmpeg 代码工程 1. 集成FFmpeg (1) 第一步：新建工程 File->NewProject->Native C++->输入工程信息->Next->Finish。 三星手机开启开发者选项 (2) 第二步：导入库文件。 1) 项目选中Project模式->app->src->main->右键new->Directory->输入jniLibs->enter。 2) 将编译好的include和lib文件夹copy->选中刚才新建的jniLibs->paste。 (3) 第三步：修改CMakeLists.txt 1) app->src->main->cpp->双击CMakeLists.txt。 2) 修改CMakeLists.txt。 # FFMpeg配置 # FFmpeg配置目录 set(JNILIBS_DIR ${CMAKE_SOURCE_DIR}/../jniLibs) # 编解码(最重要的库) add_library( avcodec SHARED IMPORTED) set_target_properties( avcodec PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libavcodec.so) # 滤镜特效处理库 add_library( avfilter SHARED IMPORTED) set_target_properties( avfilter PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libavfilter.so) # 封装格式处理库 add_library( avformat SHARED IMPORTED) set_target_properties( avformat PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libavformat.so) # 工具库(大部分库都需要这个库的支持) add_library( avutil SHARED IMPORTED) set_target_properties( avutil PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libavutil.so) # 音频采样数据格式转换库 add_library( swresample SHARED IMPORTED) set_target_properties( swresample PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libswresample.so) # 视频像素数据格式转换 add_library( swscale SHARED IMPORTED) set_target_properties( swscale PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libswscale.so) add_library( avdevice SHARED IMPORTED) set_target_properties( avdevice PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libavdevice.so) add_library( postproc SHARED IMPORTED) set_target_properties( postproc PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libpostproc.so) #set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=gnu++11\") #判断编译器类型,如果是gcc编译器,则在编译选项中加入c++11支持 if(CMAKE_COMPILER_IS_GNUCXX) set(CMAKE_CXX_FLAGS \"-std=c++11 ${CMAKE_CXX_FLAGS}\") message(STATUS \"optional:-std=c++11\") endif(CMAKE_COMPILER_IS_GNUCXX) #配置编译的头文件 include_directories(${JNILIBS_DIR}/include) . . . target_link_libraries( # Specifies the target library. myapplication avcodec swresample avfilter avformat avutil swscale avdevice postproc # Links the target library to the log library # included in the NDK. ${log-lib}) (4) 配置CPU架构类型 修改app->build.gradle： externalNativeBuild { cmake { cppFlags '' abiFilters 'armeabi' } } 发现编译失败，解决方法，修改为如下( 参考链接)： defaultConfig { ndk { abiFilters 'armeabi-v7a' } } (5) 编译成功 2. 测试案例 (1) 打印配置信息 1) 定义Java方法 新建Java类FFmpegTest，定义ffmpegTestConfig方法： // 测试FFmpeg配置 // native：标记这个方法是一个特殊方法，不是普通的java方法，而是用于与NDK进行交互的方法（C/C++语言交互） // 用native修饰方法，方法没有实现，具体的实现在C/C++里面。 public static native void ffmpegTestConfig(); 2) 定义NDK方法 在native-lib.cpp中，导入FFmpeg头文件，由于 FFmpeg 是使用 C 语言编写的，所在 C++ 文件中引用 #include 的时候，也需要包裹在 extern \"C\" { }，才能正确的编译。 #import extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include } 在native-lib.cpp中新增Java方法ffmpegTestConfig的C++实现。 extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegcompiled_FFmpegTest_ffmpegTestConfig(JNIEnv *env, jclass clazz) { const char *configuration = avcodec_configuration(); __android_log_print(ANDROID_LOG_INFO, \"ffmpeg configuration\", \"%s\", configuration); } 之所以可以这么写是因为在CMakeLists.txt中有如下配置，将Java和C/C++进行关联。 add_library( # Sets the name of the library. androidffmpegcompiled # Sets the library as a shared library. SHARED # Provides a relative path to your source file(s). native-lib.cpp) 3) MainActivity增加测试代码。 protected void onCreate(Bundle savedInstanceState) { ... FFmpegTest.ffmpegTestConfig(); } 4) 运行工程，正确打印。 I/ffmpeg configuration: --prefix=/Users/chenchangqing/Documents/code/ffmpeg/01_ffmpeg_compiled/ffmpeg-3.4-target-android/armeabi-v7a --enable-shared --enable-gpl --disable-static --disable-doc --disable-ffmpeg --disable-ffplay --disable-ffprobe --disable-ffserver --disable-doc --disable-symver --enable-small --cross-prefix=/Users/chenchangqing/Documents/code/ffmpeg/01_ffmpeg_compiled/ndk/android-ndk-r10e/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi- --target-os=android --arch=armeabi-v7a --enable-cross-compile --sysroot=/Users/chenchangqing/Documents/code/ffmpeg/01_ffmpeg_compiled/ndk/android-ndk-r10e/platforms/android-18/arch-arm --extra-cflags='-Os -fpic -marm' --enable-pic (2) 打开视频文件 1) 定义Java方法 FFmpegTest定义ffmpegVideoOpenFile方法： // 测试FFmpeg打开视频 // filePath:路径 public static native void ffmpegVideoOpenFile(String filePath); 2) 定义NDK方法 在native-lib.cpp中，导入FFmpeg头文件。 #import extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include // 导入封装格式库 #import } 在native-lib.cpp中新增Java方法ffmpegVideoOpenFile的C++实现。 extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegcompiled_FFmpegTest_ffmpegVideoOpenFile(JNIEnv *env, jclass clazz, jstring file_path) { // 第一步：注册组件 av_register_all(); // 第二步：打开封装格式文件 // 参数一：封装格式上下文 AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：打开视频地址->path const char *url = env->GetStringUTFChars(file_path, NULL); // 参数三：指定输入封装格式->默认格式 // 参数四：指定默认配置信息->默认配置 int avformat_open_input_reuslt = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_reuslt != 0){ // 失败了 // 获取错误信息 // char* error_info = NULL; // av_strerror(avformat_open_input_reuslt, error_info, 1024); __android_log_print(ANDROID_LOG_INFO, \"ffmpeg\", \"打开文件失败\"); return; } __android_log_print(ANDROID_LOG_INFO, \"ffmpeg\", \"打开文件成功\"); } 3) 增加权限 在AndroidManifest.xml增加SD卡的读写权限。 4) MainActivity增加测试代码。 注意：如果打开失败，可能读写存储设备的权限被禁用。 摩托罗拉·刀锋：设置->应用和通知->高级->权限管理器->隐私相关·读写存储设备->找到应用->如果禁用，则修改为允许。 String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String inFilePath = rootPath.concat(\"/DCIM/Camera/VID_20220220_181306412.mp4\"); FFmpegTest.ffmpegVideoOpenFile(inFilePath); "},"pages/ffmpeg/iOS_integrated_FFmpeg.html":{"url":"pages/ffmpeg/iOS_integrated_FFmpeg.html","title":"iOS集成FFmpeg","keywords":"","body":"iOS集成FFmpeg 代码工程 下载FFmpeg源码 ffmpeg-3.4下载脚本：download-ffmpeg.sh。 sh download-ffmpeg.sh 安装gas-preprocessor ffmpeg-3.4对应的gas-preprocessor.pl。 cd gas-preprocessor directory sudo cp -f gas-preprocessor.pl /usr/local/bin/ chmod 777 /usr/local/bin/gas-preprocessor.pl 注意：使用Github最新的gas-preprocessor.pl编译FFmpeg，报错\"GNU assembler not found, install/update gas-preprocessor\"，所以使用指定版本的gas-preprocessor。 编译FFmpeg ffmpeg-3.4编译脚本：ios-build-ffmpeg.sh，将编译脚本放在和源码的同一目录。 默认分别编译arm64、armv7、i386、x86_64，代码如下： sh ios-build-ffmpeg.sh 指定架构编译，可以指定arm64、armv7、i386、x86_64，代码如下： sh ios-build-ffmpeg.sh arm64 指定armv7编译时出现问题，待解决： AS libavcodec/arm/aacpsdsp_neon.o src/libavutil/arm/asm.S:50:9: error: unknown directive .arch armv7-a ^ make: *** [libavcodec/arm/aacpsdsp_neon.o] Error 1 make: *** Waiting for unfinished jobs.... 新建工程 删除Scenedelegate，参考：Xcode 11新建项目多了Scenedelegate。 导入库文件 在工程目录新建FFmpeg-3.4，拷贝已经编译好的arm64静态库文件夹至FFmpeg-3.4，删除不需要的share、lib/pkgconfig文件夹，最后将FFmpeg-3.4通过Add Files加入工程。 配置头文件 1) 复制头文件路径 选中Target>Build Setting>搜索Library Search>双击Library Search Paths复制FFmpeg lib路径>修改lib为include就是FFmpeg头文件路径： $(PROJECT_DIR)/iOSIntegrationWithFFmpeg（工程名）/FFmpeg-3.4/arm64/include 2) 配置头文件路径 选中Target>Build Setting>搜索Header Search>选中Header Search Paths>增加上面复制好头文件路径。 添加依赖库 CoreMedia.framework CoreGraphics.framework VideoToolbox.framework AudioToolbox.framework libiconv.tbd libz.tbd libbz2.tbd 添加完毕，编译成功。 简单测试 下载test.mov，加入工程，新建测试类FFmpegTest，FFmpegTest.h增加方法定义。 /// 测试FFmpeg配置 + (void)ffmpegTestConfig; /// 打开视频文件 + (void)ffmpegVideoOpenfile:(NSString*)filePath; FFmpegTest.m引入FFmpeg头文件。 // 核心库->音视频编解码库 #import // 导入封装格式库 #import FFmpegTest.m增加方法实现。 /// 测试FFmpeg配置 + (void)ffmpegTestConfig { const char *configuration = avcodec_configuration(); NSLog(@\"配置信息: %s\", configuration); } /// 打开视频文件 + (void)ffmpegVideoOpenfile:(NSString*)filePath { // 第一步：注册组件 av_register_all(); // 第二步：打开封装格式文件 // 参数一：封装格式上下文 AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：打开视频地址->path const char *url = [filePath UTF8String]; // 参数三：指定输入封装格式->默认格式 // 参数四：指定默认配置信息->默认配置 int avformat_open_input_reuslt = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_reuslt != 0){ // 失败了 // 获取错误信息 // char* error_info = NULL; // av_strerror(avformat_open_input_reuslt, error_info, 1024); NSLog(@\"打开文件失败\"); return; } NSLog(@\"打开文件成功\"); } 在ViewController引入FFmpegTest.h头文件。 #import \"FFmpegTest.h\" 在ViewController的viewDidLoad加入方法调用。 // 测试一 [FFmpegTest ffmpegTestConfig]; // 测试二 NSString *path = [[NSBundle mainBundle] pathForResource:@\"Test\" ofType:@\".mov\"]; [FFmpegTest ffmpegVideoOpenfile:path]; run工程，console正确输出。 2022-05-08 01:32:12.320527+0800 iOSIntegrationWithFFmpeg[34898:1017443] 配置信息: --target-os=darwin --arch=arm64 --cc='xcrun -sdk iphoneos clang' --as='gas-preprocessor.pl -arch aarch64 -- xcrun -sdk iphoneos clang' --enable-cross-compile --disable-debug --disable-programs --disable-doc --enable-pic --extra-cflags='-arch arm64 -mios-version-min=7.0 -fembed-bitcode' --extra-ldflags='-arch arm64 -mios-version-min=7.0 -fembed-bitcode' --prefix=/Users/chenchangqing/Documents/code/ffmpeg/01_ffmpeg_compiled/ios_build/arm64 2022-05-08 01:32:12.338556+0800 iOSIntegrationWithFFmpeg[34898:1017443] 打开文件成功 "},"pages/ffmpeg/FFmpeg_basics.html":{"url":"pages/ffmpeg/FFmpeg_basics.html","title":"FFmpeg基础知识","keywords":"","body":"FFmpeg基础知识 一、视频播放 1. 视频播放流程 通常看到视频格式：mp4、mov、flv、wmv等等…，称之为：封装格式。 2. 视频播放器 两种模式播放器： (1) 可视化界面播放器 直接用户直观操作、简单易懂。例如腾讯视频、爱奇艺视频、QQ影音、暴风影音、快播、优酷等等。 (2) 非可视化界面播放器 命令操作播放器，用户看不懂，使用起来非常麻烦。例如FFmpeg的fplay（命令）播放器（内置播放器）、vlc播放器、mplayer播放器。 3. 视频信息查看工具 MediaInfo：帮助我们查看视频完整信息，直接去AppStore下载即可，不过要3块钱。 ULtraEdit：直接查看视频二进制数据。 视频单项信息： Elecard Format Analyzer：封装格式信息工具。 Elecard Stream Eye：视频编码信息工具。 GLYUVPlayer：视频像素信息工具，最新的mac系统不支持了，这里用YuvEye替换。 Adobe Audition：音频采样数据工具。 二、音视频封装格式 1. 封装格式 mp4、mov、flv、wmv、avi、ts、mkv等等。 2. 封装格式作用 视频流+音频流按照格式进行存储在一个文件中。 3.MPEG2-TS格式 视频压缩数据格式：MPEG2-TS。特定：数据排版，不包含头文件，数据大小固定（188byte）的TS-Packet。 4. FLV格式 (1) 优势 由于它形成的文件极小、加载速度极快，使得网络观看视频文件成为可能，它的出现有效地解决了视频文件导入Flash后，使导出的SWF文件体积庞大，不能在网络上很好的使用等问题。 (2) 文件结构 FLV是一个二进制文件，由文件头（FLV header）和很多tag组成。tag又可以分成三类：audio,video,script，分别代表音频流，视频流，脚本流（关键字或者文件信息之类）。 (3) FLV文件 FLV文件=FLV头文件+ tag1+tag内容1 + tag2+tag内容2 + ...+... + tagN+tag内容N。 (4) FLV头文件 1-3： 前3个字节是文件格式标识(FLV 0x46 0x4C 0x56)。 4-4： 第4个字节是版本（0x01）。 5-5： 第5个字节的前5个bit是保留的必须是0。 6-9: 第6-9的四个字节还是保留的.其数据为 00000009。 整个文件头的长度，一般是9（3+1+1+4）。 三、视频编码数据 1. 视频编码作用 将视频像素数据（YUV、RGB）进行压缩成为视频码流，从而降低视频数据量。（减小内存暂用） 2. 视频编码格式 3. H.264视频压缩数据格式 非常复杂算法->压缩->占用内存那么少？（例如：帧间预测、帧内预测…）->提高压缩性能。 四、音频编码数据 1. 音频编码作用 将音频采样数据（PCM格式）进行压缩成为音频码流，从而降低音频数据量。（减小内存暂用） 2. 音频编码格式 3. AAC格式 AAC，全称Advanced Audio Coding，是一种专为声音数据设计的文件压缩格式。与MP3不同，它采用了全新的算法进行编码，更加高效，具有更高的“性价比”。利用AAC格式，可使人感觉声音质量没有明显降低的前提下，更加小巧。苹果ipod、诺基亚手机支持AAC格式的音频文件。 (1) 优点 相对于mp3，AAC格式的音质更佳，文件更小。 (2) 不足 AAC属于有损压缩的格式，与时下流行的APE、FLAC等无损格式相比音质存在“本质上”的差距。加之，传输速度更快的USB3.0和16G以上大容量MP3正在加速普及，也使得AAC头上“小巧”的光环不复存在。 (3) 特点 ①提升的压缩率：可以以更小的文件大小获得更高的音质； ②支持多声道：可提供最多48个全音域声道； ③更高的解析度：最高支持96KHz的采样频率； ④提升的解码效率：解码播放所占的资源更少； 五、视频像素数据 1. 作用 保存了屏幕上面每一个像素点的值。 2. 数据格式 常见格式：RGB24、RGB32、YUV420P、YUV422P、YUV444P等等…一般最常见：YUV420P。 RGB格式： 3. 数据文件大小 例如：RGB24高清视频体积？（1个小时时长）。 体积：3600 x 25 x 1920 x 1080 * 3 = 559GB（非常大）。 假设：帧率25HZ，采样精度8bit，3个字节。 4. YUV播放器 人类：对色度不敏感，对亮度敏感。 Y表示：亮度，UV表示：色度。 六、音频采样数据 1. 作用 保存了音频中的每一个采样点值。 2. 数据文件大小 例如：1分钟PCM格式歌曲。 体积：60 x 44100 x 2 x 2 = 11MB。 分析：60表示时间，44100表示采样率（一般情况下，都是这个采样率，人的耳朵能够分辨的声音），2表示声道数量，2表示采样精度16位 = 2字节。 3. 工具 Audition 4. PCM格式 七、FFmpeg安装 1. 安装弯路 参考这片文章（mac下ffmpeg安装），我没搞成功。 执行命令： brew install ffmpeg 出现错误： Error: Xcode alone is not sufficient on Big Sur. Install the Command Line Tools: xcode-select --install 开始执行xcode-select --install，安装xcode命令行，这里又个问题，为啥我有xcode还得装这个。 出现错误： Error: No such file or directory @ rb_sysopen - /Users/chenchangqing/Library/Caches/Homebrew/downloads/c1e04fd8a5516a3a63dd106e38df40a2d44af18cc3f3d366e5ba0563d2f95570--openexr-3.1.4.big_sur.bottle.tar.gz 查资料后，先后执行了以下命令： brew install openexr brew install libvmaf brew install freetype 一直处于： 又卡住了： ==> Applying configure-big_sur.diff patching file configure Hunk #1 succeeded at 9513 (offset 780 lines). ==> ./configure --prefix=/usr/local/Homebrew/Cellar/gettext/0.21 --with-included-glib --w ==> make 出现错误，依次执行： brew install sqlite brew install meson brew install harfbuzz 放弃... 2. 正确方法 最后看了这篇文章，Mac OS上使用ffmpeg的“血泪”总结，按文章执行如下步骤： brew tap homebrew-ffmpeg/ffmpeg brew install homebrew-ffmpeg/ffmpeg/ffmpeg 安装成功。 八、FFmpeg应用 提供了一套比较完整代码，开源免费。核心架构设计思想：（核心 + 插件）设计。 1. ffmpeg (1) 作用 用于对视频进行转码，将mp4->mov，mov->mp4，wmv->mp4等等。 (2) 命令格式 ffmpeg -i {指定输入文件路径} -b:v {输出视频码率} {输出文件路径}。 (3) 测试运行 下载test.mov，修改码率，mov转mp4。 ffmpeg -i test.mov -b:v 234k -b:a 64k test.mp4 (4) 常用脚本 topmp4.sh，to1080pmp4.sh (4) 案例：视频转为高质量GIF动图 ffmpeg -ss 00:00:03 -t 3 -i Test.mov -s 640x360 -r “15” dongtu.gif 1) -ss 00:00:03 表示从第 00 分钟 03 秒开始制作 GIF，如果你想从第 9 秒开始，则输入 -ss 00:00:09，或者 -ss 9，支持小数点，所以也可以输入 -ss 00:00:11.3，或者 -ss 34.6 之类的，如果不加该命令，则从 0 秒开始制作； 2) -t 3 表示把持续 3 秒的视频转换为 GIF，你可以把它改为其他数字，例如 1.5，7 等等，时间越长，GIF 体积越大，如果不加该命令，则把整个视频转为 GIF； 3) -i 表示 invert 的意思吧，转换； 4) Test.mov 就是你要转换的视频，名称最好不要有中文，不要留空格，支持多种视频格式； 5) -s 640x360 是 GIF 的分辨率，视频分辨率可能是 1080p，但你制作的 GIF 可以转为 720p 等，允许自定义，分辨率越高体积越大，如果不加该命令，则保持分辨率不变； 6) -r “15” 表示帧率，网上下载的视频帧率通常为 24，设为 15 效果挺好了，帧率越高体积越大，如果不加该命令，则保持帧率不变； 7) dongtu.gif：就是你要输出的文件，你也可以把它命名为 hello.gif 等等。 2. ffplay 格式：ffplay {文件路径}，如下： ffplay test.mov "},"pages/ffmpeg/FFmpeg_video_encoding.html":{"url":"pages/ffmpeg/FFmpeg_video_encoding.html","title":"FFmpeg视频解码","keywords":"","body":"FFmpeg视频编码 Android代码工程 iOS代码工程 一、视频编码流程 第一步：注册组件 av_register_all：例如：编码器、解码器等等。 // 第一步：注册组件 av_register_all(); 第二步：初始化封装格式上下文 // 第二步：初始化封装格式上下文 AVFormatContext *avformat_context = avformat_alloc_context(); const char *coutFilePath = env->GetStringUTFChars(out_file_path, NULL); // iOS使用 // const char *coutFilePath = [outFilePath UTF8String]; AVOutputFormat *avoutput_format = av_guess_format(NULL, coutFilePath, NULL); // 设置视频压缩数据格式类型(h264、h265、mpeg2等等...) avformat_context->oformat = avoutput_format; 第三步：打开输出文件 // 第三步：打开输出文件 // 参数一：输出流 // 参数二：输出文件 // 参数三：权限->输出到文件中 if (avio_open(&avformat_context->pb, coutFilePath, AVIO_FLAG_WRITE) 第四步：创建输出码流 // 第四步：创建输出码流 // 注意：创建了一块内存空间，并不知道他是什么类型流，希望他是视频流 AVStream *av_video_stream = avformat_new_stream(avformat_context, NULL); 第五步：初始化编码器上下文 1. 获取编码器上下文 // 5.1 获取编码器上下文 AVCodecContext *avcodec_context = av_video_stream->codec; 2. 设置视频编码器ID // 5.2 设置视频编码器ID avcodec_context->codec_id = avoutput_format->video_codec; 3. 设置为视频编码器 // 5.3 设置为视频编码器 avcodec_context->codec_type = AVMEDIA_TYPE_VIDEO; 4. 设置像素数据格式 // 5.4 设置像素数据格式 // 编码的是像素数据格式，视频像素数据格式为YUV420P(YUV422P、YUV444P等等...) // 注意：这个类型是根据你解码的时候指定的解码的视频像素数据格式类型 avcodec_context->pix_fmt = AV_PIX_FMT_YUV420P; 5. 设置视频尺寸 // 5.5 设置视频尺寸 avcodec_context->width = 640; avcodec_context->height = 352; 6. 设置视频帧率 // 5.6 设置视频帧率 // 视频帧率：25fps（每秒25帧） // 单位：fps，\"f\"表示帧数，\"ps\"表示每秒 avcodec_context->time_base.num = 1; avcodec_context->time_base.den = 25; 7. 设置视频码率 // 5.7 设置视频码率 //（1）什么是码率？ // 含义：每秒传送的比特(bit)数单位为 bps(Bit Per Second)，比特率越高，传送数据速度越快。 // 单位：bps，\"b\"表示数据量，\"ps\"表示每秒。 //（2）什么是视频码率? // 含义：视频码率就是数据传输时单位时间传送的数据位数，一般我们用的单位是kbps即千位每秒。 //（3）视频码率计算如下？ // 基本的算法是：码率（kbps）= 视频大小 - 音频大小（bit位）/ 时间（秒）。 // 例如：Test.mov时间 = 24秒，文件大小（视频+音频） = 1.73MB。 // 视频大小 = 1.34MB（文件占比：77%）= 1.34MB * 1024 * 1024 * 8 / 24 = 字节大小 = 468365字节 = 468Kbps。 // 音频大小 = 376KB（文件占比：21%）。 // 计算出来的码率 : 468Kbps，K表示1000，b表示位（bit）。 // 总结：码率越大，视频越大。 avcodec_context->bit_rate = 468000; 8. 设置GOP // 5.8 设置GOP // 影响到视频质量问题，是一组连续画面 //（1）MPEG格式画面类型 // 3种类型：I帧、P帧、B帧。 //（2）I帧： // 内部编码帧，是原始帧（原始视频数据） // 是完整画面，是关键帧（必需的有，如果没有I，那么你无法进行编码，解码）。 // 视频第1帧：视频序列中的第一个帧始终都是I帧，因为它是关键帧。 //（3）P帧 // 向前预测帧 // 预测前面的一帧类型，处理前面的一阵数据(->I帧、B帧)。 // P帧数据根据前面的一帧数据进行处理得到了P帧。 //（4）B帧 // 前后预测帧（双向预测帧），前面一帧和后面一帧的差别。 // B帧压缩率高，但是对解码性能要求较高。 //（5）总结 // I只需要考虑自己 = 1帧，P帧考虑自己+前面一帧 = 2帧，B帧考虑自己+前后帧 = 3帧 // 说白了，P帧和B帧是对I帧压缩。 // 每250帧，插入1个I帧，I帧越少，视频越小 avcodec_context->gop_size = 250; 9. 设置量化参数 // 5.9 设置量化参数 // 数学算法（高级算法），量化系数越小，视频越是清晰 // 一般情况下都是默认值，最小量化系数默认值是10，最大量化系数默认值是51 avcodec_context->qmin = 10; avcodec_context->qmax = 51; 10. 设置b帧最大值 // 5.10 设置b帧最大值 // 设置不需要B帧 avcodec_context->max_b_frames = 0; 第六步：查找视频编码器 // 第六步：查找视频编码器 AVCodec *avcodec = avcodec_find_encoder(avcodec_context->codec_id); if (avcodec == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"找不到编码器\"); // iOS使用 // NSLog(@\"找不到编码器\"); return; } __android_log_print(ANDROID_LOG_INFO, \"main\", \"编码器名称为：%s\", avcodec->name); // iOS使用 // NSLog(@\"编码器名称为：%s\", avcodec->name); 1. 出现问题 新建测试工程（稍后会介绍建工程测试），代码运行到这一步会出现“找不到编码器”，因为编译库没有依赖x264库（默认情况下FFmpeg没有编译进行h264库）。 2. 解决问题 (1) 下载源码 x264库，翻墙更快。 git clone https://code.videolan.org/videolan/x264.git (2) 下载ndk https://developer.android.google.cn/ndk/downloads/older_releases.html 我这里使用的是ndkr10e。 在x264源码的同目录下新建ndk文件交，将下载好的ndk放入ndk文件夹。 (3) 编译x264脚本 编译x264的.a静态库，指定编译平台类型：iOS平台、安卓平台、Mac平台、Windows平台等等。 android_build_x264.sh是编译脚本，将编译脚本放在和源码的同一目录，执行： sh android-build-x264.sh 执行过程会提示开机密码，看到Android h264 builds finished说明编译成功。 (4) 编译FFmpeg 修改Android的FFmpeg动态库编译脚本，将x264库其编译进去。android-build-ffmpeg.sh是原来的编译脚本，在原来的编译脚本./configure增加如下选项。 # 以下是编译x264库增加的 # 禁用所有编码器 --disable-encoders \\ # 通过libx264库启用H.264编码 --enable-libx264 \\ # 启用编码器名称 --enable-encoder=libx264 \\ # 启用几个图片编码，由于生成视频预览 --enable-encoder=mjpeg \\ --enable-encoder=png \\ #和FFmpeg动态库一起编译，指定你之前编译好的x264静态库和头文件 --extra-cflags=\"-I/Users/chenchangqing/Documents/code/ffmpeg/06_ffmpeg_video_encoding/android_build_x264/include\" \\ --extra-ldflags=\"-L/Users/chenchangqing/Documents/code/ffmpeg/06_ffmpeg_video_encoding/android_build_x264/lib\" \\ android-build-ffmpeg-x264.sh是修改后的脚本，再次编译FFmpeg库，重新生成.so动态库。 重新编译，发现错误： libavcodec/libx264.c: In function 'X264_frame': libavcodec/libx264.c:282:9: error: 'x264_bit_depth' undeclared (first use in this function) if (x264_bit_depth > 8) ^ libavcodec/libx264.c:282:9: note: each undeclared identifier is reported only once for each function it appears in libavcodec/libx264.c: In function 'X264_init_static': libavcodec/libx264.c:892:9: error: 'x264_bit_depth' undeclared (first use in this function) if (x264_bit_depth == 8) ^ make: *** [libavcodec/libx264.o] Error 1 查询资料（“x264_bit_depth”未声明），是因为ffmpeg和x264不兼容，这里不使用最新版本的x264，尝试另一个版本的x264，重新编译，再重新生成.so动态库。 再次运行测试工程，成功输出： I/main: 编码器名称为：libx264 问题解决。 3. 解决问题（iOS） 这个问题在iOS上也是存在的，这里也列出解决步骤。 (1) 下载源码 x264库，翻墙更快。 git clone https://code.videolan.org/videolan/x264.git (2) 编译x264脚本 ios-build-x264.sh是编译脚本，将编译脚本放在和源码的同一目录，执行： sh ios-build-x264.sh 注意：如果使用旧版本的x264，比如这个x264，会出现下面的问题，所以我这里使用的当时最新的x264。 Out of tree builds are impossible with config.h/x264_config.h in source dir. (3) 编译FFmpeg 修改iOS的FFmpeg库编译脚本，将x264库其编译进去。ios-build-ffmpeg.sh是原来的编译脚本，在原来的编译脚本./configure增加如下选项。 # 以下是编译x264库增加的 --enable-gpl \\ --disable-encoders \\ --enable-libx264 \\ --enable-encoder=libx264 \\ --enable-encoder=mjpeg \\ --enable-encoder=png \\ --extra-cflags=\"-I/Users/yangshaohong/Desktop/ffmpeg-test/test/thin-x264/arm64/include\" \\ --extra-ldflags=\"-L/Users/yangshaohong/Desktop/ffmpeg-test/test/thin-x264/arm64/lib\" \\ ios-build-ffmpeg-x264.sh是修改后的脚本，再次编译FFmpeg库，重新生成.a静态库。 用了最新的x264，还是出现了问题： src/libavcodec/libx264.c:282:9: error: use of undeclared identifier 'x264_bit_depth' if (x264_bit_depth > 8) ^ src/libavcodec/libx264.c:892:9: error: use of undeclared identifier 'x264_bit_depth' if (x264_bit_depth == 8) ^ src/libavcodec/libx264.c:894:14: error: use of undeclared identifier 'x264_bit_depth' else if (x264_bit_depth == 9) ^ src/libavcodec/libx264.c:896:14: error: use of undeclared identifier 'x264_bit_depth' else if (x264_bit_depth == 10) ^ 4 errors generated. make: *** [libavcodec/libx264.o] Error 1 make: *** Waiting for unfinished jobs.... 查询资料（“x264_bit_depth”未声明），是因为ffmpeg和x264不兼容，这里不使用最新版本的x264，尝试另一个版本的x264-snapshot-20180730-2245-stable.tar.bz2，重新编译，成功生成了支持h264编码的FFmpeg静态库。 注意：这里x264和ffmpeg都指定了arm64的架构。 第七步：打开视频编码器 注意：代码中的“优化步骤”是必须的，要不然编码过程会有坑。 // 第七步：打开视频编码器 // 以下是编码优化步骤，必须有，要不然编码会出问题 // 编码延时问题，编码选项->编码设置 AVDictionary *param = 0; if (avcodec_context->codec_id == AV_CODEC_ID_H264) { // 需要查看x264源码->x264.c文件 // 第一个值：预备参数 // key: preset // value: slow->慢 // value: superfast->超快 av_dict_set(&param, \"preset\", \"slow\", 0); // 第二个值：调优 // key: tune->调优 // value: zerolatency->零延迟 av_dict_set(&param, \"tune\", \"zerolatency\", 0); } // 打开编码器 if (avcodec_open2(avcodec_context, avcodec, &param) 第八步：写入文件头信息 // 第八步：写入文件头信息 avformat_write_header(avformat_context, NULL); 第九步：打开yuv文件 // 第九步：打开yuv文件 // 遇到问题：fopen Permission denied const char *cinFilePath = env->GetStringUTFChars(in_file_path, NULL); // iOS使用 // const char *cinFilePath = [inFilePath UTF8String]; int errNum = 0; FILE *in_file = fopen(cinFilePath, \"rb\"); if (in_file == NULL) { errNum = errno; __android_log_print(ANDROID_LOG_INFO, \"main\", \"文件不存在:%s,in_file:%s,errNum:%d,reason:%s\", cinFilePath, in_file, errNum, strerror(errNum)); // iOS使用 // NSLog(@\"文件不存在\"); return; } 这一步有坑，打开yuv文件（fopen）一直出现“Permission denied”错误，困扰了有一天，最后还是没有找到很好的办法，但是有个临时解决办法，就是先执行视频解码为.yuv文件，这个时候去打开（fopen）刚生成的.yuv文件，是可以成功的。 第十步：视频编码准备 // 第十步：视频编码准备 // 10.1 创建视频原始数据帧 // 作用：存储视频原始数据帧 AVFrame *av_frame = av_frame_alloc(); // 10.2 创建一个缓冲区 // 作用：用于缓存读取视频数据 // 先获取缓冲区大小 int buffer_size = av_image_get_buffer_size(avcodec_context->pix_fmt, avcodec_context->width, avcodec_context->height, 1); // 创建一个缓冲区，作用是缓存一帧视频像素数据 uint8_t *out_buffer = (uint8_t *) av_malloc(buffer_size); // 10.3 填充视频原始数据帧 av_image_fill_arrays(av_frame->data, av_frame->linesize, out_buffer, avcodec_context->pix_fmt, avcodec_context->width, avcodec_context->height, 1); // 10.4 创建压缩数据帧数据 // 作用：接收压缩数据帧数据 AVPacket *av_packet = (AVPacket *) av_malloc(buffer_size); 第十一步：循环读取视频像素数据 视频编码读取视频像素数据问题分析？ 答案如下： 比例规范：y : u : v = 4 : 1 : 1 然后规范：y = width（视频宽）* height（高） 假设：width = 100，height = 10 所以：y = width * height = 1000 所以：u = y / 4 = 1000 / 4 = 250，v = y / 4 = 1000 / 4 = 250 也就是说：一帧yuv大小 = 1500 编码的时候读取一帧数据：fread(out_buffer, 1, y_size * 3 / 2, in_file) y_size * 3 / 2 = 1000 * 3 / 2 = 1500 代码：av_frame->data[0] = out_buffer 解释：指针是从out_buffer = 0开始，所以data[0]读取范围：0-1000 代码：av_frame->data[1] = out_buffer + y_size 解释：指针是从out_buffer + y_size = 0 + 1000 = 1000开始，所以data[1]读取范围：1000-1250 代码：av_frame->data[2] = out_buffer + y_size * 5 / 4 解释：指针是从out_buffer + y_size * 5 / 4 = 0 + 1000 * 5 / 4 = 1250开始，所以data[2]读取范围：1250-1500 一帧数据->大小 = Y大小 + U大小 + V大小 假设：width = 100，height = 10 Y大小：y = width * height = 100 * 10 = 1000 U大小：u = y / 4 = 1000 / 4 = 250 V大小：v = y / 4 = 1000 / 4 = 250 一帧数据大小 = Y + U + V = 1500 视频解码计算->指针位移处理 保存Y大小： fwrite(avframe_yuv420p->data[0], 1, y_size, file_yuv420p); avframe_yuv420p->data[0]->表示Y值 读取：0->1000 保存U大小 fwrite(avframe_yuv420p->data[1], 1, u_size, file_yuv420p); avframe_yuv420p->data[1]->表示U值 读取：1000->1250 保存V大小 fwrite(avframe_yuv420p->data[2], 1, v_size, file_yuv420p); avframe_yuv420p->data[2]->表示V值 读取：1250->1500 视频编码计算->指针位移计算 分析读取数据大小？ y = 1000 数据大小 = 一帧YUV数据 = Y + U + V = 1500 数据大小 = y * 3 / 2 = 1000 * 3 / 2 = 1500 现在我们视频编码根据Y大小，求出YUV大小计算公式 out_buffer = 1500（总的数据量） 保存Y大小 av_frame->data[0] = out_buffer; 读取Y数据->1000 读取：0->1000 保存U大小 av_frame->data[1] = out_buffer + y_size; 读取U数据->250 读取：0 + 1000 -> 1250 保存V大小 av_frame->data[2] = out_buffer + y_size * 5 / 4; 读取V数据->250 读取：0 + 1000 * 5 / 4 = 1250->1500 说白了：通过Y值得到V读取起点位置 // 第十一步：循环读取视频像素数据 // 编码是否成功 int result = 0; int current_frame_index = 1; int i = 0; // 计算y的大小 int y_size = avcodec_context->width * avcodec_context->height; while (true) { // 从yuv文件里面读取缓冲区 // 读取大小：y_size * 3 / 2 if (fread(out_buffer, 1, y_size * 3 / 2, in_file) data[0] = out_buffer; // U值 av_frame->data[1] = out_buffer + y_size; // V值 av_frame->data[2] = out_buffer + y_size * 5 / 4; // 帧数 av_frame->pts = i; // 注意时间戳 i++; // 第十二步：视频编码处理 // ... current_frame_index++; } 第十二步：视频编码处理 代码位置在第十一步。 // 第十二步：视频编码处理 // 发送一帧视频像素数据 avcodec_send_frame(avcodec_context, av_frame); // 接收一帧视频像素数据，编码为视频压缩数据格式 result = avcodec_receive_packet(avcodec_context, av_packet); // 判定是否编码成功 if (result == 0) { // 编码成功 // 第十三步：将视频压缩数据写入到输出文件中 // ... } else { __android_log_print(ANDROID_LOG_INFO, \"main\", \"编码第%d帧失败2\", current_frame_index); // iOS使用 // NSLog(@\"编码第%d帧失败2\", current_frame_index); return; } 第十三步：将视频压缩数据写入到输出文件中 代码位置在第十二步。 // 第十三步：将视频压缩数据写入到输出文件中 av_packet->stream_index = av_video_stream->index; result = av_write_frame(avformat_context, av_packet); current_frame_index++; // 是否输出成功 if (result 第十四步：写入剩余帧数据 // 第十四步：写入剩余帧数据 // 作用：输出编码器中剩余AVPacket，可能没有 flush_encoder(avformat_context, 0); 第十五步：写入文件尾部信息 // 第十五步：写入文件尾部信息 av_write_trailer(avformat_context); 第十六步：释放内存，关闭编码器 // 第十六步：释放内存，关闭编码器 avcodec_close(avcodec_context); av_free(av_frame); av_free(out_buffer); av_packet_free(&av_packet); avio_close(avformat_context->pb); avformat_free_context(avformat_context); fclose(in_file); 二、新建Android视频编码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程AndroidFFmpegEncodingVideo。 2. 定义java方法 寻找MainActivity：app->src->main->java->MainActivity，增加代码如下： public native void ffmpegVideoEncode(String inFilePath, String outFilePath); public native void ffmepgDecodeVideo(String inFilePath, String outFilePath); 3. 定义NDK方法 增加android打印。 #include 在native-lib.cpp中，导入FFmpeg头文件。 extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include // 封装格式处理库 #include \"libavformat/avformat.h\" // 工具库 #include \"libavutil/imgutils.h\" // 视频像素数据格式库 #include \"libswscale/swscale.h\"} 在native-lib.cpp中新增java方法flush_encoder、ffmepgVideoEncode、ffmepgDecodeVideo的C++实现，输入MainActivity.就会有代码提示，选择正确ffmepgVideoEncode方法补全代码。 ffmepgDecodeVideo方法实现参考FFmpeg视频解码。 int flush_encoder(AVFormatContext *fmt_ctx, unsigned int stream_index) { int ret; int got_frame; AVPacket enc_pkt; if (!(fmt_ctx->streams[stream_index]->codec->codec->capabilities & CODEC_CAP_DELAY)) return 0; while (1) { enc_pkt.data = NULL; enc_pkt.size = 0; av_init_packet(&enc_pkt); ret = avcodec_encode_video2(fmt_ctx->streams[stream_index]->codec, &enc_pkt, NULL, &got_frame); av_frame_free(NULL); if (ret 三、测试Android视频编码工程 准备视频文件：test.mov 在AndroidManifest.xml增加SD卡的读写权限。 MainActivity增加测试代码，这里先进行视频解码，生成的.yuv文件后，直接对.yuv文件再进行编码。 注意：如果打开失败，可能读写存储设备的权限被禁用。 摩托罗拉·刀锋：设置->应用和通知->高级->权限管理器->隐私相关·读写存储设备->找到应用->如果禁用，则修改为允许。 import android.os.Environment; import java.io.File; import java.io.IOException; import android.util.Log; private void ffmpegVideoEncode() { String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.yuv\"); String outFilePath = downloadPath.concat(\"/test.h264\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmpegVideoEncode(inFilePath, outFilePath); } private void ffmepgDecodeVideo() { String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.mov\"); String outFilePath = downloadPath.concat(\"/test.yuv\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()){ Log.i(\"日志：\",\"存在\"); }else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmepgDecodeVideo(inFilePath, outFilePath); } ffmepgDecodeVideo(); ffmpegVideoEncode(); run工程代码，正确打印，同时正确生成.h264文件。 I/日志：: 存在 I/main: 解码器名称：h264 I/main: 当前解码第1帧 . . . I/main: 当前解码第600帧 I/日志：: 存在 I/main: 编码器名称为：libx264 I/main: 编码第1帧成功 . . . I/main: 编码第598帧成功 读取完毕... h264文件播放： ffplay test.h264 四、新建iOS视频编码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程iOSFFmpegEncodingVideo。 注意：工程使用的是支持h264编码的FFmpeg库文件。 2. 导入资源文件 资源文件就是视频解码后的.yuv文件。先将.yuv文件拷贝至工程目录下，再通过add files的方式加入工程。 3. 导入x264静态库 在工程目录新建x264，拷贝编译好的thin-x264文件夹至x264目录，只保留arm64的文件夹，删除lib文件夹中的pkgconfig，再通过add files的方式加入工程。 配置x264头文件，参考FFmpeg集成。 4. 增加视频编码方法 (1) 导入FFmpeg头文件 修改FFmpegTest.h，新增如下： //核心库 #include \"libavcodec/avcodec.h\" //封装格式处理库 #include \"libavformat/avformat.h\" //工具库 #include \"libavutil/imgutils.h\" (2) 新增视频编码方法 修改FFmpegTest.h，新增如下： /// FFmpeg视频编码 + (void)ffmpegVideoEncode:(NSString*)filePath outFilePath:(NSString*)outFilePath; 修改FFmpegTest.m，新增如下： int flush_encoder(AVFormatContext *fmt_ctx, unsigned int stream_index) { int ret; int got_frame; AVPacket enc_pkt; if (!(fmt_ctx->streams[stream_index]->codec->codec->capabilities & CODEC_CAP_DELAY)) return 0; while (1) { enc_pkt.data = NULL; enc_pkt.size = 0; av_init_packet(&enc_pkt); ret = avcodec_encode_video2(fmt_ctx->streams[stream_index]->codec, &enc_pkt, NULL, &got_frame); av_frame_free(NULL); if (ret (3) 增加方法测试 修改ViewController.m，新增测试代码如下： NSString* inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"yuv\"]; NSArray* paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES); NSString* path = [paths objectAtIndex:0]; NSString* tmpPath = [path stringByAppendingPathComponent:@\"temp\"]; [[NSFileManager defaultManager] createDirectoryAtPath:tmpPath withIntermediateDirectories:YES attributes:nil error:NULL]; NSString* outFilePath = [tmpPath stringByAppendingPathComponent:[NSString stringWithFormat:@\"test.h264\"]]; [FFmpegTest ffmpegVideoEncode:inPath outFilePath:outFilePath]; run工程代码，正确打印，同时正确生成.h264文件。 iOSFFmpegEncodingVideo[828:210395] 编码器名称为：libx264 [libx264 @ 0x107021e00] using cpu capabilities: ARMv8 NEON [libx264 @ 0x107021e00] profile High, level 3.0 [h264 @ 0x10701c200] Using AVStream.codec.time_base as a timebase hint to the muxer is deprecated. Set AVStream.time_base instead. [h264 @ 0x10701c200] Using AVStream.codec to pass codec parameters to muxers is deprecated, use AVStream.codecpar instead. [libx264 @ 0x107021e00] AVFrame.format is not set [libx264 @ 0x107021e00] AVFrame.width or height is not set 2022-04-23 00:16:34.170713+0800 iOSFFmpegEncodingVideo[828:210395] 编码第1帧成功 [libx264 @ 0x107021e00] AVFrame.format is not set [libx264 @ 0x107021e00] AVFrame.width or height is not set 2022-04-23 00:16:34.175975+0800 iOSFFmpegEncodingVideo[828:210395] 编码第2帧成功 [libx264 @ 0x107021e00] AVFrame.format is not set [libx264 @ 0x107021e00] AVFrame.width or height is not set . . . 2022-04-23 00:16:39.831069+0800 iOSFFmpegEncodingVideo[828:210395] 编码第598帧成功 2022-04-23 00:16:39.831365+0800 iOSFFmpegEncodingVideo[828:210395] 读取完毕... [libx264 @ 0x107021e00] frame I:3 Avg QP:24.64 size: 20162 [libx264 @ 0x107021e00] frame P:595 Avg QP:25.38 size: 2245 [libx264 @ 0x107021e00] mb I I16..4: 13.5% 50.9% 35.6% [libx264 @ 0x107021e00] mb P I16..4: 0.3% 0.5% 0.4% P16..4: 40.8% 12.2% 4.0% 0.0% 0.0% skip:41.7% [libx264 @ 0x107021e00] final ratefactor: 23.64 [libx264 @ 0x107021e00] 8x8 transform intra:43.8% inter:54.5% [libx264 @ 0x107021e00] coded y,uvDC,uvAC intra: 64.8% 75.5% 30.8% inter: 9.9% 13.2% 0.3% [libx264 @ 0x107021e00] i16 v,h,dc,p: 21% 21% 7% 52% [libx264 @ 0x107021e00] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 15% 13% 8% 8% 10% 11% 11% 10% 14% [libx264 @ 0x107021e00] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 15% 8% 9% 10% 11% 11% 8% 11% [libx264 @ 0x107021e00] i8c dc,h,v,p: 46% 21% 24% 10% [libx264 @ 0x107021e00] Weighted P-Frames: Y:0.3% UV:0.2% [libx264 @ 0x107021e00] kb/s:466.92 "},"pages/ffmpeg/FFmpeg_video_decoding.html":{"url":"pages/ffmpeg/FFmpeg_video_decoding.html","title":"FFmpeg音频解码","keywords":"","body":"FFmpeg视频解码 代码工程 一、视频解码流程 第一步：注册组件 av_register_all：例如：编码器、解码器等等。 // 第一步：注册组件 av_register_all(); 第二步：打开封装格式 avformat_open_input：例如：打开.mp4、.mov、.wmv文件等等。 // 第二步：打开封装格式 // 参数一：封装格式上下文 // 作用：保存整个视频信息(解码器、编码器等等...) // 信息：码率、帧率等... AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：视频路径 // 在我们iOS里面 // NSString* path = @\"test.mov\"; // const char *url = [path UTF8String] const char *url = env->GetStringUTFChars(in_file_path, NULL); // 参数三：指定输入的格式 // 参数四：设置默认参数 int avformat_open_input_result = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_result != 0){ // 安卓平台下log __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开文件失败\"); // iOS平台下log // NSLog(\"打开文件失败\"); // 不同的平台替换不同平台log日志 return; } 第三步：查找视频基本信息 avformat_find_stream_info：如果是视频解码，那么查找视频流，如果是音频解码，那么就查找音频流。 // 第三步：查找视频流，拿到视频信息 // 参数一：封装格式上下文 // 参数二：指定默认配置 int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result 第四步：查找视频解码器 avcodec_find_decoder：查找解码器。 1. 查找视频流索引位置 // 第四步：查找视频解码器 // 4.1 查找视频流索引位置 int av_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 判断流类型：视频流、音频流、字母流等等... if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_stream_index = i; break; } } 2. 获取解码器上下文 根据视频流索引，获取解码器上下文。 // 4.2 根据视频流索引，获取解码器上下文 AVCodecContext *avcodec_context = avformat_context->streams[av_stream_index]->codec; 3. 获得解码器ID 根据解码器上下文，获得解码器ID，然后查找解码器。 // 4.3 根据解码器上下文，获得解码器ID，然后查找解码器 AVCodec *avcodec = avcodec_find_decoder(avcodec_context->codec_id); 第五步：打开解码器 avcodec_open2：打开解码器。 // 第五步：打开解码器 int avcodec_open2_result = avcodec_open2(avcodec_context, avcodec, NULL); if (avcodec_open2_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开解码器失败\"); return; } // 测试一下 // 打印信息 __android_log_print(ANDROID_LOG_INFO, \"main\", \"解码器名称：%s\", avcodec->name); 第六步：定义类型转换参数 用于sws_scale()，进行音频采样数据转换操作。 1. 创建视频采样数据上下文 // 第六步：定义类型转换参数 // 6.1 创建视频采样数据帧上下文 // 参数一：源文件->原始视频像素数据格式宽 // 参数二：源文件->原始视频像素数据格式高 // 参数三：源文件->原始视频像素数据格式类型 // 参数四：目标文件->目标视频像素数据格式宽 // 参数五：目标文件->目标视频像素数据格式高 // 参数六：目标文件->目标视频像素数据格式类型 SwsContext *swscontext = sws_getContext(avcodec_context->width, avcodec_context->height, avcodec_context->pix_fmt, avcodec_context->width, avcodec_context->height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); 2. 创建视频压缩数据帧 // 6.2 创建视频压缩数据帧 // 视频压缩数据：H264 AVFrame* avframe_in = av_frame_alloc(); // 定义解码结果 int decode_result = 0; 3. 创建视频采样数据帧 // 6.3 创建视频采样数据帧 // 视频采样数据：YUV格式 AVFrame* avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型->yuv420类型 // 得到YUV420P缓冲区大小 // 参数一：视频像素数据格式类型->YUV420P格式 // 参数二：一帧视频像素数据宽 = 视频宽 // 参数三：一帧视频像素数据高 = 视频高 // 参数四：字节对齐方式->默认是1 int buffer_size = av_image_get_buffer_size(AV_PIX_FMT_YUV420P, avcodec_context->width, avcodec_context->height, 1); // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 // 参数一：目标->填充数据(avframe_yuv420p) // 参数二：目标->每一行大小 // 参数三：原始数据 // 参数四：目标->格式类型 // 参数五：宽 // 参数六：高 // 参数七：字节对齐方式 av_image_fill_arrays(avframe_yuv420p->data, avframe_yuv420p->linesize, out_buffer, AV_PIX_FMT_YUV420P, avcodec_context->width, avcodec_context->height, 1); 第七步：打开.yuv文件 // 第七步：打开.yuv文件 const char *outfile = env->GetStringUTFChars(out_file_path, NULL); FILE* file_yuv420p = fopen(outfile, \"wb+\"); if (file_yuv420p == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"输出文件打开失败\"); return; } 第八步：读取视频压缩数据帧 av_read_frame：读取视频压缩数据帧。 // 第八步：读取视频压缩数据帧 int current_index = 0; // 写入时yuv数据位置 int y_size, u_size, v_size; // 分析av_read_frame参数。 // 参数一：封装格式上下文 // 参数二：一帧压缩数据 // 如果是解码视频流，是视频压缩帧数据，例如H264 AVPacket* packet = (AVPacket*)av_malloc(sizeof(AVPacket)); while (av_read_frame(avformat_context, packet) >= 0) { // >=:读取到了 // stream_index == av_stream_index) { // 第九步：开始视频解码 // ... current_index++; __android_log_print(ANDROID_LOG_INFO, \"main\", \"当前解码第%d帧\", current_index); } } 第九步：开始视频解码 注意：代码位置在第八步。 avcodec_send_packet：发送一帧视频压缩数据。 avcodec_receive_frame：解码一帧视频数据。 // 第九步：开始视频解码 // 发送一帧视频压缩数据 avcodec_send_packet(avcodec_context, packet); // 解码一帧视频数据 decode_result = avcodec_receive_frame(avcodec_context, avframe_in); if (decode_result == 0) { // 视频解码成功 // 第十步：开始类型转换 // ... // 第十一步：写入.yuv文件 // ... } 第十步：开始类型转换 注意：代码位置在第九步。 // 第十步：开始类型转换 // 将解码出来的视频像素点数据格式统一转类型为yuv420P // 参数一：视频像素数据格式上下文 // 参数二：原来的视频像素数据格式->输入数据 // 参数三：原来的视频像素数据格式->输入画面每一行大小 // 参数四：原来的视频像素数据格式->输入画面每一行开始位置(填写：0->表示从原点开始读取) // 参数五：原来的视频像素数据格式->输入数据行数 // 参数六：转换类型后视频像素数据格式->输出数据 // 参数七：转换类型后视频像素数据格式->输出画面每一行大小 sws_scale(swscontext, (const uint8_t *const *)avframe_in->data, avframe_in->linesize, 0, avcodec_context->height, avframe_yuv420p->data, avframe_yuv420p->linesize); 第十一步：写入.yuv文件 注意：代码位置在第九步。 // 第十一步：写入.yuv文件 // 计算YUV大小 // Y表示：亮度 // UV表示：色度 // YUV420P格式规范一：Y结构表示一个像素(一个像素对应一个Y) // YUV420P格式规范二：4个像素点对应一个(U和V: 4Y = U = V) y_size = avcodec_context->width * avcodec_context->height; u_size = y_size / 4; v_size = y_size / 4; // 首先->Y数据 fwrite(avframe_yuv420p->data[0], 1, y_size, file_yuv420p); // 其次->U数据 fwrite(avframe_yuv420p->data[1], 1, u_size, file_yuv420p); // 再其次->V数据 fwrite(avframe_yuv420p->data[2], 1, v_size, file_yuv420p); 第十二步：释放内存资源，关闭解码器 // 第十二步：释放内存资源，关闭解码器 av_packet_free(&packet); fclose(file_yuv420p); av_frame_free(&avframe_in); av_frame_free(&avframe_yuv420p); free(out_buffer); avcodec_close(avcodec_context); avformat_free_context(avformat_context); 二、新建Android视频解码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程AndroidFFmpegDecodingVideo。 2. 定义java方法 寻找MainActivity：app->src->main->java->MainActivity，增加代码如下： public native void ffmepgDecodeVideo(String inFilePath, String outFilePath); 3. 定义NDK方法 增加android打印。 #include 在native-lib.cpp中，导入FFmpeg头文件。 extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include // 封装格式处理库 #include \"libavformat/avformat.h\" // 工具库 #include \"libavutil/imgutils.h\" // 视频像素数据格式库 #include \"libswscale/swscale.h\" } 在native-lib.cpp中新增java方法ffmepgDecodeVideo的C++实现，输入MainActivity.就会有代码提示，选择正确ffmepgDecodeVideo方法补全代码。 extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegdecodingvideo_MainActivity_ffmepgDecodeVideo(JNIEnv *env, jobject thiz, jstring in_file_path, jstring out_file_path) { // 这里拷贝上面的视频解码流程的代码即可。 } 三、测试Android视频解码工程 准备视频文件：test.mov 在AndroidManifest.xml增加SD卡的读写权限。 MainActivity增加测试代码。 注意：如果打开失败，可能读写存储设备的权限被禁用。 摩托罗拉·刀锋：设置->应用和通知->高级->权限管理器->隐私相关·读写存储设备->找到应用->如果禁用，则修改为允许。 import android.os.Environment; import java.io.File; import java.io.IOException; import android.util.Log; String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.mov\"); String outFilePath = downloadPath.concat(\"/test.yuv\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmepgDecodeVideo(inFilePath, outFilePath); 出现问题，待解决： I/main: 解码器名称：h264 A/libc: Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 5713 (egdecodingvideo), pid 5713 (egdecodingvideo) 增加打印pix_fmt代码： __android_log_print(ANDROID_LOG_INFO, \"main\", \"avcodec_context->pix_fmt：%d\", avcodec_context->pix_fmt); 发现avcodec_context->pix_fmt = -1，导致sws_getContext方法出错，修改sws_getContext的srcFormat参数。 SwsContext *swscontext = sws_getContext(avcodec_context->width, avcodec_context->height, AV_PIX_FMT_YUV420P, //avcodec_context->pix_fmt, avcodec_context->width, avcodec_context->height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); run工程代码，正确打印，同时正确生成yuv文件。 I/main: 解码器名称：h264 I/main: avcodec_context->width：640 I/main: avcodec_context->height：352 I/main: avcodec_context->pix_fmt：-1 I/main: 当前解码第1帧 . . . I/main: 当前解码第600帧 yuv文件太大（202.1M），不方便上传。yuv播放： ffplay -f rawvideo -video_size 640x352 /Users/chenchangqing/Documents/code/ffmpeg/resources/test.yuv "},"pages/ffmpeg/FFmpeg_audio_decoding.html":{"url":"pages/ffmpeg/FFmpeg_audio_decoding.html","title":"FFmpeg视频编码","keywords":"","body":"FFmpeg视频解码 代码工程 一、视频解码流程 第一步：注册组件 av_register_all：例如：编码器、解码器等等。 // 第一步：注册组件 av_register_all(); 第二步：打开封装格式 avformat_open_input：例如：打开.mp4、.mov、.wmv文件等等。 // 第二步：打开封装格式 // 参数一：封装格式上下文 // 作用：保存整个视频信息(解码器、编码器等等...) // 信息：码率、帧率等... AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：视频路径 // 在我们iOS里面 // NSString* path = @\"test.mov\"; // const char *url = [path UTF8String] const char *url = env->GetStringUTFChars(in_file_path, NULL); // 参数三：指定输入的格式 // 参数四：设置默认参数 int avformat_open_input_result = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_result != 0){ // 安卓平台下log __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开文件失败\"); // iOS平台下log // NSLog(\"打开文件失败\"); // 不同的平台替换不同平台log日志 return; } 第三步：查找视频基本信息 avformat_find_stream_info：如果是视频解码，那么查找视频流，如果是音频解码，那么就查找音频流。 // 第三步：查找视频流，拿到视频信息 // 参数一：封装格式上下文 // 参数二：指定默认配置 int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result 第四步：查找音频解码器 avcodec_find_decoder：查找解码器。 1. 查找音频流索引位置 // 第四步：查找音频解码器 // 4.1 查找音频流索引位置 int av_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 判断流类型：视频流、音频流、字母流等等... if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_stream_index = i; break; } } 2. 获取解码器上下文 根据音频流索引，获取解码器上下文。 // 4.2 根据音频流索引，获取解码器上下文 AVCodecContext *avcodec_context = avformat_context->streams[av_stream_index]->codec; 3. 获得解码器ID 根据解码器上下文，获得解码器ID，然后查找解码器。 // 4.3 根据解码器上下文，获得解码器ID，然后查找解码器 AVCodec *avcodec = avcodec_find_decoder(avcodec_context->codec_id); 第五步：打开解码器 avcodec_open2：打开解码器。 // 第五步：打开解码器 int avcodec_open2_result = avcodec_open2(avcodec_context, avcodec, NULL); if (avcodec_open2_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开解码器失败\"); return; } // 测试一下 // 打印信息 __android_log_print(ANDROID_LOG_INFO, \"main\", \"解码器名称：%s\", avcodec->name); 第六步：定义类型转换参数 用于swr_convert()，进行音频采样数据转换操作。 1. 创建音频采样数据上下文 // 第六步：定义类型转换参数 // 6.1 创建音频采样数据上下文 // 参数一：音频采样数据上下文 // 上下文：保存音频信息 SwrContext* swr_context = swr_alloc(); // 参数二：输出声道布局类型(立体声、环绕声、机器人等等...) // 立体声 int64_t out_ch_layout = AV_CH_LAYOUT_STEREO; // int out_ch_layout = av_get_default_channel_layout(avcodec_context->channels); // 参数三：输出采样精度（编码） // 例如：采样精度8位 = 1字节，采样精度16位 = 2字节 // 直接指定 // int out_sample_fmt = AV_SAMPLE_FMT_S16; // 动态获取，保持一致 AVSampleFormat out_sample_fmt = avcodec_context->sample_fmt; // 参数四：输出采样率(44100HZ) int out_sample_rate = avcodec_context->sample_rate; // 参数五：输入声道布局类型 int64_t in_ch_layout = av_get_default_channel_layout(avcodec_context->channels); // 参数六：输入采样精度 AVSampleFormat in_sample_fmt = avcodec_context->sample_fmt; // 参数七：输入采样率 int in_sample_rate = avcodec_context->sample_rate; // 参数八：log_offset->log日志，从那里开始统计 int log_offset = 0; // 参数九：log上下文 swr_alloc_set_opts(swr_context, out_ch_layout, out_sample_fmt, out_sample_rate, in_ch_layout, in_sample_fmt, in_sample_rate, log_offset, NULL); // 初始化音频采样数据上下文 swr_init(swr_context); 2. 创建音频压缩数据帧 // 6.2 创建音频压缩数据帧 // 音频压缩数据：acc格式、mp3格式 AVFrame* avframe_in = av_frame_alloc(); // 定义解码结果 int decode_result = 0; 3. 创建音频采样数据帧 // 6.3 创建音频采样数据帧 // 音频采样数据：PCM格式 // 缓冲区大小 = 采样率(44100HZ) * 采样精度(16位 = 2字节) int MAX_AUDIO_SIZE = 44100 * 2; uint8_t *out_buffer = (uint8_t *)av_malloc(MAX_AUDIO_SIZE); 第七步：打开.pcm文件 // 第七步：打开.yuv文件 const char *outfile = env->GetStringUTFChars(out_file_path, NULL); FILE* file_pcm = fopen(outfile, \"wb+\"); if (file_pcm == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"输出文件打开失败\"); return; } 第八步：读取视频压缩数据帧 av_read_frame：读取视频压缩数据帧。 // 第八步：读取视频压缩数据帧 int current_index = 0; // 分析av_read_frame参数。 // 参数一：封装格式上下文 // 参数二：一帧压缩数据 // 如果是解码音频流，是音频压缩帧数据，例如acc、mp3 AVPacket* packet = (AVPacket*)av_malloc(sizeof(AVPacket)); while (av_read_frame(avformat_context, packet) >= 0) { // >=:读取到了 // stream_index == av_stream_index) { // 第九步：开始音频解码 // ... current_index++; __android_log_print(ANDROID_LOG_INFO, \"main\", \"当前解码第%d帧\", current_index); } } 第九步：开始视频解码 注意：代码位置在第八步。 avcodec_send_packet：发送一帧视频压缩数据。 avcodec_receive_frame：解码一帧视频数据。 // 第九步：开始音频解码 // 发送一帧音频压缩数据 avcodec_send_packet(avcodec_context, packet); // 解码一帧视频数据 decode_result = avcodec_receive_frame(avcodec_context, avframe_in); if (decode_result == 0) { // 音频解码成功 // 第十步：开始类型转换 // ... // 第十一步：写入.pcm文件 // ... } 第十步：开始类型转换 注意：代码位置在第九步。 // 第十步：开始类型转换 // 将解码出来的音频数据格式统一转类型为PCM // 参数一：音频采样数据上下文 // 参数二：输出音频采样数据 // 参数三：输出音频采样数据->大小 // 参数四：输入音频采样数据 // 参数五：输入音频采样数据->大小 swr_convert(swr_context, &out_buffer, MAX_AUDIO_SIZE, (const uint8_t **)avframe_in->data, avframe_in->nb_samples); 第十一步：写入.pcm文件 注意：代码位置在第九步。 // 第十一步：写入.pcm文件 // 获取缓冲区实际存储大小 // 参数一：行大小 // 参数二：输出声道数量 int out_nb_channels = av_get_channel_layout_nb_channels(out_ch_layout); // 参数三：输入大小 // 参数四：输出音频采样数据格式 // 参数五：字节对齐方式 int out_buffer_size = av_samples_get_buffer_size(NULL, out_nb_channels, avframe_in->nb_samples, out_sample_fmt, 1); // 写入文件 fwrite(out_buffer, 1, out_buffer_size, file_pcm); 第十二步：释放内存资源，关闭解码器 // 第十二步：释放内存资源，关闭解码器 fclose(file_pcm); av_packet_free(&packet); swr_free(&swr_context); av_free(out_buffer); av_frame_free(&avframe_in); avcodec_close(avcodec_context); avformat_close_input(&avformat_context); 二、新建Android音频解码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程AndroidFFmpegDecodingAudio。 2. 定义java方法 寻找MainActivity：app->src->main->java->MainActivity，增加代码如下： public native void ffmepgDecodeAudio(String inFilePath, String outFilePath); 3. 定义NDK方法 增加android打印。 #include 在native-lib.cpp中，导入FFmpeg头文件。 extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include // 封装格式处理库 #include \"libavformat/avformat.h\" // 工具库 #include \"libavutil/imgutils.h\" // 视频像素数据格式库 #include \"libswscale/swscale.h\" // 音频采样数据格式库 #include \"libswresample/swresample.h\" } 在native-lib.cpp中新增java方法ffmepgDecodeAudio的C++实现，输入MainActivity.就会有代码提示，选择正确ffmepgDecodeAudio方法补全代码。 extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegdecodingaudio_MainActivity_ffmepgDecodeAudio(JNIEnv *env, jobject thiz, jstring in_file_path, jstring out_file_path) { // 这里拷贝上面的音频解码流程的代码即可。 } 三、测试Android音频解码工程 准备视频文件：test.mov 在AndroidManifest.xml增加SD卡的读写权限。 MainActivity增加测试代码。 注意：如果打开失败，可能读写存储设备的权限被禁用。 摩托罗拉·刀锋：设置->应用和通知->高级->权限管理器->隐私相关·读写存储设备->找到应用->如果禁用，则修改为允许。 import android.os.Environment; import java.io.File; import java.io.IOException; import android.util.Log; String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.mov\"); String outFilePath = downloadPath.concat(\"/test.pcm\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmepgDecodeAudio(inFilePath, outFilePath); run工程代码，正确打印，同时正确生成pcm文件。 I/main: 解码器名称：acc I/main: 当前解码第1帧 . . . I/main: 当前解码第502帧 pcm文件音频播放： ffplay -f s16le -ac 2 -ar 44100 /Users/chenchangqing/Downloads/test.pcm "},"pages/ffmpeg/FFmpeg_audio_encoding.html":{"url":"pages/ffmpeg/FFmpeg_audio_encoding.html","title":"FFmpeg音频编码","keywords":"","body":"FFmpeg音频编码 Android代码工程 iOS代码工程 一、音频编码流程 第一步：注册组件 av_register_all：例如：编码器、解码器等等。 // 第一步：注册组件 av_register_all(); 第二步：初始化封装格式上下文 // 第二步：初始化封装格式上下文 AVFormatContext *avformat_context = avformat_alloc_context(); const char *coutFilePath = env->GetStringUTFChars(out_file_path, NULL); // iOS使用 // const char *coutFilePath = [outFilePath UTF8String]; AVOutputFormat *avoutput_format = av_guess_format(NULL, coutFilePath, NULL); // 设置音频压缩数据格式类型(aac、mp3等等...) avformat_context->oformat = avoutput_format; 第三步：打开输出文件 // 第三步：打开输出文件 // 参数一：输出流 // 参数二：输出文件 // 参数三：权限->输出到文件中 if (avio_open(&avformat_context->pb, coutFilePath, AVIO_FLAG_WRITE) 第四步：创建输出码流 // 第四步：创建输出码流 // 注意：创建了一块内存空间，并不知道他是什么类型流，希望他是音频流 AVStream *av_audio_stream = avformat_new_stream(avformat_context, NULL); 第五步：初始化编码器上下文 1. 获取编码器上下文 // 第五步：初始化编码器上下文 // 5.1 获取编码器上下文 AVCodecContext *avcodec_context = av_audio_stream->codec; 2. 设置音频编码器ID // 5.2 设置音频编码器ID avcodec_context->codec_id = avoutput_format->audio_codec; run工程时，这一步出现了问题： A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x10 in tid 13086 (egencodingaudio), pid 13086 (egencodingaudio) 困挠了好几个小时，终于找到原因了，我把编码音频的输出文件后缀写成了acc，将后缀改成aac就解决了。 // 错误 String outFilePath = downloadPath.concat(\"/test.acc\"); // 正确 String outFilePath = downloadPath.concat(\"/test.aac\"); 3. 设置为音频编码器 // 5.3 设置为视频编码器 avcodec_context->codec_type = AVMEDIA_TYPE_AUDIO; 4. 设置音频数据格式等 // 5.4 设置像素数据格式 // 编码的是音频采样数据格式，视频像素数据格式为PCM // 注意：这个类型是根据你解码的时候指定的解码的音频采样数据格式类型 avcodec_context->sample_fmt = AV_SAMPLE_FMT_S16; // 设置采样率 avcodec_context->sample_rate = 44100; // 立体声 avcodec_context->channel_layout = AV_CH_LAYOUT_STEREO; // 声道数量 int channels = av_get_channel_layout_nb_channels(avcodec_context->channel_layout); avcodec_context->channels = channels; // 设置码率 // 基本的算法是：【码率】(kbps)=【视频大小 - 音频大小】(bit位) /【时间】(秒) avcodec_context->bit_rate = 128000; 第六步：查找音频编码器 // 第六步：查找音频编码器 AVCodec *avcodec = avcodec_find_encoder(avcodec_context->codec_id); if (avcodec == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"找不到编码器\"); // iOS使用 // NSLog(@\"找不到编码器\"); return; } __android_log_print(ANDROID_LOG_INFO, \"main\", \"编码器名称为：%s\", avcodec->name); // iOS使用 // NSLog(@\"编码器名称为：%s\", avcodec->name); 第七步：打开音频编码器 // 第七步：打开音频编码器 // 打开编码器 if (avcodec_open2(avcodec_context, avcodec, NULL) 1. 出现问题 新建测试工程（稍后会介绍建工程测试），代码运行到这一步会出现“打开编码器失败”，因为虽然找到了aac编码器，但是无法打开。 I/main: 编码器名称为：aac I/main: 打开编码器失败 2. 查找原因 下面我们通过打印错误日志定位错误原因。 修改MainActivity.java，新增方法： private void createAVLogFile() { // String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String logFilePath = downloadPath.concat(\"/av_log.txt\"); // 文件不存在我创建一个文件 File file = new File(logFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } } 修改native-lib.cpp，新增方法： // 这个函数作用：统计程序报错信息，FFmpeg报错信息打印到av_log文件中 // 我们将av_log保存到了sdcard（外部存储） // 或者你打印到控制台也可以，这里我们将错误信息打印到文件中 void custom_log(void *ptr, int level, const char* fmt, va_list vl) { __android_log_print(ANDROID_LOG_INFO, \"main\", fmt, vl); // 由于权限问题，还是无法将日志打印至sdcard，临时解决方案就是每次都先创建一个新的av_log.txt。 FILE *fp=fopen(\"/storage/emulated/0/Download/av_log.txt\",\"a+\"); if(fp){ vfprintf(fp,fmt,vl); fflush(fp); fclose(fp); } } 修改第七步，这样就可以打印日志至av_log.txt。 // 第七步：打开音频编码器 // 设置log错误监听函数 av_log_set_callback(custom_log); // 打开编码器 if (avcodec_open2(avcodec_context, avcodec, NULL) run测试工程，查看av_log.txt，发现编码器有问题，不支持s16（AV_SAMPLE_FMT_S16）格式。 Specified sample format s16 is invalid or not supported 3. 解决问题 通过分析，我们发现编码器有问题，那么我们需要换一个编码器。老得FFmpeg框架里面支持faac格式，新的FFmpeg框架里面fdk_aac格式。 faac和fdk_aac区别：fdk_aac编码出来音频质量高，占用内存少。 这里我们需要更换编码器为libfdk_aac，fdk_aac同时也支持s16（AV_SAMPLE_FMT_S16）格式。 (1) 下载源码 fdk-aac。 我使用的是fdk-aac-0.1.4.zip。 注意：编译0.1.5是有问题。 (2) 下载ndk https://developer.android.google.cn/ndk/downloads/older_releases.html 我这里使用的是ndkr10e。 在fdkaac源码的同目录下新建ndk文件交，将下载好的ndk放入ndk文件夹。 (3) 编译fdk-aac 编译fdk-aac的.a静态库。 android-build-fdkaac.sh是编译脚本，将编译脚本放在和源码的同一目录，执行： sh android-build-fdkaac.sh 执行过程会提示开机密码，看到Android aac builds finished说明编译成功。 (4) 编译FFmpeg 修改Android的FFmpeg动态库编译脚本，将fdkaac库其编译进去。android-build-ffmpeg.sh是原来的编译脚本，在原来的编译脚本./configure增加如下选项。 # 以下是编译fdkaac库增加的 # 禁用所有编码器 --disable-encoders \\ --enable-libfdk-aac \\ --enable-encoder=libfdk_aac \\ --enable-decoder=libfdk_aac \\ # 和FFmpeg动态库一起编译，指定你之前编译好的fdkaac静态库和头文件 --extra-cflags=\"-I/Users/chenchangqing/Documents/code/ffmpeg/07_ffmpeg_audio_encoding/android_build_fdkaac/include\" \\ --extra-ldflags=\"-L/Users/chenchangqing/Documents/code/ffmpeg/07_ffmpeg_audio_encoding/android_build_fdkaac/lib\" \\ android-build-ffmpeg-fdkaac.sh是修改后的脚本，再次编译FFmpeg库，重新生成.so动态库。 重新编译，发现错误，删除--enable-gpl \\。 libfdk_aac is incompatible with the gpl and --enable-nonfree is not specified. 查看ffmpeg-3.4/ffbuild/config.log重新编译，发现错误： /var/folders/vx/w486nkxn1dx05w199n5dl76m0000gn/T//ffconf.C7cXMk2x/test.o:test.c:function check_aacEncOpen: error: undefined reference to 'aacEncOpen' collect2: error: ld returned 1 exit status ERROR: libfdk_aac not found 哈哈，最后解决方案还是让我找到了，又耗费了几个小时，资料在这mac下编译android下aac,不愿孤独-Mac 上用NDK编译lib库的问题 no archive symbol table (run ran lib)...。 解决方案是，手动调用$NDK_HOME/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi-runlib，对生成的.a进行接口的导出。 ./arm-linux-androideabi-runlib libfdk-aac.a 经过这么一步，就可以顺利的执行支持fdk-aac的FFmpeg脚本（android-build-ffmpeg-fdkaac.sh）了。 注意：这里有个细节，在fdk-aac编译后的安装目录执行ranlib命令是无效的，所以我新建了android_build_fdkaac2文件夹，将lib和include文件夹复制进来，在执行ranlib命令就可以了，编译ffmpeg时指定fdk-aac的目录为android_build_fdkaac2即可。 4. 解决问题（iOS） 这个问题在iOS上也是存在的，这里也列出解决步骤。 (1) 下载源码 fdk-aac。 我使用的是fdk-aac-0.1.4.zip。 注意：编译0.1.5是有问题。 (2) 编译fdk-aac ios-build-fdkaac.sh是编译脚本，将编译脚本放在和源码的同一目录，执行： sh ios-build-fdkaac.sh 出现错误： configure: error: source directory already configured; run \"make distclean\" there first make: *** No rule to make target `install'. Stop. 根据提示，执行make distclean可以解决。 (3) 编译FFmpeg 修改iOS的FFmpeg库编译脚本，将fdkaac库其编译进去。ios-build-ffmpeg.sh是原来的编译脚本，在原来的编译脚本./configure增加如下选项。 # 以下是编译fdkaac库增加的 # 禁用所有编码器 --disable-encoders \\ --enable-libfdk-aac \\ --enable-encoder=libfdk_aac \\ --enable-decoder=libfdk_aac \\ # 和FFmpeg动态库一起编译，指定你之前编译好的fdkaac静态库和头文件 --extra-cflags=\"-I/Users/chenchangqing/Documents/code/ffmpeg/07_ffmpeg_audio_encoding/ios_build_fdkaac/include\" \\ --extra-ldflags=\"-L/Users/chenchangqing/Documents/code/ffmpeg/07_ffmpeg_audio_encoding/ios_build_fdkaac/lib\" \\ ios-build-ffmpeg-fdkaac.sh是修改后的脚本，再次编译FFmpeg库，重新生成.a静态库。 sh ios-build-ffmpeg-fdkaac.sh arm64 出现下面的错误，重新下载FFmpeg就可以解决了。 Out of tree builds are impossible with config.h in source dir. 注意：这里fdkaac和ffmpeg都指定了arm64的架构。 5. 使用fdk-aac编码器 // 错误 // AVCodec *avcodec = avcodec_find_encoder(avcodec_context->codec_id); // 正确 AVCodec *avcodec = avcodec_find_encoder_by_name(\"libfdk_aac\"); run工程，正常打开编码器。 I/main: 编码器名称为：libfdk_aac 第八步：写入文件头信息 // 第八步：写入文件头信息 avformat_write_header(avformat_context, NULL); 第九步：打开pcm文件 // 第九步：打开pcm文件 // 遇到问题：fopen Permission denied const char *cinFilePath = env->GetStringUTFChars(in_file_path, NULL); // iOS使用 // const char *cinFilePath = [inFilePath UTF8String]; int errNum = 0; FILE *in_file = fopen(cinFilePath, \"rb\"); if (in_file == NULL) { errNum = errno; __android_log_print(ANDROID_LOG_INFO, \"main\", \"文件不存在:%s,in_file:%s,errNum:%d,reason:%s\", cinFilePath, in_file, errNum, strerror(errNum)); // iOS使用 // NSLog(@\"文件不存在\"); return; } 这一步有坑，打开pcm文件（fopen）一直出现“Permission denied”错误，困扰了有一天，最后还是没有找到很好的办法，但是有个临时解决办法，就是先执行音频解码为.pcm文件，这个时候去打开（fopen）刚生成的.pcm文件，是可以成功的。 第十步：音频编码准备 // 第十步：音频编码准备 // 10.1 创建音频原始数据帧 // 作用：存储音频原始数据帧 AVFrame *av_frame = av_frame_alloc(); av_frame->nb_samples = avcodec_context->frame_size; av_frame->format = avcodec_context->sample_fmt; // 10.2 创建一个缓冲区 // 作用：用于缓存读取音频数据 // 先获取缓冲区大小 int buffer_size = av_samples_get_buffer_size(NULL, avcodec_context->channels, avcodec_context->frame_size, avcodec_context->sample_fmt, 1); // 创建一个缓冲区，作用是缓存一帧音频像素数据 uint8_t *out_buffer = (uint8_t *) av_malloc(buffer_size); // 10.3 填充音频原始数据帧 avcodec_fill_audio_frame(av_frame, avcodec_context->channels, avcodec_context->sample_fmt, (const uint8_t *)out_buffer, buffer_size, 1); // 10.4 创建压缩数据帧数据 // 作用：接收压缩数据帧数据 AVPacket *av_packet = (AVPacket *) av_malloc(buffer_size); 第十一步：循环读取视频像素数据 // 第十一步：循环读取音频数据 // 编码是否成功 int result = 0; int current_frame_index = 1; int i = 0; while (true) { // 从pcm文件里面读取缓冲区 if (fread(out_buffer, 1, buffer_size, in_file) data[0] = out_buffer; av_frame->pts = i; // 注意时间戳 i++; // 第十二步：音频编码处理 // ... current_frame_index++; } 第十二步：音频编码处理 代码位置在第十一步。 // 第十二步：音频编码处理 // 发送一帧音频数据 avcodec_send_frame(avcodec_context, av_frame); if (result != 0) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"Failed to send frame!\"); // iOS使用 // NSLog(@\"Failed to send frame!\", current_frame_index); return; } // 接收一帧音频数据，编码为音频压缩数据格式 result = avcodec_receive_packet(avcodec_context, av_packet); // 判定是否编码成功 if (result == 0) { // 编码成功 // 第十三步：将音频压缩数据写入到输出文件中 // ... } else { __android_log_print(ANDROID_LOG_INFO, \"main\", \"编码第%d帧失败2\", current_frame_index); // iOS使用 // NSLog(@\"编码第%d帧失败2\", current_frame_index); return; } 第十三步：将音频压缩数据写入到输出文件中 代码位置在第十二步。 // 第十三步：将音频压缩数据写入到输出文件中 av_packet->stream_index = av_audio_stream->index; result = av_write_frame(avformat_context, av_packet); // 是否输出成功 if (result 第十四步：写入剩余帧数据 增加flush_encoder方法： int flush_encoder(AVFormatContext *fmt_ctx, unsigned int stream_index) { int ret; int got_frame; AVPacket enc_pkt; if (!(fmt_ctx->streams[stream_index]->codec->codec->capabilities & CODEC_CAP_DELAY)) return 0; while (1) { enc_pkt.data = NULL; enc_pkt.size = 0; av_init_packet(&enc_pkt); ret = avcodec_encode_audio2(fmt_ctx->streams[stream_index]->codec, &enc_pkt, NULL, &got_frame); av_frame_free(NULL); if (ret 调用flush_encoder方法： // 第十四步：写入剩余帧数据 // 作用：输出编码器中剩余AVPacket，可能没有 result = flush_encoder(avformat_context, 0); if (result 第十五步：写入文件尾部信息 // 第十五步：写入文件尾部信息 av_write_trailer(avformat_context); 第十六步：释放内存，关闭编码器 // 第十六步：释放内存，关闭编码器 avcodec_close(avcodec_context); av_free(av_frame); av_free(out_buffer); av_packet_free(&av_packet); avio_close(avformat_context->pb); avformat_free_context(avformat_context); fclose(in_file); 二、新建Android音频编码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程AndroidFFmpegEncodingAudio。 2. 定义java方法 寻找MainActivity：app->src->main->java->MainActivity，增加代码如下： public native void ffmpegDecodeAudio(String inFilePath, String outFilePath); public native void ffmpegEncodeAudio(String inFilePath, String outFilePath); 3. 定义NDK方法 增加android打印。 #include 在native-lib.cpp中，导入FFmpeg头文件。 extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include // 封装格式处理库 #include \"libavformat/avformat.h\" // 工具库 #include \"libavutil/imgutils.h\" // 音频采样数据格式库 #include \"libswresample/swresample.h\" } 在native-lib.cpp中新增java方法flush_encoder、ffmpegDecodeAudio、ffmpegEncodeVideo的C++实现，输入MainActivity.就会有代码提示，选择正确ffmepgEncodeAudio方法补全代码。 ffmpegDecodeAudio方法实现参考FFmpeg视频解码。 extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegencodingaudio_MainActivity_ffmpegEncodeAudio(JNIEnv *env, jobject thiz, jstring in_file_path, jstring out_file_path) { // 这里拷贝上面的音频编码流程的代码即可。 } extern \"C\" JNIEXPORT void JNICALL Java_com_ccq_androidffmpegencodingaudio_MainActivity_ffmpegDecodeAudio(JNIEnv *env, jobject thiz, jstring in_file_path, jstring out_file_path) { } 三、测试Android音频编码工程 准备视频文件：test.mov 在AndroidManifest.xml增加SD卡的读写权限。 MainActivity增加测试代码，这里先进行视频解码，生成的.pcm文件后，直接对.pcm文件再进行编码。 注意：如果打开失败，可能读写存储设备的权限被禁用。 摩托罗拉·刀锋：设置->应用和通知->高级->权限管理器->隐私相关·读写存储设备->找到应用->如果禁用，则修改为允许。 import android.os.Environment; import java.io.File; import java.io.IOException; import android.util.Log; private void ffmpegEncodeAudio() { // String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.pcm\"); // 错误，会导致程序崩溃 // String outFilePath = downloadPath.concat(\"/test.aac\"); // 正确 String outFilePath = downloadPath.concat(\"/test.aac\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmpegEncodeAudio(inFilePath, outFilePath); } private void createAVLogFile() { // String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String logFilePath = downloadPath.concat(\"/av_log.txt\"); // 文件不存在我创建一个文件 File file = new File(logFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } } private void ffmepgDecodeAudio() { // String rootPath = Environment.getExternalStorageDirectory().getAbsolutePath(); String downloadPath = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).getAbsolutePath(); String inFilePath = downloadPath.concat(\"/test.mov\"); String outFilePath = downloadPath.concat(\"/test.pcm\"); // 文件不存在我创建一个文件 File file = new File(outFilePath); if (file.exists()) { Log.i(\"日志：\",\"存在\"); } else { try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } ffmpegDecodeAudio(inFilePath, outFilePath); } ffmepgDecodeAudio(); createAVLogFile(); ffmpegEncodeAudio(); run工程代码，正确打印，同时正确生成.h264文件。 I/日志：: 存在 I/main: 解码器名称：aac I/main: 当前解码第1帧 . . . I/main: 当前解码第1036帧 I/日志：: 存在 I/main: 编码器名称为：libfdk_aac I/main: Using AVStream.codec.time_base as a timebase hint to the muxer is deprecated. Set AVStream.time_base instead. I/main: Using AVStream.codec to pass codec parameters to muxers is deprecated, use AVStream.codecpar instead. I/main: 编码第1帧成功 . . . I/main: 编码第1035帧成功 读取完毕... I/main: Flush Encoder: Succeed to encode 1 frame! size: 363 I/main: Flush Encoder: Succeed to encode 1 frame! size: 95 I/main: Statistics: -2770196 seeks, -498852848 writeouts aac文件播放： ffplay test.aac 四、新建iOS视频编码工程 1. 新建工程 参考之前FFmpeg集成，新建ndk工程iOSFFmpegEncodingAudio。 注意：工程使用的是支持fdkacc编码的FFmpeg库文件。 2. 导入资源文件 资源文件就是音频解码后的.pcm文件。先将.pcm文件拷贝至工程目录下，再通过add files的方式加入工程。 3. 导入fdkacc静态库 在工程目录新建fdk-aac，拷贝编译好的ios_build_fdkaac/thin文件夹至fdkaac-0.1.4目录，只保留arm64的文件夹，删除lib文件夹中的pkgconfig和libfdk-aac.la，再通过add files的方式加入工程。 配置fdkaac头文件，参考FFmpeg集成。 4. 增加音编码方法 (1) 导入FFmpeg头文件 修改FFmpegTest.h，新增如下： //核心库 #include \"libavcodec/avcodec.h\" //封装格式处理库 #include \"libavformat/avformat.h\" //工具库 #include \"libavutil/imgutils.h\" #include \"libswresample/swresample.h\" (2) 新增音频编码方法 修改FFmpegTest.h，新增如下： /// FFmpeg音频编码 + (void)ffmpegAudioEncode: (NSString *)inFilePath outFilePath: (NSString *)outFilePath; 修改FFmpegTest.m，新增如下： + (void)ffmpegAudioEncode: (NSString *)inFilePath outFilePath: (NSString *)outFilePath { // 代码复制音频编码流程中的代码 // 将备注`iOS使用`的代码打开 } (3) 增加方法测试 修改ViewController.m，新增测试代码如下： NSString* inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"pcm\"]; NSArray* paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES); NSString* path = [paths objectAtIndex:0]; NSString* tmpPath = [path stringByAppendingPathComponent:@\"temp\"]; [[NSFileManager defaultManager] createDirectoryAtPath:tmpPath withIntermediateDirectories:YES attributes:nil error:NULL]; NSString* outFilePath = [tmpPath stringByAppendingPathComponent:[NSString stringWithFormat:@\"test.aac\"]]; [FFmpegTest ffmpegAudioEncode:inPath outFilePath:outFilePath]; run工程代码，正确打印，同时正确生成.aac文件。 "},"pages/ffmpeg/SDL_plays_YUV.html":{"url":"pages/ffmpeg/SDL_plays_YUV.html","title":"SDL播放YUV","keywords":"","body":"SDL播放YUV Android代码工程 Mac代码工程 iOS代码工程 一、SDL播放流程 第一步：初始化SDL多媒体框架 // 第一步：初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { LOG_I_ARGS(\"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } 第二步：初始化SDL窗口 // 第二步：初始化SDL窗口 // 参数一：窗口名称 // 参数二：窗口在屏幕上的x坐标 // 参数三：窗口在屏幕上的y坐标 // 参数四：窗口在屏幕上宽 // 参数五：窗口在屏幕上高 // 参数六：窗口状态(打开) int width = 640; int height = 352; SDL_Window* sdl_window = SDL_CreateWindow(\"SDL播放YUV视频\", SDL_WINDOWPOS_CENTERED, SDL_WINDOWPOS_CENTERED, width, height, SDL_WINDOW_OPENGL); if (sdl_window == NULL) { LOG_I_ARGS(\"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第三步：创建渲染器->渲染窗口 // 第三步：创建渲染器->渲染窗口 // 参数一：渲染目标创建->目标 // 参数二：从那里开始渲染(-1:表示从第一个位置开始) // 参数三：渲染类型(软件渲染) SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window, -1, 0); if (sdl_renderer == NULL) { LOG_I_ARGS(\"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第四步：创建纹理 // 第四步：创建纹理 // 参数一：纹理->目标渲染器 // 参数二：渲染格式->YUV格式->像素数据格式(视频)或者是音频采样数据格式(音频) // 参数三：绘制方式->频繁绘制->SDL_TEXTUREACCESS_STREAMING // 参数四：纹理宽 // 参数五：纹理高 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer, SDL_PIXELFORMAT_IYUV, SDL_TEXTUREACCESS_STREAMING, width, height); if (sdl_texture == NULL) { LOG_I_ARGS(\"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第五步：打开yuv文件 // 第五步：打开yuv文件 int errNum = 0; FILE* yuv_file = fopen(\"/storage/emulated/0/Download/test.yuv\", \"rb\"); // MAC使用 // FILE* yuv_file = fopen(\"/Users/chenchangqing/Documents/code/ffmpeg/resources/test.yuv\", \"rb\"); // iOS // NSString* inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"yuv\"]; // FILE* yuv_file = fopen([inPath UTF8String], \"rb\"); if (yuv_file == NULL) { errNum = errno; LOG_I_ARGS(\"打开文件失败：errNum:%d,reason:%s\", errNum, strerror(errNum)); // Mac使用 // printf(\"打开文件失败：errNum:%d,reason:%s\", errNum, strerror(errNum)); // 退出程序 SDL_Quit(); return 0; } 第六步：循环读取yuv视频像素数据帧 // 第六步：循环读取yuv视频像素数据帧 int y_size = width * height; // 定义缓冲区(内存空间开辟多大?) // 缓存一帧视频像素数据 = Y + U + V // Y:U:V = 4 : 1 : 1 // 假设：Y = 1.0 U = 0.25 V = 0.25 // 宽度：Y + U + V = 1.5 // 换算：Y + U + V = width * height * 1.5 char buffer_pix[y_size * 3 / 2]; // 定义渲染器区域 SDL_Rect sdl_rect; int currentIndex = 1; while (true) { // 一帧一帧读取 fread(buffer_pix, 1, y_size * 3 / 2, yuv_file); // 判定是否读取完毕 if (feof(yuv_file)){ break; } // 第七步：设置纹理数据 // ... // 第八步：将纹理数据拷贝给渲染器 // ... // 第九步：呈现画面帧 // ... // 第十步：渲染每一帧直接间隔时间 // ... printf(\"当前到了第%d帧\\n\", currentIndex); currentIndex++; } 第七步：设置纹理数据 // 第七步：设置纹理数据 // 参数一：纹理 // 参数二：渲染区域 // 参数三：需要渲染数据->视频像素数据帧 // 参数四：帧宽 SDL_UpdateTexture(sdl_texture, NULL, buffer_pix, width); 第八步：将纹理数据拷贝给渲染器 // 第八步：将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) sdl_rect.x = 100; sdl_rect.y = 100; sdl_rect.w = width; sdl_rect.h = height; SDL_RenderClear(sdl_renderer); SDL_RenderCopy(sdl_renderer, sdl_texture, NULL, &sdl_rect); 第九步：呈现画面帧 // 第九步：呈现画面帧 SDL_RenderPresent(sdl_renderer); 第十步：渲染每一帧直接间隔时间 // 第十步：渲染每一帧直接间隔时间 SDL_Delay(30); 第十一步：释放资源 // 第十一步：释放资源 fclose(yuv_file); SDL_DestroyTexture(sdl_texture); SDL_DestroyRenderer(sdl_renderer); 第十二步：退出程序 // 第十二步：退出程序 SDL_Quit(); 二、Android编译SDL 1. 下载工具包 (1) SDL http://www.libsdl.org/release/SDL2-2.0.5.tar.gz /Users/chenchangqing/Documents/code/ffmpeg/08_ffmpeg_sdl/SDL2-2.0.5 注意：由于最新的SDL编译后使用遇到无法显示视频的问题，这里使用SDL2-2.0.5。 问题：eglSwapBuffersWithDamageKHRImpl:1402 error 300d (EGL_BAD_SURFACE) (2) NDK https://dl.google.com/android/repository/android-ndk-r10e-darwin-x86_64.zip /Users/chenchangqing/Documents/code/ffmpeg/resources/ndk/android-ndk-r10e (3) SDK https://dl.google.com/android/adt/adt-bundle-linux-x86_64-20140702.zip /Users/chenchangqing/Documents/code/ffmpeg/resources/sdk/adt-bundle-linux-x86_64-20140702 (3) ANT https://dlcdn.apache.org//ant/binaries/apache-ant-1.10.12-bin.tar.gz /Users/chenchangqing/Documents/code/ffmpeg/resources/ant/apache-ant-1.10.12 2. 修改androidbuild.sh 查看SDL2-2.0.5/docs/README-android.md得知分别需要配置NDK、SDK、ANT，并且如果编译APK文件需要java环境，这里我们暂时不需要编译APK，忽略java环境即可。androidbuild.sh文件在SDL-2.0.5/build-scripts。 注意：最新版（目前2.0.20）是不需要修改androidbuild.sh的，但是需要配置SDK、NDK的环境变量。 (1) 配置NDK # NDKBUILD=`which ndk-build` NDKBUILD=\"/Users/chenchangqing/Documents/code/ffmpeg/resources/ndk/android-ndk-r10e/ndk-build\" (2) 配置SDK # ANDROID=`which android` ANDROID=\"/Users/chenchangqing/Documents/code/ffmpeg/resources/sdk/adt-bundle-linux-x86_64-20140702/sdk\" (3) 配置ANT # ANT=`which ant` ANT=\"/Users/chenchangqing/Documents/code/ffmpeg/resources/ant/apache-ant-1.10.12/bin/ant\" 3. 运行脚本 ./androidbuild.sh org.libsdl.testgles ../test/testgles.c 脚本执行完毕，分别生成了armeabi、armeabi-v7a、x86的.so动态库，SDL2-2.0.5/build/org.libsdl.testgles/libs是动态库的路径。 注意：最新版（目前2.0.20）脚本执行完毕生成的是一个Android工程，编译后生成动态库文件，暂时没研究好动态库文件的路径在哪里。 三、Android集成SDL 第一步：新建工程 File->NewProject->Native C++->输入工程信息->Next->Finish。 工程名称：AndroidSDLPlayYUV。 第二步：导入库文件。 1. 新建jniLibs文件夹 项目选中Project模式->app->src->main->右键new->Directory->输入jniLibs->enter。 同样的方式在jniLibs下心间lib文件夹，用来存放.so库文件。 2. 拷贝文件至jniLibs 拷贝SDL2-2.0.5/build/org.libsdl.testgles/libs/armeabi-v7a/libSDL2.so至jniLibs/lib。 拷贝SDL2-2.0.5/src至jniLibs。 拷贝SDL2-2.0.5/include至jniLibs。 第三步：配置SDL库 修改CMakeLists.txt。 1. 设置jniLibs # 1. 设置jniLibs set(JNILIBS_DIR ${CMAKE_SOURCE_DIR}/../jniLibs) 2. SDL核心库 # 2. SDL核心库 add_library( SDL2 SHARED IMPORTED) set_target_properties( SDL2 PROPERTIES IMPORTED_LOCATION ${JNILIBS_DIR}/lib/libSDL2.so) 3. 配置SDL_android_main.c 修改androidsdlplayyuv为SDL2main，增加${JNILIBS_DIR}/src/main/android/SDL_android_main.c。 androidsdlplayyuv为工程名称。 # 3. 配置SDL_android_main.c add_library( # Sets the name of the library. SDL2main # Sets the library as a shared library. SHARED # Provides a relative path to your source file(s). ${JNILIBS_DIR}/src/main/android/SDL_android_main.c native-lib.cpp) 4. 链接SDL2mian和SDL2库 修改androidsdlplayyuv为SDL2main，增加SDL2。 androidsdlplayyuv为工程名称。 # 4. 链接SDL2mian和SDL2库 target_link_libraries( # Specifies the target library. SDL2main SDL2 # Links the target library to the log library # included in the NDK. ${log-lib}) 注意1：3，4步完成后，然后马上编译，会出现error: undefined reference to 'SDL_main'错误，是因为native-lib.cpp还没写main函数，这里先忽略。注意2: androidsdlplayyuv库已经改成了SDL2main，MainActivity.java的loadLibrary也应该改下名称。 5. SDL头文件和源码 # 5. SDL头文件和源码 include_directories(${JNILIBS_DIR}/src) include_directories(${JNILIBS_DIR}/include) 第四步：配置CPU架构类型 修改app->build.gradle，defaultConfig增加ndk配置。 ndk { abiFilters 'armeabi-v7a' } 第五步：修改native-lib.cpp 引入头文件，增加SDL入口，新增main函数，实现SDL播放YUV。 #include #include #include \"SDL.h\" #define LOG_I_ARGS(FORMAT,...) __android_log_print(ANDROID_LOG_INFO,\"main\",FORMAT,__VA_ARGS__); #define LOG_I(FORMAT) LOG_I_ARGS(FORMAT,0); // SDL入口 extern \"C\" int main(int argc, char *argv[]) { // SDL播放YUV实现 // 拷贝SDL播放流程的代码 return 0; } 第六步：增加播放界面 拷贝SDL2-2.0.5/build/org.libsdl.testgles/src/org至java。 修改SDLActivity.java，原来 protected String[] getLibraries() { return new String[] { \"SDL2\", // \"SDL2_image\", // \"SDL2_mixer\", // \"SDL2_net\", // \"SDL2_ttf\", \"main\" }; } 修改为 protected String[] getLibraries() { return new String[] { \"SDL2\", // \"SDL2_image\", // \"SDL2_mixer\", // \"SDL2_net\", // \"SDL2_ttf\", \"SDL2main\"// 这里的名字是上一步通过add_library配置好的。 }; } 第七步：修改AndroidManifest.xml 在AndroidManifest.xml中声明MANAGE_EXTERNAL_STORAGE权限。 第八步：增加播放按钮 打开main->res->layout->activity_main.xml，点击右上角的Code，将原来的Text改为现在的Button。 第九步：修改MainActivity.java 这是最后一步，完成这一步，运行工程点击播放按钮就可以直接播放了。 注意：在/storage/emulated/0/Download文件夹下放test.yuv。 // 修改1：增加import import android.view.View; import android.content.Intent; import org.libsdl.app.SDLActivity; import android.widget.Toast; import android.os.Build; import android.provider.Settings; import android.os.Environment; public class MainActivity extends AppCompatActivity { // Used to load the 'androidsdlplayyuv' library on application startup. static { // 修改2：androidsdlplayyuv改为SDL2main System.loadLibrary(\"SDL2main\"); } private ActivityMainBinding binding; @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); binding = ActivityMainBinding.inflate(getLayoutInflater()); setContentView(binding.getRoot()); // 修改3：注释TextView，增加checkPermission // Example of a call to a native method // TextView tv = binding.sampleText; // tv.setText(stringFromJNI()); checkPermission(); } /** * 修改4：新增checkPermission方法 * 检查所有文件的权限 */ public void checkPermission() { if (Build.VERSION.SDK_INT 四、Mac集成SDL 第一步：配置SDL开发环境 1. 下载SDL2.dmg https://www.libsdl.org/release/SDL2-2.0.5.dmg 下载好了，点击安装，会得到SDL2.framework。 为了避免不必要的麻烦，这里我们依然使用2.0.5的版本。 2. 安装SDL2 将SDL2.Framework拷贝到/Library/Frameworks目录下。 第二步：新建Mac工程 新建命令行项目：New->Project->macOS->Command Line Tool，项目名称MacSDLPlayYUV。 第三步：导入SDL库 在工程目录新建SDLFramework，将SDL2.Framework拷贝到SDLFramework，通过Add的方式加入工程。 第四步：修改main.m 引入SDL头文件，在main函数拷贝“SDL播放流程”的代码即可。 注意：打印日志的方式需要修改为Mac的方式。 #import #include // 引入SDL头文件 #include // SDL入口 int main(int argc, const char * argv[]) { // SDL播放YUV实现 // ... return 0; } 五、iOS集成SDL 第一步：编译.a静态库 1. 下载SDL源码 http://www.libsdl.org/release/SDL2-2.0.5.tar.gz /Users/chenchangqing/Documents/code/ffmpeg/08_ffmpeg_sdl/SDL2-2.0.5 2. 编译SDL静态库 打开SDL2-2.0.5/Xcode-iOS/SDL工程，选择libSDL目标，再选择Any iOS Device真机编译，编译完成后可以在工程的Products看到libSDL2.a由红色变为了白色，说明静态库已经编译好了，右键show in Finder获取生成好的静态库。 注意：如果编译失败，可能是iOS编译版本不支持，修改SDL的iOS Deployment Target为9.0即可，默认是5.1.1。 第二步：新建iOS工程 删除Scenedelegate，参考：Xcode 11新建项目多了Scenedelegate。 工程名称为iOSSDLPlayYUV。 第三步：导入库文件。 在工程目录新建SDLFramework，拷贝libSDL2.a、SDL2-2.0.5/include至SDLFramework，最后将SDLFramework进入工程。 第四步：添加依赖库 CoreGraphics.framework AudioToolbox.framework AVFoundation.framework CoreAudio.framework OpenGLES.framework CoreMotion.framework GameController.framework 第五步：配置头文件 1. 复制头文件路径 选中Target>Build Setting>搜索Library Search>双击Library Search Paths复制SDLFramework路径>追加/include就是SDL头文件路径： $(PROJECT_DIR)/iOSSDLPlayYUV/SDLFramework/include 2. 配置头文件路径 选中Target>Build Setting>搜索Header Search>选中Header Search Paths>增加上面复制好头文件路径。 第六步：修改main.m 引入SDL头文件，在main函数拷贝“SDL播放流程”的代码即可。 注意：打印日志的方式需要修改为iOS的方式，需要检查下yuv的路径。目前播放还是黑屏，待解决。 #import #include // 引入SDL头文件 #include \"SDL.h\" // SDL入口 int main(int argc, char * argv[]) { // SDL播放YUV实现 // 拷贝SDL播放流程的代码 return 0; } "},"pages/ffmpeg/iOS_integrated_SDL.html":{"url":"pages/ffmpeg/iOS_integrated_SDL.html","title":"iOS集成SDL","keywords":"","body":"iOS集成SDL 代码工程 下载SDL源码 SDL2-2.0.5下载脚本：download-sdl.sh。 sh download-sdl.sh 编译SDL 打开SDL2-2.0.5/Xcode-iOS/SDL工程，选择libSDL目标，再选择Any iOS Device真机编译，编译完成后可以在工程的Products看到libSDL2.a由红色变为了白色，说明静态库已经编译好了，右键show in Finder获取生成好的静态库。 iOS文档位置：源码/docs/README-ios.md。 新建工程 删除Scenedelegate，参考：Xcode 11新建项目多了Scenedelegate。 导入库文件 在工程目录新建SDL2-2.0.5/lib，拷贝已经编译好的libSDL2.a至SDL2-2.0.5/lib，继续拷贝SDL2-2.0.5/include至SDL2-2.0.5，最后将SDL2-2.0.5通过Add Files加入工程。 配置头文件 1) 复制头文件路径 选中Target>Build Setting>搜索Library Search>双击Library Search Paths复制SDL lib路径>修改lib为include就是SDL头文件路径： $(PROJECT_DIR)/iOSIntegrationWithSDL（工程名）/SDL2-2.0.5/include 2) 配置头文件路径 选中Target>Build Setting>搜索Header Search>选中Header Search Paths>增加上面复制好头文件路径。 添加依赖库 AudioToolbox.framework AVFoundation.framework CoreAudio.framework CoreGraphics.framework CoreMotion.framework Foundation.framework GameController.framework OpenGLES.framework QuartzCore.framework UIKit.framework 添加完毕，编译成功。 简单测试 在main.m中引入SDL头文件，编译，编译成功就可以使用SDL开发了。 import \"SDL.h\" "},"pages/ffmpeg/iOS_integrated_SDL_source_code.html":{"url":"pages/ffmpeg/iOS_integrated_SDL_source_code.html","title":"iOS集成SDL（源码）","keywords":"","body":"iOS集成SDL（源码） 代码工程 下载SDL源码 SDL2-2.0.5下载脚本：download-sdl.sh。 sh download-sdl.sh iOS文档位置：源码/docs/README-ios.md。 新建工程 删除Scenedelegate，参考：Xcode 11新建项目多了Scenedelegate。 导入SDL工程 1）将SDL2-2.0.5/Xcode-iOS/SDL/SDL.xcodeproj工程通过Add Files加入工程。 2）选中Target->Build Phases->Link Binary With Libraries->点击+增加libSDL2.a。 3）选中Target>Build Setting>搜索Header Search>选中User Header Search Paths>源码include相对位置（例：../SDL2-2.0.5/include）。 添加依赖库 AudioToolbox.framework AVFoundation.framework CoreAudio.framework CoreGraphics.framework CoreMotion.framework Foundation.framework GameController.framework OpenGLES.framework QuartzCore.framework UIKit.framework 添加完毕，编译成功。 简单测试 在main.m中引入SDL头文件，编译，编译成功就可以使用SDL开发了。 import \"SDL.h\" 遇到问题 1）SDL工程编译，GCDevice报错 解决：选中SDL的PROJECT->iOS Deployment Target->修改为9.0（源码里的好像是5.1）。 GCDevice编译错误 2）引入SDL.h后无法编译 解决：User Header Search Paths配置的SDL头文件位置错误，修改正确即可。 "},"pages/ffmpeg/FFmpeg_SDL_play_video.html":{"url":"pages/ffmpeg/FFmpeg_SDL_play_video.html","title":"FFmpeg+SDL播放视频","keywords":"","body":"FFmpeg+SDL播放视频 Android工程代码 一、代码实现 第一步：注册组件 // 第一步：注册组件 av_register_all(); 第二步：打开封装格式 // 第二步：打开封装格式 // 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 // 参数一：封装格式上下文 // 作用：保存整个视频信息(解码器、编码器等等...) // 信息：码率、帧率等... AVFormatContext* avformat_context = avformat_alloc_context(); // 参数二：视频路径 const char *url = \"/storage/emulated/0/Download/test.mov\"; // 参数三：指定输入的格式 // 参数四：设置默认参数 int avformat_open_input_result = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_result != 0){ // 安卓平台下log __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开文件失败\"); // iOS平台下log // NSLog(\"打开文件失败\"); // 不同的平台替换不同平台log日志 return -1; } 第三步：查找视频流，拿到视频信息 // 第三步：查找视频流，拿到视频信息 // 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 // 参数一：封装格式上下文 // 参数二：指定默认配置 int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result 第四步：查找视频解码器 // 第四步：查找视频解码器 // 4.1 查找视频流索引位置 int av_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 判断流类型：视频流、音频流、字母流等等... if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_stream_index = i; break; } } // 4.2 根据视频流索引，获取解码器上下文 AVCodecContext *avcodec_context = avformat_context->streams[av_stream_index]->codec; // 4.3 根据解码器上下文，获得解码器ID，然后查找解码器 AVCodec *avcodec = avcodec_find_decoder(avcodec_context->codec_id); 第五步：打开解码器 // 第五步：打开解码器 int avcodec_open2_result = avcodec_open2(avcodec_context, avcodec, NULL); if (avcodec_open2_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开解码器失败\"); return -1; } // 测试一下 // 打印信息 __android_log_print(ANDROID_LOG_INFO, \"main\", \"解码器名称：%s\", avcodec->name); 第六步：定义类型转换参数 // 第六步：定义类型转换参数 // 6.1 设置图像转换像素格式为AV_PIX_FMT_YUV420P // 参数一：源文件->原始视频像素数据格式宽 // 参数二：源文件->原始视频像素数据格式高 // 参数三：源文件->原始视频像素数据格式类型 // 参数四：目标文件->目标视频像素数据格式宽 // 参数五：目标文件->目标视频像素数据格式高 // 参数六：目标文件->目标视频像素数据格式类型 SwsContext *swscontext = sws_getContext(avcodec_context->width, avcodec_context->height, avcodec_context->pix_fmt, avcodec_context->width, avcodec_context->height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); // 6.2 解码后的视频信息结构体 // 视频压缩数据：H264 AVFrame* avframe_in = av_frame_alloc(); // 定义解码结果 int decode_result = 0; // 6.3 保存转换为AV_PIX_FMT_YUV420P格式的视频帧 // 视频采样数据：YUV格式 AVFrame* avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型->yuv420类型 // 得到YUV420P缓冲区大小 // 参数一：视频像素数据格式类型->YUV420P格式 // 参数二：一帧视频像素数据宽 = 视频宽 // 参数三：一帧视频像素数据高 = 视频高 // 参数四：字节对齐方式->默认是1 int buffer_size = av_image_get_buffer_size(AV_PIX_FMT_YUV420P, avcodec_context->width, avcodec_context->height, 1); // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 // 参数一：目标->填充数据(avframe_yuv420p) // 参数二：目标->每一行大小 // 参数三：原始数据 // 参数四：目标->格式类型 // 参数五：宽 // 参数六：高 // 参数七：字节对齐方式 av_image_fill_arrays(avframe_yuv420p->data, avframe_yuv420p->linesize, out_buffer, AV_PIX_FMT_YUV420P, avcodec_context->width, avcodec_context->height, 1); 第七步：初始化SDL多媒体框架 // 第七步：初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } 第八步：初始化SDL窗口 // 第八步：初始化SDL窗口 // 参数一：窗口名称 // 参数二：窗口在屏幕上的x坐标 // 参数三：窗口在屏幕上的y坐标 // 参数四：窗口在屏幕上宽 // 参数五：窗口在屏幕上高 // 参数六：窗口状态(打开) SDL_Window* sdl_window = SDL_CreateWindow(\"SDL播放YUV视频\", SDL_WINDOWPOS_CENTERED, SDL_WINDOWPOS_CENTERED, avcodec_context->width, avcodec_context->height, SDL_WINDOW_OPENGL); if (sdl_window == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建成功width：%d，height：%d\", avcodec_context->width,avcodec_context->height); 第九步：创建渲染器->渲染窗口 // 第九步：创建渲染器->渲染窗口 // 参数一：渲染目标创建->目标 // 参数二：从那里开始渲染(-1:表示从第一个位置开始) // 参数三：渲染类型(软件渲染) SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window, -1, 0); if (sdl_renderer == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 定义渲染器区域 SDL_Rect sdl_rect; 第十步：创建纹理 // 第十步：创建纹理 // 参数一：纹理->目标渲染器 // 参数二：渲染格式->YUV格式->像素数据格式(视频)或者是音频采样数据格式(音频) // 参数三：绘制方式->频繁绘制->SDL_TEXTUREACCESS_STREAMING // 参数四：纹理宽 // 参数五：纹理高 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer, SDL_PIXELFORMAT_IYUV, SDL_TEXTUREACCESS_STREAMING, avcodec_context->width, avcodec_context->height); if (sdl_texture == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第十一步：读取视频压缩数据帧 // 第十一步：读取视频压缩数据帧 int current_index = 0; // 写入时yuv数据位置 int y_size, u_size, v_size; // 分析av_read_frame参数。 // 参数一：封装格式上下文 // 参数二：一帧压缩数据 // 如果是解码视频流，是视频压缩帧数据，例如H264 AVPacket* packet = (AVPacket*)av_malloc(sizeof(AVPacket)); while (av_read_frame(avformat_context, packet) >= 0) { // >=:读取到了 // stream_index == av_stream_index) { // 第十二步：开始视频解码 // ... current_index++; __android_log_print(ANDROID_LOG_INFO, \"main\", \"当前解码第%d帧\", current_index); } } 第十二步：开始视频解码 // 第十二步：开始视频解码 // 发送一帧视频压缩数据 avcodec_send_packet(avcodec_context, packet); // 解码一帧视频数据 decode_result = avcodec_receive_frame(avcodec_context, avframe_in); if (decode_result == 0) { // 视频解码成功 // 第十三步：开始类型转换 // ... // 第十四步：设置纹理数据 // ... // 第十五步：将纹理数据拷贝给渲染器 // ... // 第十六步：呈现画面帧 // ... // 第十七步：渲染每一帧直接间隔时间 // ... } 第十三步：开始类型转换 // 第十三步：开始类型转换 // 将解码出来的视频像素点数据格式统一转类型为yuv420P // 参数一：视频像素数据格式上下文 // 参数二：原来的视频像素数据格式->输入数据 // 参数三：原来的视频像素数据格式->输入画面每一行大小 // 参数四：原来的视频像素数据格式->输入画面每一行开始位置(填写：0->表示从原点开始读取) // 参数五：原来的视频像素数据格式->输入数据行数 // 参数六：转换类型后视频像素数据格式->输出数据 // 参数七：转换类型后视频像素数据格式->输出画面每一行大小 sws_scale(swscontext, (const uint8_t *const *)avframe_in->data, avframe_in->linesize, 0, avcodec_context->height, avframe_yuv420p->data, avframe_yuv420p->linesize); 第十四步：设置纹理数据 // 第十四步：设置纹理数据 // 参数一：纹理 // 参数二：渲染区域 // 参数三：需要渲染数据->视频像素数据帧 // 参数四：帧宽 SDL_UpdateTexture(sdl_texture, NULL, avframe_yuv420p->data[0], avframe_yuv420p->linesize[0]); 第十五步：将纹理数据拷贝给渲染器 // 第十五步：将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) sdl_rect.x = 100; sdl_rect.y = 100; sdl_rect.w = avcodec_context->width; sdl_rect.h = avcodec_context->height; SDL_RenderClear(sdl_renderer); SDL_RenderCopy(sdl_renderer, sdl_texture, NULL, &sdl_rect); 第十六步：呈现画面帧 // 第十六步：呈现画面帧 SDL_RenderPresent(sdl_renderer); 第十七步：渲染每一帧直接间隔时间 // 第十七步：渲染每一帧直接间隔时间 SDL_Delay(30); 第十八步：释放资源 // 第十八步：释放资源 SDL_DestroyTexture(sdl_texture); SDL_DestroyRenderer(sdl_renderer); 第十九步：退出程序 // 第十九步：退出程序 SDL_Quit(); 第二十步：释放内存资源，关闭解码器 // 第二十步：释放内存资源，关闭解码器 av_packet_free(&packet); av_frame_free(&avframe_in); av_frame_free(&avframe_yuv420p); free(out_buffer); avcodec_close(avcodec_context); avformat_free_context(avformat_context); 一、Android实现 第一步：Android集成SDL 参考：http://www.1221.site/FFmpeg/08_SDL%E6%92%AD%E6%94%BEYUV.html 新建工程名称为AndroidDisplayVideoWhileDecoding，按上面文章配置，能正常播放YUV文件就算成功完成了第一步。 第二步：Android集成FFmpeg 参考：http://www.1221.site/FFmpeg/02_FFmpeg%E9%9B%86%E6%88%90.html 第三步：修改native-lib.cpp #include #include #include #include #include \"SDL.h\" extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include #include \"libavformat/avformat.h\" #include #include } // SDL入口 extern \"C\" int main(int argc, char *argv[]) { // 边解码边显示视频实现 // 复制代码实现 } "},"pages/ffmpeg/FFmpeg_SDL_plays_audio.html":{"url":"pages/ffmpeg/FFmpeg_SDL_plays_audio.html","title":"FFmpeg+SDL播放音频","keywords":"","body":"FFmpeg+SDL播放音频 Android工程代码 一、代码实现 增加头文件 #include 定义一 // 定义一 // SDL读音频缓存的大小 #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 int quit = 0;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 定义二：数据包队列(链表)结构体 // 定义二：数据包队列(链表)结构体 /*-------链表节点结构体------- typedef struct AVPacketList { AVPacket pkt;//链表数据 struct AVPacketList *next;//链表后继节点 } AVPacketList; ---------------------------*/ typedef struct PacketQueue { AVPacketList *first_pkt, *last_pkt;// 队列首尾节点指针 int nb_packets;// 队列长度 int size;// 保存编码数据的缓存长度，size=packet->size SDL_mutex *qlock;// 队列互斥量，保护队列数据 SDL_cond *qready;// 队列就绪条件变量 } PacketQueue; PacketQueue audioq;// 定义全局队列对象 定义三：队列初始化函数 // 定义三：队列初始化函数 void packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue));//全零初始化队列结构体对象 q->qlock = SDL_CreateMutex();//创建互斥量对象 q->qready = SDL_CreateCond();//创建条件变量对象 } 定义四：向队列中插入数据包 // 定义四：向队列中插入数据包 int packet_queue_put(PacketQueue *q, AVPacket *pkt) { /*-------准备队列(链表)节点对象------*/ AVPacketList *pktlist;// 创建链表节点对象指针 pktlist = static_cast(av_malloc(sizeof(AVPacketList)));// 在堆上创建链表节点对象 if (!pktlist) {// 检查链表节点对象是否创建成功 return -1; } pktlist->pkt = *pkt;// 将输入数据包赋值给新建链表节点对象中的数据包对象 pktlist->next = NULL;// 链表后继指针为空 // if (av_packet_ref(pkt, pkt)qlock);// 队列互斥量加锁，保护队列数据 if (!q->last_pkt) {// 检查队列尾节点是否存在(检查队列是否为空) q->first_pkt = pktlist;// 若不存在(队列尾空)，则将当前节点作队列为首节点 } else { q->last_pkt->next = pktlist;// 若已存在尾节点，则将当前节点挂到尾节点的后继指针上，并作为新的尾节点 } q->last_pkt = pktlist;// 将当前节点作为新的尾节点 q->nb_packets++;// 队列长度+1 q->size += pktlist->pkt.size;// 更新队列编码数据的缓存长度 SDL_CondSignal(q->qready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(q->qlock);// 释放互斥量 return 0; } 定义五：从队列中提取数据包，并将提取的数据包出队列 // 定义五：从队列中提取数据包，并将提取的数据包出队列 static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block) { AVPacketList *pktlist;// 临时链表节点对象指针 int ret;// 操作结果 SDL_LockMutex(q->qlock);// 队列互斥量加锁，保护队列数据 for (;;) { if (quit) {// 检查退出进程标识 ret = -1;// 操作失败 break; } pktlist = q->first_pkt;// 传递将队列首个数据包指针 if (pktlist) {// 检查数据包是否为空(队列是否有数据) q->first_pkt = pktlist->next;// 队列首节点指针后移 if (!q->first_pkt) {// 检查首节点的后继节点是否存在 q->last_pkt = NULL;// 若不存在，则将尾节点指针置空 } q->nb_packets--;// 队列长度-1 q->size -= pktlist->pkt.size;// 更新队列编码数据的缓存长度 *pkt = pktlist->pkt;// 将队列首节点数据返回 av_free(pktlist);// 清空临时节点数据(清空首节点数据，首节点出队列) ret = 1;// 操作成功 break; } else if (!block) { ret = 0; break; } else {// 队列处于未就绪状态，此时通过SDL_CondWait函数等待qready就绪信号，并暂时对互斥量解锁 /*--------------------- * 等待队列就绪信号qready，并对互斥量暂时解锁 * 此时线程处于阻塞状态，并置于等待条件就绪的线程列表上 * 使得该线程只在临界区资源就绪后才被唤醒，而不至于线程被频繁切换 * 该函数返回时，互斥量再次被锁住，并执行后续操作 --------------------*/ SDL_CondWait(q->qready, q->qlock);// 暂时解锁互斥量并将自己阻塞，等待临界区资源就绪(等待SDL_CondSignal发出临界区资源就绪的信号) } }// end for for-loop SDL_UnlockMutex(q->qlock);// 释放互斥量 return ret; } 定义六：音频解码 /*--------------------------- * 定义六：音频解码 * 从缓存队列中提取数据包、解码，并返回解码后的数据长度(对一个完整的packet解码，将解码数据写入audio_buf缓存，并返回多帧解码数据的总长度) * aCodecCtx:音频解码器上下文 * audio_buf：保存解码一个完整的packe后的原始音频数据(缓存中可能包含多帧解码后的音频数据) * buf_size：解码后的音频数据长度，未使用 --------------------------*/ int audio_decode_frame(AVCodecContext *aCodecCtx, uint8_t *audio_buf, int buf_size) { static AVPacket pkt;// 保存从队列中提取的数据包 static AVFrame frame;// 保存从数据包中解码的音频数据 static uint8_t *audio_pkt_data = NULL;// 保存数据包编码数据缓存指针 static int audio_pkt_size = 0;// 数据包中剩余的编码数据长度 int coded_consumed_size, data_size = 0;// 每次消耗的编码数据长度[input](len1)，输出原始音频数据的缓存长度[output] for (;;) { while(audio_pkt_size>0) {// 检查缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) int got_frame = 0;// 解码操作成功标识，成功返回非零值 coded_consumed_size=avcodec_decode_audio4(aCodecCtx,&frame,&got_frame,&pkt);//解码一帧音频数据，并返回消耗的编码数据长度 if (coded_consumed_size channels,frame.nb_samples,aCodecCtx->sample_fmt,1); memcpy(audio_buf, frame.data[0], data_size);// 将解码数据复制到输出缓存 } if (data_size 定义七：音频输出回调函数 /*------Audio Callback------- * 定义七：音频输出回调函数 * sdl通过该回调函数将解码后的pcm数据送入声卡播放, * sdl通常一次会准备一组缓存pcm数据，通过该回调送入声卡，声卡根据音频pts依次播放pcm数据 * 待送入缓存的pcm数据完成播放后，再载入一组新的pcm缓存数据(每次音频输出缓存为空时，sdl就调用此函数填充音频输出缓存，并送入声卡播放) * When we begin playing audio, SDL will continually call this callback function * and ask it to fill the audio buffer with a certain number of bytes * The audio function callback takes the following parameters: * stream: A pointer to the audio buffer to be filled，输出音频数据到声卡缓存 * len: The length (in bytes) of the audio buffer,缓存长度wanted_spec.samples=SDL_AUDIO_BUFFER_SIZE(1024) --------------------------*/ void audio_callback(void *userdata, Uint8 *stream, int len) { AVCodecContext *aCodecCtx = (AVCodecContext *)userdata;// 传递用户数据 int wt_stream_len, audio_size;// 每次写入stream的数据长度，解码后的数据长度 static uint8_t audio_buf[(MAX_AUDIO_FRAME_SIZE*3)/2];// 保存解码一个packet后的多帧原始音频数据 static unsigned int audio_buf_size = 0;// 解码后的多帧音频数据长度 static unsigned int audio_buf_index = 0;// 累计写入stream的长度 while (len>0) {// 检查音频缓存的剩余长度 if (audio_buf_index >= audio_buf_size) {// 检查是否需要执行解码操作 // We have already sent all our data; get more，从缓存队列中提取数据包、解码，并返回解码后的数据长度，audio_buf缓存中可能包含多帧解码后的音频数据 audio_size = audio_decode_frame(aCodecCtx, audio_buf, audio_buf_size); if (audio_size len) {// 检查每次写入缓存的数据长度是否超过指定长度(1024) wt_stream_len = len;// 指定长度从解码的缓存中取数据 } // 每次从解码的缓存数据中以指定长度抽取数据并写入stream传递给声卡 memcpy(stream,(uint8_t*)audio_buf+audio_buf_index,wt_stream_len); len -= wt_stream_len;// 更新解码音频缓存的剩余长度 stream += wt_stream_len;// 更新缓存写入位置 audio_buf_index += wt_stream_len;// 更新累计写入缓存数据长度 }// end for while } 第一步：注册组件 /*------------------------- * 第一步：注册组件 * 注册所有ffmpeg支持的多媒体格式及编解码器 -------------------------*/ av_register_all(); 第二步：打开封装格式 /*------------------------- * 第二步：打开封装格式 * 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 * 参数一：封装格式上下文 * 参数二：视频路径 * 参数三：指定输入的格式 * 参数四：设置默认参数 --------------------------*/ AVFormatContext* avformat_context = avformat_alloc_context();// 参数一：封装格式上下文 const char *url = \"/storage/emulated/0/Download/test.mov\";// 参数二：视频路径 int avformat_open_input_result = avformat_open_input(&avformat_context, url, NULL, NULL); if (avformat_open_input_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"查找音视频流\\n\"); return -1; } 第三步：查找码流 /*------------------------- * 第三步：查找码流 * 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 * 参数一：封装格式上下文 * 参数二：指定默认配置 -------------------------*/ int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result 第四步：查找解码器 // 第四步：查找解码器 // 视频流类型标号初始化为-1 int av_video_stream_index = -1; // 音频流类型标号初始化为-1 int av_audio_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 若文件中包含有视频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_video_stream_index = i; } // 若文件中包含有音频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_audio_stream_index = i; } } // 检查文件中是否存在视频流 if (av_video_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到视频流\\n\"); return -1; } // 检查文件中是否存在音频流 if (av_audio_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到音频流\\n\"); return -1; } // 根据流类型标号从avformat_context->streams中取得流对应的解码器上下文 AVCodecContext *video_avcodec_context = avformat_context->streams[av_video_stream_index]->codec; AVCodecContext *audio_avcodec_context = avformat_context->streams[av_audio_stream_index]->codec; // 根据流对应的解码器上下文查找对应的解码器，返回对应的解码器(信息结构体) AVCodec *video_avcodec = avcodec_find_decoder(video_avcodec_context->codec_id); AVCodec *audio_avcodec = avcodec_find_decoder(audio_avcodec_context->codec_id); // 检查视频解码器 if (!video_avcodec) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没找到视频解码器\\n\"); return -1; } // 检查音频解码器 if (!audio_avcodec) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没找到音频解码器\\n\"); return -1; } 第五步：打开解码器 // 第五步：打开解码器 // 打开视频解码器 int avcodec_open2_result = avcodec_open2(video_avcodec_context, video_avcodec, NULL); if (avcodec_open2_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开视频解码器失败\\n\"); return -1; } // 打开音频解码器 avcodec_open2_result = avcodec_open2(audio_avcodec_context, audio_avcodec, NULL); if (avcodec_open2_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"打开音频解码器失败\\n\"); return -1; } // 打印解码器信息 __android_log_print(ANDROID_LOG_INFO, \"main\", \"视频解码器：%s\\n\", video_avcodec->name); __android_log_print(ANDROID_LOG_INFO, \"main\", \"音频解码器：%s\\n\", audio_avcodec->name); 第六步：定义类型转换参数 /*------------------------- * 第六步：定义类型转换参数 * 参数一：原始视频像素数据格式宽 * 参数二：原始视频像素数据格式高 * 参数三：原始视频像素数据格式类型 * 参数四：目标视频像素数据格式宽 * 参数五：目标视频像素数据格式高 * 参数六：目标视频像素数据格式类型 -------------------------*/ // 设置图像转换像素格式为AV_PIX_FMT_YUV420P SwsContext *swscontext = sws_getContext(video_avcodec_context->width, video_avcodec_context->height, video_avcodec_context->pix_fmt, video_avcodec_context->width, video_avcodec_context->height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); // 保存音视频解码后的数据，如状态信息、编解码器信息、宏块类型表，QP表，运动矢量表等数据 AVFrame* avframe_in = av_frame_alloc(); // 定义解码结果 int decode_result = 0; // AV_PIX_FMT_YUV420P格式的视频帧 AVFrame* avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型 int buffer_size =av_image_get_buffer_size(AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 video_avcodec_context->width,// 一帧视频像素数据宽 = 视频宽 video_avcodec_context->height,// 一帧视频像素数据高 = 视频高 1);// 字节对齐方式，默认是1 // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 av_image_fill_arrays(avframe_yuv420p->data,// 目标视频帧数据 avframe_yuv420p->linesize,// 目标视频帧行大小 out_buffer,// 原始数据 AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 video_avcodec_context->width,// 视频宽 video_avcodec_context->height,//视频高 1);// 字节对齐方式 第七步：初始化SDL多媒体框架 // 第七步：初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } 第八步：缓存队列初始化 // 第八步：缓存队列初始化 packet_queue_init(&audioq); 第九步：设置音频播放参数 // 第九步：设置音频播放参数 // SDL_AudioSpec a structure that contains the audio output format，创建 SDL_AudioSpec 结构体，设置音频播放数据 SDL_AudioSpec wanted_spec, spec; // 创建SDL_AudioSpec结构体，设置音频播放参数 // 采样频率 DSP frequency -- samples per second wanted_spec.freq = audio_avcodec_context->sample_rate; // 采样格式 Audio data format wanted_spec.format = AUDIO_S16SYS; // 声道数 Number of channels: 1 mono, 2 stereo wanted_spec.channels = audio_avcodec_context->channels; wanted_spec.silence = 0;// 无输出时是否静音 // 默认每次读音频缓存的大小，推荐值为 512~8192，ffplay使用的是1024 wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE; // 设置取音频数据的回调接口函数 the function to call when the audio device needs more data wanted_spec.callback = audio_callback; // 传递用户数据 wanted_spec.userdata = audio_avcodec_context; 第十步：打开音频设备 /*-------------------------- * 第十步：打开音频设备 * 以指定参数打开音频设备，并返回与指定参数最为接近的参数，该参数为设备实际支持的音频参数 * Opens the audio device with the desired parameters(wanted_spec) * return another specs we actually be using * and not guaranteed to get what we asked for --------------------------*/ if (SDL_OpenAudio(&wanted_spec, &spec) 第十一步：初始化SDL窗口 // 第十一步：初始化SDL窗口 SDL_Window* sdl_window = SDL_CreateWindow(\"FFmpeg+SDL播放视频\",// 参数一：窗口名称 SDL_WINDOWPOS_CENTERED,// 参数二：窗口在屏幕上的x坐标 SDL_WINDOWPOS_CENTERED,// 参数三：窗口在屏幕上的y坐标 video_avcodec_context->width,// 参数四：窗口在屏幕上宽 video_avcodec_context->height,// 参数五：窗口在屏幕上高 SDL_WINDOW_OPENGL);// 参数六：窗口状态(打开) if (sdl_window == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建成功，width：%d，height：%d\", video_avcodec_context->width, video_avcodec_context->height); 第十二步：创建渲染器 // 第十二步：创建渲染器 // 定义渲染器区域 SDL_Rect sdl_rect; SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window,// 渲染目标创建 -1, // 从那里开始渲染(-1:表示从第一个位置开始) 0);// 渲染类型(软件渲染) if (sdl_renderer == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第十三步：创建纹理 // 第十三步：创建纹理 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer,// 渲染器 SDL_PIXELFORMAT_IYUV,// 像素数据格式 SDL_TEXTUREACCESS_STREAMING,// 绘制方式：频繁绘制- video_avcodec_context->width,// 纹理宽 video_avcodec_context->height);// 纹理高 if (sdl_texture == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } 第十四步：读取视频压缩数据帧 // 第十四步：读取视频压缩数据帧 int video_current_index = 0; // 负责保存压缩编码数据相关信息的结构体,每帧图像由一到多个packet包组成 AVPacket* packet = (AVPacket*)av_malloc(sizeof(AVPacket)); // 从文件中依次读取每个图像编码数据包，并存储在AVPacket数据结构中，>=:读取到了，= 0) { // 检查数据包类型是否是视频流 if (packet->stream_index == av_video_stream_index) { /*----------------------- * 第十五步：视频解码 * 解码完整的一帧数据，decode_result返回true * 可能无法通过只解码一个packet就获得一个完整的视频帧frame，可能需要读取多个packet才行 * avcodec_receive_frame()会在解码到完整的一帧时，decode_result为true -----------------------*/ // ... video_current_index++; __android_log_print(ANDROID_LOG_INFO, \"main\", \"当前解码视频第%d帧\", video_current_index); } // 检查数据包类型是否是音频流 else if (packet->stream_index == av_audio_stream_index) { // 第二十一步：向缓存队列中填充编码数据包 // ... } // 字幕流类型标识 else { // 释放AVPacket数据结构中编码数据指针 av_packet_free(&packet); } /*------------------------ * 第二十二步：获取SDL事件 * 在每次循环中从SDL后台队列取事件并填充到SDL_Event对象中 * SDL的事件系统使得你可以接收用户的输入，从而完成一些控制操作 ------------------------*/ // ... } 第十五步：视频解码 /*----------------------- * 第十五步：视频解码 * 解码完整的一帧数据，decode_result返回true * 可能无法通过只解码一个packet就获得一个完整的视频帧frame，可能需要读取多个packet才行 * avcodec_receive_frame()会在解码到完整的一帧时，decode_result为true -----------------------*/ // 发送一帧视频压缩数据 avcodec_send_packet(video_avcodec_context, packet); // 解码一帧视频数据 decode_result = avcodec_receive_frame(video_avcodec_context, avframe_in); if (decode_result == 0) { // 视频解码成功 // 第十六步：开始类型转换 // ... // 第十七步：设置纹理数据 // ... // 第十八步：将纹理数据拷贝给渲染器 // ... // 第十九步：呈现画面帧 // ... // 第二十步：渲染每一帧直接间隔时间 // ... } 第十六步：开始类型转换 // 第十六步：开始类型转换 // 将解码出来的视频像素点数据格式统一转类型为yuv420P sws_scale(swscontext,// 视频像素数据格式上下文 (const uint8_t *const *)avframe_in->data,// 输入数据 avframe_in->linesize,// 输入画面每一行大小 0,// 输入画面每一行开始位置(0表示从原点开始读取) video_avcodec_context->height,// 输入数据行数 avframe_yuv420p->data,// 输出数据 avframe_yuv420p->linesize);// 输出画面每一行大小 第十七步：设置纹理数据 // 第十七步：设置纹理数据 SDL_UpdateTexture(sdl_texture, // 纹理 NULL,// 渲染区域 avframe_yuv420p->data[0],// 需要渲染数据：视频像素数据帧 avframe_yuv420p->linesize[0]);// 帧宽 第十八步：将纹理数据拷贝给渲染器 // 第十八步：将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) sdl_rect.x = 100; sdl_rect.y = 100; sdl_rect.w = video_avcodec_context->width; sdl_rect.h = video_avcodec_context->height; SDL_RenderClear(sdl_renderer); SDL_RenderCopy(sdl_renderer, sdl_texture, NULL, &sdl_rect); 第十九步：呈现画面帧 // 第十九步：呈现画面帧 SDL_RenderPresent(sdl_renderer); 第二十步：渲染每一帧直接间隔时间 // 第二十步：渲染每一帧直接间隔时间 SDL_Delay(30); 第二十一步：向缓存队列中填充编码数据包 // 第二十一步：向缓存队列中填充编码数据包 packet_queue_put(&audioq, packet); 第二十二步：获取SDL事件 /*------------------------ * 第二十二步：获取SDL事件 * 在每次循环中从SDL后台队列取事件并填充到SDL_Event对象中 * SDL的事件系统使得你可以接收用户的输入，从而完成一些控制操作 ------------------------*/ SDL_Event event;//SDL事件对象 SDL_PollEvent(&event); switch (event.type) {//检查SDL事件对象 case SDL_QUIT://退出事件 quit = 1;//退出进程标识置1 SDL_Quit();//退出操作 exit(0);//结束进程 break; default: break; }// end for switch 第二十三步：释放资源，退出程序 // 第二十三步：释放资源，退出程序 av_packet_free(&packet); av_frame_free(&avframe_in); av_frame_free(&avframe_yuv420p); free(out_buffer); avcodec_close(video_avcodec_context); avformat_free_context(avformat_context); SDL_DestroyTexture(sdl_texture); SDL_DestroyRenderer(sdl_renderer); SDL_Quit(); 一、Android实现 第一步：Android集成SDL 参考：http://www.1221.site/FFmpeg/08_SDL%E6%92%AD%E6%94%BEYUV.html 新建工程名称为AndroidDisplayVideoWhileDecoding，按上面文章配置，能正常播放YUV文件就算成功完成了第一步。 第二步：Android集成FFmpeg 参考：http://www.1221.site/FFmpeg/02_FFmpeg%E9%9B%86%E6%88%90.html 第三步：修改native-lib.cpp #include #include #include #include #include \"SDL.h\" #include extern \"C\" { // 引入头文件 // 核心库->音视频编解码库 #include #include \"libavformat/avformat.h\" #include #include } // 定义一 // SDL读音频缓存的大小 #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 int quit = 0;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 // SDL入口 extern \"C\" int main(int argc, char *argv[]) { // 边解码边播放音视频实现 // 复制代码实现 } "},"pages/ffmpeg/FFmpeg_SDL_creates_thread.html":{"url":"pages/ffmpeg/FFmpeg_SDL_creates_thread.html","title":"FFmpeg+SDL创建线程","keywords":"","body":"FFmpeg+SDL创建线程 Android工程代码 ffmpeg播放器实现详解 - 创建线程：https://www.cnblogs.com/breakpointlab/p/13508271.html 源代码一览 #include #include #include #include #include \"SDL.h\" #include #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 #define MAX_AUDIOQ_SIZE (5 * 16 * 1024) #define MAX_VIDEOQ_SIZE (5 * 256 * 1024) #define FF_ALLOC_EVENT (SDL_USEREVENT) #define FF_REFRESH_EVENT (SDL_USEREVENT + 1) #define FF_QUIT_EVENT (SDL_USEREVENT + 2) #define VIDEO_PICTURE_QUEUE_SIZE 1 extern \"C\" { #include #include \"libavformat/avformat.h\" #include #include #include } /*-------链表节点结构体-------- typedef struct AVPacketList { AVPacket pkt;//链表数据 struct AVPacketList *next;//链表后继节点 } AVPacketList; ---------------------------*/ // 数据包队列(链表)结构体 typedef struct PacketQueue { AVPacketList *first_pkt, *last_pkt;// 队列首尾节点指针 int nb_packets;// 队列长度 int size;// 保存编码数据的缓存长度，size=packet->size SDL_mutex *qlock;// 队列互斥量，保护队列数据 SDL_cond *qready;// 队列就绪条件变量 } PacketQueue; // 图像帧结构体 typedef struct VideoPicture { AVFrame *avframe_yuv420p; int width, height;//Source height & width. int allocated;//是否分配内存空间，视频帧转换为SDL overlay标识 } VideoPicture; typedef struct VideoState { AVFormatContext *pFormatCtx;// 保存文件容器封装信息及码流参数的结构体 AVStream *video_st;// 视频流信息结构体 AVStream *audio_st;//音频流信息结构体 struct SwsContext *sws_ctx;// 描述转换器参数的结构体 PacketQueue videoq;// 视频编码数据包队列(编码数据队列，以链表方式实现) VideoPicture pictq[VIDEO_PICTURE_QUEUE_SIZE]; int pictq_size, pictq_rindex, pictq_windex;// 队列长度，读/写位置索引 SDL_mutex *pictq_lock;// 队列读写锁对象，保护图像帧队列数据 SDL_cond *pictq_ready;// 队列就绪条件变量 SDL_Rect sdl_rect; SDL_Renderer* sdl_renderer; SDL_Texture* sdl_texture; PacketQueue audioq;// 音频编码数据包队列(编码数据队列，以链表方式实现) uint8_t audio_buf[(MAX_AUDIO_FRAME_SIZE*3)/2];//保存解码一个packet后的多帧原始音频数据(解码数据队列，以数组方式实现) unsigned int audio_buf_size;//解码后的多帧音频数据长度 unsigned int audio_buf_index;//累计写入stream的长度 uint8_t *audio_pkt_data;//编码数据缓存指针位置 int audio_pkt_size;//缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) AVPacket audio_pkt;//保存从队列中提取的数据包 AVFrame audio_frame;//保存从数据包中解码的音频数据 int video_width; int video_height; char filename[1024];// 输入文件完整路径名 int videoStream, audioStream;// 音视频流类型标号 SDL_Thread *parse_tid;// 编码数据包解析线程id SDL_Thread *decode_tid;// 解码线程id int quit;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 } VideoState;// Since we only have one decoding thread, the Big Struct can be global in case we need it. VideoState *global_video_state; // 定时器触发的回调函数 static Uint32 sdl_refresh_timer_cb(Uint32 interval, void *opaque) { SDL_Event event;//SDL事件对象 event.type = FF_REFRESH_EVENT;//视频显示刷新事件 event.user.data1 = opaque;//传递用户数据 SDL_PushEvent(&event);//发送事件 return 0; // 0 means stop timer. } /*--------------------------- * Schedule a video refresh in 'delay' ms. * 告诉sdl在指定的延时后来推送一个 FF_REFRESH_EVENT 事件 * 这个事件将在事件队列里触发sdl_refresh_timer_cb函数的调用 --------------------------*/ static void schedule_refresh(VideoState *is, int delay) { SDL_AddTimer(delay, sdl_refresh_timer_cb, is);//在指定的时间(ms)后回调用户指定的函数 } // 视频(图像)帧渲染 void video_display(VideoState *is) { SDL_Rect rect;// SDL矩形对象 VideoPicture *vp;// 图像帧结构体指针 vp = &is->pictq[is->pictq_rindex];//从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // 设置纹理数据 SDL_UpdateTexture(is->sdl_texture, // 纹理 NULL,// 渲染区域 vp->avframe_yuv420p->data[0],// 需要渲染数据：视频像素数据帧 vp->avframe_yuv420p->linesize[0]);// 帧宽 // 将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) is->sdl_rect.x = 100; is->sdl_rect.y = 100; is->sdl_rect.w = vp->width; is->sdl_rect.h = vp->height; SDL_RenderClear(is->sdl_renderer); SDL_RenderCopy(is->sdl_renderer, is->sdl_texture, NULL, &is->sdl_rect); // 呈现画面帧 SDL_RenderPresent(is->sdl_renderer); }// end for if }// end for video_display // 显示刷新函数(FF_REFRESH_EVENT响应函数) void video_refresh_timer(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 // vp is used in later tutorials for synchronization. if (is->video_st) { if (is->pictq_size == 0) {// 检查图像帧队列是否有待显示图像 schedule_refresh(is, 1); } else {// 刷新图像 /*------------------------- * Now, normally here goes a ton of code about timing, etc. * we're just going to guess at a delay for now. * You can increase and decrease this value and hard code the timing * but I don't suggest that ;) We'll learn how to do it for real later.. ------------------------*/ schedule_refresh(is, 40);// 设置显示下一帧图像的刷新时间，通过定时器timer方式触发 // Show the picture! video_display(is);// 图像帧渲染 // Update queue for next picture! if (++is->pictq_rindex == VIDEO_PICTURE_QUEUE_SIZE) {// 更新并检查图像帧队列读位置索引 is->pictq_rindex = 0;// 重置读位置索引 } SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 is->pictq_size--;// 更新图像帧队列长度 SDL_CondSignal(is->pictq_ready);// 发送队列就绪信号 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } } else { schedule_refresh(is, 100); } } // 数据包队列初始化函数 void packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue));// 全零初始化队列结构体对象 q->qlock = SDL_CreateMutex();// 创建互斥量对象 q->qready = SDL_CreateCond();// 创建条件变量对象 } // 向队列中插入数据包 int packet_queue_put(PacketQueue *q, AVPacket *pkt) { /*-------准备队列(链表)节点对象------*/ AVPacketList *pktlist=(AVPacketList *)av_malloc(sizeof(AVPacketList));// 在堆上创建链表节点对象 if (!pktlist) {// 检查链表节点对象是否创建成功 return -1; } pktlist->pkt = *pkt;// 将输入数据包赋值给新建链表节点对象中的数据包对象 pktlist->next = NULL;// 链表后继指针为空 // if (av_packet_ref(pkt, pkt) qlock);// 队列互斥量加锁，保护队列数据 if (!q->last_pkt) {// 检查队列尾节点是否存在(检查队列是否为空) q->first_pkt = pktlist;// 若不存在(队列尾空)，则将当前节点作队列为首节点 } else { q->last_pkt->next = pktlist;// 若已存在尾节点，则将当前节点挂到尾节点的后继指针上，并作为新的尾节点 } q->last_pkt = pktlist;// 将当前节点作为新的尾节点 q->nb_packets++;// 队列长度+1 q->size += pktlist->pkt.size;// 更新队列编码数据的缓存长度 SDL_CondSignal(q->qready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(q->qlock);// 释放互斥量 return 0; } // 从队列中提取数据包，并将提取的数据包出队列 static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block) { AVPacketList *pktlist;// 临时链表节点对象指针 int ret;// 操作结果 SDL_LockMutex(q->qlock);// 队列互斥量加锁，保护队列数据 for (;;) { if (global_video_state->quit) {// 检查退出进程标识 ret = -1;// 操作失败 break; }//end for if pktlist = q->first_pkt;// 传递将队列首个数据包指针 if (pktlist) {// 检查数据包是否为空(队列是否有数据) q->first_pkt = pktlist->next;// 队列首节点指针后移 if (!q->first_pkt) {// 检查首节点的后继节点是否存在 q->last_pkt = NULL;// 若不存在，则将尾节点指针置空 } q->nb_packets--;// 队列长度-1 q->size -= pktlist->pkt.size;// 更新队列编码数据的缓存长度 *pkt = pktlist->pkt;// 将队列首节点数据返回 av_free(pktlist);// 清空临时节点数据(清空首节点数据，首节点出队列) ret = 1;// 操作成功 break; } else if (!block) { ret = 0; break; } else {// 队列处于未就绪状态，此时通过SDL_CondWait函数等待qready就绪信号，并暂时对互斥量解锁 /*--------------------- * 等待队列就绪信号qready，并对互斥量暂时解锁 * 此时线程处于阻塞状态，并置于等待条件就绪的线程列表上 * 使得该线程只在临界区资源就绪后才被唤醒，而不至于线程被频繁切换 * 该函数返回时，互斥量再次被锁住，并执行后续操作 --------------------*/ SDL_CondWait(q->qready, q->qlock);// 暂时解锁互斥量并将自己阻塞，等待临界区资源就绪(等待SDL_CondSignal发出临界区资源就绪的信号) } }//end for for-loop SDL_UnlockMutex(q->qlock);// 释放互斥量 return ret; } // 创建/重置图像帧，为图像帧分配内存空间 void alloc_picture(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {// 检查图像帧是否已存在 // We already have one make another, bigger/smaller. av_frame_free(&vp->avframe_yuv420p); } vp->width = is->video_st->codec->width;// 设置图像帧宽度 vp->height = is->video_st->codec->height;// 设置图像帧高度 SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 vp->allocated = 1;// 图像帧像素缓冲区已分配内存 // AV_PIX_FMT_YUV420P格式的视频帧 vp->avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型 int buffer_size =av_image_get_buffer_size(AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 一帧视频像素数据宽 = 视频宽 is->video_st->codec->height,// 一帧视频像素数据高 = 视频高 1);// 字节对齐方式，默认是1 // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 av_image_fill_arrays(vp->avframe_yuv420p->data,// 目标视频帧数据 vp->avframe_yuv420p->linesize,// 目标视频帧行大小 out_buffer,// 原始数据 AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 视频宽 is->video_st->codec->height,//视频高 1);// 字节对齐方式 SDL_CondSignal(is->pictq_ready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } /*--------------------------- * queue_picture：图像帧插入队列等待渲染 * @is：全局状态参数集 * @pFrame：保存图像解码数据的结构体 * 1、首先检查图像帧队列(数组)是否存在空间插入新的图像，若没有足够的空间插入图像则使当前线程休眠等待 * 2、在初始化的条件下，队列(数组)中VideoPicture的bmp对象(YUV overlay)尚未分配空间，通过FF_ALLOC_EVENT事件的方法调用alloc_picture分配空间 * 3、当队列(数组)中所有VideoPicture的bmp对象(YUV overlay)均已分配空间的情况下，直接跳过步骤2向bmp对象拷贝像素数据，像素数据在进行格式转换后执行拷贝操作 ---------------------------*/ int queue_picture(VideoState *is, AVFrame *pFrame) { /*--------1、检查队列是否有插入空间-------*/ // Wait until we have space for a new pic. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (is->pictq_size >= VIDEO_PICTURE_QUEUE_SIZE && !is->quit) {// 检查队列当前长度 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待pictq_ready信号 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } /*-------2、初始化/重置YUV overlay-------*/ // windex is set to 0 initially. VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列中抽取图像帧对象 // Allocate or resize the buffer，检查YUV overlay是否已存在，否则初始化YUV overlay，分配像素缓存空间 if (!vp->avframe_yuv420p || vp->width!=is->video_st->codec->width || vp->height!=is->video_st->codec->height) { vp->allocated = 0;// 图像帧未分配空间 // We have to do it in the main thread. SDL_Event event;// SDL事件对象 event.type = FF_ALLOC_EVENT;//指定分配图像帧内存事件 event.user.data1 = is;//传递用户数据 SDL_PushEvent(&event);//发送SDL事件 // Wait until we have a picture allocated. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (!vp->allocated && !is->quit) {// 检查当前图像帧是否已初始化 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待alloc_picture发送pictq_ready信号唤醒当前线程 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } }// end for if /*--------3、拷贝视频帧到YUV overlay-------*/ // We have a place to put our picture on the queue. if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // Convert the image into YUV format that SDL uses，将解码后的图像帧转换为AV_PIX_FMT_YUV420P格式，并拷贝到图像帧队列 sws_scale(is->sws_ctx, (uint8_t const * const *)pFrame->data, pFrame->linesize, 0, is->video_st->codec->height, vp->avframe_yuv420p->data, vp->avframe_yuv420p->linesize); // Now we inform our display thread that we have a pic ready. if (++is->pictq_windex == VIDEO_PICTURE_QUEUE_SIZE) {//更新并检查当前图像帧队列写入位置 is->pictq_windex = 0;//重置图像帧队列写入位置 } SDL_LockMutex(is->pictq_lock);//锁定队列读写锁，保护队列数据 is->pictq_size++;//更新图像帧队列长度 SDL_UnlockMutex(is->pictq_lock);//释放队列读写锁 }// end for if return 0; } int video_current_index = 0; // 视频解码线程函数 int decode_thread(void *arg) { VideoState *is = (VideoState *) arg;// 传递用户数据 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 int frameFinished;// 解码操作是否成功标识 // Allocate video frame，为解码后的视频信息结构体分配空间并完成初始化操作(结构体中的图像缓存按照下面两步手动安装) AVFrame *pFrame = av_frame_alloc(); for (;;) { if (packet_queue_get(&is->videoq,packet,1)video_st->codec, pFrame, &frameFinished, packet); // Did we get a video frame，检查是否解码出完整一帧图像 if (frameFinished) { if (queue_picture(is, pFrame)audio_pkt;// 保存从队列中提取的数据包 for (;;) { while (is->audio_pkt_size>0) {// 检查缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) int got_frame = 0;// 解码操作成功标识，成功返回非零值 // 解码一帧音频数据，并返回消耗的编码数据长度 coded_consumed_size = avcodec_decode_audio4(is->audio_st->codec, &is->audio_frame, &got_frame, pkt); if (coded_consumed_size audio_pkt_size = 0;// 更新编码数据缓存长度 break; } if (got_frame) {// 检查解码操作是否成功 // 计算解码后音频数据长度[output] data_size = av_samples_get_buffer_size(NULL, is->audio_st->codec->channels, is->audio_frame.nb_samples, is->audio_st->codec->sample_fmt, 1); memcpy(is->audio_buf, is->audio_frame.data[0], data_size);// 将解码数据复制到输出缓存 } is->audio_pkt_data += coded_consumed_size;// 更新编码数据缓存指针位置 is->audio_pkt_size -= coded_consumed_size;// 更新缓存中剩余的编码数据长度 if (data_size data) {// 检查数据包是否已从队列中提取 av_packet_unref(pkt);// 释放pkt中保存的编码数据 } if (is->quit) {// 检查退出进程标识 return -1; } // Next packet，从队列中提取数据包到pkt if (packet_queue_get(&is->audioq, pkt, 1) audio_pkt_data = pkt->data;// 传递编码数据缓存指针 is->audio_pkt_size = pkt->size;// 传递编码数据缓存长度 } } /*------Audio Callback------- * 音频输出回调函数，sdl通过该回调函数将解码后的pcm数据送入声卡播放, * sdl通常一次会准备一组缓存pcm数据，通过该回调送入声卡，声卡根据音频pts依次播放pcm数据 * 待送入缓存的pcm数据完成播放后，再载入一组新的pcm缓存数据(每次音频输出缓存为空时，sdl就调用此函数填充音频输出缓存，并送入声卡播放) * When we begin playing audio, SDL will continually call this callback function * and ask it to fill the audio buffer with a certain number of bytes * The audio function callback takes the following parameters: * stream: A pointer to the audio buffer to be filled，输出音频数据到声卡缓存 * len: The length (in bytes) of the audio buffer,缓存长度wanted_spec.samples=SDL_AUDIO_BUFFER_SIZE(1024) --------------------------*/ void audio_callback(void *userdata, Uint8 *stream, int len) { VideoState *is = (VideoState *) userdata;// 传递用户数据 int wt_stream_len, audio_size;// 每次写入stream的数据长度，解码后的数据长度 while (len > 0) {//检查音频缓存的剩余长度 if (is->audio_buf_index >= is->audio_buf_size) {// 检查是否需要执行解码操作 // We have already sent all our data; get more，从缓存队列中提取数据包、解码，并返回解码后的数据长度，audio_buf缓存中可能包含多帧解码后的音频数据 audio_size = audio_decode_frame(is); if (audio_size audio_buf_size = 1024; memset(is->audio_buf, 0, is->audio_buf_size);// 全零重置缓冲区 } else { is->audio_buf_size = audio_size;// 返回packet中包含的原始音频数据长度(多帧) } is->audio_buf_index = 0;// 初始化累计写入缓存长度 }//end for if wt_stream_len=is->audio_buf_size-is->audio_buf_index;// 计算解码缓存剩余长度 if (wt_stream_len > len) {// 检查每次写入缓存的数据长度是否超过指定长度(1024) wt_stream_len = len;// 指定长度从解码的缓存中取数据 } // 每次从解码的缓存数据中以指定长度抽取数据并写入stream传递给声卡 memcpy(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, wt_stream_len); len -= wt_stream_len;// 更新解码音频缓存的剩余长度 stream += wt_stream_len;// 更新缓存写入位置 is->audio_buf_index += wt_stream_len;// 更新累计写入缓存数据长度 }//end for while } // 根据指定类型打开流，找到对应的解码器、创建对应的音频配置、保存关键信息到 VideoState、启动音频和视频线程 int stream_component_open(VideoState *is, int stream_index) { AVFormatContext *pFormatCtx = is->pFormatCtx;// 传递文件容器的封装信息及码流参数 AVCodecContext *codecCtx = NULL;// 解码器上下文对象，解码器依赖的相关环境、状态、资源以及参数集的接口指针 AVCodec *codec = NULL;// 保存编解码器信息的结构体，提供编码与解码的公共接口，可以看作是编码器与解码器的一个全局变量 //检查输入的流类型是否在合理范围内 if (stream_index = pFormatCtx->nb_streams) { return -1; } // Get a pointer to the codec context for the video stream. codecCtx = pFormatCtx->streams[stream_index]->codec;// 取得解码器上下文 if (codecCtx->codec_type == AVMEDIA_TYPE_AUDIO) {//检查解码器类型是否为音频解码器 SDL_AudioSpec wanted_spec, spec;//SDL_AudioSpec a structure that contains the audio output format，创建 SDL_AudioSpec 结构体，设置音频播放数据 // Set audio settings from codec info,SDL_AudioSpec a structure that contains the audio output format // 创建SDL_AudioSpec结构体，设置音频播放参数 wanted_spec.freq = codecCtx->sample_rate;//采样频率 DSP frequency -- samples per second wanted_spec.format = AUDIO_S16SYS;//采样格式 Audio data format wanted_spec.channels = codecCtx->channels;//声道数 Number of channels: 1 mono, 2 stereo wanted_spec.silence = 0;//无输出时是否静音 //默认每次读音频缓存的大小，推荐值为 512~8192，ffplay使用的是1024 specifies a unit of audio data refers to the size of the audio buffer in sample frames wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE; wanted_spec.callback = audio_callback;//设置读取音频数据的回调接口函数 the function to call when the audio device needs more data wanted_spec.userdata = is;//传递用户数据 /*--------------------------- * 以指定参数打开音频设备，并返回与指定参数最为接近的参数，该参数为设备实际支持的音频参数 * Opens the audio device with the desired parameters(wanted_spec) * return another specs we actually be using * and not guaranteed to get what we asked for --------------------------*/ if (SDL_OpenAudio(&wanted_spec, &spec) codec_id); AVDictionary *optionsDict = NULL; if (!codec || (avcodec_open2(codecCtx, codec, &optionsDict) name); // 检查解码器类型 switch(codecCtx->codec_type) { case AVMEDIA_TYPE_AUDIO:// 音频解码器 is->audioStream = stream_index;// 音频流类型标号初始化 is->audio_st = pFormatCtx->streams[stream_index]; is->audio_buf_size = 0;// 解码后的多帧音频数据长度 is->audio_buf_index = 0;//累 计写入stream的长度 memset(&is->audio_pkt, 0, sizeof(is->audio_pkt)); packet_queue_init(&is->audioq);// 音频数据包队列初始化 SDL_PauseAudio(0);// audio callback starts running again，开启音频设备，如果这时候没有获得数据那么它就静音 break; case AVMEDIA_TYPE_VIDEO:// 视频解码器 is->videoStream = stream_index;// 视频流类型标号初始化 is->video_st = pFormatCtx->streams[stream_index]; packet_queue_init(&is->videoq);// 视频数据包队列初始化 is->decode_tid = SDL_CreateThread(decode_thread,\"视频解码线程\" ,is);// 创建视频解码线程 // Initialize SWS context for software scaling，设置图像转换像素格式为AV_PIX_FMT_YUV420P is->sws_ctx = sws_getContext(is->video_st->codec->width, is->video_st->codec->height, is->video_st->codec->pix_fmt, is->video_st->codec->width, is->video_st->codec->height, AV_PIX_FMT_YUV420P, SWS_BILINEAR, NULL, NULL, NULL); break; default: break; } return 0; } // 编码数据包解析线程函数(从视频文件中解析出音视频编码数据单元，一个AVPacket的data通常对应一个NAL) int parse_thread(void *arg) { VideoState *is = (VideoState *)arg;// 传递用户参数 global_video_state = is;// 传递全局状态参量结构体 /*------------------------- * 打开封装格式 * 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 * 参数一：封装格式上下文 * 参数二：视频路径 * 参数三：指定输入的格式 * 参数四：设置默认参数 --------------------------*/ AVFormatContext *avformat_context = NULL;// 参数一：封装格式上下文 int avformat_open_input_result = avformat_open_input(&avformat_context, is->filename, NULL, NULL); if (avformat_open_input_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"查找音视频流\\n\"); return -1; } is->pFormatCtx = avformat_context;//传递文件容器封装信息及码流参数 /*------------------------- * 查找码流 * 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 * 参数一：封装格式上下文 * 参数二：指定默认配置 -------------------------*/ int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result videoStream = -1;//视频流类型标号初始化为-1 is->audioStream = -1;//音频流类型标号初始化为-1 // 视频流类型标号初始化为-1 int av_video_stream_index = -1; // 音频流类型标号初始化为-1 int av_audio_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 若文件中包含有视频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_video_stream_index = i; } // 若文件中包含有音频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_audio_stream_index = i; } } // 检查文件中是否存在视频流 if (av_video_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到视频流\\n\"); goto fail;//跳转至异常处理 return -1; } // 检查文件中是否存在音频流 if (av_audio_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到音频流\\n\"); goto fail;//跳转至异常处理 return -1; } stream_component_open(is, av_audio_stream_index);// 根据指定类型打开音频流 stream_component_open(is, av_video_stream_index);// 根据指定类型打开视频流 // Main decode loop. for (;;) { if (is->quit) {//检查退出进程标识 break; } // Seek stuff goes here，检查音视频编码数据包队列长度是否溢出 if (is->audioq.size > MAX_AUDIOQ_SIZE || is->videoq.size > MAX_VIDEOQ_SIZE) { SDL_Delay(10); continue; } /*----------------------- * read in a packet and store it in the AVPacket struct * ffmpeg allocates the internal data for us,which is pointed to by packet.data * this is freed by the av_free_packet() -----------------------*/ // 负责保存压缩编码数据相关信息的结构体,每帧图像由一到多个packet包组成 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 if (av_read_frame(is->pFormatCtx, packet) pFormatCtx->pb->error == 0) { SDL_Delay(100); // No error; wait for user input. continue; } else { break; } } // Is this a packet from the video stream? if (packet->stream_index == is->videoStream) {// 检查数据包是否为视频类型 packet_queue_put(&is->videoq, packet);// 向队列中插入数据包 } else if (packet->stream_index == is->audioStream) {// 检查数据包是否为音频类型 packet_queue_put(&is->audioq, packet);// 向队列中插入数据包 } else {// 检查数据包是否为字幕类型 av_packet_unref(packet);// 释放packet中保存的(字幕)编码数据 } } // All done - wait for it. while (!is->quit) { SDL_Delay(100); } fail:// 异常处理 if (1) { SDL_Event event;// SDL事件对象 event.type = FF_QUIT_EVENT;// 指定退出事件类型 event.user.data1 = is;// 传递用户数据 SDL_PushEvent(&event);// 将该事件对象压入SDL后台事件队列 } return 0; } int init_sdl(VideoState *is) { // 初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } // 初始化SDL窗口 SDL_Window* sdl_window = SDL_CreateWindow(\"FFmpeg+SDL播放视频\",// 参数一：窗口名称 SDL_WINDOWPOS_CENTERED,// 参数二：窗口在屏幕上的x坐标 SDL_WINDOWPOS_CENTERED,// 参数三：窗口在屏幕上的y坐标 is->video_width,// 参数四：窗口在屏幕上宽 is->video_height,// 参数五：窗口在屏幕上高 SDL_WINDOW_OPENGL);// 参数六：窗口状态(打开) if (sdl_window == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建渲染器 // 定义渲染器区域 SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window,// 渲染目标创建 -1, // 从那里开始渲染(-1:表示从第一个位置开始) 0);// 渲染类型(软件渲染) if (sdl_renderer == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建纹理 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer,// 渲染器 SDL_PIXELFORMAT_IYUV,// 像素数据格式 SDL_TEXTUREACCESS_STREAMING,// 绘制方式：频繁绘制- is->video_width,// 纹理宽 is->video_height);// 纹理高 if (sdl_texture == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } is->sdl_renderer = sdl_renderer; is->sdl_texture = sdl_texture; return 0; } // SDL入口 extern \"C\" int main(int argc, char *argv[]) { /*------------------------- * 注册组件 * 注册所有ffmpeg支持的多媒体格式及编解码器 -------------------------*/ av_register_all(); // 创建全局状态对象 VideoState *is= (VideoState *)av_mallocz(sizeof(VideoState)); av_strlcpy(is->filename, \"/storage/emulated/0/Download/test.mov\", sizeof(is->filename));// 复制视频文件路径名 is->video_width = 640; is->video_height = 352; is->pictq_lock = SDL_CreateMutex();// 创建编码数据包队列互斥锁对象 is->pictq_ready = SDL_CreateCond();// 创建编码数据包队列就绪条件对象 int init_sdl_result = init_sdl(is); if (init_sdl_result parse_tid = SDL_CreateThread(parse_thread, \"编码数据包解析线程\", is); if (!is->parse_tid) {// 检查线程是否创建成功 av_free(is); return -1; } // SDL事件对象 SDL_Event event; for (;;) {// SDL事件循环 SDL_WaitEvent(&event);// 主线程阻塞，等待事件到来 switch(event.type) {// 事件到来后唤醒主线程，检查事件类型 case FF_QUIT_EVENT: case SDL_QUIT:// 退出进程事件 is->quit = 1; // If the video has finished playing, then both the picture and audio queues are waiting for more data. // Make them stop waiting and terminate normally.. avcodec_close(is->video_st->codec); avformat_free_context(is->pFormatCtx); SDL_CondSignal(is->audioq.qready);// 发出队列就绪信号避免死锁 SDL_CondSignal(is->videoq.qready); SDL_DestroyTexture(is->sdl_texture); SDL_DestroyRenderer(is->sdl_renderer); SDL_Quit(); return 0; case FF_ALLOC_EVENT: alloc_picture(event.user.data1);// 分配视频帧事件响应函数 break; case FF_REFRESH_EVENT:// 视频显示刷新事件 video_refresh_timer(event.user.data1);// 视频显示刷新事件响应函数 break; default: break; } } return 0; } "},"pages/ffmpeg/FFmpeg_SDL_synchronized_video.html":{"url":"pages/ffmpeg/FFmpeg_SDL_synchronized_video.html","title":"FFmpeg+SDL同步视频","keywords":"","body":"FFmpeg+SDL同步视频 Android工程代码 ffmpeg播放器实现详解 - 视频同步控制：https://www.cnblogs.com/breakpointlab/p/15771348.html 源代码一览 #include #include #include #include #include \"SDL.h\" #include #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 #define AV_SYNC_THRESHOLD 0.01//前后两帧间的显示时间间隔的最小值0.01s #define AV_NOSYNC_THRESHOLD 10.0//最小刷新间隔时间10ms #define MAX_AUDIOQ_SIZE (5 * 16 * 1024) #define MAX_VIDEOQ_SIZE (5 * 256 * 1024) #define FF_ALLOC_EVENT (SDL_USEREVENT) #define FF_REFRESH_EVENT (SDL_USEREVENT + 1) #define FF_QUIT_EVENT (SDL_USEREVENT + 2) #define VIDEO_PICTURE_QUEUE_SIZE 1 extern \"C\" { #include #include \"libavformat/avformat.h\" #include #include #include #include } /*-------链表节点结构体-------- typedef struct AVPacketList { AVPacket pkt;//链表数据 struct AVPacketList *next;//链表后继节点 } AVPacketList; ---------------------------*/ // 数据包队列(链表)结构体 typedef struct PacketQueue { AVPacketList *first_pkt, *last_pkt;// 队列首尾节点指针 int nb_packets;// 队列长度 int size;// 保存编码数据的缓存长度，size=packet->size SDL_mutex *qlock;// 队列互斥量，保护队列数据 SDL_cond *qready;// 队列就绪条件变量 } PacketQueue; // 图像帧结构体 typedef struct VideoPicture { AVFrame *avframe_yuv420p; int width, height;//Source height & width. int allocated;//是否分配内存空间，视频帧转换为SDL overlay标识 double pts;//当前图像帧的绝对显示时间戳 } VideoPicture; typedef struct VideoState { AVFormatContext *pFormatCtx;// 保存文件容器封装信息及码流参数的结构体 AVStream *video_st;// 视频流信息结构体 AVStream *audio_st;//音频流信息结构体 struct SwsContext *sws_ctx;// 描述转换器参数的结构体 PacketQueue videoq;// 视频编码数据包队列(编码数据队列，以链表方式实现) VideoPicture pictq[VIDEO_PICTURE_QUEUE_SIZE]; int pictq_size, pictq_rindex, pictq_windex;// 队列长度，读/写位置索引 SDL_mutex *pictq_lock;// 队列读写锁对象，保护图像帧队列数据 SDL_cond *pictq_ready;// 队列就绪条件变量 SDL_Rect sdl_rect; SDL_Renderer* sdl_renderer; SDL_Texture* sdl_texture; PacketQueue audioq;// 音频编码数据包队列(编码数据队列，以链表方式实现) uint8_t audio_buf[(MAX_AUDIO_FRAME_SIZE*3)/2];//保存解码一个packet后的多帧原始音频数据(解码数据队列，以数组方式实现) unsigned int audio_buf_size;//解码后的多帧音频数据长度 unsigned int audio_buf_index;//累计写入stream的长度 uint8_t *audio_pkt_data;//编码数据缓存指针位置 int audio_pkt_size;//缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) AVPacket audio_pkt;//保存从队列中提取的数据包 AVFrame audio_frame;//保存从数据包中解码的音频数据 int video_width; int video_height; char filename[1024];// 输入文件完整路径名 int videoStream, audioStream;// 音视频流类型标号 SDL_Thread *parse_tid;// 编码数据包解析线程id SDL_Thread *decode_tid;// 解码线程id int quit;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 //video/audio_clock save pts of last decoded frame/predicted pts of next decoded frame double video_clock;//keep track of how much time has passed according to the video double audio_clock; double frame_timer;//视频播放到当前帧时的累计已播放时间 double frame_last_pts;//上一帧图像的显示时间戳，用于在video_refersh_timer中保存上一帧的pts值 double frame_last_delay;//上一帧图像的动态刷新延迟时间 } VideoState;// Since we only have one decoding thread, the Big Struct can be global in case we need it. VideoState *global_video_state; /*------取得当前播放音频数据的pts------ * 音视频同步的原理是根据音频的pts来控制视频的播放 * 也就是说在视频解码一帧后，是否显示以及显示多长时间，是通过该帧的PTS与同时正在播放的音频的PTS比较而来的 * 如果音频的PTS较大，则视频准备完毕立即刷新，否则等待 * * 因为pcm数据采用audio_callback回调方式进行播放 * 对于音频播放我们只能得到写入回调函数前缓存音频帧的pts，而无法得到当前播放帧的pts(需要采用当前播放音频帧的pts作为参考时钟) * 考虑到音频的大小与播放时间成正比(相同采样率)，那么当前时刻正在播放的音频帧pts(位于回调函数缓存中) * 就可以根据已送入声卡的pcm数据长度、缓存中剩余pcm数据长度，缓存长度及采样率进行推算了 --------------------------------*/ double get_audio_clock(VideoState *is) { double pts=is->audio_clock;//Maintained in the audio thread，取得解码操作完成时的当前播放时间戳 //还未(送入声卡)播放的剩余原始音频数据长度，等于解码后的多帧原始音频数据长度-累计送入声卡的长度 int hw_buf_size=is->audio_buf_size-is->audio_buf_index;//计算当前音频解码数据缓存索引位置 int bytes_per_sec=0;//每秒的原始音频字节数 int pcm_bytes=is->audio_st->codec->channels*2;//每组原始音频数据字节数=声道数*每声道数据字节数 if (is->audio_st) { bytes_per_sec=is->audio_st->codec->sample_rate*pcm_bytes;//计算每秒的原始音频字节数 } if (bytes_per_sec) {//检查每秒的原始音频字节数 pts-=(double)hw_buf_size/bytes_per_sec;//根据送入声卡缓存的索引位置，往前倒推计算当前时刻的音频播放时间戳pts } return pts;//返回当前正在播放的音频时间戳 } // 定时器触发的回调函数 static Uint32 sdl_refresh_timer_cb(Uint32 interval, void *opaque) { SDL_Event event;//SDL事件对象 event.type = FF_REFRESH_EVENT;//视频显示刷新事件 event.user.data1 = opaque;//传递用户数据 SDL_PushEvent(&event);//发送事件 return 0; // 0 means stop timer. } /*--------------------------- * Schedule a video refresh in 'delay' ms. * 告诉sdl在指定的延时后来推送一个 FF_REFRESH_EVENT 事件 * 这个事件将在事件队列里触发sdl_refresh_timer_cb函数的调用 --------------------------*/ static void schedule_refresh(VideoState *is, int delay) { SDL_AddTimer(delay, sdl_refresh_timer_cb, is);//在指定的时间(ms)后回调用户指定的函数 } // 视频(图像)帧渲染 void video_display(VideoState *is) { SDL_Rect rect;// SDL矩形对象 VideoPicture *vp;// 图像帧结构体指针 vp = &is->pictq[is->pictq_rindex];//从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // 设置纹理数据 SDL_UpdateTexture(is->sdl_texture, // 纹理 NULL,// 渲染区域 vp->avframe_yuv420p->data[0],// 需要渲染数据：视频像素数据帧 vp->avframe_yuv420p->linesize[0]);// 帧宽 // 将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) is->sdl_rect.x = 100; is->sdl_rect.y = 100; is->sdl_rect.w = vp->width; is->sdl_rect.h = vp->height; SDL_RenderClear(is->sdl_renderer); SDL_RenderCopy(is->sdl_renderer, is->sdl_texture, NULL, &is->sdl_rect); // 呈现画面帧 SDL_RenderPresent(is->sdl_renderer); }// end for if }// end for video_display // 显示刷新函数(FF_REFRESH_EVENT响应函数) int video_current_index = 0; void video_refresh_timer(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp;//图像帧对象 //delay-前后帧间的显示时间间隔，diff-图像帧显示与音频帧播放间的时间差 //sync_threshold-前后帧间的最小时间差，actual_delay-当前帧-下已帧的显示时间间隔(动态时间、真实时间、绝对时间) double delay,diff,sync_threshold,actual_delay,ref_clock;//ref_clock-音频时间戳 if (is->video_st) { if (is->pictq_size == 0) {// 检查图像帧队列是否有待显示图像 schedule_refresh(is, 1);//若队列为空，则发送显示刷新事件并再次进入video_refresh_timer函数 } else {// 刷新图像 vp = &is->pictq[is->pictq_rindex];//从显示队列中取得等待显示的图像帧 //计算当前帧和前一帧显示(pts)的间隔时间(显示时间戳的差值) delay = vp->pts - is->frame_last_pts;//The pts from last time，前后帧间的时间差 if (delay = 1.0) {//检查时间间隔是否在合理范围 // If incorrect delay, use previous one delay = is->frame_last_delay;//沿用之前的动态刷新间隔时间 } // Save for next time is->frame_last_delay = delay;//保存上一帧图像的动态刷新延迟时间 is->frame_last_pts = vp->pts;//保存上一帧图像的显示时间戳 // Update delay to sync to audio，取得声音播放时间戳(作为视频同步的参考时间) ref_clock=get_audio_clock(is);//根据Audio clock来判断Video播放的快慢，获取当前播放声音的时间戳 //也就是说在diff这段时间中声音是匀速发生的，但是在delay这段时间frame的显示可能就会有快慢的区别 diff=vp->pts-ref_clock;//计算图像帧显示与音频帧播放间的时间差 //根据时间差调整播放下一帧的延迟时间，以实现同步 Skip or repeat the frame，Take delay into account sync_threshold=(delay>AV_SYNC_THRESHOLD) ? delay : AV_SYNC_THRESHOLD;//比较前后两帧间的显示时间间隔与最小时间间隔 //判断音视频不同步条件，即音视频间的时间差 & 前后帧间的时间差该阈值则为快进模式，不存在音视频同步问题 if (fabs(diff)=sync_threshold) {//比较两帧画面间的显示时间与两帧画面间声音的播放时间，快了，加倍delay delay=2*delay; } }//如果diff(明显)大于AV_NOSYNC_THRESHOLD，即快进的模式了，画面跳动太大，不存在音视频同步的问题了 //更新视频播放到当前帧时的已播放时间值(所有图像帧动态播放累计时间值-真实值)，frame_timer一直累加在播放过程中我们计算的延时 is->frame_timer+=delay; //每次计算frame_timer与系统时间的差值(以系统时间为基准时间)，将frame_timer与系统时间(绝对时间)相关联的目的 actual_delay=is->frame_timer-(av_gettime()/1000000.0);//Computer the REAL delay if (actual_delay pts: %f，ref_clock：%f，actual_delay：%f\", video_current_index, vp->pts, ref_clock, actual_delay); // Update queue for next picture! if (++is->pictq_rindex == VIDEO_PICTURE_QUEUE_SIZE) {// 更新并检查图像帧队列读位置索引 is->pictq_rindex = 0;// 重置读位置索引 } SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 is->pictq_size--;// 更新图像帧队列长度 SDL_CondSignal(is->pictq_ready);// 发送队列就绪信号 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } } else { schedule_refresh(is, 100); } } // 数据包队列初始化函数 void packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue));// 全零初始化队列结构体对象 q->qlock = SDL_CreateMutex();// 创建互斥量对象 q->qready = SDL_CreateCond();// 创建条件变量对象 } // 向队列中插入数据包 int packet_queue_put(PacketQueue *q, AVPacket *pkt) { /*-------准备队列(链表)节点对象------*/ AVPacketList *pktlist=(AVPacketList *)av_malloc(sizeof(AVPacketList));// 在堆上创建链表节点对象 if (!pktlist) {// 检查链表节点对象是否创建成功 return -1; } pktlist->pkt = *pkt;// 将输入数据包赋值给新建链表节点对象中的数据包对象 pktlist->next = NULL;// 链表后继指针为空 // if (av_packet_ref(pkt, pkt) qlock);// 队列互斥量加锁，保护队列数据 if (!q->last_pkt) {// 检查队列尾节点是否存在(检查队列是否为空) q->first_pkt = pktlist;// 若不存在(队列尾空)，则将当前节点作队列为首节点 } else { q->last_pkt->next = pktlist;// 若已存在尾节点，则将当前节点挂到尾节点的后继指针上，并作为新的尾节点 } q->last_pkt = pktlist;// 将当前节点作为新的尾节点 q->nb_packets++;// 队列长度+1 q->size += pktlist->pkt.size;// 更新队列编码数据的缓存长度 SDL_CondSignal(q->qready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(q->qlock);// 释放互斥量 return 0; } // 从队列中提取数据包，并将提取的数据包出队列 static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block) { AVPacketList *pktlist;// 临时链表节点对象指针 int ret;// 操作结果 SDL_LockMutex(q->qlock);// 队列互斥量加锁，保护队列数据 for (;;) { if (global_video_state->quit) {// 检查退出进程标识 ret = -1;// 操作失败 break; }//end for if pktlist = q->first_pkt;// 传递将队列首个数据包指针 if (pktlist) {// 检查数据包是否为空(队列是否有数据) q->first_pkt = pktlist->next;// 队列首节点指针后移 if (!q->first_pkt) {// 检查首节点的后继节点是否存在 q->last_pkt = NULL;// 若不存在，则将尾节点指针置空 } q->nb_packets--;// 队列长度-1 q->size -= pktlist->pkt.size;// 更新队列编码数据的缓存长度 *pkt = pktlist->pkt;// 将队列首节点数据返回 av_free(pktlist);// 清空临时节点数据(清空首节点数据，首节点出队列) ret = 1;// 操作成功 break; } else if (!block) { ret = 0; break; } else {// 队列处于未就绪状态，此时通过SDL_CondWait函数等待qready就绪信号，并暂时对互斥量解锁 /*--------------------- * 等待队列就绪信号qready，并对互斥量暂时解锁 * 此时线程处于阻塞状态，并置于等待条件就绪的线程列表上 * 使得该线程只在临界区资源就绪后才被唤醒，而不至于线程被频繁切换 * 该函数返回时，互斥量再次被锁住，并执行后续操作 --------------------*/ SDL_CondWait(q->qready, q->qlock);// 暂时解锁互斥量并将自己阻塞，等待临界区资源就绪(等待SDL_CondSignal发出临界区资源就绪的信号) } }//end for for-loop SDL_UnlockMutex(q->qlock);// 释放互斥量 return ret; } // 创建/重置图像帧，为图像帧分配内存空间 void alloc_picture(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {// 检查图像帧是否已存在 // We already have one make another, bigger/smaller. av_frame_free(&vp->avframe_yuv420p); } vp->width = is->video_st->codec->width;// 设置图像帧宽度 vp->height = is->video_st->codec->height;// 设置图像帧高度 SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 vp->allocated = 1;// 图像帧像素缓冲区已分配内存 // AV_PIX_FMT_YUV420P格式的视频帧 vp->avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型 int buffer_size =av_image_get_buffer_size(AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 一帧视频像素数据宽 = 视频宽 is->video_st->codec->height,// 一帧视频像素数据高 = 视频高 1);// 字节对齐方式，默认是1 // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 av_image_fill_arrays(vp->avframe_yuv420p->data,// 目标视频帧数据 vp->avframe_yuv420p->linesize,// 目标视频帧行大小 out_buffer,// 原始数据 AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 视频宽 is->video_st->codec->height,//视频高 1);// 字节对齐方式 SDL_CondSignal(is->pictq_ready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } /*--------------------------- * queue_picture：图像帧插入队列等待渲染 * @is：全局状态参数集 * @pFrame：保存图像解码数据的结构体 * 1、首先检查图像帧队列(数组)是否存在空间插入新的图像，若没有足够的空间插入图像则使当前线程休眠等待 * 2、在初始化的条件下，队列(数组)中VideoPicture的bmp对象(YUV overlay)尚未分配空间，通过FF_ALLOC_EVENT事件的方法调用alloc_picture分配空间 * 3、当队列(数组)中所有VideoPicture的bmp对象(YUV overlay)均已分配空间的情况下，直接跳过步骤2向bmp对象拷贝像素数据，像素数据在进行格式转换后执行拷贝操作 ---------------------------*/ int queue_picture(VideoState *is, AVFrame *pFrame, double pts) { /*--------1、检查队列是否有插入空间-------*/ // Wait until we have space for a new pic. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (is->pictq_size >= VIDEO_PICTURE_QUEUE_SIZE && !is->quit) {// 检查队列当前长度 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待pictq_ready信号 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } /*-------2、初始化/重置YUV overlay-------*/ // windex is set to 0 initially. VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列中抽取图像帧对象 // Allocate or resize the buffer，检查YUV overlay是否已存在，否则初始化YUV overlay，分配像素缓存空间 if (!vp->avframe_yuv420p || vp->width!=is->video_st->codec->width || vp->height!=is->video_st->codec->height) { vp->allocated = 0;// 图像帧未分配空间 // We have to do it in the main thread. SDL_Event event;// SDL事件对象 event.type = FF_ALLOC_EVENT;//指定分配图像帧内存事件 event.user.data1 = is;//传递用户数据 SDL_PushEvent(&event);//发送SDL事件 // Wait until we have a picture allocated. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (!vp->allocated && !is->quit) {// 检查当前图像帧是否已初始化 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待alloc_picture发送pictq_ready信号唤醒当前线程 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } }// end for if /*--------3、拷贝视频帧到YUV overlay-------*/ // We have a place to put our picture on the queue. if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // Convert the image into YUV format that SDL uses，将解码后的图像帧转换为AV_PIX_FMT_YUV420P格式，并拷贝到图像帧队列 sws_scale(is->sws_ctx, (uint8_t const * const *)pFrame->data, pFrame->linesize, 0, is->video_st->codec->height, vp->avframe_yuv420p->data, vp->avframe_yuv420p->linesize); vp->pts = pts;//传递当前图像帧的绝对显示时间戳 // Now we inform our display thread that we have a pic ready. if (++is->pictq_windex == VIDEO_PICTURE_QUEUE_SIZE) {//更新并检查当前图像帧队列写入位置 is->pictq_windex = 0;//重置图像帧队列写入位置 } SDL_LockMutex(is->pictq_lock);//锁定队列读写锁，保护队列数据 is->pictq_size++;//更新图像帧队列长度 SDL_UnlockMutex(is->pictq_lock);//释放队列读写锁 }// end for if return 0; } /*--------------------------- * 更新内部视频播放计时器(记录视频已经播时间(video_clock)） * @is：全局状态参数集 * @src_frame：当前(输入的)(待更新的)图像帧对象 * @pts：当前图像帧的显示时间戳 * update the PTS to be in sync ---------------------------*/ double synchronize_video(VideoState *is, AVFrame *src_frame, double pts) { /*----------检查显示时间戳----------*/ if (pts != 0) {//检查显示时间戳是否有效 // If we have pts, set video clock to it. is->video_clock = pts;//用显示时间戳更新已播放时间 } else {//若获取不到显示时间戳 // If we aren't given a pts, set it to the clock. pts = is->video_clock;//用已播放时间更新显示时间戳 } /*--------更新视频已经播时间--------*/ // Update the video clock，若该帧要重复显示(取决于repeat_pict)，则全局视频播放时序video_clock应加上重复显示的数量*帧率 double frame_delay = av_q2d(is->video_st->codec->time_base);//该帧显示完将要花费的时间 // If we are repeating a frame, adjust clock accordingly,若存在重复帧，则在正常播放的前后两帧图像间安排渲染重复帧 frame_delay += src_frame->repeat_pict*(frame_delay*0.5);//计算渲染重复帧的时值(类似于音符时值) is->video_clock += frame_delay;//更新视频播放时间 // printf(\"repeat_pict=%d \\n\",src_frame->repeat_pict); return pts;//此时返回的值即为下一帧将要开始显示的时间戳 } // 视频解码线程函数 int decode_thread(void *arg) { VideoState *is = (VideoState *) arg;// 传递用户数据 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 int frameFinished;// 解码操作是否成功标识 // Allocate video frame，为解码后的视频信息结构体分配空间并完成初始化操作(结构体中的图像缓存按照下面两步手动安装) AVFrame *pFrame = av_frame_alloc(); double pts;//当前桢在整个视频中的(绝对)时间位置 for (;;) { if (packet_queue_get(&is->videoq,packet,1)pts;// Save global pts to be stored in pFrame in first call. /*----------------------- * Decode video frame，解码完整的一帧数据，并将frameFinished设置为true * 可能无法通过只解码一个packet就获得一个完整的视频帧frame，可能需要读取多个packet才行 * avcodec_decode_video2()会在解码到完整的一帧时设置frameFinished为真 * Technically a packet can contain partial frames or other bits of data * ffmpeg's parser ensures that the packets we get contain either complete or multiple frames * convert the packet to a frame for us and set frameFinisned for us when we have the next frame -----------------------*/ avcodec_decode_video2(is->video_st->codec, pFrame, &frameFinished, packet); //取得编码数据包中的显示时间戳PTS(int64_t),并暂时保存在pts(double)中 // if (packet->dts==AV_NOPTS_VALUE && pFrame->opaque && *(uint64_t*)pFrame->opaque!=AV_NOPTS_VALUE) { // pts = *(uint64_t *)pFrame->opaque; // } else if (packet->dts != AV_NOPTS_VALUE) { // pts = packet->dts; // } else { // pts = 0; // } pts=av_frame_get_best_effort_timestamp(pFrame);//取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 /*------------------------- * 在解码线程函数中计算当前图像帧的显示时间戳 * 1、取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 * 2、根据PTS*time_base来计算当前桢在整个视频中的显示时间戳，即PTS*(1/framerate) * av_q2d把AVRatioal结构转换成double的函数， * 用于计算视频源每个图像帧显示的间隔时间(1/framerate),即返回(time_base->num/time_base->den) -------------------------*/ //根据pts=PTS*time_base={numerator=1,denominator=25}计算当前桢在整个视频中的显示时间戳 pts*=av_q2d(is->video_st->time_base);//time_base为AVRational有理数结构体{num=1,den=25}，记录了视频源每个图像帧显示的间隔时间 // Did we get a video frame，检查是否解码出完整一帧图像 if (frameFinished) { pts = synchronize_video(is, pFrame, pts);//检查当前帧的显示时间戳pts并更新内部视频播放计时器(记录视频已经播时间(video_clock)） if (queue_picture(is, pFrame, pts)audio_pkt;// 保存从队列中提取的数据包 double pts;//音频播放时间戳 for (;;) { while (is->audio_pkt_size>0) {// 检查缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) int got_frame = 0;// 解码操作成功标识，成功返回非零值 // 解码一帧音频数据，并返回消耗的编码数据长度 coded_consumed_size = avcodec_decode_audio4(is->audio_st->codec, &is->audio_frame, &got_frame, pkt); if (coded_consumed_size audio_pkt_size = 0;// 更新编码数据缓存长度 break; } if (got_frame) {// 检查解码操作是否成功 // 计算解码后音频数据长度[output] data_size = av_samples_get_buffer_size(NULL, is->audio_st->codec->channels, is->audio_frame.nb_samples, is->audio_st->codec->sample_fmt, 1); memcpy(is->audio_buf, is->audio_frame.data[0], data_size);// 将解码数据复制到输出缓存 } is->audio_pkt_data += coded_consumed_size;// 更新编码数据缓存指针位置 is->audio_pkt_size -= coded_consumed_size;// 更新缓存中剩余的编码数据长度 if (data_size audio_clock;//用每次更新的音频播放时间更新音频PTS *pts_ptr=pts; /*--------------------- * 当一个packet中包含多个音频帧时 * 通过[解码后音频原始数据长度]及[采样率]来推算一个packet中其他音频帧的播放时间戳pts * 采样频率44.1kHz，量化位数16位，意味着每秒采集数据44.1k个，每个数据占2字节 --------------------*/ pcm_bytes=2*is->audio_st->codec->channels;//计算每组音频采样数据的字节数=每个声道音频采样字节数*声道数 /*----更新audio_clock--- * 一个pkt包含多个音频frame，同时一个pkt对应一个pts(pkt->pts) * 因此，该pkt中包含的多个音频帧的时间戳由以下公式推断得出 * bytes_per_sec=pcm_bytes*is->audio_st->codec->sample_rate * 从pkt中不断的解码，推断(一个pkt中)每帧数据的pts并累加到音频播放时钟 --------------------*/ is->audio_clock+=(double)data_size/(double)(pcm_bytes*is->audio_st->codec->sample_rate); // We have data, return it and come back for more later. return data_size;// 返回解码数据缓存长度 } if (pkt->data) {// 检查数据包是否已从队列中提取 av_packet_unref(pkt);// 释放pkt中保存的编码数据 } if (is->quit) {// 检查退出进程标识 return -1; } // Next packet，从队列中提取数据包到pkt if (packet_queue_get(&is->audioq, pkt, 1) audio_pkt_data = pkt->data;// 传递编码数据缓存指针 is->audio_pkt_size = pkt->size;// 传递编码数据缓存长度 // If update, update the audio clock w/pts if (pkt->pts != AV_NOPTS_VALUE) {//检查音频播放时间戳 //获得一个新的packet的时候，更新audio_clock，用packet中的pts更新audio_clock(一个pkt对应一个pts) is->audio_clock=pkt->pts*av_q2d(is->audio_st->time_base);//更新音频已经播的时间 } } } /*------Audio Callback------- * 音频输出回调函数，sdl通过该回调函数将解码后的pcm数据送入声卡播放, * sdl通常一次会准备一组缓存pcm数据，通过该回调送入声卡，声卡根据音频pts依次播放pcm数据 * 待送入缓存的pcm数据完成播放后，再载入一组新的pcm缓存数据(每次音频输出缓存为空时，sdl就调用此函数填充音频输出缓存，并送入声卡播放) * When we begin playing audio, SDL will continually call this callback function * and ask it to fill the audio buffer with a certain number of bytes * The audio function callback takes the following parameters: * stream: A pointer to the audio buffer to be filled，输出音频数据到声卡缓存 * len: The length (in bytes) of the audio buffer,缓存长度wanted_spec.samples=SDL_AUDIO_BUFFER_SIZE(1024) --------------------------*/ void audio_callback(void *userdata, Uint8 *stream, int len) { VideoState *is = (VideoState *) userdata;// 传递用户数据 int wt_stream_len, audio_size;// 每次写入stream的数据长度，解码后的数据长度 double pts;//音频时间戳 while (len > 0) {//检查音频缓存的剩余长度 if (is->audio_buf_index >= is->audio_buf_size) {// 检查是否需要执行解码操作 // We have already sent all our data; get more，从缓存队列中提取数据包、解码，并返回解码后的数据长度，audio_buf缓存中可能包含多帧解码后的音频数据 audio_size = audio_decode_frame(is, &pts); if (audio_size audio_buf_size = 1024; memset(is->audio_buf, 0, is->audio_buf_size);// 全零重置缓冲区 } else { is->audio_buf_size = audio_size;// 返回packet中包含的原始音频数据长度(多帧) } is->audio_buf_index = 0;// 初始化累计写入缓存长度 }//end for if wt_stream_len=is->audio_buf_size-is->audio_buf_index;// 计算解码缓存剩余长度 if (wt_stream_len > len) {// 检查每次写入缓存的数据长度是否超过指定长度(1024) wt_stream_len = len;// 指定长度从解码的缓存中取数据 } // 每次从解码的缓存数据中以指定长度抽取数据并写入stream传递给声卡 memcpy(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, wt_stream_len); len -= wt_stream_len;// 更新解码音频缓存的剩余长度 stream += wt_stream_len;// 更新缓存写入位置 is->audio_buf_index += wt_stream_len;// 更新累计写入缓存数据长度 }//end for while } // 根据指定类型打开流，找到对应的解码器、创建对应的音频配置、保存关键信息到 VideoState、启动音频和视频线程 int stream_component_open(VideoState *is, int stream_index) { AVFormatContext *pFormatCtx = is->pFormatCtx;// 传递文件容器的封装信息及码流参数 AVCodecContext *codecCtx = NULL;// 解码器上下文对象，解码器依赖的相关环境、状态、资源以及参数集的接口指针 AVCodec *codec = NULL;// 保存编解码器信息的结构体，提供编码与解码的公共接口，可以看作是编码器与解码器的一个全局变量 //检查输入的流类型是否在合理范围内 if (stream_index = pFormatCtx->nb_streams) { return -1; } // Get a pointer to the codec context for the video stream. codecCtx = pFormatCtx->streams[stream_index]->codec;// 取得解码器上下文 if (codecCtx->codec_type == AVMEDIA_TYPE_AUDIO) {//检查解码器类型是否为音频解码器 SDL_AudioSpec wanted_spec, spec;//SDL_AudioSpec a structure that contains the audio output format，创建 SDL_AudioSpec 结构体，设置音频播放数据 // Set audio settings from codec info,SDL_AudioSpec a structure that contains the audio output format // 创建SDL_AudioSpec结构体，设置音频播放参数 wanted_spec.freq = codecCtx->sample_rate;//采样频率 DSP frequency -- samples per second wanted_spec.format = AUDIO_S16SYS;//采样格式 Audio data format wanted_spec.channels = codecCtx->channels;//声道数 Number of channels: 1 mono, 2 stereo wanted_spec.silence = 0;//无输出时是否静音 //默认每次读音频缓存的大小，推荐值为 512~8192，ffplay使用的是1024 specifies a unit of audio data refers to the size of the audio buffer in sample frames wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE; wanted_spec.callback = audio_callback;//设置读取音频数据的回调接口函数 the function to call when the audio device needs more data wanted_spec.userdata = is;//传递用户数据 /*--------------------------- * 以指定参数打开音频设备，并返回与指定参数最为接近的参数，该参数为设备实际支持的音频参数 * Opens the audio device with the desired parameters(wanted_spec) * return another specs we actually be using * and not guaranteed to get what we asked for --------------------------*/ if (SDL_OpenAudio(&wanted_spec, &spec) codec_id); AVDictionary *optionsDict = NULL; if (!codec || (avcodec_open2(codecCtx, codec, &optionsDict) name); // 检查解码器类型 switch(codecCtx->codec_type) { case AVMEDIA_TYPE_AUDIO:// 音频解码器 is->audioStream = stream_index;// 音频流类型标号初始化 is->audio_st = pFormatCtx->streams[stream_index]; is->audio_buf_size = 0;// 解码后的多帧音频数据长度 is->audio_buf_index = 0;//累 计写入stream的长度 memset(&is->audio_pkt, 0, sizeof(is->audio_pkt)); packet_queue_init(&is->audioq);// 音频数据包队列初始化 SDL_PauseAudio(0);// audio callback starts running again，开启音频设备，如果这时候没有获得数据那么它就静音 break; case AVMEDIA_TYPE_VIDEO:// 视频解码器 is->videoStream = stream_index;// 视频流类型标号初始化 is->video_st = pFormatCtx->streams[stream_index]; //以系统时间为基准，初始化播放到当前帧的已播放时间值，该值为真实时间值、动态时间值、绝对时间值 is->frame_timer=(double)av_gettime()/1000000.0; is->frame_last_delay = 40e-3;//初始化上一帧图像的动态刷新延迟时间 packet_queue_init(&is->videoq);// 视频数据包队列初始化 is->decode_tid = SDL_CreateThread(decode_thread,\"视频解码线程\" ,is);// 创建视频解码线程 // Initialize SWS context for software scaling，设置图像转换像素格式为AV_PIX_FMT_YUV420P is->sws_ctx = sws_getContext(is->video_st->codec->width, is->video_st->codec->height, is->video_st->codec->pix_fmt, is->video_st->codec->width, is->video_st->codec->height, AV_PIX_FMT_YUV420P, SWS_BILINEAR, NULL, NULL, NULL); break; default: break; } return 0; } // 编码数据包解析线程函数(从视频文件中解析出音视频编码数据单元，一个AVPacket的data通常对应一个NAL) int parse_thread(void *arg) { VideoState *is = (VideoState *)arg;// 传递用户参数 global_video_state = is;// 传递全局状态参量结构体 /*------------------------- * 打开封装格式 * 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 * 参数一：封装格式上下文 * 参数二：视频路径 * 参数三：指定输入的格式 * 参数四：设置默认参数 --------------------------*/ AVFormatContext *avformat_context = NULL;// 参数一：封装格式上下文 int avformat_open_input_result = avformat_open_input(&avformat_context, is->filename, NULL, NULL); if (avformat_open_input_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"查找音视频流\\n\"); return -1; } is->pFormatCtx = avformat_context;//传递文件容器封装信息及码流参数 /*------------------------- * 查找码流 * 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 * 参数一：封装格式上下文 * 参数二：指定默认配置 -------------------------*/ int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result videoStream = -1;//视频流类型标号初始化为-1 is->audioStream = -1;//音频流类型标号初始化为-1 // 视频流类型标号初始化为-1 int av_video_stream_index = -1; // 音频流类型标号初始化为-1 int av_audio_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 若文件中包含有视频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_video_stream_index = i; } // 若文件中包含有音频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_audio_stream_index = i; } } // 检查文件中是否存在视频流 if (av_video_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到视频流\\n\"); goto fail;//跳转至异常处理 return -1; } // 检查文件中是否存在音频流 if (av_audio_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到音频流\\n\"); goto fail;//跳转至异常处理 return -1; } stream_component_open(is, av_audio_stream_index);// 根据指定类型打开音频流 stream_component_open(is, av_video_stream_index);// 根据指定类型打开视频流 // Main decode loop. for (;;) { if (is->quit) {//检查退出进程标识 break; } // Seek stuff goes here，检查音视频编码数据包队列长度是否溢出 if (is->audioq.size > MAX_AUDIOQ_SIZE || is->videoq.size > MAX_VIDEOQ_SIZE) { SDL_Delay(10); continue; } /*----------------------- * read in a packet and store it in the AVPacket struct * ffmpeg allocates the internal data for us,which is pointed to by packet.data * this is freed by the av_free_packet() -----------------------*/ // 负责保存压缩编码数据相关信息的结构体,每帧图像由一到多个packet包组成 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 if (av_read_frame(is->pFormatCtx, packet) pFormatCtx->pb->error == 0) { SDL_Delay(100); // No error; wait for user input. continue; } else { break; } } // Is this a packet from the video stream? if (packet->stream_index == is->videoStream) {// 检查数据包是否为视频类型 packet_queue_put(&is->videoq, packet);// 向队列中插入数据包 } else if (packet->stream_index == is->audioStream) {// 检查数据包是否为音频类型 packet_queue_put(&is->audioq, packet);// 向队列中插入数据包 } else {// 检查数据包是否为字幕类型 av_packet_unref(packet);// 释放packet中保存的(字幕)编码数据 } } // All done - wait for it. while (!is->quit) { SDL_Delay(100); } fail:// 异常处理 if (1) { SDL_Event event;// SDL事件对象 event.type = FF_QUIT_EVENT;// 指定退出事件类型 event.user.data1 = is;// 传递用户数据 SDL_PushEvent(&event);// 将该事件对象压入SDL后台事件队列 } return 0; } int init_sdl(VideoState *is) { // 初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } // 初始化SDL窗口 SDL_Window* sdl_window = SDL_CreateWindow(\"FFmpeg+SDL播放视频\",// 参数一：窗口名称 SDL_WINDOWPOS_CENTERED,// 参数二：窗口在屏幕上的x坐标 SDL_WINDOWPOS_CENTERED,// 参数三：窗口在屏幕上的y坐标 is->video_width,// 参数四：窗口在屏幕上宽 is->video_height,// 参数五：窗口在屏幕上高 SDL_WINDOW_OPENGL);// 参数六：窗口状态(打开) if (sdl_window == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建渲染器 // 定义渲染器区域 SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window,// 渲染目标创建 -1, // 从那里开始渲染(-1:表示从第一个位置开始) 0);// 渲染类型(软件渲染) if (sdl_renderer == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建纹理 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer,// 渲染器 SDL_PIXELFORMAT_IYUV,// 像素数据格式 SDL_TEXTUREACCESS_STREAMING,// 绘制方式：频繁绘制- is->video_width,// 纹理宽 is->video_height);// 纹理高 if (sdl_texture == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } is->sdl_renderer = sdl_renderer; is->sdl_texture = sdl_texture; return 0; } // SDL入口 extern \"C\" int main(int argc, char *argv[]) { /*------------------------- * 注册组件 * 注册所有ffmpeg支持的多媒体格式及编解码器 -------------------------*/ av_register_all(); // 创建全局状态对象 VideoState *is= (VideoState *)av_mallocz(sizeof(VideoState)); av_strlcpy(is->filename, \"/storage/emulated/0/Download/test.mov\", sizeof(is->filename));// 复制视频文件路径名 is->video_width = 640; is->video_height = 352; is->pictq_lock = SDL_CreateMutex();// 创建编码数据包队列互斥锁对象 is->pictq_ready = SDL_CreateCond();// 创建编码数据包队列就绪条件对象 int init_sdl_result = init_sdl(is); if (init_sdl_result parse_tid = SDL_CreateThread(parse_thread, \"编码数据包解析线程\", is); if (!is->parse_tid) {// 检查线程是否创建成功 av_free(is); return -1; } // SDL事件对象 SDL_Event event; for (;;) {// SDL事件循环 SDL_WaitEvent(&event);// 主线程阻塞，等待事件到来 switch(event.type) {// 事件到来后唤醒主线程，检查事件类型 case FF_QUIT_EVENT: case SDL_QUIT:// 退出进程事件 is->quit = 1; // If the video has finished playing, then both the picture and audio queues are waiting for more data. // Make them stop waiting and terminate normally.. avcodec_close(is->video_st->codec); avformat_free_context(is->pFormatCtx); SDL_CondSignal(is->audioq.qready);// 发出队列就绪信号避免死锁 SDL_CondSignal(is->videoq.qready); SDL_DestroyTexture(is->sdl_texture); SDL_DestroyRenderer(is->sdl_renderer); SDL_Quit(); return 0; case FF_ALLOC_EVENT: alloc_picture(event.user.data1);// 分配视频帧事件响应函数 break; case FF_REFRESH_EVENT:// 视频显示刷新事件 video_refresh_timer(event.user.data1);// 视频显示刷新事件响应函数 break; default: break; } } return 0; } "},"pages/ffmpeg/FFmpeg_SDL_synchronized_audio.html":{"url":"pages/ffmpeg/FFmpeg_SDL_synchronized_audio.html","title":"FFmpeg+SDL同步音频","keywords":"","body":"FFmpeg+SDL同步音频 Android工程代码 iOS工程代码 ffmpeg播放器实现详解 - 音频同步控制：https://www.cnblogs.com/breakpointlab/p/15791998.html 源代码一览 #include #include #include #include #include \"SDL.h\" #include #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 #define AV_SYNC_THRESHOLD 0.01//前后两帧间的显示时间间隔的最小值0.01s #define AV_NOSYNC_THRESHOLD 10.0//最小刷新间隔时间10ms #define MAX_AUDIOQ_SIZE (5 * 16 * 1024) #define MAX_VIDEOQ_SIZE (5 * 256 * 1024) #define FF_ALLOC_EVENT (SDL_USEREVENT) #define FF_REFRESH_EVENT (SDL_USEREVENT + 1) #define FF_QUIT_EVENT (SDL_USEREVENT + 2) #define VIDEO_PICTURE_QUEUE_SIZE 1 #define SAMPLE_CORRECTION_PERCENT_MAX 10 #define AUDIO_DIFF_AVG_NB 20 extern \"C\" { #include #include \"libavformat/avformat.h\" #include #include #include #include #include #include } uint64_t global_video_pkt_pts = AV_NOPTS_VALUE; enum {//同步时钟源 AV_SYNC_AUDIO_MASTER,//音频时钟为主同步源 AV_SYNC_VIDEO_MASTER,//视频时钟为主同步源 AV_SYNC_EXTERNAL_MASTER,//外部时钟为主同步源 }; #define DEFAULT_AV_SYNC_TYPE AV_SYNC_AUDIO_MASTER//指定以视频时钟为主同步源(时间基准) /*-------链表节点结构体-------- typedef struct AVPacketList { AVPacket pkt;//链表数据 struct AVPacketList *next;//链表后继节点 } AVPacketList; ---------------------------*/ // 数据包队列(链表)结构体 typedef struct PacketQueue { AVPacketList *first_pkt, *last_pkt;// 队列首尾节点指针 int nb_packets;// 队列长度 int size;// 保存编码数据的缓存长度，size=packet->size SDL_mutex *qlock;// 队列互斥量，保护队列数据 SDL_cond *qready;// 队列就绪条件变量 } PacketQueue; // 图像帧结构体 typedef struct VideoPicture { AVFrame *avframe_yuv420p; int width, height;//Source height & width. int allocated;//是否分配内存空间，视频帧转换为SDL overlay标识 double pts;//当前图像帧的绝对显示时间戳 } VideoPicture; typedef struct VideoState { AVFormatContext *pFormatCtx;// 保存文件容器封装信息及码流参数的结构体 AVStream *video_st;// 视频流信息结构体 AVStream *audio_st;//音频流信息结构体 struct SwsContext *sws_ctx;// 描述转换器参数的结构体 struct SwsContext *sws_ctx_audio; PacketQueue videoq;// 视频编码数据包队列(编码数据队列，以链表方式实现) VideoPicture pictq[VIDEO_PICTURE_QUEUE_SIZE]; int pictq_size, pictq_rindex, pictq_windex;// 队列长度，读/写位置索引 SDL_mutex *pictq_lock;// 队列读写锁对象，保护图像帧队列数据 SDL_cond *pictq_ready;// 队列就绪条件变量 SDL_Rect sdl_rect; SDL_Renderer* sdl_renderer; SDL_Texture* sdl_texture; PacketQueue audioq;// 音频编码数据包队列(编码数据队列，以链表方式实现) uint8_t audio_buf[(MAX_AUDIO_FRAME_SIZE*3)/2];//保存解码一个packet后的多帧原始音频数据(解码数据队列，以数组方式实现) unsigned int audio_buf_size;//解码后的多帧音频数据长度 unsigned int audio_buf_index;//累计写入stream的长度 uint8_t *audio_pkt_data;//编码数据缓存指针位置 int audio_pkt_size;//缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) AVPacket audio_pkt;//保存从队列中提取的数据包 AVFrame audio_frame;//保存从数据包中解码的音频数据 int video_width; int video_height; char filename[1024];// 输入文件完整路径名 int videoStream, audioStream;// 音视频流类型标号 SDL_Thread *parse_tid;// 编码数据包解析线程id SDL_Thread *decode_tid;// 解码线程id int quit;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 //video/audio_clock save pts of last decoded frame/predicted pts of next decoded frame double video_clock;//keep track of how much time has passed according to the video double audio_clock; double frame_timer;//视频播放到当前帧时的累计已播放时间 double frame_last_pts;//上一帧图像的显示时间戳，用于在video_refersh_timer中保存上一帧的pts值 double frame_last_delay;//上一帧图像的动态刷新延迟时间 int av_sync_type;//主同步源类型 double external_clock;//External clock base int64_t external_clock_time;//外部时钟的绝对时间 double audio_diff_cum;//音频时钟与同步源累计时差，sed for AV difference average computation double audio_diff_avg_coef;//音频时钟与同步源时差均值加权系数 double audio_diff_threshold;//音频时钟与同步源时差均值阈值 int audio_diff_avg_count;//音频不同步计数(音频时钟与主同步源存在不同步的次数) int audio_hw_buf_size; double video_current_pts;//当前帧显示时间戳，Current displayed pts (different from video_clock if frame fifos are used) int64_t video_current_pts_time;//取得video_current_pts的系统时间，time (av_gettime) at which we updated video_current_pts - used to have running video pts } VideoState;// Since we only have one decoding thread, the Big Struct can be global in case we need it. VideoState *global_video_state; /*------取得当前播放音频数据的pts------ * 音视频同步的原理是根据音频的pts来控制视频的播放 * 也就是说在视频解码一帧后，是否显示以及显示多长时间，是通过该帧的PTS与同时正在播放的音频的PTS比较而来的 * 如果音频的PTS较大，则视频准备完毕立即刷新，否则等待 * * 因为pcm数据采用audio_callback回调方式进行播放 * 对于音频播放我们只能得到写入回调函数前缓存音频帧的pts，而无法得到当前播放帧的pts(需要采用当前播放音频帧的pts作为参考时钟) * 考虑到音频的大小与播放时间成正比(相同采样率)，那么当前时刻正在播放的音频帧pts(位于回调函数缓存中) * 就可以根据已送入声卡的pcm数据长度、缓存中剩余pcm数据长度，缓存长度及采样率进行推算了 --------------------------------*/ double get_audio_clock(VideoState *is) { double pts=is->audio_clock;//Maintained in the audio thread，取得解码操作完成时的当前播放时间戳 //还未(送入声卡)播放的剩余原始音频数据长度，等于解码后的多帧原始音频数据长度-累计送入声卡的长度 int hw_buf_size=is->audio_buf_size-is->audio_buf_index;//计算当前音频解码数据缓存索引位置 int bytes_per_sec=0;//每秒的原始音频字节数 int pcm_bytes=is->audio_st->codec->channels*2;//每组原始音频数据字节数=声道数*每声道数据字节数 if (is->audio_st) { bytes_per_sec=is->audio_st->codec->sample_rate*pcm_bytes;//计算每秒的原始音频字节数 } if (bytes_per_sec) {//检查每秒的原始音频字节数 pts-=(double)hw_buf_size/bytes_per_sec;//根据送入声卡缓存的索引位置，往前倒推计算当前时刻的音频播放时间戳pts } return pts;//返回当前正在播放的音频时间戳 } /*-----------取得视频时钟----------- * 即取得当前播放视频帧的pts，以视频时钟pts作为音视频同步基准，return the current time offset of the video currently being played * 该值为当前帧时间戳pts+一个微小的修正值delta * 因为在ms的级别上，在毫秒级别上，若取得视频时钟(即当前帧pts)的时刻，与调用视频时钟的时刻(如将音频同步到该视频pts时刻)存在延迟 * 那么，视频时钟需要在被调用时进行修正，修正值delta为 * delta=[取得视频时钟的时刻值video_current_pts_time] 到 [调用get_video_clock时刻值] 的间隔时间 * 通常情况下，都会选择以外部时钟或音频时钟作为主同步源，以视频同步到音频或外部时钟为首选同步方案 * 以视频时钟作为主同步源的同步方案，属于3种基本的同步方案(同步到音频、同步到视频、同步到外部时钟) * 本利仅为展示同步到视频时钟的方法，一般情况下同步到视频时钟仅作为辅助的同步方案 --------------------------------*/ double get_video_clock(VideoState *is) { double delta=(av_gettime()-is->video_current_pts_time)/1000000.0; //pts_of_last_frame+(Current_time-time_elapsed_since_pts_value_was_set) return is->video_current_pts+delta; } //取得系统时间，以系统时钟作为同步基准 double get_external_clock(VideoState *is) { return av_gettime()/1000000.0;//取得系统当前时间，以1/1000000秒为单位，便于在各个平台移植 } //取得主时钟(基准时钟) double get_master_clock(VideoState *is) { if (is->av_sync_type == AV_SYNC_VIDEO_MASTER) { return get_video_clock(is);//返回视频时钟 } else if (is->av_sync_type == AV_SYNC_AUDIO_MASTER) { return get_audio_clock(is);//返回音频时钟 } else { return get_external_clock(is);//返回系统时钟 } } /*--------------------------- * return the wanted number of samples to get better sync if sync_type is video or external master clock * 通常情况下会以音频或系统时钟为主同步源，只有在音频或系统时钟失效的情况下才以视频为主同步源 * 该函数比对音频时钟与主同步源的时差，通过动态丢帧(或插值)部分音频数据，以起到减少(或增加)音频播放时长，减少与主同步源时差的作用 * 该函数对音频缓存数据进行丢帧(或插值)，返回丢帧(或插值)后的音频数据长度 * 因为音频同步可能带来输出声音不连续等副作用，该函数通过音频不同步次数(audio_diff_avg_count)及时差均值(avg_diff)来约束音频的同步过程 ---------------------------*/ int synchronize_audio(VideoState *is, short *samples, int samples_size, double pts) { double ref_clock;//主同步源(基准时钟) int pcm_bytes=is->audio_st->codec->channels*2;//每组音频数据字节数=声道数*每声道数据字节数 /* if not master, then we try to remove or add samples to correct the clock */ if (is->av_sync_type != AV_SYNC_AUDIO_MASTER) {//检查主同步源，若同步源不是音频时钟的情况下，执行以下代码 double diff, avg_diff;//diff-音频帧播放间与主同步源时差，avg_diff-采样不同步平均值 int wanted_size, min_size, max_size;//经过丢帧(或插值)后的缓存长度，缓存长度最大/最小值 ref_clock = get_master_clock(is);//取得当前主同步源，以主同步源为基准时间 diff = get_audio_clock(is) - ref_clock;//计算音频时钟与当前主同步源的时差 if (diffaudio_diff_cum=diff+is->audio_diff_avg_coef*is->audio_diff_cum; if (is->audio_diff_avg_countaudio_diff_avg_count++;//音频不同步计数更新 } else {//当音频不同步次数超过阈值限定后，触发音频同步操作 avg_diff=is->audio_diff_cum*(1.0-is->audio_diff_avg_coef);//计算时差均值(等比级数几何平均数) if (fabs(avg_diff)>=is->audio_diff_threshold) {//比对时差均值与时差阈值 wanted_size=samples_size+((int)(diff*is->audio_st->codec->sample_rate)*pcm_bytes);//根据时差换算同步后的缓存长度 min_size=samples_size*((100-SAMPLE_CORRECTION_PERCENT_MAX)/100);//同步后的缓存长度最小值 max_size=samples_size*((100+SAMPLE_CORRECTION_PERCENT_MAX)/100);//同步后的缓存长度最大值 if (wanted_sizemax_size) {//若同步后缓存长度>最小缓存长度 wanted_size=max_size;//用最大缓存长度作为同步后的缓存长度 } if (wanted_sizesamples_size) {//若同步后缓存长度大于当前缓存长度 //Add samples by copying final sample，通过复制最后一个音频数据进行插值 //int nb=samples_size-wanted_size; int nb=wanted_size-samples_size;//计算插值后缓存长度与原始缓存长度间的差值(需要插值的音频数据组数) uint8_t *samples_end=(uint8_t*)samples+samples_size-pcm_bytes;//取得缓存末端数据指针 uint8_t *q=samples_end+pcm_bytes;//初始插值位置|||q| while (nb>0) {//检查插值音频组数(每组包括两个声道的pcm数据) memcpy(q,samples_end,pcm_bytes);//在samples原始缓存后追加插值 q += pcm_bytes;//更新插值位置 nb -= pcm_bytes;//更新插值组数 } samples_size=wanted_size;//返回音频同步后的缓存长度 } } } } else { // Difference is too big, reset diff stuff，时差过大，重置时差累计值 is->audio_diff_avg_count = 0;//音频不同步计数重置 is->audio_diff_cum = 0;//音频累计时差重置 } }//end for if (is->av_sync_type != AV_SYNC_AUDIO_MASTER) return samples_size;//返回发送到声卡的音频缓存字节数 } // 定时器触发的回调函数 static Uint32 sdl_refresh_timer_cb(Uint32 interval, void *opaque) { SDL_Event event;//SDL事件对象 event.type = FF_REFRESH_EVENT;//视频显示刷新事件 event.user.data1 = opaque;//传递用户数据 SDL_PushEvent(&event);//发送事件 return 0; // 0 means stop timer. } /*--------------------------- * Schedule a video refresh in 'delay' ms. * 告诉sdl在指定的延时后来推送一个 FF_REFRESH_EVENT 事件 * 这个事件将在事件队列里触发sdl_refresh_timer_cb函数的调用 --------------------------*/ static void schedule_refresh(VideoState *is, int delay) { SDL_AddTimer(delay, sdl_refresh_timer_cb, is);//在指定的时间(ms)后回调用户指定的函数 } // 视频(图像)帧渲染 void video_display(VideoState *is) { SDL_Rect rect;// SDL矩形对象 VideoPicture *vp;// 图像帧结构体指针 vp = &is->pictq[is->pictq_rindex];//从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // 设置纹理数据 SDL_UpdateTexture(is->sdl_texture, // 纹理 NULL,// 渲染区域 vp->avframe_yuv420p->data[0],// 需要渲染数据：视频像素数据帧 vp->avframe_yuv420p->linesize[0]);// 帧宽 // 将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) is->sdl_rect.x = 100; is->sdl_rect.y = 100; is->sdl_rect.w = vp->width; is->sdl_rect.h = vp->height; SDL_RenderClear(is->sdl_renderer); SDL_RenderCopy(is->sdl_renderer, is->sdl_texture, NULL, &is->sdl_rect); // 呈现画面帧 SDL_RenderPresent(is->sdl_renderer); }// end for if }// end for video_display // 显示刷新函数(FF_REFRESH_EVENT响应函数) int video_current_index = 0; void video_refresh_timer(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp;//图像帧对象 //delay-前后帧间的显示时间间隔，diff-图像帧显示与音频帧播放间的时间差 //sync_threshold-前后帧间的最小时间差，actual_delay-当前帧-下已帧的显示时间间隔(动态时间、真实时间、绝对时间) double delay,diff,sync_threshold,actual_delay,ref_clock;//ref_clock-音频时间戳 if (is->video_st) { if (is->pictq_size == 0) {// 检查图像帧队列是否有待显示图像 schedule_refresh(is, 1);//若队列为空，则发送显示刷新事件并再次进入video_refresh_timer函数 } else {// 刷新图像 vp = &is->pictq[is->pictq_rindex];//从显示队列中取得等待显示的图像帧 is->video_current_pts = vp->pts;//取得当前帧的显示时间戳 is->video_current_pts_time = av_gettime();//取得系统时间，作为当前帧播放的时间基准 //计算当前帧和前一帧显示(pts)的间隔时间(显示时间戳的差值) //计算当前帧和前一帧显示(pts)的间隔时间(显示时间戳的差值) delay = vp->pts - is->frame_last_pts;//The pts from last time，前后帧间的时间差 if (delay = 1.0) {//检查时间间隔是否在合理范围 // If incorrect delay, use previous one delay = is->frame_last_delay;//沿用之前的动态刷新间隔时间 } // Save for next time is->frame_last_delay = delay;//保存上一帧图像的动态刷新延迟时间 is->frame_last_pts = vp->pts;//保存上一帧图像的显示时间戳 // Update delay to sync to audio，取得声音播放时间戳(作为视频同步的参考时间) // Update delay to sync to audio，取得声音播放时间戳(作为视频同步的参考时间) if (is->av_sync_type != AV_SYNC_VIDEO_MASTER) {//检查主同步时钟源 ref_clock = get_master_clock(is);//根据Audio clock来判断Video播放的快慢，获取当前播放声音的时间戳 //也就是说在diff这段时间中声音是匀速发生的，但是在delay这段时间frame的显示可能就会有快慢的区别 diff = vp->pts - ref_clock;//计算图像帧显示与音频帧播放间的时间差 //根据时间差调整播放下一帧的延迟时间，以实现同步 Skip or repeat the frame，Take delay into account sync_threshold = (delay > AV_SYNC_THRESHOLD) ? delay : AV_SYNC_THRESHOLD;//比较前后两帧间的显示时间间隔与最小时间间隔 //判断音视频不同步条件，即音视频间的时间差 & 前后帧间的时间差该阈值则为快进模式，不存在音视频同步问题 if (fabs(diff) = sync_threshold) {//比较两帧画面间的显示时间与两帧画面间声音的播放时间，快了，加倍delay delay = 2 * delay; } }//如果diff(明显)大于AV_NOSYNC_THRESHOLD，即快进的模式了，画面跳动太大，不存在音视频同步的问题了 } //更新视频播放到当前帧时的已播放时间值(所有图像帧动态播放累计时间值-真实值)，frame_timer一直累加在播放过程中我们计算的延时 is->frame_timer+=delay; //每次计算frame_timer与系统时间的差值(以系统时间为基准时间)，将frame_timer与系统时间(绝对时间)相关联的目的 actual_delay=is->frame_timer-(av_gettime()/1000000.0);//Computer the REAL delay if (actual_delay pts: %f，ref_clock：%f，actual_delay：%f\", video_current_index, vp->pts, ref_clock, actual_delay); // Update queue for next picture! if (++is->pictq_rindex == VIDEO_PICTURE_QUEUE_SIZE) {// 更新并检查图像帧队列读位置索引 is->pictq_rindex = 0;// 重置读位置索引 } SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 is->pictq_size--;// 更新图像帧队列长度 SDL_CondSignal(is->pictq_ready);// 发送队列就绪信号 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } } else { schedule_refresh(is, 100); } } // 数据包队列初始化函数 void packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue));// 全零初始化队列结构体对象 q->qlock = SDL_CreateMutex();// 创建互斥量对象 q->qready = SDL_CreateCond();// 创建条件变量对象 } // 向队列中插入数据包 int packet_queue_put(PacketQueue *q, AVPacket *pkt) { /*-------准备队列(链表)节点对象------*/ AVPacketList *pktlist=(AVPacketList *)av_malloc(sizeof(AVPacketList));// 在堆上创建链表节点对象 if (!pktlist) {// 检查链表节点对象是否创建成功 return -1; } pktlist->pkt = *pkt;// 将输入数据包赋值给新建链表节点对象中的数据包对象 pktlist->next = NULL;// 链表后继指针为空 // if (av_packet_ref(pkt, pkt) qlock);// 队列互斥量加锁，保护队列数据 if (!q->last_pkt) {// 检查队列尾节点是否存在(检查队列是否为空) q->first_pkt = pktlist;// 若不存在(队列尾空)，则将当前节点作队列为首节点 } else { q->last_pkt->next = pktlist;// 若已存在尾节点，则将当前节点挂到尾节点的后继指针上，并作为新的尾节点 } q->last_pkt = pktlist;// 将当前节点作为新的尾节点 q->nb_packets++;// 队列长度+1 q->size += pktlist->pkt.size;// 更新队列编码数据的缓存长度 SDL_CondSignal(q->qready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(q->qlock);// 释放互斥量 return 0; } // 从队列中提取数据包，并将提取的数据包出队列 static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block) { AVPacketList *pktlist;// 临时链表节点对象指针 int ret;// 操作结果 SDL_LockMutex(q->qlock);// 队列互斥量加锁，保护队列数据 for (;;) { if (global_video_state->quit) {// 检查退出进程标识 ret = -1;// 操作失败 break; }//end for if pktlist = q->first_pkt;// 传递将队列首个数据包指针 if (pktlist) {// 检查数据包是否为空(队列是否有数据) q->first_pkt = pktlist->next;// 队列首节点指针后移 if (!q->first_pkt) {// 检查首节点的后继节点是否存在 q->last_pkt = NULL;// 若不存在，则将尾节点指针置空 } q->nb_packets--;// 队列长度-1 q->size -= pktlist->pkt.size;// 更新队列编码数据的缓存长度 *pkt = pktlist->pkt;// 将队列首节点数据返回 av_free(pktlist);// 清空临时节点数据(清空首节点数据，首节点出队列) ret = 1;// 操作成功 break; } else if (!block) { ret = 0; break; } else {// 队列处于未就绪状态，此时通过SDL_CondWait函数等待qready就绪信号，并暂时对互斥量解锁 /*--------------------- * 等待队列就绪信号qready，并对互斥量暂时解锁 * 此时线程处于阻塞状态，并置于等待条件就绪的线程列表上 * 使得该线程只在临界区资源就绪后才被唤醒，而不至于线程被频繁切换 * 该函数返回时，互斥量再次被锁住，并执行后续操作 --------------------*/ SDL_CondWait(q->qready, q->qlock);// 暂时解锁互斥量并将自己阻塞，等待临界区资源就绪(等待SDL_CondSignal发出临界区资源就绪的信号) } }//end for for-loop SDL_UnlockMutex(q->qlock);// 释放互斥量 return ret; } // 创建/重置图像帧，为图像帧分配内存空间 void alloc_picture(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {// 检查图像帧是否已存在 // We already have one make another, bigger/smaller. av_frame_free(&vp->avframe_yuv420p); } vp->width = is->video_st->codec->width;// 设置图像帧宽度 vp->height = is->video_st->codec->height;// 设置图像帧高度 SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 vp->allocated = 1;// 图像帧像素缓冲区已分配内存 // AV_PIX_FMT_YUV420P格式的视频帧 vp->avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型 int buffer_size =av_image_get_buffer_size(AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 一帧视频像素数据宽 = 视频宽 is->video_st->codec->height,// 一帧视频像素数据高 = 视频高 1);// 字节对齐方式，默认是1 // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 av_image_fill_arrays(vp->avframe_yuv420p->data,// 目标视频帧数据 vp->avframe_yuv420p->linesize,// 目标视频帧行大小 out_buffer,// 原始数据 AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 视频宽 is->video_st->codec->height,//视频高 1);// 字节对齐方式 SDL_CondSignal(is->pictq_ready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } /*--------------------------- * queue_picture：图像帧插入队列等待渲染 * @is：全局状态参数集 * @pFrame：保存图像解码数据的结构体 * 1、首先检查图像帧队列(数组)是否存在空间插入新的图像，若没有足够的空间插入图像则使当前线程休眠等待 * 2、在初始化的条件下，队列(数组)中VideoPicture的bmp对象(YUV overlay)尚未分配空间，通过FF_ALLOC_EVENT事件的方法调用alloc_picture分配空间 * 3、当队列(数组)中所有VideoPicture的bmp对象(YUV overlay)均已分配空间的情况下，直接跳过步骤2向bmp对象拷贝像素数据，像素数据在进行格式转换后执行拷贝操作 ---------------------------*/ int queue_picture(VideoState *is, AVFrame *pFrame, double pts) { /*--------1、检查队列是否有插入空间-------*/ // Wait until we have space for a new pic. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (is->pictq_size >= VIDEO_PICTURE_QUEUE_SIZE && !is->quit) {// 检查队列当前长度 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待pictq_ready信号 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } /*-------2、初始化/重置YUV overlay-------*/ // windex is set to 0 initially. VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列中抽取图像帧对象 // Allocate or resize the buffer，检查YUV overlay是否已存在，否则初始化YUV overlay，分配像素缓存空间 if (!vp->avframe_yuv420p || vp->width!=is->video_st->codec->width || vp->height!=is->video_st->codec->height) { vp->allocated = 0;// 图像帧未分配空间 // We have to do it in the main thread. SDL_Event event;// SDL事件对象 event.type = FF_ALLOC_EVENT;//指定分配图像帧内存事件 event.user.data1 = is;//传递用户数据 SDL_PushEvent(&event);//发送SDL事件 // Wait until we have a picture allocated. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (!vp->allocated && !is->quit) {// 检查当前图像帧是否已初始化 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待alloc_picture发送pictq_ready信号唤醒当前线程 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } }// end for if /*--------3、拷贝视频帧到YUV overlay-------*/ // We have a place to put our picture on the queue. if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // Convert the image into YUV format that SDL uses，将解码后的图像帧转换为AV_PIX_FMT_YUV420P格式，并拷贝到图像帧队列 sws_scale(is->sws_ctx, (uint8_t const * const *)pFrame->data, pFrame->linesize, 0, is->video_st->codec->height, vp->avframe_yuv420p->data, vp->avframe_yuv420p->linesize); vp->pts = pts;//传递当前图像帧的绝对显示时间戳 // Now we inform our display thread that we have a pic ready. if (++is->pictq_windex == VIDEO_PICTURE_QUEUE_SIZE) {//更新并检查当前图像帧队列写入位置 is->pictq_windex = 0;//重置图像帧队列写入位置 } SDL_LockMutex(is->pictq_lock);//锁定队列读写锁，保护队列数据 is->pictq_size++;//更新图像帧队列长度 SDL_UnlockMutex(is->pictq_lock);//释放队列读写锁 }// end for if return 0; } /*--------------------------- * 更新内部视频播放计时器(记录视频已经播时间(video_clock)） * @is：全局状态参数集 * @src_frame：当前(输入的)(待更新的)图像帧对象 * @pts：当前图像帧的显示时间戳 * update the PTS to be in sync ---------------------------*/ double synchronize_video(VideoState *is, AVFrame *src_frame, double pts) { /*----------检查显示时间戳----------*/ if (pts != 0) {//检查显示时间戳是否有效 // If we have pts, set video clock to it. is->video_clock = pts;//用显示时间戳更新已播放时间 } else {//若获取不到显示时间戳 // If we aren't given a pts, set it to the clock. pts = is->video_clock;//用已播放时间更新显示时间戳 } /*--------更新视频已经播时间--------*/ // Update the video clock，若该帧要重复显示(取决于repeat_pict)，则全局视频播放时序video_clock应加上重复显示的数量*帧率 double frame_delay = av_q2d(is->video_st->codec->time_base);//该帧显示完将要花费的时间 // If we are repeating a frame, adjust clock accordingly,若存在重复帧，则在正常播放的前后两帧图像间安排渲染重复帧 frame_delay += src_frame->repeat_pict*(frame_delay*0.5);//计算渲染重复帧的时值(类似于音符时值) is->video_clock += frame_delay;//更新视频播放时间 // printf(\"repeat_pict=%d \\n\",src_frame->repeat_pict); return pts;//此时返回的值即为下一帧将要开始显示的时间戳 } // These are called whenever we allocate a frame buffer. We use this to store the global_pts in a frame at the time it is allocated. int our_get_buffer(struct AVCodecContext *c, AVFrame *pic, int flags) { int ret = avcodec_default_get_buffer2(c, pic, 0); uint64_t *pts = (uint64_t *)av_malloc(sizeof(uint64_t)); *pts = global_video_pkt_pts; pic->opaque = pts; return ret; } // 视频解码线程函数 int decode_thread(void *arg) { VideoState *is = (VideoState *) arg;// 传递用户数据 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 int frameFinished;// 解码操作是否成功标识 // Allocate video frame，为解码后的视频信息结构体分配空间并完成初始化操作(结构体中的图像缓存按照下面两步手动安装) AVFrame *pFrame = av_frame_alloc(); double pts;//当前桢在整个视频中的(绝对)时间位置 for (;;) { if (packet_queue_get(&is->videoq,packet,1)pts;// Save global pts to be stored in pFrame in first call. /*----------------------- * Decode video frame，解码完整的一帧数据，并将frameFinished设置为true * 可能无法通过只解码一个packet就获得一个完整的视频帧frame，可能需要读取多个packet才行 * avcodec_decode_video2()会在解码到完整的一帧时设置frameFinished为真 * Technically a packet can contain partial frames or other bits of data * ffmpeg's parser ensures that the packets we get contain either complete or multiple frames * convert the packet to a frame for us and set frameFinisned for us when we have the next frame -----------------------*/ avcodec_decode_video2(is->video_st->codec, pFrame, &frameFinished, packet); //取得编码数据包中的显示时间戳PTS(int64_t),并暂时保存在pts(double)中 // if (packet->dts==AV_NOPTS_VALUE && pFrame->opaque && *(uint64_t*)pFrame->opaque!=AV_NOPTS_VALUE) { // pts = *(uint64_t *)pFrame->opaque; // } else if (packet->dts != AV_NOPTS_VALUE) { // pts = packet->dts; // } else { // pts = 0; // } pts=av_frame_get_best_effort_timestamp(pFrame);//取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 /*------------------------- * 在解码线程函数中计算当前图像帧的显示时间戳 * 1、取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 * 2、根据PTS*time_base来计算当前桢在整个视频中的显示时间戳，即PTS*(1/framerate) * av_q2d把AVRatioal结构转换成double的函数， * 用于计算视频源每个图像帧显示的间隔时间(1/framerate),即返回(time_base->num/time_base->den) -------------------------*/ //根据pts=PTS*time_base={numerator=1,denominator=25}计算当前桢在整个视频中的显示时间戳 pts*=av_q2d(is->video_st->time_base);//time_base为AVRational有理数结构体{num=1,den=25}，记录了视频源每个图像帧显示的间隔时间 // Did we get a video frame，检查是否解码出完整一帧图像 if (frameFinished) { pts = synchronize_video(is, pFrame, pts);//检查当前帧的显示时间戳pts并更新内部视频播放计时器(记录视频已经播时间(video_clock)） if (queue_picture(is, pFrame, pts)sws_ctx_audio, \"in_channel_layout\", src_ch_layout, 0); av_opt_set_int(is->sws_ctx_audio, \"out_channel_layout\", dst_ch_layout, 0); av_opt_set_int(is->sws_ctx_audio, \"in_sample_rate\", src_rate, 0); av_opt_set_int(is->sws_ctx_audio, \"out_sample_rate\", dst_rate, 0); av_opt_set_sample_fmt(is->sws_ctx_audio, \"in_sample_fmt\", src_sample_fmt, 0); av_opt_set_sample_fmt(is->sws_ctx_audio, \"out_sample_fmt\", dst_sample_fmt, 0); int ret;//返回结果 // Initialize the resampling context. if ((ret = swr_init((struct SwrContext *) is->sws_ctx_audio)) sws_ctx_audio,src_rate)+src_nb_samples,dst_rate,src_rate,AV_ROUND_UP); //Convert to destination format. ret=swr_convert((struct SwrContext*)is->sws_ctx_audio,dst_data,dst_nb_samples,(const uint8_t **)decoded_frame.data,src_nb_samples); if (retaudio_buf, dst_data[0], dst_bufsize); if (src_data) { av_freep(&src_data[0]); } av_freep(&src_data); if (dst_data) { av_freep(&dst_data[0]); } av_freep(&dst_data); return dst_bufsize; } // 音频解码函数，从缓存队列中提取数据包、解码，并返回解码后的数据长度(对一个完整的packet解码，将解码数据写入audio_buf缓存，并返回多帧解码数据的总长度) int audio_decode_frame(VideoState *is, double *pts_ptr) { int coded_consumed_size,data_size=0,pcm_bytes;// 每次消耗的编码数据长度[input](len1)，输出原始音频数据的缓存长度[output]，每组音频采样数据的字节数 AVPacket *pkt = &is->audio_pkt;// 保存从队列中提取的数据包 double pts;//音频播放时间戳 for (;;) { while (is->audio_pkt_size>0) {// 检查缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) int got_frame = 0;// 解码操作成功标识，成功返回非零值 // 解码一帧音频数据，并返回消耗的编码数据长度 coded_consumed_size = avcodec_decode_audio4(is->audio_st->codec, &is->audio_frame, &got_frame, pkt); if (coded_consumed_size audio_pkt_size = 0;// 更新编码数据缓存长度 break; } if (got_frame) {// 检查解码操作是否成功 if (is->audio_frame.format != AV_SAMPLE_FMT_S16) {//检查音频数据格式是否为16位采样格式 //当音频数据不为16位采样格式情况下，采用decode_frame_from_packet计算解码数据长度 data_size=decode_frame_from_packet(is, is->audio_frame); __android_log_print(ANDROID_LOG_INFO, \"main\", \"音频数据格式是采样格式:%d\",is->audio_frame.format); } else {//计算解码后音频数据长度[output] data_size=av_samples_get_buffer_size(NULL,is->audio_st->codec->channels,is->audio_frame.nb_samples,is->audio_st->codec->sample_fmt, 1); memcpy(is->audio_buf,is->audio_frame.data[0],data_size);//将解码数据复制到输出缓存 } } is->audio_pkt_data += coded_consumed_size;// 更新编码数据缓存指针位置 is->audio_pkt_size -= coded_consumed_size;// 更新缓存中剩余的编码数据长度 if (data_size audio_clock;//用每次更新的音频播放时间更新音频PTS *pts_ptr=pts; /*--------------------- * 当一个packet中包含多个音频帧时 * 通过[解码后音频原始数据长度]及[采样率]来推算一个packet中其他音频帧的播放时间戳pts * 采样频率44.1kHz，量化位数16位，意味着每秒采集数据44.1k个，每个数据占2字节 --------------------*/ pcm_bytes=2*is->audio_st->codec->channels;//计算每组音频采样数据的字节数=每个声道音频采样字节数*声道数 /*----更新audio_clock--- * 一个pkt包含多个音频frame，同时一个pkt对应一个pts(pkt->pts) * 因此，该pkt中包含的多个音频帧的时间戳由以下公式推断得出 * bytes_per_sec=pcm_bytes*is->audio_st->codec->sample_rate * 从pkt中不断的解码，推断(一个pkt中)每帧数据的pts并累加到音频播放时钟 --------------------*/ is->audio_clock+=(double)data_size/(double)(pcm_bytes*is->audio_st->codec->sample_rate); // We have data, return it and come back for more later. return data_size;// 返回解码数据缓存长度 } if (pkt->data) {// 检查数据包是否已从队列中提取 av_packet_unref(pkt);// 释放pkt中保存的编码数据 } if (is->quit) {// 检查退出进程标识 return -1; } // Next packet，从队列中提取数据包到pkt if (packet_queue_get(&is->audioq, pkt, 1) audio_pkt_data = pkt->data;// 传递编码数据缓存指针 is->audio_pkt_size = pkt->size;// 传递编码数据缓存长度 // If update, update the audio clock w/pts if (pkt->pts != AV_NOPTS_VALUE) {//检查音频播放时间戳 //获得一个新的packet的时候，更新audio_clock，用packet中的pts更新audio_clock(一个pkt对应一个pts) is->audio_clock=pkt->pts*av_q2d(is->audio_st->time_base);//更新音频已经播的时间 } } } /*------Audio Callback------- * 音频输出回调函数，sdl通过该回调函数将解码后的pcm数据送入声卡播放, * sdl通常一次会准备一组缓存pcm数据，通过该回调送入声卡，声卡根据音频pts依次播放pcm数据 * 待送入缓存的pcm数据完成播放后，再载入一组新的pcm缓存数据(每次音频输出缓存为空时，sdl就调用此函数填充音频输出缓存，并送入声卡播放) * When we begin playing audio, SDL will continually call this callback function * and ask it to fill the audio buffer with a certain number of bytes * The audio function callback takes the following parameters: * stream: A pointer to the audio buffer to be filled，输出音频数据到声卡缓存 * len: The length (in bytes) of the audio buffer,缓存长度wanted_spec.samples=SDL_AUDIO_BUFFER_SIZE(1024) --------------------------*/ void audio_callback(void *userdata, Uint8 *stream, int len) { VideoState *is = (VideoState *) userdata;// 传递用户数据 int wt_stream_len, audio_size;// 每次写入stream的数据长度，解码后的数据长度 double pts;//音频时间戳 while (len > 0) {//检查音频缓存的剩余长度 if (is->audio_buf_index >= is->audio_buf_size) {// 检查是否需要执行解码操作 // We have already sent all our data; get more，从缓存队列中提取数据包、解码，并返回解码后的数据长度，audio_buf缓存中可能包含多帧解码后的音频数据 audio_size = audio_decode_frame(is, &pts); if (audio_size audio_buf_size = 1024; memset(is->audio_buf, 0, is->audio_buf_size);// 全零重置缓冲区 } else { //在回调函数中增加音频同步过程，即对音频数据缓存进行丢帧(或插值)，以起到降低音频时钟与主同步源时差的目的 audio_size=synchronize_audio(is,(int16_t*)is->audio_buf,audio_size,pts);//返回音频同步后的缓存长度 is->audio_buf_size = audio_size;// 返回packet中包含的原始音频数据长度(多帧) } is->audio_buf_index = 0;// 初始化累计写入缓存长度 }//end for if wt_stream_len=is->audio_buf_size-is->audio_buf_index;// 计算解码缓存剩余长度 if (wt_stream_len > len) {// 检查每次写入缓存的数据长度是否超过指定长度(1024) wt_stream_len = len;// 指定长度从解码的缓存中取数据 } // 每次从解码的缓存数据中以指定长度抽取数据并写入stream传递给声卡 memcpy(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, wt_stream_len); len -= wt_stream_len;// 更新解码音频缓存的剩余长度 stream += wt_stream_len;// 更新缓存写入位置 is->audio_buf_index += wt_stream_len;// 更新累计写入缓存数据长度 }//end for while } // 根据指定类型打开流，找到对应的解码器、创建对应的音频配置、保存关键信息到 VideoState、启动音频和视频线程 int stream_component_open(VideoState *is, int stream_index) { AVFormatContext *pFormatCtx = is->pFormatCtx;// 传递文件容器的封装信息及码流参数 AVCodecContext *codecCtx = NULL;// 解码器上下文对象，解码器依赖的相关环境、状态、资源以及参数集的接口指针 AVCodec *codec = NULL;// 保存编解码器信息的结构体，提供编码与解码的公共接口，可以看作是编码器与解码器的一个全局变量 //检查输入的流类型是否在合理范围内 if (stream_index = pFormatCtx->nb_streams) { return -1; } // Get a pointer to the codec context for the video stream. codecCtx = pFormatCtx->streams[stream_index]->codec;// 取得解码器上下文 if (codecCtx->codec_type == AVMEDIA_TYPE_AUDIO) {//检查解码器类型是否为音频解码器 SDL_AudioSpec wanted_spec, spec;//SDL_AudioSpec a structure that contains the audio output format，创建 SDL_AudioSpec 结构体，设置音频播放数据 // Set audio settings from codec info,SDL_AudioSpec a structure that contains the audio output format // 创建SDL_AudioSpec结构体，设置音频播放参数 wanted_spec.freq = codecCtx->sample_rate;//采样频率 DSP frequency -- samples per second wanted_spec.format = AUDIO_S16SYS;//采样格式 Audio data format wanted_spec.channels = codecCtx->channels;//声道数 Number of channels: 1 mono, 2 stereo wanted_spec.silence = 0;//无输出时是否静音 //默认每次读音频缓存的大小，推荐值为 512~8192，ffplay使用的是1024 specifies a unit of audio data refers to the size of the audio buffer in sample frames wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE; wanted_spec.callback = audio_callback;//设置读取音频数据的回调接口函数 the function to call when the audio device needs more data wanted_spec.userdata = is;//传递用户数据 /*--------------------------- * 以指定参数打开音频设备，并返回与指定参数最为接近的参数，该参数为设备实际支持的音频参数 * Opens the audio device with the desired parameters(wanted_spec) * return another specs we actually be using * and not guaranteed to get what we asked for --------------------------*/ if (SDL_OpenAudio(&wanted_spec, &spec) audio_hw_buf_size = spec.size; } /*----------------------- * Find the decoder for the video stream，根据视频流对应的解码器上下文查找对应的解码器，返回对应的解码器(信息结构体) * The stream's information about the codec is in what we call the \"codec context. * This contains all the information about the codec that the stream is using -----------------------*/ codec = avcodec_find_decoder(codecCtx->codec_id); AVDictionary *optionsDict = NULL; if (!codec || (avcodec_open2(codecCtx, codec, &optionsDict) name); // 检查解码器类型 switch(codecCtx->codec_type) { case AVMEDIA_TYPE_AUDIO:// 音频解码器 is->audioStream = stream_index;// 音频流类型标号初始化 is->audio_st = pFormatCtx->streams[stream_index]; is->audio_buf_size = 0;// 解码后的多帧音频数据长度 is->audio_buf_index = 0;//累 计写入stream的长度 // Averaging filter for audio sync. is->audio_diff_avg_coef=exp(log(0.01/AUDIO_DIFF_AVG_NB));//音频时钟与主同步源累计时差加权系数 is->audio_diff_avg_count=0;//音频不同步计数初始化 // Correct audio only if larger error than this. is->audio_diff_threshold=2.0*SDL_AUDIO_BUFFER_SIZE/codecCtx->sample_rate; is->sws_ctx_audio = (struct SwsContext *) swr_alloc(); if (!is->sws_ctx_audio) { fprintf(stderr, \"Could not allocate resampler context\\n\"); return -1; } memset(&is->audio_pkt, 0, sizeof(is->audio_pkt)); packet_queue_init(&is->audioq);// 音频数据包队列初始化 SDL_PauseAudio(0);// audio callback starts running again，开启音频设备，如果这时候没有获得数据那么它就静音 break; case AVMEDIA_TYPE_VIDEO:// 视频解码器 is->videoStream = stream_index;// 视频流类型标号初始化 is->video_st = pFormatCtx->streams[stream_index]; //以系统时间为基准，初始化播放到当前帧的已播放时间值，该值为真实时间值、动态时间值、绝对时间值 is->frame_timer=(double)av_gettime()/1000000.0; is->frame_last_delay = 40e-3;//初始化上一帧图像的动态刷新延迟时间 is->video_current_pts_time = av_gettime();//取得系统当前时间 packet_queue_init(&is->videoq);// 视频数据包队列初始化 is->decode_tid = SDL_CreateThread(decode_thread,\"视频解码线程\" ,is);// 创建视频解码线程 // Initialize SWS context for software scaling，设置图像转换像素格式为AV_PIX_FMT_YUV420P is->sws_ctx = sws_getContext(is->video_st->codec->width, is->video_st->codec->height, is->video_st->codec->pix_fmt, is->video_st->codec->width, is->video_st->codec->height, AV_PIX_FMT_YUV420P, SWS_BILINEAR, NULL, NULL, NULL); codecCtx->get_buffer2 = our_get_buffer; break; default: break; } return 0; } // 编码数据包解析线程函数(从视频文件中解析出音视频编码数据单元，一个AVPacket的data通常对应一个NAL) int parse_thread(void *arg) { VideoState *is = (VideoState *)arg;// 传递用户参数 global_video_state = is;// 传递全局状态参量结构体 /*------------------------- * 打开封装格式 * 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 * 参数一：封装格式上下文 * 参数二：视频路径 * 参数三：指定输入的格式 * 参数四：设置默认参数 --------------------------*/ AVFormatContext *avformat_context = NULL;// 参数一：封装格式上下文 int avformat_open_input_result = avformat_open_input(&avformat_context, is->filename, NULL, NULL); if (avformat_open_input_result != 0){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"查找音视频流\\n\"); return -1; } is->pFormatCtx = avformat_context;//传递文件容器封装信息及码流参数 /*------------------------- * 查找码流 * 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 * 参数一：封装格式上下文 * 参数二：指定默认配置 -------------------------*/ int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result videoStream = -1;//视频流类型标号初始化为-1 is->audioStream = -1;//音频流类型标号初始化为-1 // 视频流类型标号初始化为-1 int av_video_stream_index = -1; // 音频流类型标号初始化为-1 int av_audio_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 若文件中包含有视频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_video_stream_index = i; } // 若文件中包含有音频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_audio_stream_index = i; } } // 检查文件中是否存在视频流 if (av_video_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到视频流\\n\"); goto fail;//跳转至异常处理 return -1; } // 检查文件中是否存在音频流 if (av_audio_stream_index == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到音频流\\n\"); goto fail;//跳转至异常处理 return -1; } stream_component_open(is, av_audio_stream_index);// 根据指定类型打开音频流 stream_component_open(is, av_video_stream_index);// 根据指定类型打开视频流 // Main decode loop. for (;;) { if (is->quit) {//检查退出进程标识 break; } // Seek stuff goes here，检查音视频编码数据包队列长度是否溢出 if (is->audioq.size > MAX_AUDIOQ_SIZE || is->videoq.size > MAX_VIDEOQ_SIZE) { SDL_Delay(10); continue; } /*----------------------- * read in a packet and store it in the AVPacket struct * ffmpeg allocates the internal data for us,which is pointed to by packet.data * this is freed by the av_free_packet() -----------------------*/ // 负责保存压缩编码数据相关信息的结构体,每帧图像由一到多个packet包组成 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 if (av_read_frame(is->pFormatCtx, packet) pFormatCtx->pb->error == 0) { SDL_Delay(100); // No error; wait for user input. continue; } else { break; } } // Is this a packet from the video stream? if (packet->stream_index == is->videoStream) {// 检查数据包是否为视频类型 packet_queue_put(&is->videoq, packet);// 向队列中插入数据包 } else if (packet->stream_index == is->audioStream) {// 检查数据包是否为音频类型 packet_queue_put(&is->audioq, packet);// 向队列中插入数据包 } else {// 检查数据包是否为字幕类型 av_packet_unref(packet);// 释放packet中保存的(字幕)编码数据 } } // All done - wait for it. while (!is->quit) { SDL_Delay(100); } fail:// 异常处理 if (1) { SDL_Event event;// SDL事件对象 event.type = FF_QUIT_EVENT;// 指定退出事件类型 event.user.data1 = is;// 传递用户数据 SDL_PushEvent(&event);// 将该事件对象压入SDL后台事件队列 } return 0; } int init_sdl(VideoState *is) { // 初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } // 初始化SDL窗口 SDL_Window* sdl_window = SDL_CreateWindow(\"FFmpeg+SDL播放视频\",// 参数一：窗口名称 SDL_WINDOWPOS_CENTERED,// 参数二：窗口在屏幕上的x坐标 SDL_WINDOWPOS_CENTERED,// 参数三：窗口在屏幕上的y坐标 is->video_width,// 参数四：窗口在屏幕上宽 is->video_height,// 参数五：窗口在屏幕上高 SDL_WINDOW_OPENGL);// 参数六：窗口状态(打开) if (sdl_window == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建渲染器 // 定义渲染器区域 SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window,// 渲染目标创建 -1, // 从那里开始渲染(-1:表示从第一个位置开始) 0);// 渲染类型(软件渲染) if (sdl_renderer == NULL){ __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建纹理 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer,// 渲染器 SDL_PIXELFORMAT_IYUV,// 像素数据格式 SDL_TEXTUREACCESS_STREAMING,// 绘制方式：频繁绘制- is->video_width,// 纹理宽 is->video_height);// 纹理高 if (sdl_texture == NULL) { __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } is->sdl_renderer = sdl_renderer; is->sdl_texture = sdl_texture; return 0; } // SDL入口 extern \"C\" int main(int argc, char *argv[]) { /*------------------------- * 注册组件 * 注册所有ffmpeg支持的多媒体格式及编解码器 -------------------------*/ av_register_all(); // 创建全局状态对象 VideoState *is= (VideoState *)av_mallocz(sizeof(VideoState)); av_strlcpy(is->filename, \"/storage/emulated/0/Download/test.mov\", sizeof(is->filename));// 复制视频文件路径名 is->video_width = 640; is->video_height = 352; av_strlcpy(is->filename, \"/storage/emulated/0/DCIM/Camera/TG-2022-04-13-160703582.mp4\", sizeof(is->filename));// 复制视频文件路径名 is->video_width = 720; is->video_height = 1280; is->pictq_lock = SDL_CreateMutex();// 创建编码数据包队列互斥锁对象 is->pictq_ready = SDL_CreateCond();// 创建编码数据包队列就绪条件对象 int init_sdl_result = init_sdl(is); if (init_sdl_result av_sync_type = DEFAULT_AV_SYNC_TYPE;//指定主同步源 // 创建编码数据包解析线程 is->parse_tid = SDL_CreateThread(parse_thread, \"编码数据包解析线程\", is); if (!is->parse_tid) {// 检查线程是否创建成功 av_free(is); return -1; } // SDL事件对象 SDL_Event event; for (;;) {// SDL事件循环 SDL_WaitEvent(&event);// 主线程阻塞，等待事件到来 switch(event.type) {// 事件到来后唤醒主线程，检查事件类型 case FF_QUIT_EVENT: case SDL_QUIT:// 退出进程事件 is->quit = 1; // If the video has finished playing, then both the picture and audio queues are waiting for more data. // Make them stop waiting and terminate normally.. avcodec_close(is->video_st->codec); avformat_free_context(is->pFormatCtx); SDL_CondSignal(is->audioq.qready);// 发出队列就绪信号避免死锁 SDL_CondSignal(is->videoq.qready); SDL_DestroyTexture(is->sdl_texture); SDL_DestroyRenderer(is->sdl_renderer); SDL_Quit(); return 0; case FF_ALLOC_EVENT: alloc_picture(event.user.data1);// 分配视频帧事件响应函数 break; case FF_REFRESH_EVENT:// 视频显示刷新事件 video_refresh_timer(event.user.data1);// 视频显示刷新事件响应函数 break; default: break; } } return 0; } "},"pages/ffmpeg/FFmpeg_SDL_fast_forward_and_rewind.html":{"url":"pages/ffmpeg/FFmpeg_SDL_fast_forward_and_rewind.html","title":"FFmpeg+SDL快进快退","keywords":"","body":"FFmpeg+SDL快进快退 iOS工程代码 ffmpeg播放器实现详解 - 快进快退控制：https://www.cnblogs.com/breakpointlab/p/15807316.html 源代码一览 #import #import \"AppDelegate.h\" #import \"SDL.h\" //#include //#include //#include #include #include \"SDL.h\" #include \"SDL_thread.h\" #include \"SDL_syswm.h\" #import \"ViewController.h\" #define SDL_AUDIO_BUFFER_SIZE 1024 #define MAX_AUDIO_FRAME_SIZE 192000 #define AV_SYNC_THRESHOLD 0.01//前后两帧间的显示时间间隔的最小值0.01s #define AV_NOSYNC_THRESHOLD 10.0//最小刷新间隔时间10ms #define MAX_AUDIOQ_SIZE (5 * 16 * 1024) #define MAX_VIDEOQ_SIZE (5 * 256 * 1024) #define FF_ALLOC_EVENT (SDL_USEREVENT) #define FF_REFRESH_EVENT (SDL_USEREVENT + 1) #define FF_QUIT_EVENT (SDL_USEREVENT + 2) #define VIDEO_PICTURE_QUEUE_SIZE 1 #define SAMPLE_CORRECTION_PERCENT_MAX 10 #define AUDIO_DIFF_AVG_NB 20 //extern \"C\" { #include #include \"libavformat/avformat.h\" #include #include #include #include #include #include //} AVPacket flush_pkt;//在执行[快进]/[快退]操作后，ffmpeg需要执行重置解码器操作 uint64_t global_video_pkt_pts = AV_NOPTS_VALUE; enum {//同步时钟源 AV_SYNC_AUDIO_MASTER,//音频时钟为主同步源 AV_SYNC_VIDEO_MASTER,//视频时钟为主同步源 AV_SYNC_EXTERNAL_MASTER,//外部时钟为主同步源 }; #define DEFAULT_AV_SYNC_TYPE AV_SYNC_AUDIO_MASTER//指定以视频时钟为主同步源(时间基准) SDL_Window *screen;//SDL绘图表面 /*-------链表节点结构体-------- typedef struct AVPacketList { AVPacket pkt;//链表数据 struct AVPacketList *next;//链表后继节点 } AVPacketList; ---------------------------*/ // 数据包队列(链表)结构体 typedef struct PacketQueue { AVPacketList *first_pkt, *last_pkt;// 队列首尾节点指针 int nb_packets;// 队列长度 int size;// 保存编码数据的缓存长度，size=packet->size SDL_mutex *qlock;// 队列互斥量，保护队列数据 SDL_cond *qready;// 队列就绪条件变量 } PacketQueue; // 图像帧结构体 typedef struct VideoPicture { AVFrame *avframe_yuv420p; int width, height;//Source height & width. int allocated;//是否分配内存空间，视频帧转换为SDL overlay标识 double pts;//当前图像帧的绝对显示时间戳 } VideoPicture; typedef struct VideoState { AVFormatContext *pFormatCtx;// 保存文件容器封装信息及码流参数的结构体 AVStream *video_st;// 视频流信息结构体 AVStream *audio_st;//音频流信息结构体 struct SwsContext *sws_ctx;// 描述转换器参数的结构体 struct SwsContext *sws_ctx_audio; PacketQueue videoq;// 视频编码数据包队列(编码数据队列，以链表方式实现) VideoPicture pictq[VIDEO_PICTURE_QUEUE_SIZE]; int pictq_size, pictq_rindex, pictq_windex;// 队列长度，读/写位置索引 SDL_mutex *pictq_lock;// 队列读写锁对象，保护图像帧队列数据 SDL_cond *pictq_ready;// 队列就绪条件变量 SDL_Rect sdl_rect; SDL_Renderer* sdl_renderer; SDL_Texture* sdl_texture; PacketQueue audioq;// 音频编码数据包队列(编码数据队列，以链表方式实现) uint8_t audio_buf[(MAX_AUDIO_FRAME_SIZE*3)/2];//保存解码一个packet后的多帧原始音频数据(解码数据队列，以数组方式实现) unsigned int audio_buf_size;//解码后的多帧音频数据长度 unsigned int audio_buf_index;//累计写入stream的长度 uint8_t *audio_pkt_data;//编码数据缓存指针位置 int audio_pkt_size;//缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) AVPacket audio_pkt;//保存从队列中提取的数据包 AVFrame audio_frame;//保存从数据包中解码的音频数据 int video_width; int video_height; char filename[1024];// 输入文件完整路径名 int videoStream, audioStream;// 音视频流类型标号 SDL_Thread *parse_tid;// 编码数据包解析线程id SDL_Thread *decode_tid;// 解码线程id int quit;// 全局退出进程标识，在界面上点了退出后，告诉线程退出 //video/audio_clock save pts of last decoded frame/predicted pts of next decoded frame double video_clock;//keep track of how much time has passed according to the video double audio_clock; double frame_timer;//视频播放到当前帧时的累计已播放时间 double frame_last_pts;//上一帧图像的显示时间戳，用于在video_refersh_timer中保存上一帧的pts值 double frame_last_delay;//上一帧图像的动态刷新延迟时间 int av_sync_type;//主同步源类型 double external_clock;//External clock base int64_t external_clock_time;//外部时钟的绝对时间 double audio_diff_cum;//音频时钟与同步源累计时差，sed for AV difference average computation double audio_diff_avg_coef;//音频时钟与同步源时差均值加权系数 double audio_diff_threshold;//音频时钟与同步源时差均值阈值 int audio_diff_avg_count;//音频不同步计数(音频时钟与主同步源存在不同步的次数) int audio_hw_buf_size; double video_current_pts;//当前帧显示时间戳，Current displayed pts (different from video_clock if frame fifos are used) int64_t video_current_pts_time;//取得video_current_pts的系统时间，time (av_gettime) at which we updated video_current_pts - used to have running video pts int seek_req;//[快进]/[后退]操作开启标志位 int seek_flags;//[快进]/[后退]操作类型标志位 int64_t seek_pos;//[快进]/[后退]操作后的参考时间戳 } VideoState;// Since we only have one decoding thread, the Big Struct can be global in case we need it. VideoState *global_video_state; /*------取得当前播放音频数据的pts------ * 音视频同步的原理是根据音频的pts来控制视频的播放 * 也就是说在视频解码一帧后，是否显示以及显示多长时间，是通过该帧的PTS与同时正在播放的音频的PTS比较而来的 * 如果音频的PTS较大，则视频准备完毕立即刷新，否则等待 * * 因为pcm数据采用audio_callback回调方式进行播放 * 对于音频播放我们只能得到写入回调函数前缓存音频帧的pts，而无法得到当前播放帧的pts(需要采用当前播放音频帧的pts作为参考时钟) * 考虑到音频的大小与播放时间成正比(相同采样率)，那么当前时刻正在播放的音频帧pts(位于回调函数缓存中) * 就可以根据已送入声卡的pcm数据长度、缓存中剩余pcm数据长度，缓存长度及采样率进行推算了 --------------------------------*/ double get_audio_clock(VideoState *is) { double pts=is->audio_clock;//Maintained in the audio thread，取得解码操作完成时的当前播放时间戳 //还未(送入声卡)播放的剩余原始音频数据长度，等于解码后的多帧原始音频数据长度-累计送入声卡的长度 int hw_buf_size=is->audio_buf_size-is->audio_buf_index;//计算当前音频解码数据缓存索引位置 int bytes_per_sec=0;//每秒的原始音频字节数 int pcm_bytes=is->audio_st->codec->channels*2;//每组原始音频数据字节数=声道数*每声道数据字节数 if (is->audio_st) { bytes_per_sec=is->audio_st->codec->sample_rate*pcm_bytes;//计算每秒的原始音频字节数 } if (bytes_per_sec) {//检查每秒的原始音频字节数 pts-=(double)hw_buf_size/bytes_per_sec;//根据送入声卡缓存的索引位置，往前倒推计算当前时刻的音频播放时间戳pts } return pts;//返回当前正在播放的音频时间戳 } /*-----------取得视频时钟----------- * 即取得当前播放视频帧的pts，以视频时钟pts作为音视频同步基准，return the current time offset of the video currently being played * 该值为当前帧时间戳pts+一个微小的修正值delta * 因为在ms的级别上，在毫秒级别上，若取得视频时钟(即当前帧pts)的时刻，与调用视频时钟的时刻(如将音频同步到该视频pts时刻)存在延迟 * 那么，视频时钟需要在被调用时进行修正，修正值delta为 * delta=[取得视频时钟的时刻值video_current_pts_time] 到 [调用get_video_clock时刻值] 的间隔时间 * 通常情况下，都会选择以外部时钟或音频时钟作为主同步源，以视频同步到音频或外部时钟为首选同步方案 * 以视频时钟作为主同步源的同步方案，属于3种基本的同步方案(同步到音频、同步到视频、同步到外部时钟) * 本利仅为展示同步到视频时钟的方法，一般情况下同步到视频时钟仅作为辅助的同步方案 --------------------------------*/ double get_video_clock(VideoState *is) { double delta=(av_gettime()-is->video_current_pts_time)/1000000.0; //pts_of_last_frame+(Current_time-time_elapsed_since_pts_value_was_set) return is->video_current_pts+delta; } //取得系统时间，以系统时钟作为同步基准 double get_external_clock(VideoState *is) { return av_gettime()/1000000.0;//取得系统当前时间，以1/1000000秒为单位，便于在各个平台移植 } //取得主时钟(基准时钟) double get_master_clock(VideoState *is) { if (is->av_sync_type == AV_SYNC_VIDEO_MASTER) { return get_video_clock(is);//返回视频时钟 } else if (is->av_sync_type == AV_SYNC_AUDIO_MASTER) { return get_audio_clock(is);//返回音频时钟 } else { return get_external_clock(is);//返回系统时钟 } } /*--------------------------- * return the wanted number of samples to get better sync if sync_type is video or external master clock * 通常情况下会以音频或系统时钟为主同步源，只有在音频或系统时钟失效的情况下才以视频为主同步源 * 该函数比对音频时钟与主同步源的时差，通过动态丢帧(或插值)部分音频数据，以起到减少(或增加)音频播放时长，减少与主同步源时差的作用 * 该函数对音频缓存数据进行丢帧(或插值)，返回丢帧(或插值)后的音频数据长度 * 因为音频同步可能带来输出声音不连续等副作用，该函数通过音频不同步次数(audio_diff_avg_count)及时差均值(avg_diff)来约束音频的同步过程 ---------------------------*/ int synchronize_audio(VideoState *is, short *samples, int samples_size, double pts) { double ref_clock;//主同步源(基准时钟) int pcm_bytes=is->audio_st->codec->channels*2;//每组音频数据字节数=声道数*每声道数据字节数 /* if not master, then we try to remove or add samples to correct the clock */ if (is->av_sync_type != AV_SYNC_AUDIO_MASTER) {//检查主同步源，若同步源不是音频时钟的情况下，执行以下代码 double diff, avg_diff;//diff-音频帧播放间与主同步源时差，avg_diff-采样不同步平均值 int wanted_size, min_size, max_size;//经过丢帧(或插值)后的缓存长度，缓存长度最大/最小值 ref_clock = get_master_clock(is);//取得当前主同步源，以主同步源为基准时间 diff = get_audio_clock(is) - ref_clock;//计算音频时钟与当前主同步源的时差 if (diffaudio_diff_cum=diff+is->audio_diff_avg_coef*is->audio_diff_cum; if (is->audio_diff_avg_countaudio_diff_avg_count++;//音频不同步计数更新 } else {//当音频不同步次数超过阈值限定后，触发音频同步操作 avg_diff=is->audio_diff_cum*(1.0-is->audio_diff_avg_coef);//计算时差均值(等比级数几何平均数) if (fabs(avg_diff)>=is->audio_diff_threshold) {//比对时差均值与时差阈值 wanted_size=samples_size+((int)(diff*is->audio_st->codec->sample_rate)*pcm_bytes);//根据时差换算同步后的缓存长度 min_size=samples_size*((100-SAMPLE_CORRECTION_PERCENT_MAX)/100);//同步后的缓存长度最小值 max_size=samples_size*((100+SAMPLE_CORRECTION_PERCENT_MAX)/100);//同步后的缓存长度最大值 if (wanted_sizemax_size) {//若同步后缓存长度>最小缓存长度 wanted_size=max_size;//用最大缓存长度作为同步后的缓存长度 } if (wanted_sizesamples_size) {//若同步后缓存长度大于当前缓存长度 //Add samples by copying final sample，通过复制最后一个音频数据进行插值 //int nb=samples_size-wanted_size; int nb=wanted_size-samples_size;//计算插值后缓存长度与原始缓存长度间的差值(需要插值的音频数据组数) uint8_t *samples_end=(uint8_t*)samples+samples_size-pcm_bytes;//取得缓存末端数据指针 uint8_t *q=samples_end+pcm_bytes;//初始插值位置|||q| while (nb>0) {//检查插值音频组数(每组包括两个声道的pcm数据) memcpy(q,samples_end,pcm_bytes);//在samples原始缓存后追加插值 q += pcm_bytes;//更新插值位置 nb -= pcm_bytes;//更新插值组数 } samples_size=wanted_size;//返回音频同步后的缓存长度 } } } } else { // Difference is too big, reset diff stuff，时差过大，重置时差累计值 is->audio_diff_avg_count = 0;//音频不同步计数重置 is->audio_diff_cum = 0;//音频累计时差重置 } }//end for if (is->av_sync_type != AV_SYNC_AUDIO_MASTER) return samples_size;//返回发送到声卡的音频缓存字节数 } // 定时器触发的回调函数 static Uint32 sdl_refresh_timer_cb(Uint32 interval, void *opaque) { SDL_Event event;//SDL事件对象 event.type = FF_REFRESH_EVENT;//视频显示刷新事件 event.user.data1 = opaque;//传递用户数据 SDL_PushEvent(&event);//发送事件 return 0; // 0 means stop timer. } /*--------------------------- * Schedule a video refresh in 'delay' ms. * 告诉sdl在指定的延时后来推送一个 FF_REFRESH_EVENT 事件 * 这个事件将在事件队列里触发sdl_refresh_timer_cb函数的调用 --------------------------*/ static void schedule_refresh(VideoState *is, int delay) { SDL_AddTimer(delay, sdl_refresh_timer_cb, is);//在指定的时间(ms)后回调用户指定的函数 } // 视频(图像)帧渲染 void video_display(VideoState *is) { SDL_Rect rect;// SDL矩形对象 VideoPicture *vp;// 图像帧结构体指针 float aspect_ratio;//宽度/高度比 int w, h, x, y;//窗口尺寸及起始位置 vp = &is->pictq[is->pictq_rindex];//从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 if (is->video_st->codec->sample_aspect_ratio.num == 0) { aspect_ratio = 0; } else { aspect_ratio = av_q2d(is->video_st->codec->sample_aspect_ratio) * is->video_st->codec->width / is->video_st->codec->height; } if (aspect_ratio video_st->codec->width / (float) is->video_st->codec->height; } // SDL中获取屏幕尺寸 SDL_DisplayMode DM; SDL_GetCurrentDisplayMode(0, &DM); h = DM.h; w = ((int) rint(h * aspect_ratio)) & -3; if (w > DM.w) { w = DM.w; h = ((int) rint(w / aspect_ratio)) & -3; } x = (DM.w - w) / 2; y = (DM.h - h) / 2; // 设置纹理数据 SDL_UpdateTexture(is->sdl_texture, // 纹理 NULL,// 渲染区域 vp->avframe_yuv420p->data[0],// 需要渲染数据：视频像素数据帧 vp->avframe_yuv420p->linesize[0]);// 帧宽 // 将纹理数据拷贝给渲染器 // 设置左上角位置(全屏) is->sdl_rect.x = x; is->sdl_rect.y = y; is->sdl_rect.w = w; is->sdl_rect.h = h; SDL_RenderClear(is->sdl_renderer); SDL_RenderCopy(is->sdl_renderer, is->sdl_texture, NULL, &is->sdl_rect); // 呈现画面帧 SDL_RenderPresent(is->sdl_renderer); }// end for if }// end for video_display // 显示刷新函数(FF_REFRESH_EVENT响应函数) int video_current_index = 0; void video_refresh_timer(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp;//图像帧对象 //delay-前后帧间的显示时间间隔，diff-图像帧显示与音频帧播放间的时间差 //sync_threshold-前后帧间的最小时间差，actual_delay-当前帧-下已帧的显示时间间隔(动态时间、真实时间、绝对时间) double delay,diff,sync_threshold,actual_delay,ref_clock;//ref_clock-音频时间戳 if (is->video_st) { if (is->pictq_size == 0) {// 检查图像帧队列是否有待显示图像 schedule_refresh(is, 1);//若队列为空，则发送显示刷新事件并再次进入video_refresh_timer函数 } else {// 刷新图像 vp = &is->pictq[is->pictq_rindex];//从显示队列中取得等待显示的图像帧 is->video_current_pts = vp->pts;//取得当前帧的显示时间戳 is->video_current_pts_time = av_gettime();//取得系统时间，作为当前帧播放的时间基准 //计算当前帧和前一帧显示(pts)的间隔时间(显示时间戳的差值) //计算当前帧和前一帧显示(pts)的间隔时间(显示时间戳的差值) delay = vp->pts - is->frame_last_pts;//The pts from last time，前后帧间的时间差 if (delay = 1.0) {//检查时间间隔是否在合理范围 // If incorrect delay, use previous one delay = is->frame_last_delay;//沿用之前的动态刷新间隔时间 } // Save for next time is->frame_last_delay = delay;//保存上一帧图像的动态刷新延迟时间 is->frame_last_pts = vp->pts;//保存上一帧图像的显示时间戳 // Update delay to sync to audio，取得声音播放时间戳(作为视频同步的参考时间) // Update delay to sync to audio，取得声音播放时间戳(作为视频同步的参考时间) if (is->av_sync_type != AV_SYNC_VIDEO_MASTER) {//检查主同步时钟源 ref_clock = get_master_clock(is);//根据Audio clock来判断Video播放的快慢，获取当前播放声音的时间戳 //也就是说在diff这段时间中声音是匀速发生的，但是在delay这段时间frame的显示可能就会有快慢的区别 diff = vp->pts - ref_clock;//计算图像帧显示与音频帧播放间的时间差 //根据时间差调整播放下一帧的延迟时间，以实现同步 Skip or repeat the frame，Take delay into account sync_threshold = (delay > AV_SYNC_THRESHOLD) ? delay : AV_SYNC_THRESHOLD;//比较前后两帧间的显示时间间隔与最小时间间隔 //判断音视频不同步条件，即音视频间的时间差 & 前后帧间的时间差该阈值则为快进模式，不存在音视频同步问题 if (fabs(diff) = sync_threshold) {//比较两帧画面间的显示时间与两帧画面间声音的播放时间，快了，加倍delay delay = 2 * delay; } }//如果diff(明显)大于AV_NOSYNC_THRESHOLD，即快进的模式了，画面跳动太大，不存在音视频同步的问题了 } //更新视频播放到当前帧时的已播放时间值(所有图像帧动态播放累计时间值-真实值)，frame_timer一直累加在播放过程中我们计算的延时 is->frame_timer+=delay; //每次计算frame_timer与系统时间的差值(以系统时间为基准时间)，将frame_timer与系统时间(绝对时间)相关联的目的 actual_delay=is->frame_timer-(av_gettime()/1000000.0);//Computer the REAL delay if (actual_delay pts: %f，ref_clock：%f，actual_delay：%f\", video_current_index, vp->pts, ref_clock, actual_delay); // Update queue for next picture! if (++is->pictq_rindex == VIDEO_PICTURE_QUEUE_SIZE) {// 更新并检查图像帧队列读位置索引 is->pictq_rindex = 0;// 重置读位置索引 } SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 is->pictq_size--;// 更新图像帧队列长度 SDL_CondSignal(is->pictq_ready);// 发送队列就绪信号 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } } else { schedule_refresh(is, 100); } } // 数据包队列初始化函数 void packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue));// 全零初始化队列结构体对象 q->qlock = SDL_CreateMutex();// 创建互斥量对象 q->qready = SDL_CreateCond();// 创建条件变量对象 } // 向队列中插入数据包 int packet_queue_put(PacketQueue *q, AVPacket *pkt) { /*-------准备队列(链表)节点对象------*/ AVPacketList *pktlist=(AVPacketList *)av_malloc(sizeof(AVPacketList));// 在堆上创建链表节点对象 if (!pktlist) {// 检查链表节点对象是否创建成功 return -1; } pktlist->pkt = *pkt;// 将输入数据包赋值给新建链表节点对象中的数据包对象 pktlist->next = NULL;// 链表后继指针为空 // if (av_packet_ref(pkt, pkt) qlock);// 队列互斥量加锁，保护队列数据 if (!q->last_pkt) {// 检查队列尾节点是否存在(检查队列是否为空) q->first_pkt = pktlist;// 若不存在(队列尾空)，则将当前节点作队列为首节点 } else { q->last_pkt->next = pktlist;// 若已存在尾节点，则将当前节点挂到尾节点的后继指针上，并作为新的尾节点 } q->last_pkt = pktlist;// 将当前节点作为新的尾节点 q->nb_packets++;// 队列长度+1 q->size += pktlist->pkt.size;// 更新队列编码数据的缓存长度 SDL_CondSignal(q->qready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(q->qlock);// 释放互斥量 return 0; } // 从队列中提取数据包，并将提取的数据包出队列 static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block) { AVPacketList *pktlist;// 临时链表节点对象指针 int ret;// 操作结果 SDL_LockMutex(q->qlock);// 队列互斥量加锁，保护队列数据 for (;;) { if (global_video_state->quit) {// 检查退出进程标识 ret = -1;// 操作失败 break; }//end for if pktlist = q->first_pkt;// 传递将队列首个数据包指针 if (pktlist) {// 检查数据包是否为空(队列是否有数据) q->first_pkt = pktlist->next;// 队列首节点指针后移 if (!q->first_pkt) {// 检查首节点的后继节点是否存在 q->last_pkt = NULL;// 若不存在，则将尾节点指针置空 } q->nb_packets--;// 队列长度-1 q->size -= pktlist->pkt.size;// 更新队列编码数据的缓存长度 *pkt = pktlist->pkt;// 将队列首节点数据返回 av_free(pktlist);// 清空临时节点数据(清空首节点数据，首节点出队列) ret = 1;// 操作成功 break; } else if (!block) { ret = 0; break; } else {// 队列处于未就绪状态，此时通过SDL_CondWait函数等待qready就绪信号，并暂时对互斥量解锁 /*--------------------- * 等待队列就绪信号qready，并对互斥量暂时解锁 * 此时线程处于阻塞状态，并置于等待条件就绪的线程列表上 * 使得该线程只在临界区资源就绪后才被唤醒，而不至于线程被频繁切换 * 该函数返回时，互斥量再次被锁住，并执行后续操作 --------------------*/ SDL_CondWait(q->qready, q->qlock);// 暂时解锁互斥量并将自己阻塞，等待临界区资源就绪(等待SDL_CondSignal发出临界区资源就绪的信号) } }//end for for-loop SDL_UnlockMutex(q->qlock);// 释放互斥量 return ret; } // 创建/重置图像帧，为图像帧分配内存空间 void alloc_picture(void *userdata) { VideoState *is = (VideoState *)userdata;// 传递用户数据 VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列(数组)中提取图像帧结构对象 if (vp->avframe_yuv420p) {// 检查图像帧是否已存在 // We already have one make another, bigger/smaller. av_frame_free(&vp->avframe_yuv420p); } vp->width = is->video_st->codec->width;// 设置图像帧宽度 vp->height = is->video_st->codec->height;// 设置图像帧高度 SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护画布的像素数据 vp->allocated = 1;// 图像帧像素缓冲区已分配内存 // AV_PIX_FMT_YUV420P格式的视频帧 vp->avframe_yuv420p = av_frame_alloc(); // 给缓冲区设置类型 int buffer_size =av_image_get_buffer_size(AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 一帧视频像素数据宽 = 视频宽 is->video_st->codec->height,// 一帧视频像素数据高 = 视频高 1);// 字节对齐方式，默认是1 // 开辟一块内存空间 uint8_t *out_buffer = (uint8_t *)av_malloc(buffer_size); // 向avframe_yuv420p填充数据 av_image_fill_arrays(vp->avframe_yuv420p->data,// 目标视频帧数据 vp->avframe_yuv420p->linesize,// 目标视频帧行大小 out_buffer,// 原始数据 AV_PIX_FMT_YUV420P,// 视频像素数据格式类型 is->video_st->codec->width,// 视频宽 is->video_st->codec->height,//视频高 1);// 字节对齐方式 SDL_CondSignal(is->pictq_ready);// 给等待线程发出消息，通知队列已就绪 SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 } /*--------------------------- * queue_picture：图像帧插入队列等待渲染 * @is：全局状态参数集 * @pFrame：保存图像解码数据的结构体 * 1、首先检查图像帧队列(数组)是否存在空间插入新的图像，若没有足够的空间插入图像则使当前线程休眠等待 * 2、在初始化的条件下，队列(数组)中VideoPicture的bmp对象(YUV overlay)尚未分配空间，通过FF_ALLOC_EVENT事件的方法调用alloc_picture分配空间 * 3、当队列(数组)中所有VideoPicture的bmp对象(YUV overlay)均已分配空间的情况下，直接跳过步骤2向bmp对象拷贝像素数据，像素数据在进行格式转换后执行拷贝操作 ---------------------------*/ int queue_picture(VideoState *is, AVFrame *pFrame, double pts) { /*--------1、检查队列是否有插入空间-------*/ // Wait until we have space for a new pic. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (is->pictq_size >= VIDEO_PICTURE_QUEUE_SIZE && !is->quit) {// 检查队列当前长度 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待pictq_ready信号 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } /*-------2、初始化/重置YUV overlay-------*/ // windex is set to 0 initially. VideoPicture *vp=&is->pictq[is->pictq_windex];// 从图像帧队列中抽取图像帧对象 // Allocate or resize the buffer，检查YUV overlay是否已存在，否则初始化YUV overlay，分配像素缓存空间 if (!vp->avframe_yuv420p || vp->width!=is->video_st->codec->width || vp->height!=is->video_st->codec->height) { vp->allocated = 0;// 图像帧未分配空间 // We have to do it in the main thread. SDL_Event event;// SDL事件对象 event.type = FF_ALLOC_EVENT;//指定分配图像帧内存事件 event.user.data1 = is;//传递用户数据 SDL_PushEvent(&event);//发送SDL事件 // Wait until we have a picture allocated. SDL_LockMutex(is->pictq_lock);// 锁定互斥量，保护图像帧队列 while (!vp->allocated && !is->quit) {// 检查当前图像帧是否已初始化 SDL_CondWait(is->pictq_ready, is->pictq_lock);// 线程休眠等待alloc_picture发送pictq_ready信号唤醒当前线程 } SDL_UnlockMutex(is->pictq_lock);// 释放互斥量 if (is->quit) {// 检查进程退出标识 return -1; } }// end for if /*--------3、拷贝视频帧到YUV overlay-------*/ // We have a place to put our picture on the queue. if (vp->avframe_yuv420p) {//检查像素数据指针是否有效 // Convert the image into YUV format that SDL uses，将解码后的图像帧转换为AV_PIX_FMT_YUV420P格式，并拷贝到图像帧队列 sws_scale(is->sws_ctx, (uint8_t const * const *)pFrame->data, pFrame->linesize, 0, is->video_st->codec->height, vp->avframe_yuv420p->data, vp->avframe_yuv420p->linesize); vp->pts = pts;//传递当前图像帧的绝对显示时间戳 // Now we inform our display thread that we have a pic ready. if (++is->pictq_windex == VIDEO_PICTURE_QUEUE_SIZE) {//更新并检查当前图像帧队列写入位置 is->pictq_windex = 0;//重置图像帧队列写入位置 } SDL_LockMutex(is->pictq_lock);//锁定队列读写锁，保护队列数据 is->pictq_size++;//更新图像帧队列长度 SDL_UnlockMutex(is->pictq_lock);//释放队列读写锁 }// end for if return 0; } /*--------------------------- * 更新内部视频播放计时器(记录视频已经播时间(video_clock)） * @is：全局状态参数集 * @src_frame：当前(输入的)(待更新的)图像帧对象 * @pts：当前图像帧的显示时间戳 * update the PTS to be in sync ---------------------------*/ double synchronize_video(VideoState *is, AVFrame *src_frame, double pts) { /*----------检查显示时间戳----------*/ if (pts != 0) {//检查显示时间戳是否有效 // If we have pts, set video clock to it. is->video_clock = pts;//用显示时间戳更新已播放时间 } else {//若获取不到显示时间戳 // If we aren't given a pts, set it to the clock. pts = is->video_clock;//用已播放时间更新显示时间戳 } /*--------更新视频已经播时间--------*/ // Update the video clock，若该帧要重复显示(取决于repeat_pict)，则全局视频播放时序video_clock应加上重复显示的数量*帧率 double frame_delay = av_q2d(is->video_st->codec->time_base);//该帧显示完将要花费的时间 // If we are repeating a frame, adjust clock accordingly,若存在重复帧，则在正常播放的前后两帧图像间安排渲染重复帧 frame_delay += src_frame->repeat_pict*(frame_delay*0.5);//计算渲染重复帧的时值(类似于音符时值) is->video_clock += frame_delay;//更新视频播放时间 // printf(\"repeat_pict=%d \\n\",src_frame->repeat_pict); return pts;//此时返回的值即为下一帧将要开始显示的时间戳 } // These are called whenever we allocate a frame buffer. We use this to store the global_pts in a frame at the time it is allocated. int our_get_buffer(struct AVCodecContext *c, AVFrame *pic, int flags) { int ret = avcodec_default_get_buffer2(c, pic, 0); uint64_t *pts = (uint64_t *)av_malloc(sizeof(uint64_t)); *pts = global_video_pkt_pts; pic->opaque = pts; return ret; } // 视频解码线程函数 int decode_thread(void *arg) { VideoState *is = (VideoState *) arg;// 传递用户数据 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 int frameFinished;// 解码操作是否成功标识 // Allocate video frame，为解码后的视频信息结构体分配空间并完成初始化操作(结构体中的图像缓存按照下面两步手动安装) AVFrame *pFrame = av_frame_alloc(); double pts;//当前桢在整个视频中的(绝对)时间位置 for (;;) { if (packet_queue_get(&is->videoq,packet,1)data == flush_pkt.data) {//检查是否需要重新解码 avcodec_flush_buffers(is->video_st->codec);//重新解码前需要重置解码器 continue; } pts = 0;//(绝对)显示时间戳初始化 global_video_pkt_pts = packet->pts;// Save global pts to be stored in pFrame in first call. /*----------------------- * Decode video frame，解码完整的一帧数据，并将frameFinished设置为true * 可能无法通过只解码一个packet就获得一个完整的视频帧frame，可能需要读取多个packet才行 * avcodec_decode_video2()会在解码到完整的一帧时设置frameFinished为真 * Technically a packet can contain partial frames or other bits of data * ffmpeg's parser ensures that the packets we get contain either complete or multiple frames * convert the packet to a frame for us and set frameFinisned for us when we have the next frame -----------------------*/ avcodec_decode_video2(is->video_st->codec, pFrame, &frameFinished, packet); //取得编码数据包中的显示时间戳PTS(int64_t),并暂时保存在pts(double)中 // if (packet->dts==AV_NOPTS_VALUE && pFrame->opaque && *(uint64_t*)pFrame->opaque!=AV_NOPTS_VALUE) { // pts = *(uint64_t *)pFrame->opaque; // } else if (packet->dts != AV_NOPTS_VALUE) { // pts = packet->dts; // } else { // pts = 0; // } pts=av_frame_get_best_effort_timestamp(pFrame);//取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 /*------------------------- * 在解码线程函数中计算当前图像帧的显示时间戳 * 1、取得编码数据包中的图像帧显示序号PTS(int64_t),并暂时保存在pts(double)中 * 2、根据PTS*time_base来计算当前桢在整个视频中的显示时间戳，即PTS*(1/framerate) * av_q2d把AVRatioal结构转换成double的函数， * 用于计算视频源每个图像帧显示的间隔时间(1/framerate),即返回(time_base->num/time_base->den) -------------------------*/ //根据pts=PTS*time_base={numerator=1,denominator=25}计算当前桢在整个视频中的显示时间戳 pts*=av_q2d(is->video_st->time_base);//time_base为AVRational有理数结构体{num=1,den=25}，记录了视频源每个图像帧显示的间隔时间 // Did we get a video frame，检查是否解码出完整一帧图像 if (frameFinished) { pts = synchronize_video(is, pFrame, pts);//检查当前帧的显示时间戳pts并更新内部视频播放计时器(记录视频已经播时间(video_clock)） if (queue_picture(is, pFrame, pts)sws_ctx_audio, \"in_channel_layout\", src_ch_layout, 0); av_opt_set_int(is->sws_ctx_audio, \"out_channel_layout\", dst_ch_layout, 0); av_opt_set_int(is->sws_ctx_audio, \"in_sample_rate\", src_rate, 0); av_opt_set_int(is->sws_ctx_audio, \"out_sample_rate\", dst_rate, 0); av_opt_set_sample_fmt(is->sws_ctx_audio, \"in_sample_fmt\", src_sample_fmt, 0); av_opt_set_sample_fmt(is->sws_ctx_audio, \"out_sample_fmt\", dst_sample_fmt, 0); int ret;//返回结果 // Initialize the resampling context. if ((ret = swr_init((struct SwrContext *) is->sws_ctx_audio)) sws_ctx_audio,src_rate)+src_nb_samples,dst_rate,src_rate,AV_ROUND_UP); //Convert to destination format. ret=swr_convert((struct SwrContext*)is->sws_ctx_audio,dst_data,dst_nb_samples,(const uint8_t **)decoded_frame.data,src_nb_samples); if (retaudio_buf, dst_data[0], dst_bufsize); if (src_data) { av_freep(&src_data[0]); } av_freep(&src_data); if (dst_data) { av_freep(&dst_data[0]); } av_freep(&dst_data); return dst_bufsize; } // 音频解码函数，从缓存队列中提取数据包、解码，并返回解码后的数据长度(对一个完整的packet解码，将解码数据写入audio_buf缓存，并返回多帧解码数据的总长度) int audio_decode_frame(VideoState *is, double *pts_ptr) { int coded_consumed_size,data_size=0,pcm_bytes;// 每次消耗的编码数据长度[input](len1)，输出原始音频数据的缓存长度[output]，每组音频采样数据的字节数 AVPacket *pkt = &is->audio_pkt;// 保存从队列中提取的数据包 double pts;//音频播放时间戳 for (;;) { while (is->audio_pkt_size>0) {// 检查缓存中剩余的编码数据长度(是否已完成一个完整的pakcet包的解码，一个数据包中可能包含多个音频编码帧) int got_frame = 0;// 解码操作成功标识，成功返回非零值 // 解码一帧音频数据，并返回消耗的编码数据长度 coded_consumed_size = avcodec_decode_audio4(is->audio_st->codec, &is->audio_frame, &got_frame, pkt); if (coded_consumed_size audio_pkt_size = 0;// 更新编码数据缓存长度 break; } if (got_frame) {// 检查解码操作是否成功 if (is->audio_frame.format != AV_SAMPLE_FMT_S16) {//检查音频数据格式是否为16位采样格式 //当音频数据不为16位采样格式情况下，采用decode_frame_from_packet计算解码数据长度 data_size=decode_frame_from_packet(is, is->audio_frame); } else {//计算解码后音频数据长度[output] data_size=av_samples_get_buffer_size(NULL,is->audio_st->codec->channels,is->audio_frame.nb_samples,is->audio_st->codec->sample_fmt, 1); memcpy(is->audio_buf,is->audio_frame.data[0],data_size);//将解码数据复制到输出缓存 } } is->audio_pkt_data += coded_consumed_size;// 更新编码数据缓存指针位置 is->audio_pkt_size -= coded_consumed_size;// 更新缓存中剩余的编码数据长度 if (data_size audio_clock;//用每次更新的音频播放时间更新音频PTS *pts_ptr=pts; /*--------------------- * 当一个packet中包含多个音频帧时 * 通过[解码后音频原始数据长度]及[采样率]来推算一个packet中其他音频帧的播放时间戳pts * 采样频率44.1kHz，量化位数16位，意味着每秒采集数据44.1k个，每个数据占2字节 --------------------*/ pcm_bytes=2*is->audio_st->codec->channels;//计算每组音频采样数据的字节数=每个声道音频采样字节数*声道数 /*----更新audio_clock--- * 一个pkt包含多个音频frame，同时一个pkt对应一个pts(pkt->pts) * 因此，该pkt中包含的多个音频帧的时间戳由以下公式推断得出 * bytes_per_sec=pcm_bytes*is->audio_st->codec->sample_rate * 从pkt中不断的解码，推断(一个pkt中)每帧数据的pts并累加到音频播放时钟 --------------------*/ is->audio_clock+=(double)data_size/(double)(pcm_bytes*is->audio_st->codec->sample_rate); // We have data, return it and come back for more later. return data_size;// 返回解码数据缓存长度 } if (pkt->data) {// 检查数据包是否已从队列中提取 av_packet_unref(pkt);// 释放pkt中保存的编码数据 } if (is->quit) {// 检查退出进程标识 return -1; } // Next packet，从队列中提取数据包到pkt if (packet_queue_get(&is->audioq, pkt, 1) data == flush_pkt.data) {//检查是否需要重新解码 avcodec_flush_buffers(is->audio_st->codec);//重新解码前需要重置解码器 continue; } is->audio_pkt_data = pkt->data;// 传递编码数据缓存指针 is->audio_pkt_size = pkt->size;// 传递编码数据缓存长度 // If update, update the audio clock w/pts if (pkt->pts != AV_NOPTS_VALUE) {//检查音频播放时间戳 //获得一个新的packet的时候，更新audio_clock，用packet中的pts更新audio_clock(一个pkt对应一个pts) is->audio_clock=pkt->pts*av_q2d(is->audio_st->time_base);//更新音频已经播的时间 } } } /*------Audio Callback------- * 音频输出回调函数，sdl通过该回调函数将解码后的pcm数据送入声卡播放, * sdl通常一次会准备一组缓存pcm数据，通过该回调送入声卡，声卡根据音频pts依次播放pcm数据 * 待送入缓存的pcm数据完成播放后，再载入一组新的pcm缓存数据(每次音频输出缓存为空时，sdl就调用此函数填充音频输出缓存，并送入声卡播放) * When we begin playing audio, SDL will continually call this callback function * and ask it to fill the audio buffer with a certain number of bytes * The audio function callback takes the following parameters: * stream: A pointer to the audio buffer to be filled，输出音频数据到声卡缓存 * len: The length (in bytes) of the audio buffer,缓存长度wanted_spec.samples=SDL_AUDIO_BUFFER_SIZE(1024) --------------------------*/ void audio_callback(void *userdata, Uint8 *stream, int len) { VideoState *is = (VideoState *) userdata;// 传递用户数据 int wt_stream_len, audio_size;// 每次写入stream的数据长度，解码后的数据长度 double pts;//音频时间戳 while (len > 0) {//检查音频缓存的剩余长度 if (is->audio_buf_index >= is->audio_buf_size) {// 检查是否需要执行解码操作 // We have already sent all our data; get more，从缓存队列中提取数据包、解码，并返回解码后的数据长度，audio_buf缓存中可能包含多帧解码后的音频数据 audio_size = audio_decode_frame(is, &pts); if (audio_size audio_buf_size = 1024; memset(is->audio_buf, 0, is->audio_buf_size);// 全零重置缓冲区 } else { //在回调函数中增加音频同步过程，即对音频数据缓存进行丢帧(或插值)，以起到降低音频时钟与主同步源时差的目的 audio_size=synchronize_audio(is,(int16_t*)is->audio_buf,audio_size,pts);//返回音频同步后的缓存长度 is->audio_buf_size = audio_size;// 返回packet中包含的原始音频数据长度(多帧) } is->audio_buf_index = 0;// 初始化累计写入缓存长度 }//end for if wt_stream_len=is->audio_buf_size-is->audio_buf_index;// 计算解码缓存剩余长度 if (wt_stream_len > len) {// 检查每次写入缓存的数据长度是否超过指定长度(1024) wt_stream_len = len;// 指定长度从解码的缓存中取数据 } // 每次从解码的缓存数据中以指定长度抽取数据并写入stream传递给声卡 memcpy(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, wt_stream_len); len -= wt_stream_len;// 更新解码音频缓存的剩余长度 stream += wt_stream_len;// 更新缓存写入位置 is->audio_buf_index += wt_stream_len;// 更新累计写入缓存数据长度 }//end for while } // 根据指定类型打开流，找到对应的解码器、创建对应的音频配置、保存关键信息到 VideoState、启动音频和视频线程 int stream_component_open(VideoState *is, int stream_index) { AVFormatContext *pFormatCtx = is->pFormatCtx;// 传递文件容器的封装信息及码流参数 AVCodecContext *codecCtx = NULL;// 解码器上下文对象，解码器依赖的相关环境、状态、资源以及参数集的接口指针 AVCodec *codec = NULL;// 保存编解码器信息的结构体，提供编码与解码的公共接口，可以看作是编码器与解码器的一个全局变量 //检查输入的流类型是否在合理范围内 if (stream_index = pFormatCtx->nb_streams) { return -1; } // Get a pointer to the codec context for the video stream. codecCtx = pFormatCtx->streams[stream_index]->codec;// 取得解码器上下文 if (codecCtx->codec_type == AVMEDIA_TYPE_AUDIO) {//检查解码器类型是否为音频解码器 SDL_AudioSpec wanted_spec, spec;//SDL_AudioSpec a structure that contains the audio output format，创建 SDL_AudioSpec 结构体，设置音频播放数据 // Set audio settings from codec info,SDL_AudioSpec a structure that contains the audio output format // 创建SDL_AudioSpec结构体，设置音频播放参数 wanted_spec.freq = codecCtx->sample_rate;//采样频率 DSP frequency -- samples per second wanted_spec.format = AUDIO_S16SYS;//采样格式 Audio data format wanted_spec.channels = codecCtx->channels;//声道数 Number of channels: 1 mono, 2 stereo wanted_spec.silence = 0;//无输出时是否静音 //默认每次读音频缓存的大小，推荐值为 512~8192，ffplay使用的是1024 specifies a unit of audio data refers to the size of the audio buffer in sample frames wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE; wanted_spec.callback = audio_callback;//设置读取音频数据的回调接口函数 the function to call when the audio device needs more data wanted_spec.userdata = is;//传递用户数据 /*--------------------------- * 以指定参数打开音频设备，并返回与指定参数最为接近的参数，该参数为设备实际支持的音频参数 * Opens the audio device with the desired parameters(wanted_spec) * return another specs we actually be using * and not guaranteed to get what we asked for --------------------------*/ if (SDL_OpenAudio(&wanted_spec, &spec) audio_hw_buf_size = spec.size; } /*----------------------- * Find the decoder for the video stream，根据视频流对应的解码器上下文查找对应的解码器，返回对应的解码器(信息结构体) * The stream's information about the codec is in what we call the \"codec context. * This contains all the information about the codec that the stream is using -----------------------*/ codec = avcodec_find_decoder(codecCtx->codec_id); AVDictionary *optionsDict = NULL; if (!codec || (avcodec_open2(codecCtx, codec, &optionsDict) name); // 检查解码器类型 switch(codecCtx->codec_type) { case AVMEDIA_TYPE_AUDIO:// 音频解码器 is->audioStream = stream_index;// 音频流类型标号初始化 is->audio_st = pFormatCtx->streams[stream_index]; is->audio_buf_size = 0;// 解码后的多帧音频数据长度 is->audio_buf_index = 0;//累 计写入stream的长度 // Averaging filter for audio sync. is->audio_diff_avg_coef=exp(log(0.01/AUDIO_DIFF_AVG_NB));//音频时钟与主同步源累计时差加权系数 is->audio_diff_avg_count=0;//音频不同步计数初始化 // Correct audio only if larger error than this. is->audio_diff_threshold=2.0*SDL_AUDIO_BUFFER_SIZE/codecCtx->sample_rate; is->sws_ctx_audio = (struct SwsContext *) swr_alloc(); if (!is->sws_ctx_audio) { fprintf(stderr, \"Could not allocate resampler context\\n\"); return -1; } memset(&is->audio_pkt, 0, sizeof(is->audio_pkt)); packet_queue_init(&is->audioq);// 音频数据包队列初始化 SDL_PauseAudio(0);// audio callback starts running again，开启音频设备，如果这时候没有获得数据那么它就静音 break; case AVMEDIA_TYPE_VIDEO:// 视频解码器 is->videoStream = stream_index;// 视频流类型标号初始化 is->video_st = pFormatCtx->streams[stream_index]; //以系统时间为基准，初始化播放到当前帧的已播放时间值，该值为真实时间值、动态时间值、绝对时间值 is->frame_timer=(double)av_gettime()/1000000.0; is->frame_last_delay = 40e-3;//初始化上一帧图像的动态刷新延迟时间 is->video_current_pts_time = av_gettime();//取得系统当前时间 packet_queue_init(&is->videoq);// 视频数据包队列初始化 is->decode_tid = SDL_CreateThread(decode_thread,\"视频解码线程\" ,is);// 创建视频解码线程 // Initialize SWS context for software scaling，设置图像转换像素格式为AV_PIX_FMT_YUV420P is->sws_ctx = sws_getContext(is->video_st->codec->width, is->video_st->codec->height, is->video_st->codec->pix_fmt, is->video_st->codec->width, is->video_st->codec->height, AV_PIX_FMT_YUV420P, SWS_BILINEAR, NULL, NULL, NULL); codecCtx->get_buffer2 = our_get_buffer; break; default: break; } return 0; } //清除队列缓存，释放队列中所有动态分配的内存 static void packet_queue_flush(PacketQueue *q) { AVPacketList *pkt, *pkttmp;//队列当前节点，临时节点 SDL_LockMutex(q->qlock);//锁定互斥量 for (pkt = q->first_pkt; pkt != NULL; pkt = pkttmp) {//遍历队列所有节点 pkttmp = pkt->next;//队列头节点后移 av_packet_unref(&pkt->pkt);//当前节点引用计数-1 av_freep(&pkt);//释放当前节点缓存 } q->last_pkt = NULL;//队列尾节点指针置零 q->first_pkt = NULL;//队列头节点指针置零 q->nb_packets = 0;//队列长度置零 q->size = 0;//队列编码数据的缓存长度置零 SDL_UnlockMutex(q->qlock);//互斥量解锁 } // 编码数据包解析线程函数(从视频文件中解析出音视频编码数据单元，一个AVPacket的data通常对应一个NAL) int parse_thread(void *arg) { VideoState *is = (VideoState *)arg;// 传递用户参数 global_video_state = is;// 传递全局状态参量结构体 /*------------------------- * 打开封装格式 * 打开视频文件，读文件头内容，取得文件容器的封装信息及码流参数并存储在avformat_context中 * 参数一：封装格式上下文 * 参数二：视频路径 * 参数三：指定输入的格式 * 参数四：设置默认参数 --------------------------*/ AVFormatContext *avformat_context = NULL;// 参数一：封装格式上下文 int avformat_open_input_result = avformat_open_input(&avformat_context, is->filename, NULL, NULL); if (avformat_open_input_result != 0){ // __android_log_print(ANDROID_LOG_INFO, \"main\", \"查找音视频流\\n\"); return -1; } is->pFormatCtx = avformat_context;//传递文件容器封装信息及码流参数 /*------------------------- * 查找码流 * 取得文件中保存的码流信息，并填充到avformat_context->stream 字段 * 参数一：封装格式上下文 * 参数二：指定默认配置 -------------------------*/ int avformat_find_stream_info_result = avformat_find_stream_info(avformat_context, NULL); if (avformat_find_stream_info_result videoStream = -1;//视频流类型标号初始化为-1 is->audioStream = -1;//音频流类型标号初始化为-1 // 视频流类型标号初始化为-1 int av_video_stream_index = -1; // 音频流类型标号初始化为-1 int av_audio_stream_index = -1; for (int i = 0; i nb_streams; ++i) { // 若文件中包含有视频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){ av_video_stream_index = i; } // 若文件中包含有音频流 if (avformat_context->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){ av_audio_stream_index = i; } } // 检查文件中是否存在视频流 if (av_video_stream_index == -1) { // __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到视频流\\n\"); goto fail;//跳转至异常处理 return -1; } // 检查文件中是否存在音频流 if (av_audio_stream_index == -1) { // __android_log_print(ANDROID_LOG_INFO, \"main\", \"没有找到音频流\\n\"); goto fail;//跳转至异常处理 return -1; } stream_component_open(is, av_audio_stream_index);// 根据指定类型打开音频流 stream_component_open(is, av_video_stream_index);// 根据指定类型打开视频流 // Main decode loop. for (;;) { if (is->quit) {//检查退出进程标识 break; } // Seek stuff goes here if (is->seek_req) {//检查[快进]/[快退]操作标志位是否开启 int stream_index= -1;//初始化音视频流类型标号 int64_t seek_target = is->seek_pos;//取得[快进]/[快退]操作后的参考时间戳 if (is->videoStream >= 0) {//检查是否取得视频流类型标号 stream_index = is->videoStream;//取得视频流类型标号 } else if (is->audioStream >= 0) {//检查是否取得音频流类型标号 stream_index = is->audioStream;//取得音频流类型标号 } if (stream_index >= 0){//检查是否取得音视频流类型标号 //时间单位转换，将seek_target的单位由AV_TIME_BASE_Q转换为time_base seek_target= av_rescale_q(seek_target, AV_TIME_BASE_Q, avformat_context->streams[stream_index]->time_base); } //根据[快进]/[快退]操作后的时间戳，跳到指定帧(该函数只能跳到离指定帧最近的关键帧) if (av_seek_frame(is->pFormatCtx, stream_index, seek_target, is->seek_flags) pFormatCtx->filename); } else {//在执行[快进]/[快退]操作后，立刻清空缓存队列，并重置音视频解码器 if (is->audioStream >= 0) {//检查是否取得音频流类型标号 packet_queue_flush(&is->audioq);//清除音频队列缓存，释放队列中所有动态分配的内存 packet_queue_put(&is->audioq, &flush_pkt);//将flush_pkt插入音频数据包队列，执行重置音频解码器操作avcodec_flush_buffers } if (is->videoStream >= 0) {//取得视频流类型标号 packet_queue_flush(&is->videoq);//清除视频队列缓存，释放队列中所有动态分配的内存 packet_queue_put(&is->videoq, &flush_pkt);//将flush_pkt插入视频数据包队列，执行重置视频解码器操作avcodec_flush_buffers } } is->seek_req = 0;//关闭[快进]/[快退]操作标志位 }//end for if (is->seek_req) // Seek stuff goes here，检查音视频编码数据包队列长度是否溢出 if (is->audioq.size > MAX_AUDIOQ_SIZE || is->videoq.size > MAX_VIDEOQ_SIZE) { SDL_Delay(10); continue; } /*----------------------- * read in a packet and store it in the AVPacket struct * ffmpeg allocates the internal data for us,which is pointed to by packet.data * this is freed by the av_free_packet() -----------------------*/ // 负责保存压缩编码数据相关信息的结构体,每帧图像由一到多个packet包组成 AVPacket pkt, *packet = &pkt;// 在栈上创建临时数据包对象并关联指针 if (av_read_frame(is->pFormatCtx, packet) pFormatCtx->pb->error == 0) { SDL_Delay(100); // No error; wait for user input. continue; } else { break; } } // Is this a packet from the video stream? if (packet->stream_index == is->videoStream) {// 检查数据包是否为视频类型 packet_queue_put(&is->videoq, packet);// 向队列中插入数据包 } else if (packet->stream_index == is->audioStream) {// 检查数据包是否为音频类型 packet_queue_put(&is->audioq, packet);// 向队列中插入数据包 } else {// 检查数据包是否为字幕类型 av_packet_unref(packet);// 释放packet中保存的(字幕)编码数据 } } // All done - wait for it. while (!is->quit) { SDL_Delay(100); } fail:// 异常处理 if (1) { SDL_Event event;// SDL事件对象 event.type = FF_QUIT_EVENT;// 指定退出事件类型 event.user.data1 = is;// 传递用户数据 SDL_PushEvent(&event);// 将该事件对象压入SDL后台事件队列 } return 0; } int init_sdl(VideoState *is) { // 初始化SDL多媒体框架 if (SDL_Init( SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER ) == -1) { // __android_log_print(ANDROID_LOG_INFO, \"main\", \"初始化失败：%s\", SDL_GetError()); // Mac使用 // printf(\"初始化失败：%s\", SDL_GetError()); return -1; } // SDL中获取屏幕尺寸 SDL_DisplayMode DM; SDL_GetCurrentDisplayMode(0, &DM); // 初始化SDL窗口 SDL_Window* sdl_window = SDL_CreateWindow(\"FFmpeg+SDL播放视频\",// 参数一：窗口名称 SDL_WINDOWPOS_CENTERED,// 参数二：窗口在屏幕上的x坐标 SDL_WINDOWPOS_CENTERED,// 参数三：窗口在屏幕上的y坐标 DM.w,// 参数四：窗口在屏幕上宽 DM.h,// 参数五：窗口在屏幕上高 SDL_WINDOW_OPENGL);// 参数六：窗口状态(打开) if (sdl_window == NULL){ // __android_log_print(ANDROID_LOG_INFO, \"main\", \"窗口创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"窗口创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } screen = sdl_window; // 创建渲染器 // 定义渲染器区域 SDL_Renderer* sdl_renderer = SDL_CreateRenderer(sdl_window,// 渲染目标创建 -1, // 从那里开始渲染(-1:表示从第一个位置开始) 0);// 渲染类型(软件渲染) if (sdl_renderer == NULL){ // __android_log_print(ANDROID_LOG_INFO, \"main\", \"渲染器创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"渲染器创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } // 创建纹理 SDL_Texture* sdl_texture = SDL_CreateTexture(sdl_renderer,// 渲染器 SDL_PIXELFORMAT_IYUV,// 像素数据格式 SDL_TEXTUREACCESS_STREAMING,// 绘制方式：频繁绘制- is->video_width,// 纹理宽 is->video_height);// 纹理高 if (sdl_texture == NULL) { // __android_log_print(ANDROID_LOG_INFO, \"main\", \"纹理创建失败：%s\", SDL_GetError()); // Mac使用 // printf(\"纹理创建失败： %s\\n\", SDL_GetError()); // 退出程序 SDL_Quit(); return -1; } is->sdl_renderer = sdl_renderer; is->sdl_texture = sdl_texture; return 0; } //设置[快进]/[快退]状态参数 void stream_seek(VideoState *is, int64_t pos, int rel) { if (!is->seek_req) {//检查[快进]/[快退]操作标志位是否开启 is->seek_pos = pos;//更新[快进]/[快退]后的参考时间戳 is->seek_flags = rel seek_req = 1;//开启[快进]/[快退]标志位 } } int init_controller(VideoState *is) { SDL_Window *sdl_window2 = SDL_GL_GetCurrentWindow(); SDL_SysWMinfo systemWindowInfo; SDL_VERSION(&systemWindowInfo.version); if (!SDL_GetWindowWMInfo(sdl_window2, &systemWindowInfo)) { return -1; } ViewController *vc = [ViewController new]; vc.tapFastForwardButtonBlock = ^{ printf(\"前进10秒\\n\"); if (global_video_state) { double incr, pos; incr = 10.0; pos = get_master_clock(global_video_state);//取得当前主同步源时间戳 pos += incr;//根据键盘操作更新主同步源时间戳(AV_TIME_BASE为时间戳基准值) stream_seek(global_video_state,(int64_t)(pos*AV_TIME_BASE),incr);//根据主同步源时间戳设置查找位置 } }; vc.tapFastBackwardButtonBlock = ^{ printf(\"后退10秒\\n\"); if (global_video_state) { double incr, pos; incr = -10.0; pos = get_master_clock(global_video_state);//取得当前主同步源时间戳 pos += incr;//根据键盘操作更新主同步源时间戳(AV_TIME_BASE为时间戳基准值) stream_seek(global_video_state,(int64_t)(pos*AV_TIME_BASE),incr);//根据主同步源时间戳设置查找位置 } }; UIWindow *customWindow = [[UIWindow alloc] initWithFrame:[UIScreen mainScreen].bounds]; customWindow.rootViewController = vc; [customWindow makeKeyAndVisible]; [systemWindowInfo.info.uikit.window addSubview:customWindow]; return 0; } // SDL入口 //extern \"C\" int main(int argc, char *argv[]) { /*------------------------- * 注册组件 * 注册所有ffmpeg支持的多媒体格式及编解码器 -------------------------*/ av_register_all(); // 创建全局状态对象 VideoState *is= (VideoState *)av_mallocz(sizeof(VideoState)); NSString* inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"mov\"]; av_strlcpy(is->filename, [inPath UTF8String], sizeof(is->filename));// 复制视频文件路径名 is->video_width = 640; is->video_height = 352; // av_strlcpy(is->filename, \"/storage/emulated/0/DCIM/Camera/TG-2022-04-13-160703582.mp4\", sizeof(is->filename));// 复制视频文件路径名 // is->video_width = 720; // is->video_height = 1280; is->pictq_lock = SDL_CreateMutex();// 创建编码数据包队列互斥锁对象 is->pictq_ready = SDL_CreateCond();// 创建编码数据包队列就绪条件对象 int init_sdl_result = init_sdl(is); if (init_sdl_result av_sync_type = DEFAULT_AV_SYNC_TYPE;//指定主同步源 // 创建编码数据包解析线程 is->parse_tid = SDL_CreateThread(parse_thread, \"编码数据包解析线程\", is); if (!is->parse_tid) {// 检查线程是否创建成功 av_free(is); return -1; } av_init_packet(&flush_pkt);//初始化flush_pkt //将flush_pkt的data成员指定为\"FLUSH\"，当数据包队列中某个包的data成员取值为\"FLUSH\"，执行重置解码器操作 flush_pkt.data = (unsigned char *) \"FLUSH\"; // SDL事件对象 SDL_Event event; for (;;) {// SDL事件循环 SDL_WaitEvent(&event);// 主线程阻塞，等待事件到来 switch(event.type) {// 事件到来后唤醒主线程，检查事件类型 case FF_QUIT_EVENT: case SDL_QUIT:// 退出进程事件 is->quit = 1; // If the video has finished playing, then both the picture and audio queues are waiting for more data. // Make them stop waiting and terminate normally.. avcodec_close(is->video_st->codec); avformat_free_context(is->pFormatCtx); SDL_CondSignal(is->audioq.qready);// 发出队列就绪信号避免死锁 SDL_CondSignal(is->videoq.qready); SDL_DestroyTexture(is->sdl_texture); SDL_DestroyRenderer(is->sdl_renderer); SDL_Quit(); return 0; case FF_ALLOC_EVENT: alloc_picture(event.user.data1);// 分配视频帧事件响应函数 break; case FF_REFRESH_EVENT:// 视频显示刷新事件 video_refresh_timer(event.user.data1);// 视频显示刷新事件响应函数 break; default: break; } } return 0; } "},"pages/ffmpeg/ffplay_source_code.html":{"url":"pages/ffmpeg/ffplay_source_code.html","title":"ffplay源码","keywords":"","body":"ffplay源码 源代码一览 /* * Copyright (c) 2003 Fabrice Bellard * * This file is part of FFmpeg. * * FFmpeg is free software; you can redistribute it and/or * modify it under the terms of the GNU Lesser General Public * License as published by the Free Software Foundation; either * version 2.1 of the License, or (at your option) any later version. * * FFmpeg is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU * Lesser General Public License for more details. * * You should have received a copy of the GNU Lesser General Public * License along with FFmpeg; if not, write to the Free Software * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA */ /** * @file * simple media player based on the FFmpeg libraries */ #include \"config.h\" #include #include #include #include #include #include \"libavutil/avstring.h\" #include \"libavutil/eval.h\" #include \"libavutil/mathematics.h\" #include \"libavutil/pixdesc.h\" #include \"libavutil/imgutils.h\" #include \"libavutil/dict.h\" #include \"libavutil/parseutils.h\" #include \"libavutil/samplefmt.h\" #include \"libavutil/avassert.h\" #include \"libavutil/time.h\" #include \"libavformat/avformat.h\" #include \"libavdevice/avdevice.h\" #include \"libswscale/swscale.h\" #include \"libavutil/opt.h\" #include \"libavcodec/avfft.h\" #include \"libswresample/swresample.h\" #if CONFIG_AVFILTER # include \"libavfilter/avfilter.h\" # include \"libavfilter/buffersink.h\" # include \"libavfilter/buffersrc.h\" #endif #include #include #include \"cmdutils.h\" #include const char program_name[] = \"ffplay\"; const int program_birth_year = 2003; #define MAX_QUEUE_SIZE (15 * 1024 * 1024) #define MIN_FRAMES 25 #define EXTERNAL_CLOCK_MIN_FRAMES 2 #define EXTERNAL_CLOCK_MAX_FRAMES 10 /* Minimum SDL audio buffer size, in samples. */ #define SDL_AUDIO_MIN_BUFFER_SIZE 512 /* Calculate actual buffer size keeping in mind not cause too frequent audio callbacks */ #define SDL_AUDIO_MAX_CALLBACKS_PER_SEC 30 /* Step size for volume control in dB */ #define SDL_VOLUME_STEP (0.75) /* no AV sync correction is done if below the minimum AV sync threshold */ #define AV_SYNC_THRESHOLD_MIN 0.04 /* AV sync correction is done if above the maximum AV sync threshold */ #define AV_SYNC_THRESHOLD_MAX 0.1 /* If a frame duration is longer than this, it will not be duplicated to compensate AV sync */ #define AV_SYNC_FRAMEDUP_THRESHOLD 0.1 /* no AV correction is done if too big error */ #define AV_NOSYNC_THRESHOLD 10.0 /* maximum audio speed change to get correct sync */ #define SAMPLE_CORRECTION_PERCENT_MAX 10 /* external clock speed adjustment constants for realtime sources based on buffer fullness */ #define EXTERNAL_CLOCK_SPEED_MIN 0.900 #define EXTERNAL_CLOCK_SPEED_MAX 1.010 #define EXTERNAL_CLOCK_SPEED_STEP 0.001 /* we use about AUDIO_DIFF_AVG_NB A-V differences to make the average */ #define AUDIO_DIFF_AVG_NB 20 /* polls for possible required screen refresh at least this often, should be less than 1/fps */ #define REFRESH_RATE 0.01 /* NOTE: the size must be big enough to compensate the hardware audio buffersize size */ /* TODO: We assume that a decoded and resampled frame fits into this buffer */ #define SAMPLE_ARRAY_SIZE (8 * 65536) #define CURSOR_HIDE_DELAY 1000000 #define USE_ONEPASS_SUBTITLE_RENDER 1 static unsigned sws_flags = SWS_BICUBIC; typedef struct MyAVPacketList { AVPacket pkt; struct MyAVPacketList *next; int serial; } MyAVPacketList; typedef struct PacketQueue { MyAVPacketList *first_pkt, *last_pkt; int nb_packets; int size; int64_t duration; int abort_request; int serial; SDL_mutex *mutex; SDL_cond *cond; } PacketQueue; #define VIDEO_PICTURE_QUEUE_SIZE 3 #define SUBPICTURE_QUEUE_SIZE 16 #define SAMPLE_QUEUE_SIZE 9 #define FRAME_QUEUE_SIZE FFMAX(SAMPLE_QUEUE_SIZE, FFMAX(VIDEO_PICTURE_QUEUE_SIZE, SUBPICTURE_QUEUE_SIZE)) typedef struct AudioParams { int freq; int channels; int64_t channel_layout; enum AVSampleFormat fmt; int frame_size; int bytes_per_sec; } AudioParams; typedef struct Clock { double pts; /* clock base */ double pts_drift; /* clock base minus time at which we updated the clock */ double last_updated; double speed; int serial; /* clock is based on a packet with this serial */ int paused; int *queue_serial; /* pointer to the current packet queue serial, used for obsolete clock detection */ } Clock; /* Common struct for handling all types of decoded data and allocated render buffers. */ typedef struct Frame { AVFrame *frame; AVSubtitle sub; int serial; double pts; /* presentation timestamp for the frame */ double duration; /* estimated duration of the frame */ int64_t pos; /* byte position of the frame in the input file */ int width; int height; int format; AVRational sar; int uploaded; int flip_v; } Frame; typedef struct FrameQueue { Frame queue[FRAME_QUEUE_SIZE]; int rindex; int windex; int size; int max_size; int keep_last; int rindex_shown; SDL_mutex *mutex; SDL_cond *cond; PacketQueue *pktq; } FrameQueue; enum { AV_SYNC_AUDIO_MASTER, /* default choice */ AV_SYNC_VIDEO_MASTER, AV_SYNC_EXTERNAL_CLOCK, /* synchronize to an external clock */ }; typedef struct Decoder { AVPacket pkt; PacketQueue *queue; AVCodecContext *avctx; int pkt_serial; int finished; int packet_pending; SDL_cond *empty_queue_cond; int64_t start_pts; AVRational start_pts_tb; int64_t next_pts; AVRational next_pts_tb; SDL_Thread *decoder_tid; } Decoder; typedef struct VideoState { SDL_Thread *read_tid; AVInputFormat *iformat; int abort_request; int force_refresh; int paused; int last_paused; int queue_attachments_req; int seek_req; int seek_flags; int64_t seek_pos; int64_t seek_rel; int read_pause_return; AVFormatContext *ic; int realtime; Clock audclk; Clock vidclk; Clock extclk; FrameQueue pictq; FrameQueue subpq; FrameQueue sampq; Decoder auddec; Decoder viddec; Decoder subdec; int audio_stream; int av_sync_type; double audio_clock; int audio_clock_serial; double audio_diff_cum; /* used for AV difference average computation */ double audio_diff_avg_coef; double audio_diff_threshold; int audio_diff_avg_count; AVStream *audio_st; PacketQueue audioq; int audio_hw_buf_size; uint8_t *audio_buf; uint8_t *audio_buf1; unsigned int audio_buf_size; /* in bytes */ unsigned int audio_buf1_size; int audio_buf_index; /* in bytes */ int audio_write_buf_size; int audio_volume; int muted; struct AudioParams audio_src; #if CONFIG_AVFILTER struct AudioParams audio_filter_src; #endif struct AudioParams audio_tgt; struct SwrContext *swr_ctx; int frame_drops_early; int frame_drops_late; enum ShowMode { SHOW_MODE_NONE = -1, SHOW_MODE_VIDEO = 0, SHOW_MODE_WAVES, SHOW_MODE_RDFT, SHOW_MODE_NB } show_mode; int16_t sample_array[SAMPLE_ARRAY_SIZE]; int sample_array_index; int last_i_start; RDFTContext *rdft; int rdft_bits; FFTSample *rdft_data; int xpos; double last_vis_time; SDL_Texture *vis_texture; SDL_Texture *sub_texture; SDL_Texture *vid_texture; int subtitle_stream; AVStream *subtitle_st; PacketQueue subtitleq; double frame_timer; double frame_last_returned_time; double frame_last_filter_delay; int video_stream; AVStream *video_st; PacketQueue videoq; double max_frame_duration; // maximum duration of a frame - above this, we consider the jump a timestamp discontinuity struct SwsContext *img_convert_ctx; struct SwsContext *sub_convert_ctx; int eof; char *filename; int width, height, xleft, ytop; int step; #if CONFIG_AVFILTER int vfilter_idx; AVFilterContext *in_video_filter; // the first filter in the video chain AVFilterContext *out_video_filter; // the last filter in the video chain AVFilterContext *in_audio_filter; // the first filter in the audio chain AVFilterContext *out_audio_filter; // the last filter in the audio chain AVFilterGraph *agraph; // audio filter graph #endif int last_video_stream, last_audio_stream, last_subtitle_stream; SDL_cond *continue_read_thread; } VideoState; /* options specified by the user */ static AVInputFormat *file_iformat; static const char *input_filename; static const char *window_title; static int default_width = 640; static int default_height = 480; static int screen_width = 0; static int screen_height = 0; static int screen_left = SDL_WINDOWPOS_CENTERED; static int screen_top = SDL_WINDOWPOS_CENTERED; static int audio_disable; static int video_disable; static int subtitle_disable; static const char* wanted_stream_spec[AVMEDIA_TYPE_NB] = {0}; static int seek_by_bytes = -1; static float seek_interval = 10; static int display_disable; static int borderless; static int alwaysontop; static int startup_volume = 100; static int show_status = 1; static int av_sync_type = AV_SYNC_AUDIO_MASTER; static int64_t start_time = AV_NOPTS_VALUE; static int64_t duration = AV_NOPTS_VALUE; static int fast = 0; static int genpts = 0; static int lowres = 0; static int decoder_reorder_pts = -1; static int autoexit; static int exit_on_keydown; static int exit_on_mousedown; static int loop = 1; static int framedrop = -1; static int infinite_buffer = -1; static enum ShowMode show_mode = SHOW_MODE_NONE; static const char *audio_codec_name; static const char *subtitle_codec_name; static const char *video_codec_name; double rdftspeed = 0.02; static int64_t cursor_last_shown; static int cursor_hidden = 0; #if CONFIG_AVFILTER static const char **vfilters_list = NULL; static int nb_vfilters = 0; static char *afilters = NULL; #endif static int autorotate = 1; static int find_stream_info = 1; static int filter_nbthreads = 0; /* current context */ static int is_full_screen; static int64_t audio_callback_time; static AVPacket flush_pkt; #define FF_QUIT_EVENT (SDL_USEREVENT + 2) static SDL_Window *window; static SDL_Renderer *renderer; static SDL_RendererInfo renderer_info = {0}; static SDL_AudioDeviceID audio_dev; static const struct TextureFormatEntry { enum AVPixelFormat format; int texture_fmt; } sdl_texture_format_map[] = { { AV_PIX_FMT_RGB8, SDL_PIXELFORMAT_RGB332 }, { AV_PIX_FMT_RGB444, SDL_PIXELFORMAT_RGB444 }, { AV_PIX_FMT_RGB555, SDL_PIXELFORMAT_RGB555 }, { AV_PIX_FMT_BGR555, SDL_PIXELFORMAT_BGR555 }, { AV_PIX_FMT_RGB565, SDL_PIXELFORMAT_RGB565 }, { AV_PIX_FMT_BGR565, SDL_PIXELFORMAT_BGR565 }, { AV_PIX_FMT_RGB24, SDL_PIXELFORMAT_RGB24 }, { AV_PIX_FMT_BGR24, SDL_PIXELFORMAT_BGR24 }, { AV_PIX_FMT_0RGB32, SDL_PIXELFORMAT_RGB888 }, { AV_PIX_FMT_0BGR32, SDL_PIXELFORMAT_BGR888 }, { AV_PIX_FMT_NE(RGB0, 0BGR), SDL_PIXELFORMAT_RGBX8888 }, { AV_PIX_FMT_NE(BGR0, 0RGB), SDL_PIXELFORMAT_BGRX8888 }, { AV_PIX_FMT_RGB32, SDL_PIXELFORMAT_ARGB8888 }, { AV_PIX_FMT_RGB32_1, SDL_PIXELFORMAT_RGBA8888 }, { AV_PIX_FMT_BGR32, SDL_PIXELFORMAT_ABGR8888 }, { AV_PIX_FMT_BGR32_1, SDL_PIXELFORMAT_BGRA8888 }, { AV_PIX_FMT_YUV420P, SDL_PIXELFORMAT_IYUV }, { AV_PIX_FMT_YUYV422, SDL_PIXELFORMAT_YUY2 }, { AV_PIX_FMT_UYVY422, SDL_PIXELFORMAT_UYVY }, { AV_PIX_FMT_NONE, SDL_PIXELFORMAT_UNKNOWN }, }; #if CONFIG_AVFILTER static int opt_add_vfilter(void *optctx, const char *opt, const char *arg) { GROW_ARRAY(vfilters_list, nb_vfilters); vfilters_list[nb_vfilters - 1] = arg; return 0; } #endif static inline int cmp_audio_fmts(enum AVSampleFormat fmt1, int64_t channel_count1, enum AVSampleFormat fmt2, int64_t channel_count2) { /* If channel count == 1, planar and non-planar formats are the same */ if (channel_count1 == 1 && channel_count2 == 1) return av_get_packed_sample_fmt(fmt1) != av_get_packed_sample_fmt(fmt2); else return channel_count1 != channel_count2 || fmt1 != fmt2; } static inline int64_t get_valid_channel_layout(int64_t channel_layout, int channels) { if (channel_layout && av_get_channel_layout_nb_channels(channel_layout) == channels) return channel_layout; else return 0; } static int packet_queue_put_private(PacketQueue *q, AVPacket *pkt) { MyAVPacketList *pkt1; if (q->abort_request) return -1; pkt1 = av_malloc(sizeof(MyAVPacketList)); if (!pkt1) return -1; pkt1->pkt = *pkt; pkt1->next = NULL; if (pkt == &flush_pkt) q->serial++; pkt1->serial = q->serial; if (!q->last_pkt) q->first_pkt = pkt1; else q->last_pkt->next = pkt1; q->last_pkt = pkt1; q->nb_packets++; q->size += pkt1->pkt.size + sizeof(*pkt1); q->duration += pkt1->pkt.duration; /* XXX: should duplicate packet data in DV case */ SDL_CondSignal(q->cond); return 0; } static int packet_queue_put(PacketQueue *q, AVPacket *pkt) { int ret; SDL_LockMutex(q->mutex); ret = packet_queue_put_private(q, pkt); SDL_UnlockMutex(q->mutex); if (pkt != &flush_pkt && ret data = NULL; pkt->size = 0; pkt->stream_index = stream_index; return packet_queue_put(q, pkt); } /* packet queue handling */ static int packet_queue_init(PacketQueue *q) { memset(q, 0, sizeof(PacketQueue)); q->mutex = SDL_CreateMutex(); if (!q->mutex) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateMutex(): %s\\n\", SDL_GetError()); return AVERROR(ENOMEM); } q->cond = SDL_CreateCond(); if (!q->cond) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateCond(): %s\\n\", SDL_GetError()); return AVERROR(ENOMEM); } q->abort_request = 1; return 0; } static void packet_queue_flush(PacketQueue *q) { MyAVPacketList *pkt, *pkt1; SDL_LockMutex(q->mutex); for (pkt = q->first_pkt; pkt; pkt = pkt1) { pkt1 = pkt->next; av_packet_unref(&pkt->pkt); av_freep(&pkt); } q->last_pkt = NULL; q->first_pkt = NULL; q->nb_packets = 0; q->size = 0; q->duration = 0; SDL_UnlockMutex(q->mutex); } static void packet_queue_destroy(PacketQueue *q) { packet_queue_flush(q); SDL_DestroyMutex(q->mutex); SDL_DestroyCond(q->cond); } static void packet_queue_abort(PacketQueue *q) { SDL_LockMutex(q->mutex); q->abort_request = 1; SDL_CondSignal(q->cond); SDL_UnlockMutex(q->mutex); } static void packet_queue_start(PacketQueue *q) { SDL_LockMutex(q->mutex); q->abort_request = 0; packet_queue_put_private(q, &flush_pkt); SDL_UnlockMutex(q->mutex); } /* return 0 if packet. */ static int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block, int *serial) { MyAVPacketList *pkt1; int ret; SDL_LockMutex(q->mutex); for (;;) { if (q->abort_request) { ret = -1; break; } pkt1 = q->first_pkt; if (pkt1) { q->first_pkt = pkt1->next; if (!q->first_pkt) q->last_pkt = NULL; q->nb_packets--; q->size -= pkt1->pkt.size + sizeof(*pkt1); q->duration -= pkt1->pkt.duration; *pkt = pkt1->pkt; if (serial) *serial = pkt1->serial; av_free(pkt1); ret = 1; break; } else if (!block) { ret = 0; break; } else { SDL_CondWait(q->cond, q->mutex); } } SDL_UnlockMutex(q->mutex); return ret; } static void decoder_init(Decoder *d, AVCodecContext *avctx, PacketQueue *queue, SDL_cond *empty_queue_cond) { memset(d, 0, sizeof(Decoder)); d->avctx = avctx; d->queue = queue; d->empty_queue_cond = empty_queue_cond; d->start_pts = AV_NOPTS_VALUE; d->pkt_serial = -1; } static int decoder_decode_frame(Decoder *d, AVFrame *frame, AVSubtitle *sub) { int ret = AVERROR(EAGAIN); for (;;) { AVPacket pkt; if (d->queue->serial == d->pkt_serial) { do { if (d->queue->abort_request) return -1; switch (d->avctx->codec_type) { case AVMEDIA_TYPE_VIDEO: ret = avcodec_receive_frame(d->avctx, frame); if (ret >= 0) { if (decoder_reorder_pts == -1) { frame->pts = frame->best_effort_timestamp; } else if (!decoder_reorder_pts) { frame->pts = frame->pkt_dts; } } break; case AVMEDIA_TYPE_AUDIO: ret = avcodec_receive_frame(d->avctx, frame); if (ret >= 0) { AVRational tb = (AVRational){1, frame->sample_rate}; if (frame->pts != AV_NOPTS_VALUE) frame->pts = av_rescale_q(frame->pts, d->avctx->pkt_timebase, tb); else if (d->next_pts != AV_NOPTS_VALUE) frame->pts = av_rescale_q(d->next_pts, d->next_pts_tb, tb); if (frame->pts != AV_NOPTS_VALUE) { d->next_pts = frame->pts + frame->nb_samples; d->next_pts_tb = tb; } } break; } if (ret == AVERROR_EOF) { d->finished = d->pkt_serial; avcodec_flush_buffers(d->avctx); return 0; } if (ret >= 0) return 1; } while (ret != AVERROR(EAGAIN)); } do { if (d->queue->nb_packets == 0) SDL_CondSignal(d->empty_queue_cond); if (d->packet_pending) { av_packet_move_ref(&pkt, &d->pkt); d->packet_pending = 0; } else { if (packet_queue_get(d->queue, &pkt, 1, &d->pkt_serial) queue->serial != d->pkt_serial); if (pkt.data == flush_pkt.data) { avcodec_flush_buffers(d->avctx); d->finished = 0; d->next_pts = d->start_pts; d->next_pts_tb = d->start_pts_tb; } else { if (d->avctx->codec_type == AVMEDIA_TYPE_SUBTITLE) { int got_frame = 0; ret = avcodec_decode_subtitle2(d->avctx, sub, &got_frame, &pkt); if (ret packet_pending = 1; av_packet_move_ref(&d->pkt, &pkt); } ret = got_frame ? 0 : (pkt.data ? AVERROR(EAGAIN) : AVERROR_EOF); } } else { if (avcodec_send_packet(d->avctx, &pkt) == AVERROR(EAGAIN)) { av_log(d->avctx, AV_LOG_ERROR, \"Receive_frame and send_packet both returned EAGAIN, which is an API violation.\\n\"); d->packet_pending = 1; av_packet_move_ref(&d->pkt, &pkt); } } av_packet_unref(&pkt); } } } static void decoder_destroy(Decoder *d) { av_packet_unref(&d->pkt); avcodec_free_context(&d->avctx); } static void frame_queue_unref_item(Frame *vp) { av_frame_unref(vp->frame); avsubtitle_free(&vp->sub); } static int frame_queue_init(FrameQueue *f, PacketQueue *pktq, int max_size, int keep_last) { int i; memset(f, 0, sizeof(FrameQueue)); if (!(f->mutex = SDL_CreateMutex())) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateMutex(): %s\\n\", SDL_GetError()); return AVERROR(ENOMEM); } if (!(f->cond = SDL_CreateCond())) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateCond(): %s\\n\", SDL_GetError()); return AVERROR(ENOMEM); } f->pktq = pktq; f->max_size = FFMIN(max_size, FRAME_QUEUE_SIZE); f->keep_last = !!keep_last; for (i = 0; i max_size; i++) if (!(f->queue[i].frame = av_frame_alloc())) return AVERROR(ENOMEM); return 0; } static void frame_queue_destory(FrameQueue *f) { int i; for (i = 0; i max_size; i++) { Frame *vp = &f->queue[i]; frame_queue_unref_item(vp); av_frame_free(&vp->frame); } SDL_DestroyMutex(f->mutex); SDL_DestroyCond(f->cond); } static void frame_queue_signal(FrameQueue *f) { SDL_LockMutex(f->mutex); SDL_CondSignal(f->cond); SDL_UnlockMutex(f->mutex); } static Frame *frame_queue_peek(FrameQueue *f) { return &f->queue[(f->rindex + f->rindex_shown) % f->max_size]; } static Frame *frame_queue_peek_next(FrameQueue *f) { return &f->queue[(f->rindex + f->rindex_shown + 1) % f->max_size]; } static Frame *frame_queue_peek_last(FrameQueue *f) { return &f->queue[f->rindex]; } static Frame *frame_queue_peek_writable(FrameQueue *f) { /* wait until we have space to put a new frame */ SDL_LockMutex(f->mutex); while (f->size >= f->max_size && !f->pktq->abort_request) { SDL_CondWait(f->cond, f->mutex); } SDL_UnlockMutex(f->mutex); if (f->pktq->abort_request) return NULL; return &f->queue[f->windex]; } static Frame *frame_queue_peek_readable(FrameQueue *f) { /* wait until we have a readable a new frame */ SDL_LockMutex(f->mutex); while (f->size - f->rindex_shown pktq->abort_request) { SDL_CondWait(f->cond, f->mutex); } SDL_UnlockMutex(f->mutex); if (f->pktq->abort_request) return NULL; return &f->queue[(f->rindex + f->rindex_shown) % f->max_size]; } static void frame_queue_push(FrameQueue *f) { if (++f->windex == f->max_size) f->windex = 0; SDL_LockMutex(f->mutex); f->size++; SDL_CondSignal(f->cond); SDL_UnlockMutex(f->mutex); } static void frame_queue_next(FrameQueue *f) { if (f->keep_last && !f->rindex_shown) { f->rindex_shown = 1; return; } frame_queue_unref_item(&f->queue[f->rindex]); if (++f->rindex == f->max_size) f->rindex = 0; SDL_LockMutex(f->mutex); f->size--; SDL_CondSignal(f->cond); SDL_UnlockMutex(f->mutex); } /* return the number of undisplayed frames in the queue */ static int frame_queue_nb_remaining(FrameQueue *f) { return f->size - f->rindex_shown; } /* return last shown position */ static int64_t frame_queue_last_pos(FrameQueue *f) { Frame *fp = &f->queue[f->rindex]; if (f->rindex_shown && fp->serial == f->pktq->serial) return fp->pos; else return -1; } static void decoder_abort(Decoder *d, FrameQueue *fq) { packet_queue_abort(d->queue); frame_queue_signal(fq); SDL_WaitThread(d->decoder_tid, NULL); d->decoder_tid = NULL; packet_queue_flush(d->queue); } static inline void fill_rectangle(int x, int y, int w, int h) { SDL_Rect rect; rect.x = x; rect.y = y; rect.w = w; rect.h = h; if (w && h) SDL_RenderFillRect(renderer, &rect); } static int realloc_texture(SDL_Texture **texture, Uint32 new_format, int new_width, int new_height, SDL_BlendMode blendmode, int init_texture) { Uint32 format; int access, w, h; if (!*texture || SDL_QueryTexture(*texture, &format, &access, &w, &h) scr_width) { width = scr_width; height = av_rescale(width, aspect_ratio.den, aspect_ratio.num) & ~1; } x = (scr_width - width) / 2; y = (scr_height - height) / 2; rect->x = scr_xleft + x; rect->y = scr_ytop + y; rect->w = FFMAX((int)width, 1); rect->h = FFMAX((int)height, 1); } static void get_sdl_pix_fmt_and_blendmode(int format, Uint32 *sdl_pix_fmt, SDL_BlendMode *sdl_blendmode) { int i; *sdl_blendmode = SDL_BLENDMODE_NONE; *sdl_pix_fmt = SDL_PIXELFORMAT_UNKNOWN; if (format == AV_PIX_FMT_RGB32 || format == AV_PIX_FMT_RGB32_1 || format == AV_PIX_FMT_BGR32 || format == AV_PIX_FMT_BGR32_1) *sdl_blendmode = SDL_BLENDMODE_BLEND; for (i = 0; i format, &sdl_pix_fmt, &sdl_blendmode); if (realloc_texture(tex, sdl_pix_fmt == SDL_PIXELFORMAT_UNKNOWN ? SDL_PIXELFORMAT_ARGB8888 : sdl_pix_fmt, frame->width, frame->height, sdl_blendmode, 0) width, frame->height, frame->format, frame->width, frame->height, AV_PIX_FMT_BGRA, sws_flags, NULL, NULL, NULL); if (*img_convert_ctx != NULL) { uint8_t *pixels[4]; int pitch[4]; if (!SDL_LockTexture(*tex, NULL, (void **)pixels, pitch)) { sws_scale(*img_convert_ctx, (const uint8_t * const *)frame->data, frame->linesize, 0, frame->height, pixels, pitch); SDL_UnlockTexture(*tex); } } else { av_log(NULL, AV_LOG_FATAL, \"Cannot initialize the conversion context\\n\"); ret = -1; } break; case SDL_PIXELFORMAT_IYUV: if (frame->linesize[0] > 0 && frame->linesize[1] > 0 && frame->linesize[2] > 0) { ret = SDL_UpdateYUVTexture(*tex, NULL, frame->data[0], frame->linesize[0], frame->data[1], frame->linesize[1], frame->data[2], frame->linesize[2]); } else if (frame->linesize[0] linesize[1] linesize[2] data[0] + frame->linesize[0] * (frame->height - 1), -frame->linesize[0], frame->data[1] + frame->linesize[1] * (AV_CEIL_RSHIFT(frame->height, 1) - 1), -frame->linesize[1], frame->data[2] + frame->linesize[2] * (AV_CEIL_RSHIFT(frame->height, 1) - 1), -frame->linesize[2]); } else { av_log(NULL, AV_LOG_ERROR, \"Mixed negative and positive linesizes are not supported.\\n\"); return -1; } break; default: if (frame->linesize[0] data[0] + frame->linesize[0] * (frame->height - 1), -frame->linesize[0]); } else { ret = SDL_UpdateTexture(*tex, NULL, frame->data[0], frame->linesize[0]); } break; } return ret; } static void set_sdl_yuv_conversion_mode(AVFrame *frame) { #if SDL_VERSION_ATLEAST(2,0,8) SDL_YUV_CONVERSION_MODE mode = SDL_YUV_CONVERSION_AUTOMATIC; if (frame && (frame->format == AV_PIX_FMT_YUV420P || frame->format == AV_PIX_FMT_YUYV422 || frame->format == AV_PIX_FMT_UYVY422)) { if (frame->color_range == AVCOL_RANGE_JPEG) mode = SDL_YUV_CONVERSION_JPEG; else if (frame->colorspace == AVCOL_SPC_BT709) mode = SDL_YUV_CONVERSION_BT709; else if (frame->colorspace == AVCOL_SPC_BT470BG || frame->colorspace == AVCOL_SPC_SMPTE170M || frame->colorspace == AVCOL_SPC_SMPTE240M) mode = SDL_YUV_CONVERSION_BT601; } SDL_SetYUVConversionMode(mode); #endif } static void video_image_display(VideoState *is) { Frame *vp; Frame *sp = NULL; SDL_Rect rect; vp = frame_queue_peek_last(&is->pictq); if (is->subtitle_st) { if (frame_queue_nb_remaining(&is->subpq) > 0) { sp = frame_queue_peek(&is->subpq); if (vp->pts >= sp->pts + ((float) sp->sub.start_display_time / 1000)) { if (!sp->uploaded) { uint8_t* pixels[4]; int pitch[4]; int i; if (!sp->width || !sp->height) { sp->width = vp->width; sp->height = vp->height; } if (realloc_texture(&is->sub_texture, SDL_PIXELFORMAT_ARGB8888, sp->width, sp->height, SDL_BLENDMODE_BLEND, 1) sub.num_rects; i++) { AVSubtitleRect *sub_rect = sp->sub.rects[i]; sub_rect->x = av_clip(sub_rect->x, 0, sp->width ); sub_rect->y = av_clip(sub_rect->y, 0, sp->height); sub_rect->w = av_clip(sub_rect->w, 0, sp->width - sub_rect->x); sub_rect->h = av_clip(sub_rect->h, 0, sp->height - sub_rect->y); is->sub_convert_ctx = sws_getCachedContext(is->sub_convert_ctx, sub_rect->w, sub_rect->h, AV_PIX_FMT_PAL8, sub_rect->w, sub_rect->h, AV_PIX_FMT_BGRA, 0, NULL, NULL, NULL); if (!is->sub_convert_ctx) { av_log(NULL, AV_LOG_FATAL, \"Cannot initialize the conversion context\\n\"); return; } if (!SDL_LockTexture(is->sub_texture, (SDL_Rect *)sub_rect, (void **)pixels, pitch)) { sws_scale(is->sub_convert_ctx, (const uint8_t * const *)sub_rect->data, sub_rect->linesize, 0, sub_rect->h, pixels, pitch); SDL_UnlockTexture(is->sub_texture); } } sp->uploaded = 1; } } else sp = NULL; } } calculate_display_rect(&rect, is->xleft, is->ytop, is->width, is->height, vp->width, vp->height, vp->sar); if (!vp->uploaded) { if (upload_texture(&is->vid_texture, vp->frame, &is->img_convert_ctx) uploaded = 1; vp->flip_v = vp->frame->linesize[0] frame); SDL_RenderCopyEx(renderer, is->vid_texture, NULL, &rect, 0, NULL, vp->flip_v ? SDL_FLIP_VERTICAL : 0); set_sdl_yuv_conversion_mode(NULL); if (sp) { #if USE_ONEPASS_SUBTITLE_RENDER SDL_RenderCopy(renderer, is->sub_texture, NULL, &rect); #else int i; double xratio = (double)rect.w / (double)sp->width; double yratio = (double)rect.h / (double)sp->height; for (i = 0; i sub.num_rects; i++) { SDL_Rect *sub_rect = (SDL_Rect*)sp->sub.rects[i]; SDL_Rect target = {.x = rect.x + sub_rect->x * xratio, .y = rect.y + sub_rect->y * yratio, .w = sub_rect->w * xratio, .h = sub_rect->h * yratio}; SDL_RenderCopy(renderer, is->sub_texture, sub_rect, &target); } #endif } } static inline int compute_mod(int a, int b) { return a height; rdft_bits++) ; nb_freq = 1 audio_tgt.channels; nb_display_channels = channels; if (!s->paused) { int data_used= s->show_mode == SHOW_MODE_WAVES ? s->width : (2*nb_freq); n = 2 * channels; delay = s->audio_write_buf_size; delay /= n; /* to be more precise, we take into account the time spent since the last buffer computation */ if (audio_callback_time) { time_diff = av_gettime_relative() - audio_callback_time; delay -= (time_diff * s->audio_tgt.freq) / 1000000; } delay += 2 * data_used; if (delay sample_array_index - delay * channels, SAMPLE_ARRAY_SIZE); if (s->show_mode == SHOW_MODE_WAVES) { h = INT_MIN; for (i = 0; i sample_array[idx]; int b = s->sample_array[(idx + 4 * channels) % SAMPLE_ARRAY_SIZE]; int c = s->sample_array[(idx + 5 * channels) % SAMPLE_ARRAY_SIZE]; int d = s->sample_array[(idx + 9 * channels) % SAMPLE_ARRAY_SIZE]; int score = a - d; if (h last_i_start = i_start; } else { i_start = s->last_i_start; } if (s->show_mode == SHOW_MODE_WAVES) { SDL_SetRenderDrawColor(renderer, 255, 255, 255, 255); /* total height for one channel */ h = s->height / nb_display_channels; /* graph height / 2 */ h2 = (h * 9) / 20; for (ch = 0; ch ytop + ch * h + (h / 2); /* position of center line */ for (x = 0; x width; x++) { y = (s->sample_array[i] * h2) >> 15; if (y xleft + x, ys, 1, y); i += channels; if (i >= SAMPLE_ARRAY_SIZE) i -= SAMPLE_ARRAY_SIZE; } } SDL_SetRenderDrawColor(renderer, 0, 0, 255, 255); for (ch = 1; ch ytop + ch * h; fill_rectangle(s->xleft, y, s->width, 1); } } else { if (realloc_texture(&s->vis_texture, SDL_PIXELFORMAT_ARGB8888, s->width, s->height, SDL_BLENDMODE_NONE, 1) rdft_bits) { av_rdft_end(s->rdft); av_free(s->rdft_data); s->rdft = av_rdft_init(rdft_bits, DFT_R2C); s->rdft_bits = rdft_bits; s->rdft_data = av_malloc_array(nb_freq, 4 *sizeof(*s->rdft_data)); } if (!s->rdft || !s->rdft_data){ av_log(NULL, AV_LOG_ERROR, \"Failed to allocate buffers for RDFT, switching to waves display\\n\"); s->show_mode = SHOW_MODE_WAVES; } else { FFTSample *data[2]; SDL_Rect rect = {.x = s->xpos, .y = 0, .w = 1, .h = s->height}; uint32_t *pixels; int pitch; for (ch = 0; ch rdft_data + 2 * nb_freq * ch; i = i_start + ch; for (x = 0; x sample_array[i] * (1.0 - w * w); i += channels; if (i >= SAMPLE_ARRAY_SIZE) i -= SAMPLE_ARRAY_SIZE; } av_rdft_calc(s->rdft, data[ch]); } /* Least efficient way to do this, we should of course * directly access it but it is more than fast enough. */ if (!SDL_LockTexture(s->vis_texture, &rect, (void **)&pixels, &pitch)) { pitch >>= 2; pixels += pitch * s->height; for (y = 0; y height; y++) { double w = 1 / sqrt(nb_freq); int a = sqrt(w * sqrt(data[0][2 * y + 0] * data[0][2 * y + 0] + data[0][2 * y + 1] * data[0][2 * y + 1])); int b = (nb_display_channels == 2 ) ? sqrt(w * hypot(data[1][2 * y + 0], data[1][2 * y + 1])) : a; a = FFMIN(a, 255); b = FFMIN(b, 255); pixels -= pitch; *pixels = (a > 1); } SDL_UnlockTexture(s->vis_texture); } SDL_RenderCopy(renderer, s->vis_texture, NULL, NULL); } if (!s->paused) s->xpos++; if (s->xpos >= s->width) s->xpos= s->xleft; } } static void stream_component_close(VideoState *is, int stream_index) { AVFormatContext *ic = is->ic; AVCodecParameters *codecpar; if (stream_index = ic->nb_streams) return; codecpar = ic->streams[stream_index]->codecpar; switch (codecpar->codec_type) { case AVMEDIA_TYPE_AUDIO: decoder_abort(&is->auddec, &is->sampq); SDL_CloseAudioDevice(audio_dev); decoder_destroy(&is->auddec); swr_free(&is->swr_ctx); av_freep(&is->audio_buf1); is->audio_buf1_size = 0; is->audio_buf = NULL; if (is->rdft) { av_rdft_end(is->rdft); av_freep(&is->rdft_data); is->rdft = NULL; is->rdft_bits = 0; } break; case AVMEDIA_TYPE_VIDEO: decoder_abort(&is->viddec, &is->pictq); decoder_destroy(&is->viddec); break; case AVMEDIA_TYPE_SUBTITLE: decoder_abort(&is->subdec, &is->subpq); decoder_destroy(&is->subdec); break; default: break; } ic->streams[stream_index]->discard = AVDISCARD_ALL; switch (codecpar->codec_type) { case AVMEDIA_TYPE_AUDIO: is->audio_st = NULL; is->audio_stream = -1; break; case AVMEDIA_TYPE_VIDEO: is->video_st = NULL; is->video_stream = -1; break; case AVMEDIA_TYPE_SUBTITLE: is->subtitle_st = NULL; is->subtitle_stream = -1; break; default: break; } } static void stream_close(VideoState *is) { /* XXX: use a special url_shutdown call to abort parse cleanly */ is->abort_request = 1; SDL_WaitThread(is->read_tid, NULL); /* close each stream */ if (is->audio_stream >= 0) stream_component_close(is, is->audio_stream); if (is->video_stream >= 0) stream_component_close(is, is->video_stream); if (is->subtitle_stream >= 0) stream_component_close(is, is->subtitle_stream); avformat_close_input(&is->ic); packet_queue_destroy(&is->videoq); packet_queue_destroy(&is->audioq); packet_queue_destroy(&is->subtitleq); /* free all pictures */ frame_queue_destory(&is->pictq); frame_queue_destory(&is->sampq); frame_queue_destory(&is->subpq); SDL_DestroyCond(is->continue_read_thread); sws_freeContext(is->img_convert_ctx); sws_freeContext(is->sub_convert_ctx); av_free(is->filename); if (is->vis_texture) SDL_DestroyTexture(is->vis_texture); if (is->vid_texture) SDL_DestroyTexture(is->vid_texture); if (is->sub_texture) SDL_DestroyTexture(is->sub_texture); av_free(is); } static void do_exit(VideoState *is) { if (is) { stream_close(is); } if (renderer) SDL_DestroyRenderer(renderer); if (window) SDL_DestroyWindow(window); uninit_opts(); #if CONFIG_AVFILTER av_freep(&vfilters_list); #endif avformat_network_deinit(); if (show_status) printf(\"\\n\"); SDL_Quit(); av_log(NULL, AV_LOG_QUIET, \"%s\", \"\"); exit(0); } static void sigterm_handler(int sig) { exit(123); } static void set_default_window_size(int width, int height, AVRational sar) { SDL_Rect rect; int max_width = screen_width ? screen_width : INT_MAX; int max_height = screen_height ? screen_height : INT_MAX; if (max_width == INT_MAX && max_height == INT_MAX) max_height = height; calculate_display_rect(&rect, 0, 0, max_width, max_height, width, height, sar); default_width = rect.w; default_height = rect.h; } static int video_open(VideoState *is) { int w,h; w = screen_width ? screen_width : default_width; h = screen_height ? screen_height : default_height; if (!window_title) window_title = input_filename; SDL_SetWindowTitle(window, window_title); SDL_SetWindowSize(window, w, h); SDL_SetWindowPosition(window, screen_left, screen_top); if (is_full_screen) SDL_SetWindowFullscreen(window, SDL_WINDOW_FULLSCREEN_DESKTOP); SDL_ShowWindow(window); is->width = w; is->height = h; return 0; } /* display the current picture, if any */ static void video_display(VideoState *is) { if (!is->width) video_open(is); SDL_SetRenderDrawColor(renderer, 0, 0, 0, 255); SDL_RenderClear(renderer); if (is->audio_st && is->show_mode != SHOW_MODE_VIDEO) video_audio_display(is); else if (is->video_st) video_image_display(is); SDL_RenderPresent(renderer); } static double get_clock(Clock *c) { if (*c->queue_serial != c->serial) return NAN; if (c->paused) { return c->pts; } else { double time = av_gettime_relative() / 1000000.0; return c->pts_drift + time - (time - c->last_updated) * (1.0 - c->speed); } } static void set_clock_at(Clock *c, double pts, int serial, double time) { c->pts = pts; c->last_updated = time; c->pts_drift = c->pts - time; c->serial = serial; } static void set_clock(Clock *c, double pts, int serial) { double time = av_gettime_relative() / 1000000.0; set_clock_at(c, pts, serial, time); } static void set_clock_speed(Clock *c, double speed) { set_clock(c, get_clock(c), c->serial); c->speed = speed; } static void init_clock(Clock *c, int *queue_serial) { c->speed = 1.0; c->paused = 0; c->queue_serial = queue_serial; set_clock(c, NAN, -1); } static void sync_clock_to_slave(Clock *c, Clock *slave) { double clock = get_clock(c); double slave_clock = get_clock(slave); if (!isnan(slave_clock) && (isnan(clock) || fabs(clock - slave_clock) > AV_NOSYNC_THRESHOLD)) set_clock(c, slave_clock, slave->serial); } static int get_master_sync_type(VideoState *is) { if (is->av_sync_type == AV_SYNC_VIDEO_MASTER) { if (is->video_st) return AV_SYNC_VIDEO_MASTER; else return AV_SYNC_AUDIO_MASTER; } else if (is->av_sync_type == AV_SYNC_AUDIO_MASTER) { if (is->audio_st) return AV_SYNC_AUDIO_MASTER; else return AV_SYNC_EXTERNAL_CLOCK; } else { return AV_SYNC_EXTERNAL_CLOCK; } } /* get the current master clock value */ static double get_master_clock(VideoState *is) { double val; switch (get_master_sync_type(is)) { case AV_SYNC_VIDEO_MASTER: val = get_clock(&is->vidclk); break; case AV_SYNC_AUDIO_MASTER: val = get_clock(&is->audclk); break; default: val = get_clock(&is->extclk); break; } return val; } static void check_external_clock_speed(VideoState *is) { if (is->video_stream >= 0 && is->videoq.nb_packets audio_stream >= 0 && is->audioq.nb_packets extclk, FFMAX(EXTERNAL_CLOCK_SPEED_MIN, is->extclk.speed - EXTERNAL_CLOCK_SPEED_STEP)); } else if ((is->video_stream videoq.nb_packets > EXTERNAL_CLOCK_MAX_FRAMES) && (is->audio_stream audioq.nb_packets > EXTERNAL_CLOCK_MAX_FRAMES)) { set_clock_speed(&is->extclk, FFMIN(EXTERNAL_CLOCK_SPEED_MAX, is->extclk.speed + EXTERNAL_CLOCK_SPEED_STEP)); } else { double speed = is->extclk.speed; if (speed != 1.0) set_clock_speed(&is->extclk, speed + EXTERNAL_CLOCK_SPEED_STEP * (1.0 - speed) / fabs(1.0 - speed)); } } /* seek in the stream */ static void stream_seek(VideoState *is, int64_t pos, int64_t rel, int seek_by_bytes) { if (!is->seek_req) { is->seek_pos = pos; is->seek_rel = rel; is->seek_flags &= ~AVSEEK_FLAG_BYTE; if (seek_by_bytes) is->seek_flags |= AVSEEK_FLAG_BYTE; is->seek_req = 1; SDL_CondSignal(is->continue_read_thread); } } /* pause or resume the video */ static void stream_toggle_pause(VideoState *is) { if (is->paused) { is->frame_timer += av_gettime_relative() / 1000000.0 - is->vidclk.last_updated; if (is->read_pause_return != AVERROR(ENOSYS)) { is->vidclk.paused = 0; } set_clock(&is->vidclk, get_clock(&is->vidclk), is->vidclk.serial); } set_clock(&is->extclk, get_clock(&is->extclk), is->extclk.serial); is->paused = is->audclk.paused = is->vidclk.paused = is->extclk.paused = !is->paused; } static void toggle_pause(VideoState *is) { stream_toggle_pause(is); is->step = 0; } static void toggle_mute(VideoState *is) { is->muted = !is->muted; } static void update_volume(VideoState *is, int sign, double step) { double volume_level = is->audio_volume ? (20 * log(is->audio_volume / (double)SDL_MIX_MAXVOLUME) / log(10)) : -1000.0; int new_volume = lrint(SDL_MIX_MAXVOLUME * pow(10.0, (volume_level + sign * step) / 20.0)); is->audio_volume = av_clip(is->audio_volume == new_volume ? (is->audio_volume + sign) : new_volume, 0, SDL_MIX_MAXVOLUME); } static void step_to_next_frame(VideoState *is) { /* if the stream is paused unpause it, then step */ if (is->paused) stream_toggle_pause(is); is->step = 1; } static double compute_target_delay(double delay, VideoState *is) { double sync_threshold, diff = 0; /* update delay to follow master synchronisation source */ if (get_master_sync_type(is) != AV_SYNC_VIDEO_MASTER) { /* if video is slave, we try to correct big delays by duplicating or deleting a frame */ diff = get_clock(&is->vidclk) - get_master_clock(is); /* skip or repeat frame. We take into account the delay to compute the threshold. I still don't know if it is the best guess */ sync_threshold = FFMAX(AV_SYNC_THRESHOLD_MIN, FFMIN(AV_SYNC_THRESHOLD_MAX, delay)); if (!isnan(diff) && fabs(diff) max_frame_duration) { if (diff = sync_threshold && delay > AV_SYNC_FRAMEDUP_THRESHOLD) delay = delay + diff; else if (diff >= sync_threshold) delay = 2 * delay; } } av_log(NULL, AV_LOG_TRACE, \"video: delay=%0.3f A-V=%f\\n\", delay, -diff); return delay; } static double vp_duration(VideoState *is, Frame *vp, Frame *nextvp) { if (vp->serial == nextvp->serial) { double duration = nextvp->pts - vp->pts; if (isnan(duration) || duration is->max_frame_duration) return vp->duration; else return duration; } else { return 0.0; } } static void update_video_pts(VideoState *is, double pts, int64_t pos, int serial) { /* update current video pts */ set_clock(&is->vidclk, pts, serial); sync_clock_to_slave(&is->extclk, &is->vidclk); } /* called to display each frame */ static void video_refresh(void *opaque, double *remaining_time) { VideoState *is = opaque; double time; Frame *sp, *sp2; if (!is->paused && get_master_sync_type(is) == AV_SYNC_EXTERNAL_CLOCK && is->realtime) check_external_clock_speed(is); if (!display_disable && is->show_mode != SHOW_MODE_VIDEO && is->audio_st) { time = av_gettime_relative() / 1000000.0; if (is->force_refresh || is->last_vis_time + rdftspeed last_vis_time = time; } *remaining_time = FFMIN(*remaining_time, is->last_vis_time + rdftspeed - time); } if (is->video_st) { retry: if (frame_queue_nb_remaining(&is->pictq) == 0) { // nothing to do, no picture to display in the queue } else { double last_duration, duration, delay; Frame *vp, *lastvp; /* dequeue the picture */ lastvp = frame_queue_peek_last(&is->pictq); vp = frame_queue_peek(&is->pictq); if (vp->serial != is->videoq.serial) { frame_queue_next(&is->pictq); goto retry; } if (lastvp->serial != vp->serial) is->frame_timer = av_gettime_relative() / 1000000.0; if (is->paused) goto display; /* compute nominal last_duration */ last_duration = vp_duration(is, lastvp, vp); delay = compute_target_delay(last_duration, is); time= av_gettime_relative()/1000000.0; if (time frame_timer + delay) { *remaining_time = FFMIN(is->frame_timer + delay - time, *remaining_time); goto display; } is->frame_timer += delay; if (delay > 0 && time - is->frame_timer > AV_SYNC_THRESHOLD_MAX) is->frame_timer = time; SDL_LockMutex(is->pictq.mutex); if (!isnan(vp->pts)) update_video_pts(is, vp->pts, vp->pos, vp->serial); SDL_UnlockMutex(is->pictq.mutex); if (frame_queue_nb_remaining(&is->pictq) > 1) { Frame *nextvp = frame_queue_peek_next(&is->pictq); duration = vp_duration(is, vp, nextvp); if(!is->step && (framedrop>0 || (framedrop && get_master_sync_type(is) != AV_SYNC_VIDEO_MASTER)) && time > is->frame_timer + duration){ is->frame_drops_late++; frame_queue_next(&is->pictq); goto retry; } } if (is->subtitle_st) { while (frame_queue_nb_remaining(&is->subpq) > 0) { sp = frame_queue_peek(&is->subpq); if (frame_queue_nb_remaining(&is->subpq) > 1) sp2 = frame_queue_peek_next(&is->subpq); else sp2 = NULL; if (sp->serial != is->subtitleq.serial || (is->vidclk.pts > (sp->pts + ((float) sp->sub.end_display_time / 1000))) || (sp2 && is->vidclk.pts > (sp2->pts + ((float) sp2->sub.start_display_time / 1000)))) { if (sp->uploaded) { int i; for (i = 0; i sub.num_rects; i++) { AVSubtitleRect *sub_rect = sp->sub.rects[i]; uint8_t *pixels; int pitch, j; if (!SDL_LockTexture(is->sub_texture, (SDL_Rect *)sub_rect, (void **)&pixels, &pitch)) { for (j = 0; j h; j++, pixels += pitch) memset(pixels, 0, sub_rect->w sub_texture); } } } frame_queue_next(&is->subpq); } else { break; } } } frame_queue_next(&is->pictq); is->force_refresh = 1; if (is->step && !is->paused) stream_toggle_pause(is); } display: /* display picture */ if (!display_disable && is->force_refresh && is->show_mode == SHOW_MODE_VIDEO && is->pictq.rindex_shown) video_display(is); } is->force_refresh = 0; if (show_status) { static int64_t last_time; int64_t cur_time; int aqsize, vqsize, sqsize; double av_diff; cur_time = av_gettime_relative(); if (!last_time || (cur_time - last_time) >= 30000) { aqsize = 0; vqsize = 0; sqsize = 0; if (is->audio_st) aqsize = is->audioq.size; if (is->video_st) vqsize = is->videoq.size; if (is->subtitle_st) sqsize = is->subtitleq.size; av_diff = 0; if (is->audio_st && is->video_st) av_diff = get_clock(&is->audclk) - get_clock(&is->vidclk); else if (is->video_st) av_diff = get_master_clock(is) - get_clock(&is->vidclk); else if (is->audio_st) av_diff = get_master_clock(is) - get_clock(&is->audclk); av_log(NULL, AV_LOG_INFO, \"%7.2f %s:%7.3f fd=%4d aq=%5dKB vq=%5dKB sq=%5dB f=%\"PRId64\"/%\"PRId64\" \\r\", get_master_clock(is), (is->audio_st && is->video_st) ? \"A-V\" : (is->video_st ? \"M-V\" : (is->audio_st ? \"M-A\" : \" \")), av_diff, is->frame_drops_early + is->frame_drops_late, aqsize / 1024, vqsize / 1024, sqsize, is->video_st ? is->viddec.avctx->pts_correction_num_faulty_dts : 0, is->video_st ? is->viddec.avctx->pts_correction_num_faulty_pts : 0); fflush(stdout); last_time = cur_time; } } } static int queue_picture(VideoState *is, AVFrame *src_frame, double pts, double duration, int64_t pos, int serial) { Frame *vp; #if defined(DEBUG_SYNC) printf(\"frame_type=%c pts=%0.3f\\n\", av_get_picture_type_char(src_frame->pict_type), pts); #endif if (!(vp = frame_queue_peek_writable(&is->pictq))) return -1; vp->sar = src_frame->sample_aspect_ratio; vp->uploaded = 0; vp->width = src_frame->width; vp->height = src_frame->height; vp->format = src_frame->format; vp->pts = pts; vp->duration = duration; vp->pos = pos; vp->serial = serial; set_default_window_size(vp->width, vp->height, vp->sar); av_frame_move_ref(vp->frame, src_frame); frame_queue_push(&is->pictq); return 0; } static int get_video_frame(VideoState *is, AVFrame *frame) { int got_picture; if ((got_picture = decoder_decode_frame(&is->viddec, frame, NULL)) pts != AV_NOPTS_VALUE) dpts = av_q2d(is->video_st->time_base) * frame->pts; frame->sample_aspect_ratio = av_guess_sample_aspect_ratio(is->ic, is->video_st, frame); if (framedrop>0 || (framedrop && get_master_sync_type(is) != AV_SYNC_VIDEO_MASTER)) { if (frame->pts != AV_NOPTS_VALUE) { double diff = dpts - get_master_clock(is); if (!isnan(diff) && fabs(diff) frame_last_filter_delay viddec.pkt_serial == is->vidclk.serial && is->videoq.nb_packets) { is->frame_drops_early++; av_frame_unref(frame); got_picture = 0; } } } } return got_picture; } #if CONFIG_AVFILTER static int configure_filtergraph(AVFilterGraph *graph, const char *filtergraph, AVFilterContext *source_ctx, AVFilterContext *sink_ctx) { int ret, i; int nb_filters = graph->nb_filters; AVFilterInOut *outputs = NULL, *inputs = NULL; if (filtergraph) { outputs = avfilter_inout_alloc(); inputs = avfilter_inout_alloc(); if (!outputs || !inputs) { ret = AVERROR(ENOMEM); goto fail; } outputs->name = av_strdup(\"in\"); outputs->filter_ctx = source_ctx; outputs->pad_idx = 0; outputs->next = NULL; inputs->name = av_strdup(\"out\"); inputs->filter_ctx = sink_ctx; inputs->pad_idx = 0; inputs->next = NULL; if ((ret = avfilter_graph_parse_ptr(graph, filtergraph, &inputs, &outputs, NULL)) nb_filters - nb_filters; i++) FFSWAP(AVFilterContext*, graph->filters[i], graph->filters[i + nb_filters]); ret = avfilter_graph_config(graph, NULL); fail: avfilter_inout_free(&outputs); avfilter_inout_free(&inputs); return ret; } static int configure_video_filters(AVFilterGraph *graph, VideoState *is, const char *vfilters, AVFrame *frame) { enum AVPixelFormat pix_fmts[FF_ARRAY_ELEMS(sdl_texture_format_map)]; char sws_flags_str[512] = \"\"; char buffersrc_args[256]; int ret; AVFilterContext *filt_src = NULL, *filt_out = NULL, *last_filter = NULL; AVCodecParameters *codecpar = is->video_st->codecpar; AVRational fr = av_guess_frame_rate(is->ic, is->video_st, NULL); AVDictionaryEntry *e = NULL; int nb_pix_fmts = 0; int i, j; for (i = 0; i key, \"sws_flags\")) { av_strlcatf(sws_flags_str, sizeof(sws_flags_str), \"%s=%s:\", \"flags\", e->value); } else av_strlcatf(sws_flags_str, sizeof(sws_flags_str), \"%s=%s:\", e->key, e->value); } if (strlen(sws_flags_str)) sws_flags_str[strlen(sws_flags_str)-1] = '\\0'; graph->scale_sws_opts = av_strdup(sws_flags_str); snprintf(buffersrc_args, sizeof(buffersrc_args), \"video_size=%dx%d:pix_fmt=%d:time_base=%d/%d:pixel_aspect=%d/%d\", frame->width, frame->height, frame->format, is->video_st->time_base.num, is->video_st->time_base.den, codecpar->sample_aspect_ratio.num, FFMAX(codecpar->sample_aspect_ratio.den, 1)); if (fr.num && fr.den) av_strlcatf(buffersrc_args, sizeof(buffersrc_args), \":frame_rate=%d/%d\", fr.num, fr.den); if ((ret = avfilter_graph_create_filter(&filt_src, avfilter_get_by_name(\"buffer\"), \"ffplay_buffer\", buffersrc_args, NULL, graph)) video_st); if (fabs(theta - 90) 1.0) { char rotate_buf[64]; snprintf(rotate_buf, sizeof(rotate_buf), \"%f*PI/180\", theta); INSERT_FILT(\"rotate\", rotate_buf); } } if ((ret = configure_filtergraph(graph, vfilters, filt_src, last_filter)) in_video_filter = filt_src; is->out_video_filter = filt_out; fail: return ret; } static int configure_audio_filters(VideoState *is, const char *afilters, int force_output_format) { static const enum AVSampleFormat sample_fmts[] = { AV_SAMPLE_FMT_S16, AV_SAMPLE_FMT_NONE }; int sample_rates[2] = { 0, -1 }; int64_t channel_layouts[2] = { 0, -1 }; int channels[2] = { 0, -1 }; AVFilterContext *filt_asrc = NULL, *filt_asink = NULL; char aresample_swr_opts[512] = \"\"; AVDictionaryEntry *e = NULL; char asrc_args[256]; int ret; avfilter_graph_free(&is->agraph); if (!(is->agraph = avfilter_graph_alloc())) return AVERROR(ENOMEM); is->agraph->nb_threads = filter_nbthreads; while ((e = av_dict_get(swr_opts, \"\", e, AV_DICT_IGNORE_SUFFIX))) av_strlcatf(aresample_swr_opts, sizeof(aresample_swr_opts), \"%s=%s:\", e->key, e->value); if (strlen(aresample_swr_opts)) aresample_swr_opts[strlen(aresample_swr_opts)-1] = '\\0'; av_opt_set(is->agraph, \"aresample_swr_opts\", aresample_swr_opts, 0); ret = snprintf(asrc_args, sizeof(asrc_args), \"sample_rate=%d:sample_fmt=%s:channels=%d:time_base=%d/%d\", is->audio_filter_src.freq, av_get_sample_fmt_name(is->audio_filter_src.fmt), is->audio_filter_src.channels, 1, is->audio_filter_src.freq); if (is->audio_filter_src.channel_layout) snprintf(asrc_args + ret, sizeof(asrc_args) - ret, \":channel_layout=0x%\"PRIx64, is->audio_filter_src.channel_layout); ret = avfilter_graph_create_filter(&filt_asrc, avfilter_get_by_name(\"abuffer\"), \"ffplay_abuffer\", asrc_args, NULL, is->agraph); if (ret agraph); if (ret audio_tgt.channel_layout; channels [0] = is->audio_tgt.channels; sample_rates [0] = is->audio_tgt.freq; if ((ret = av_opt_set_int(filt_asink, \"all_channel_counts\", 0, AV_OPT_SEARCH_CHILDREN)) agraph, afilters, filt_asrc, filt_asink)) in_audio_filter = filt_asrc; is->out_audio_filter = filt_asink; end: if (ret agraph); return ret; } #endif /* CONFIG_AVFILTER */ static int audio_thread(void *arg) { VideoState *is = arg; AVFrame *frame = av_frame_alloc(); Frame *af; #if CONFIG_AVFILTER int last_serial = -1; int64_t dec_channel_layout; int reconfigure; #endif int got_frame = 0; AVRational tb; int ret = 0; if (!frame) return AVERROR(ENOMEM); do { if ((got_frame = decoder_decode_frame(&is->auddec, frame, NULL)) sample_rate}; #if CONFIG_AVFILTER dec_channel_layout = get_valid_channel_layout(frame->channel_layout, frame->channels); reconfigure = cmp_audio_fmts(is->audio_filter_src.fmt, is->audio_filter_src.channels, frame->format, frame->channels) || is->audio_filter_src.channel_layout != dec_channel_layout || is->audio_filter_src.freq != frame->sample_rate || is->auddec.pkt_serial != last_serial; if (reconfigure) { char buf1[1024], buf2[1024]; av_get_channel_layout_string(buf1, sizeof(buf1), -1, is->audio_filter_src.channel_layout); av_get_channel_layout_string(buf2, sizeof(buf2), -1, dec_channel_layout); av_log(NULL, AV_LOG_DEBUG, \"Audio frame changed from rate:%d ch:%d fmt:%s layout:%s serial:%d to rate:%d ch:%d fmt:%s layout:%s serial:%d\\n\", is->audio_filter_src.freq, is->audio_filter_src.channels, av_get_sample_fmt_name(is->audio_filter_src.fmt), buf1, last_serial, frame->sample_rate, frame->channels, av_get_sample_fmt_name(frame->format), buf2, is->auddec.pkt_serial); is->audio_filter_src.fmt = frame->format; is->audio_filter_src.channels = frame->channels; is->audio_filter_src.channel_layout = dec_channel_layout; is->audio_filter_src.freq = frame->sample_rate; last_serial = is->auddec.pkt_serial; if ((ret = configure_audio_filters(is, afilters, 1)) in_audio_filter, frame)) out_audio_filter, frame, 0)) >= 0) { tb = av_buffersink_get_time_base(is->out_audio_filter); #endif if (!(af = frame_queue_peek_writable(&is->sampq))) goto the_end; af->pts = (frame->pts == AV_NOPTS_VALUE) ? NAN : frame->pts * av_q2d(tb); af->pos = frame->pkt_pos; af->serial = is->auddec.pkt_serial; af->duration = av_q2d((AVRational){frame->nb_samples, frame->sample_rate}); av_frame_move_ref(af->frame, frame); frame_queue_push(&is->sampq); #if CONFIG_AVFILTER if (is->audioq.serial != is->auddec.pkt_serial) break; } if (ret == AVERROR_EOF) is->auddec.finished = is->auddec.pkt_serial; #endif } } while (ret >= 0 || ret == AVERROR(EAGAIN) || ret == AVERROR_EOF); the_end: #if CONFIG_AVFILTER avfilter_graph_free(&is->agraph); #endif av_frame_free(&frame); return ret; } static int decoder_start(Decoder *d, int (*fn)(void *), const char *thread_name, void* arg) { packet_queue_start(d->queue); d->decoder_tid = SDL_CreateThread(fn, thread_name, arg); if (!d->decoder_tid) { av_log(NULL, AV_LOG_ERROR, \"SDL_CreateThread(): %s\\n\", SDL_GetError()); return AVERROR(ENOMEM); } return 0; } static int video_thread(void *arg) { VideoState *is = arg; AVFrame *frame = av_frame_alloc(); double pts; double duration; int ret; AVRational tb = is->video_st->time_base; AVRational frame_rate = av_guess_frame_rate(is->ic, is->video_st, NULL); #if CONFIG_AVFILTER AVFilterGraph *graph = NULL; AVFilterContext *filt_out = NULL, *filt_in = NULL; int last_w = 0; int last_h = 0; enum AVPixelFormat last_format = -2; int last_serial = -1; int last_vfilter_idx = 0; #endif if (!frame) return AVERROR(ENOMEM); for (;;) { ret = get_video_frame(is, frame); if (ret width || last_h != frame->height || last_format != frame->format || last_serial != is->viddec.pkt_serial || last_vfilter_idx != is->vfilter_idx) { av_log(NULL, AV_LOG_DEBUG, \"Video frame changed from size:%dx%d format:%s serial:%d to size:%dx%d format:%s serial:%d\\n\", last_w, last_h, (const char *)av_x_if_null(av_get_pix_fmt_name(last_format), \"none\"), last_serial, frame->width, frame->height, (const char *)av_x_if_null(av_get_pix_fmt_name(frame->format), \"none\"), is->viddec.pkt_serial); avfilter_graph_free(&graph); graph = avfilter_graph_alloc(); if (!graph) { ret = AVERROR(ENOMEM); goto the_end; } graph->nb_threads = filter_nbthreads; if ((ret = configure_video_filters(graph, is, vfilters_list ? vfilters_list[is->vfilter_idx] : NULL, frame)) in_video_filter; filt_out = is->out_video_filter; last_w = frame->width; last_h = frame->height; last_format = frame->format; last_serial = is->viddec.pkt_serial; last_vfilter_idx = is->vfilter_idx; frame_rate = av_buffersink_get_frame_rate(filt_out); } ret = av_buffersrc_add_frame(filt_in, frame); if (ret = 0) { is->frame_last_returned_time = av_gettime_relative() / 1000000.0; ret = av_buffersink_get_frame_flags(filt_out, frame, 0); if (ret viddec.finished = is->viddec.pkt_serial; ret = 0; break; } is->frame_last_filter_delay = av_gettime_relative() / 1000000.0 - is->frame_last_returned_time; if (fabs(is->frame_last_filter_delay) > AV_NOSYNC_THRESHOLD / 10.0) is->frame_last_filter_delay = 0; tb = av_buffersink_get_time_base(filt_out); #endif duration = (frame_rate.num && frame_rate.den ? av_q2d((AVRational){frame_rate.den, frame_rate.num}) : 0); pts = (frame->pts == AV_NOPTS_VALUE) ? NAN : frame->pts * av_q2d(tb); ret = queue_picture(is, frame, pts, duration, frame->pkt_pos, is->viddec.pkt_serial); av_frame_unref(frame); #if CONFIG_AVFILTER if (is->videoq.serial != is->viddec.pkt_serial) break; } #endif if (ret subpq))) return 0; if ((got_subtitle = decoder_decode_frame(&is->subdec, NULL, &sp->sub)) sub.format == 0) { if (sp->sub.pts != AV_NOPTS_VALUE) pts = sp->sub.pts / (double)AV_TIME_BASE; sp->pts = pts; sp->serial = is->subdec.pkt_serial; sp->width = is->subdec.avctx->width; sp->height = is->subdec.avctx->height; sp->uploaded = 0; /* now we can update the picture count */ frame_queue_push(&is->subpq); } else if (got_subtitle) { avsubtitle_free(&sp->sub); } } return 0; } /* copy samples for viewing in editor window */ static void update_sample_display(VideoState *is, short *samples, int samples_size) { int size, len; size = samples_size / sizeof(short); while (size > 0) { len = SAMPLE_ARRAY_SIZE - is->sample_array_index; if (len > size) len = size; memcpy(is->sample_array + is->sample_array_index, samples, len * sizeof(short)); samples += len; is->sample_array_index += len; if (is->sample_array_index >= SAMPLE_ARRAY_SIZE) is->sample_array_index = 0; size -= len; } } /* return the wanted number of samples to get better sync if sync_type is video * or external master clock */ static int synchronize_audio(VideoState *is, int nb_samples) { int wanted_nb_samples = nb_samples; /* if not master, then we try to remove or add samples to correct the clock */ if (get_master_sync_type(is) != AV_SYNC_AUDIO_MASTER) { double diff, avg_diff; int min_nb_samples, max_nb_samples; diff = get_clock(&is->audclk) - get_master_clock(is); if (!isnan(diff) && fabs(diff) audio_diff_cum = diff + is->audio_diff_avg_coef * is->audio_diff_cum; if (is->audio_diff_avg_count audio_diff_avg_count++; } else { /* estimate the A-V difference */ avg_diff = is->audio_diff_cum * (1.0 - is->audio_diff_avg_coef); if (fabs(avg_diff) >= is->audio_diff_threshold) { wanted_nb_samples = nb_samples + (int)(diff * is->audio_src.freq); min_nb_samples = ((nb_samples * (100 - SAMPLE_CORRECTION_PERCENT_MAX) / 100)); max_nb_samples = ((nb_samples * (100 + SAMPLE_CORRECTION_PERCENT_MAX) / 100)); wanted_nb_samples = av_clip(wanted_nb_samples, min_nb_samples, max_nb_samples); } av_log(NULL, AV_LOG_TRACE, \"diff=%f adiff=%f sample_diff=%d apts=%0.3f %f\\n\", diff, avg_diff, wanted_nb_samples - nb_samples, is->audio_clock, is->audio_diff_threshold); } } else { /* too big difference : may be initial PTS errors, so reset A-V filter */ is->audio_diff_avg_count = 0; is->audio_diff_cum = 0; } } return wanted_nb_samples; } /** * Decode one audio frame and return its uncompressed size. * * The processed audio frame is decoded, converted if required, and * stored in is->audio_buf, with size in bytes given by the return * value. */ static int audio_decode_frame(VideoState *is) { int data_size, resampled_data_size; int64_t dec_channel_layout; av_unused double audio_clock0; int wanted_nb_samples; Frame *af; if (is->paused) return -1; do { #if defined(_WIN32) while (frame_queue_nb_remaining(&is->sampq) == 0) { if ((av_gettime_relative() - audio_callback_time) > 1000000LL * is->audio_hw_buf_size / is->audio_tgt.bytes_per_sec / 2) return -1; av_usleep (1000); } #endif if (!(af = frame_queue_peek_readable(&is->sampq))) return -1; frame_queue_next(&is->sampq); } while (af->serial != is->audioq.serial); data_size = av_samples_get_buffer_size(NULL, af->frame->channels, af->frame->nb_samples, af->frame->format, 1); dec_channel_layout = (af->frame->channel_layout && af->frame->channels == av_get_channel_layout_nb_channels(af->frame->channel_layout)) ? af->frame->channel_layout : av_get_default_channel_layout(af->frame->channels); wanted_nb_samples = synchronize_audio(is, af->frame->nb_samples); if (af->frame->format != is->audio_src.fmt || dec_channel_layout != is->audio_src.channel_layout || af->frame->sample_rate != is->audio_src.freq || (wanted_nb_samples != af->frame->nb_samples && !is->swr_ctx)) { swr_free(&is->swr_ctx); is->swr_ctx = swr_alloc_set_opts(NULL, is->audio_tgt.channel_layout, is->audio_tgt.fmt, is->audio_tgt.freq, dec_channel_layout, af->frame->format, af->frame->sample_rate, 0, NULL); if (!is->swr_ctx || swr_init(is->swr_ctx) frame->sample_rate, av_get_sample_fmt_name(af->frame->format), af->frame->channels, is->audio_tgt.freq, av_get_sample_fmt_name(is->audio_tgt.fmt), is->audio_tgt.channels); swr_free(&is->swr_ctx); return -1; } is->audio_src.channel_layout = dec_channel_layout; is->audio_src.channels = af->frame->channels; is->audio_src.freq = af->frame->sample_rate; is->audio_src.fmt = af->frame->format; } if (is->swr_ctx) { const uint8_t **in = (const uint8_t **)af->frame->extended_data; uint8_t **out = &is->audio_buf1; int out_count = (int64_t)wanted_nb_samples * is->audio_tgt.freq / af->frame->sample_rate + 256; int out_size = av_samples_get_buffer_size(NULL, is->audio_tgt.channels, out_count, is->audio_tgt.fmt, 0); int len2; if (out_size frame->nb_samples) { if (swr_set_compensation(is->swr_ctx, (wanted_nb_samples - af->frame->nb_samples) * is->audio_tgt.freq / af->frame->sample_rate, wanted_nb_samples * is->audio_tgt.freq / af->frame->sample_rate) audio_buf1, &is->audio_buf1_size, out_size); if (!is->audio_buf1) return AVERROR(ENOMEM); len2 = swr_convert(is->swr_ctx, out, out_count, in, af->frame->nb_samples); if (len2 swr_ctx) swr_ctx); } is->audio_buf = is->audio_buf1; resampled_data_size = len2 * is->audio_tgt.channels * av_get_bytes_per_sample(is->audio_tgt.fmt); } else { is->audio_buf = af->frame->data[0]; resampled_data_size = data_size; } audio_clock0 = is->audio_clock; /* update the audio clock with the pts */ if (!isnan(af->pts)) is->audio_clock = af->pts + (double) af->frame->nb_samples / af->frame->sample_rate; else is->audio_clock = NAN; is->audio_clock_serial = af->serial; #ifdef DEBUG { static double last_clock; printf(\"audio: delay=%0.3f clock=%0.3f clock0=%0.3f\\n\", is->audio_clock - last_clock, is->audio_clock, audio_clock0); last_clock = is->audio_clock; } #endif return resampled_data_size; } /* prepare a new audio buffer */ static void sdl_audio_callback(void *opaque, Uint8 *stream, int len) { VideoState *is = opaque; int audio_size, len1; audio_callback_time = av_gettime_relative(); while (len > 0) { if (is->audio_buf_index >= is->audio_buf_size) { audio_size = audio_decode_frame(is); if (audio_size audio_buf = NULL; is->audio_buf_size = SDL_AUDIO_MIN_BUFFER_SIZE / is->audio_tgt.frame_size * is->audio_tgt.frame_size; } else { if (is->show_mode != SHOW_MODE_VIDEO) update_sample_display(is, (int16_t *)is->audio_buf, audio_size); is->audio_buf_size = audio_size; } is->audio_buf_index = 0; } len1 = is->audio_buf_size - is->audio_buf_index; if (len1 > len) len1 = len; if (!is->muted && is->audio_buf && is->audio_volume == SDL_MIX_MAXVOLUME) memcpy(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, len1); else { memset(stream, 0, len1); if (!is->muted && is->audio_buf) SDL_MixAudioFormat(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, AUDIO_S16SYS, len1, is->audio_volume); } len -= len1; stream += len1; is->audio_buf_index += len1; } is->audio_write_buf_size = is->audio_buf_size - is->audio_buf_index; /* Let's assume the audio driver that is used by SDL has two periods. */ if (!isnan(is->audio_clock)) { set_clock_at(&is->audclk, is->audio_clock - (double)(2 * is->audio_hw_buf_size + is->audio_write_buf_size) / is->audio_tgt.bytes_per_sec, is->audio_clock_serial, audio_callback_time / 1000000.0); sync_clock_to_slave(&is->extclk, &is->audclk); } } static int audio_open(void *opaque, int64_t wanted_channel_layout, int wanted_nb_channels, int wanted_sample_rate, struct AudioParams *audio_hw_params) { SDL_AudioSpec wanted_spec, spec; const char *env; static const int next_nb_channels[] = {0, 0, 1, 6, 2, 6, 4, 6}; static const int next_sample_rates[] = {0, 44100, 48000, 96000, 192000}; int next_sample_rate_idx = FF_ARRAY_ELEMS(next_sample_rates) - 1; env = SDL_getenv(\"SDL_AUDIO_CHANNELS\"); if (env) { wanted_nb_channels = atoi(env); wanted_channel_layout = av_get_default_channel_layout(wanted_nb_channels); } if (!wanted_channel_layout || wanted_nb_channels != av_get_channel_layout_nb_channels(wanted_channel_layout)) { wanted_channel_layout = av_get_default_channel_layout(wanted_nb_channels); wanted_channel_layout &= ~AV_CH_LAYOUT_STEREO_DOWNMIX; } wanted_nb_channels = av_get_channel_layout_nb_channels(wanted_channel_layout); wanted_spec.channels = wanted_nb_channels; wanted_spec.freq = wanted_sample_rate; if (wanted_spec.freq = wanted_spec.freq) next_sample_rate_idx--; wanted_spec.format = AUDIO_S16SYS; wanted_spec.silence = 0; wanted_spec.samples = FFMAX(SDL_AUDIO_MIN_BUFFER_SIZE, 2 fmt = AV_SAMPLE_FMT_S16; audio_hw_params->freq = spec.freq; audio_hw_params->channel_layout = wanted_channel_layout; audio_hw_params->channels = spec.channels; audio_hw_params->frame_size = av_samples_get_buffer_size(NULL, audio_hw_params->channels, 1, audio_hw_params->fmt, 1); audio_hw_params->bytes_per_sec = av_samples_get_buffer_size(NULL, audio_hw_params->channels, audio_hw_params->freq, audio_hw_params->fmt, 1); if (audio_hw_params->bytes_per_sec frame_size ic; AVCodecContext *avctx; AVCodec *codec; const char *forced_codec_name = NULL; AVDictionary *opts = NULL; AVDictionaryEntry *t = NULL; int sample_rate, nb_channels; int64_t channel_layout; int ret = 0; int stream_lowres = lowres; if (stream_index = ic->nb_streams) return -1; avctx = avcodec_alloc_context3(NULL); if (!avctx) return AVERROR(ENOMEM); ret = avcodec_parameters_to_context(avctx, ic->streams[stream_index]->codecpar); if (ret pkt_timebase = ic->streams[stream_index]->time_base; codec = avcodec_find_decoder(avctx->codec_id); switch(avctx->codec_type){ case AVMEDIA_TYPE_AUDIO : is->last_audio_stream = stream_index; forced_codec_name = audio_codec_name; break; case AVMEDIA_TYPE_SUBTITLE: is->last_subtitle_stream = stream_index; forced_codec_name = subtitle_codec_name; break; case AVMEDIA_TYPE_VIDEO : is->last_video_stream = stream_index; forced_codec_name = video_codec_name; break; } if (forced_codec_name) codec = avcodec_find_decoder_by_name(forced_codec_name); if (!codec) { if (forced_codec_name) av_log(NULL, AV_LOG_WARNING, \"No codec could be found with name '%s'\\n\", forced_codec_name); else av_log(NULL, AV_LOG_WARNING, \"No decoder could be found for codec %s\\n\", avcodec_get_name(avctx->codec_id)); ret = AVERROR(EINVAL); goto fail; } avctx->codec_id = codec->id; if (stream_lowres > codec->max_lowres) { av_log(avctx, AV_LOG_WARNING, \"The maximum value for lowres supported by the decoder is %d\\n\", codec->max_lowres); stream_lowres = codec->max_lowres; } avctx->lowres = stream_lowres; if (fast) avctx->flags2 |= AV_CODEC_FLAG2_FAST; opts = filter_codec_opts(codec_opts, avctx->codec_id, ic, ic->streams[stream_index], codec); if (!av_dict_get(opts, \"threads\", NULL, 0)) av_dict_set(&opts, \"threads\", \"auto\", 0); if (stream_lowres) av_dict_set_int(&opts, \"lowres\", stream_lowres, 0); if (avctx->codec_type == AVMEDIA_TYPE_VIDEO || avctx->codec_type == AVMEDIA_TYPE_AUDIO) av_dict_set(&opts, \"refcounted_frames\", \"1\", 0); if ((ret = avcodec_open2(avctx, codec, &opts)) key); ret = AVERROR_OPTION_NOT_FOUND; goto fail; } is->eof = 0; ic->streams[stream_index]->discard = AVDISCARD_DEFAULT; switch (avctx->codec_type) { case AVMEDIA_TYPE_AUDIO: #if CONFIG_AVFILTER { AVFilterContext *sink; is->audio_filter_src.freq = avctx->sample_rate; is->audio_filter_src.channels = avctx->channels; is->audio_filter_src.channel_layout = get_valid_channel_layout(avctx->channel_layout, avctx->channels); is->audio_filter_src.fmt = avctx->sample_fmt; if ((ret = configure_audio_filters(is, afilters, 0)) out_audio_filter; sample_rate = av_buffersink_get_sample_rate(sink); nb_channels = av_buffersink_get_channels(sink); channel_layout = av_buffersink_get_channel_layout(sink); } #else sample_rate = avctx->sample_rate; nb_channels = avctx->channels; channel_layout = avctx->channel_layout; #endif /* prepare audio output */ if ((ret = audio_open(is, channel_layout, nb_channels, sample_rate, &is->audio_tgt)) audio_hw_buf_size = ret; is->audio_src = is->audio_tgt; is->audio_buf_size = 0; is->audio_buf_index = 0; /* init averaging filter */ is->audio_diff_avg_coef = exp(log(0.01) / AUDIO_DIFF_AVG_NB); is->audio_diff_avg_count = 0; /* since we do not have a precise anough audio FIFO fullness, we correct audio sync only if larger than this threshold */ is->audio_diff_threshold = (double)(is->audio_hw_buf_size) / is->audio_tgt.bytes_per_sec; is->audio_stream = stream_index; is->audio_st = ic->streams[stream_index]; decoder_init(&is->auddec, avctx, &is->audioq, is->continue_read_thread); if ((is->ic->iformat->flags & (AVFMT_NOBINSEARCH | AVFMT_NOGENSEARCH | AVFMT_NO_BYTE_SEEK)) && !is->ic->iformat->read_seek) { is->auddec.start_pts = is->audio_st->start_time; is->auddec.start_pts_tb = is->audio_st->time_base; } if ((ret = decoder_start(&is->auddec, audio_thread, \"audio_decoder\", is)) video_stream = stream_index; is->video_st = ic->streams[stream_index]; decoder_init(&is->viddec, avctx, &is->videoq, is->continue_read_thread); if ((ret = decoder_start(&is->viddec, video_thread, \"video_decoder\", is)) queue_attachments_req = 1; break; case AVMEDIA_TYPE_SUBTITLE: is->subtitle_stream = stream_index; is->subtitle_st = ic->streams[stream_index]; decoder_init(&is->subdec, avctx, &is->subtitleq, is->continue_read_thread); if ((ret = decoder_start(&is->subdec, subtitle_thread, \"subtitle_decoder\", is)) abort_request; } static int stream_has_enough_packets(AVStream *st, int stream_id, PacketQueue *queue) { return stream_id abort_request || (st->disposition & AV_DISPOSITION_ATTACHED_PIC) || queue->nb_packets > MIN_FRAMES && (!queue->duration || av_q2d(st->time_base) * queue->duration > 1.0); } static int is_realtime(AVFormatContext *s) { if( !strcmp(s->iformat->name, \"rtp\") || !strcmp(s->iformat->name, \"rtsp\") || !strcmp(s->iformat->name, \"sdp\") ) return 1; if(s->pb && ( !strncmp(s->url, \"rtp:\", 4) || !strncmp(s->url, \"udp:\", 4) ) ) return 1; return 0; } /* this thread gets the stream from the disk or the network */ static int read_thread(void *arg) { VideoState *is = arg; AVFormatContext *ic = NULL; int err, i, ret; int st_index[AVMEDIA_TYPE_NB]; AVPacket pkt1, *pkt = &pkt1; int64_t stream_start_time; int pkt_in_play_range = 0; AVDictionaryEntry *t; SDL_mutex *wait_mutex = SDL_CreateMutex(); int scan_all_pmts_set = 0; int64_t pkt_ts; if (!wait_mutex) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateMutex(): %s\\n\", SDL_GetError()); ret = AVERROR(ENOMEM); goto fail; } memset(st_index, -1, sizeof(st_index)); is->eof = 0; ic = avformat_alloc_context(); if (!ic) { av_log(NULL, AV_LOG_FATAL, \"Could not allocate context.\\n\"); ret = AVERROR(ENOMEM); goto fail; } ic->interrupt_callback.callback = decode_interrupt_cb; ic->interrupt_callback.opaque = is; if (!av_dict_get(format_opts, \"scan_all_pmts\", NULL, AV_DICT_MATCH_CASE)) { av_dict_set(&format_opts, \"scan_all_pmts\", \"1\", AV_DICT_DONT_OVERWRITE); scan_all_pmts_set = 1; } err = avformat_open_input(&ic, is->filename, is->iformat, &format_opts); if (err filename, err); ret = -1; goto fail; } if (scan_all_pmts_set) av_dict_set(&format_opts, \"scan_all_pmts\", NULL, AV_DICT_MATCH_CASE); if ((t = av_dict_get(format_opts, \"\", NULL, AV_DICT_IGNORE_SUFFIX))) { av_log(NULL, AV_LOG_ERROR, \"Option %s not found.\\n\", t->key); ret = AVERROR_OPTION_NOT_FOUND; goto fail; } is->ic = ic; if (genpts) ic->flags |= AVFMT_FLAG_GENPTS; av_format_inject_global_side_data(ic); if (find_stream_info) { AVDictionary **opts = setup_find_stream_info_opts(ic, codec_opts); int orig_nb_streams = ic->nb_streams; err = avformat_find_stream_info(ic, opts); for (i = 0; i filename); ret = -1; goto fail; } } if (ic->pb) ic->pb->eof_reached = 0; // FIXME hack, ffplay maybe should not use avio_feof() to test for the end if (seek_by_bytes iformat->flags & AVFMT_TS_DISCONT) && strcmp(\"ogg\", ic->iformat->name); is->max_frame_duration = (ic->iformat->flags & AVFMT_TS_DISCONT) ? 10.0 : 3600.0; if (!window_title && (t = av_dict_get(ic->metadata, \"title\", NULL, 0))) window_title = av_asprintf(\"%s - %s\", t->value, input_filename); /* if seeking requested, we execute it */ if (start_time != AV_NOPTS_VALUE) { int64_t timestamp; timestamp = start_time; /* add the stream start time */ if (ic->start_time != AV_NOPTS_VALUE) timestamp += ic->start_time; ret = avformat_seek_file(ic, -1, INT64_MIN, timestamp, INT64_MAX, 0); if (ret filename, (double)timestamp / AV_TIME_BASE); } } is->realtime = is_realtime(ic); if (show_status) av_dump_format(ic, 0, is->filename, 0); for (i = 0; i nb_streams; i++) { AVStream *st = ic->streams[i]; enum AVMediaType type = st->codecpar->codec_type; st->discard = AVDISCARD_ALL; if (type >= 0 && wanted_stream_spec[type] && st_index[type] == -1) if (avformat_match_stream_specifier(ic, st, wanted_stream_spec[type]) > 0) st_index[type] = i; } for (i = 0; i = 0 ? st_index[AVMEDIA_TYPE_AUDIO] : st_index[AVMEDIA_TYPE_VIDEO]), NULL, 0); is->show_mode = show_mode; if (st_index[AVMEDIA_TYPE_VIDEO] >= 0) { AVStream *st = ic->streams[st_index[AVMEDIA_TYPE_VIDEO]]; AVCodecParameters *codecpar = st->codecpar; AVRational sar = av_guess_sample_aspect_ratio(ic, st, NULL); if (codecpar->width) set_default_window_size(codecpar->width, codecpar->height, sar); } /* open the streams */ if (st_index[AVMEDIA_TYPE_AUDIO] >= 0) { stream_component_open(is, st_index[AVMEDIA_TYPE_AUDIO]); } ret = -1; if (st_index[AVMEDIA_TYPE_VIDEO] >= 0) { ret = stream_component_open(is, st_index[AVMEDIA_TYPE_VIDEO]); } if (is->show_mode == SHOW_MODE_NONE) is->show_mode = ret >= 0 ? SHOW_MODE_VIDEO : SHOW_MODE_RDFT; if (st_index[AVMEDIA_TYPE_SUBTITLE] >= 0) { stream_component_open(is, st_index[AVMEDIA_TYPE_SUBTITLE]); } if (is->video_stream audio_stream filename); ret = -1; goto fail; } if (infinite_buffer realtime) infinite_buffer = 1; for (;;) { if (is->abort_request) break; if (is->paused != is->last_paused) { is->last_paused = is->paused; if (is->paused) is->read_pause_return = av_read_pause(ic); else av_read_play(ic); } #if CONFIG_RTSP_DEMUXER || CONFIG_MMSH_PROTOCOL if (is->paused && (!strcmp(ic->iformat->name, \"rtsp\") || (ic->pb && !strncmp(input_filename, \"mmsh:\", 5)))) { /* wait 10 ms to avoid trying to get another packet */ /* XXX: horrible */ SDL_Delay(10); continue; } #endif if (is->seek_req) { int64_t seek_target = is->seek_pos; int64_t seek_min = is->seek_rel > 0 ? seek_target - is->seek_rel + 2: INT64_MIN; int64_t seek_max = is->seek_rel seek_rel - 2: INT64_MAX; // FIXME the +-2 is due to rounding being not done in the correct direction in generation // of the seek_pos/seek_rel variables ret = avformat_seek_file(is->ic, -1, seek_min, seek_target, seek_max, is->seek_flags); if (ret ic->url); } else { if (is->audio_stream >= 0) { packet_queue_flush(&is->audioq); packet_queue_put(&is->audioq, &flush_pkt); } if (is->subtitle_stream >= 0) { packet_queue_flush(&is->subtitleq); packet_queue_put(&is->subtitleq, &flush_pkt); } if (is->video_stream >= 0) { packet_queue_flush(&is->videoq); packet_queue_put(&is->videoq, &flush_pkt); } if (is->seek_flags & AVSEEK_FLAG_BYTE) { set_clock(&is->extclk, NAN, 0); } else { set_clock(&is->extclk, seek_target / (double)AV_TIME_BASE, 0); } } is->seek_req = 0; is->queue_attachments_req = 1; is->eof = 0; if (is->paused) step_to_next_frame(is); } if (is->queue_attachments_req) { if (is->video_st && is->video_st->disposition & AV_DISPOSITION_ATTACHED_PIC) { AVPacket copy = { 0 }; if ((ret = av_packet_ref(&copy, &is->video_st->attached_pic)) videoq, &copy); packet_queue_put_nullpacket(&is->videoq, is->video_stream); } is->queue_attachments_req = 0; } /* if the queue are full, no need to read more */ if (infinite_bufferaudioq.size + is->videoq.size + is->subtitleq.size > MAX_QUEUE_SIZE || (stream_has_enough_packets(is->audio_st, is->audio_stream, &is->audioq) && stream_has_enough_packets(is->video_st, is->video_stream, &is->videoq) && stream_has_enough_packets(is->subtitle_st, is->subtitle_stream, &is->subtitleq)))) { /* wait 10 ms */ SDL_LockMutex(wait_mutex); SDL_CondWaitTimeout(is->continue_read_thread, wait_mutex, 10); SDL_UnlockMutex(wait_mutex); continue; } if (!is->paused && (!is->audio_st || (is->auddec.finished == is->audioq.serial && frame_queue_nb_remaining(&is->sampq) == 0)) && (!is->video_st || (is->viddec.finished == is->videoq.serial && frame_queue_nb_remaining(&is->pictq) == 0))) { if (loop != 1 && (!loop || --loop)) { stream_seek(is, start_time != AV_NOPTS_VALUE ? start_time : 0, 0, 0); } else if (autoexit) { ret = AVERROR_EOF; goto fail; } } ret = av_read_frame(ic, pkt); if (ret pb)) && !is->eof) { if (is->video_stream >= 0) packet_queue_put_nullpacket(&is->videoq, is->video_stream); if (is->audio_stream >= 0) packet_queue_put_nullpacket(&is->audioq, is->audio_stream); if (is->subtitle_stream >= 0) packet_queue_put_nullpacket(&is->subtitleq, is->subtitle_stream); is->eof = 1; } if (ic->pb && ic->pb->error) break; SDL_LockMutex(wait_mutex); SDL_CondWaitTimeout(is->continue_read_thread, wait_mutex, 10); SDL_UnlockMutex(wait_mutex); continue; } else { is->eof = 0; } /* check if packet is in play range specified by user, then queue, otherwise discard */ stream_start_time = ic->streams[pkt->stream_index]->start_time; pkt_ts = pkt->pts == AV_NOPTS_VALUE ? pkt->dts : pkt->pts; pkt_in_play_range = duration == AV_NOPTS_VALUE || (pkt_ts - (stream_start_time != AV_NOPTS_VALUE ? stream_start_time : 0)) * av_q2d(ic->streams[pkt->stream_index]->time_base) - (double)(start_time != AV_NOPTS_VALUE ? start_time : 0) / 1000000 stream_index == is->audio_stream && pkt_in_play_range) { packet_queue_put(&is->audioq, pkt); } else if (pkt->stream_index == is->video_stream && pkt_in_play_range && !(is->video_st->disposition & AV_DISPOSITION_ATTACHED_PIC)) { packet_queue_put(&is->videoq, pkt); } else if (pkt->stream_index == is->subtitle_stream && pkt_in_play_range) { packet_queue_put(&is->subtitleq, pkt); } else { av_packet_unref(pkt); } } ret = 0; fail: if (ic && !is->ic) avformat_close_input(&ic); if (ret != 0) { SDL_Event event; event.type = FF_QUIT_EVENT; event.user.data1 = is; SDL_PushEvent(&event); } SDL_DestroyMutex(wait_mutex); return 0; } static VideoState *stream_open(const char *filename, AVInputFormat *iformat) { VideoState *is; is = av_mallocz(sizeof(VideoState)); if (!is) return NULL; is->last_video_stream = is->video_stream = -1; is->last_audio_stream = is->audio_stream = -1; is->last_subtitle_stream = is->subtitle_stream = -1; is->filename = av_strdup(filename); if (!is->filename) goto fail; is->iformat = iformat; is->ytop = 0; is->xleft = 0; /* start video display */ if (frame_queue_init(&is->pictq, &is->videoq, VIDEO_PICTURE_QUEUE_SIZE, 1) subpq, &is->subtitleq, SUBPICTURE_QUEUE_SIZE, 0) sampq, &is->audioq, SAMPLE_QUEUE_SIZE, 1) videoq) audioq) subtitleq) continue_read_thread = SDL_CreateCond())) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateCond(): %s\\n\", SDL_GetError()); goto fail; } init_clock(&is->vidclk, &is->videoq.serial); init_clock(&is->audclk, &is->audioq.serial); init_clock(&is->extclk, &is->extclk.serial); is->audio_clock_serial = -1; if (startup_volume 100) av_log(NULL, AV_LOG_WARNING, \"-volume=%d > 100, setting to 100\\n\", startup_volume); startup_volume = av_clip(startup_volume, 0, 100); startup_volume = av_clip(SDL_MIX_MAXVOLUME * startup_volume / 100, 0, SDL_MIX_MAXVOLUME); is->audio_volume = startup_volume; is->muted = 0; is->av_sync_type = av_sync_type; is->read_tid = SDL_CreateThread(read_thread, \"read_thread\", is); if (!is->read_tid) { av_log(NULL, AV_LOG_FATAL, \"SDL_CreateThread(): %s\\n\", SDL_GetError()); fail: stream_close(is); return NULL; } return is; } static void stream_cycle_channel(VideoState *is, int codec_type) { AVFormatContext *ic = is->ic; int start_index, stream_index; int old_index; AVStream *st; AVProgram *p = NULL; int nb_streams = is->ic->nb_streams; if (codec_type == AVMEDIA_TYPE_VIDEO) { start_index = is->last_video_stream; old_index = is->video_stream; } else if (codec_type == AVMEDIA_TYPE_AUDIO) { start_index = is->last_audio_stream; old_index = is->audio_stream; } else { start_index = is->last_subtitle_stream; old_index = is->subtitle_stream; } stream_index = start_index; if (codec_type != AVMEDIA_TYPE_VIDEO && is->video_stream != -1) { p = av_find_program_from_stream(ic, NULL, is->video_stream); if (p) { nb_streams = p->nb_stream_indexes; for (start_index = 0; start_index stream_index[start_index] == stream_index) break; if (start_index == nb_streams) start_index = -1; stream_index = start_index; } } for (;;) { if (++stream_index >= nb_streams) { if (codec_type == AVMEDIA_TYPE_SUBTITLE) { stream_index = -1; is->last_subtitle_stream = -1; goto the_end; } if (start_index == -1) return; stream_index = 0; } if (stream_index == start_index) return; st = is->ic->streams[p ? p->stream_index[stream_index] : stream_index]; if (st->codecpar->codec_type == codec_type) { /* check that parameters are OK */ switch (codec_type) { case AVMEDIA_TYPE_AUDIO: if (st->codecpar->sample_rate != 0 && st->codecpar->channels != 0) goto the_end; break; case AVMEDIA_TYPE_VIDEO: case AVMEDIA_TYPE_SUBTITLE: goto the_end; default: break; } } } the_end: if (p && stream_index != -1) stream_index = p->stream_index[stream_index]; av_log(NULL, AV_LOG_INFO, \"Switch %s stream from #%d to #%d\\n\", av_get_media_type_string(codec_type), old_index, stream_index); stream_component_close(is, old_index); stream_component_open(is, stream_index); } static void toggle_full_screen(VideoState *is) { is_full_screen = !is_full_screen; SDL_SetWindowFullscreen(window, is_full_screen ? SDL_WINDOW_FULLSCREEN_DESKTOP : 0); } static void toggle_audio_display(VideoState *is) { int next = is->show_mode; do { next = (next + 1) % SHOW_MODE_NB; } while (next != is->show_mode && (next == SHOW_MODE_VIDEO && !is->video_st || next != SHOW_MODE_VIDEO && !is->audio_st)); if (is->show_mode != next) { is->force_refresh = 1; is->show_mode = next; } } static void refresh_loop_wait_event(VideoState *is, SDL_Event *event) { double remaining_time = 0.0; SDL_PumpEvents(); while (!SDL_PeepEvents(event, 1, SDL_GETEVENT, SDL_FIRSTEVENT, SDL_LASTEVENT)) { if (!cursor_hidden && av_gettime_relative() - cursor_last_shown > CURSOR_HIDE_DELAY) { SDL_ShowCursor(0); cursor_hidden = 1; } if (remaining_time > 0.0) av_usleep((int64_t)(remaining_time * 1000000.0)); remaining_time = REFRESH_RATE; if (is->show_mode != SHOW_MODE_NONE && (!is->paused || is->force_refresh)) video_refresh(is, &remaining_time); SDL_PumpEvents(); } } static void seek_chapter(VideoState *is, int incr) { int64_t pos = get_master_clock(is) * AV_TIME_BASE; int i; if (!is->ic->nb_chapters) return; /* find the current chapter */ for (i = 0; i ic->nb_chapters; i++) { AVChapter *ch = is->ic->chapters[i]; if (av_compare_ts(pos, AV_TIME_BASE_Q, ch->start, ch->time_base) = is->ic->nb_chapters) return; av_log(NULL, AV_LOG_VERBOSE, \"Seeking to chapter %d.\\n\", i); stream_seek(is, av_rescale_q(is->ic->chapters[i]->start, is->ic->chapters[i]->time_base, AV_TIME_BASE_Q), 0, 0); } /* handle an event sent by the GUI */ static void event_loop(VideoState *cur_stream) { SDL_Event event; double incr, pos, frac; for (;;) { double x; refresh_loop_wait_event(cur_stream, &event); switch (event.type) { case SDL_KEYDOWN: if (exit_on_keydown || event.key.keysym.sym == SDLK_ESCAPE || event.key.keysym.sym == SDLK_q) { do_exit(cur_stream); break; } // If we don't yet have a window, skip all key events, because read_thread might still be initializing... if (!cur_stream->width) continue; switch (event.key.keysym.sym) { case SDLK_f: toggle_full_screen(cur_stream); cur_stream->force_refresh = 1; break; case SDLK_p: case SDLK_SPACE: toggle_pause(cur_stream); break; case SDLK_m: toggle_mute(cur_stream); break; case SDLK_KP_MULTIPLY: case SDLK_0: update_volume(cur_stream, 1, SDL_VOLUME_STEP); break; case SDLK_KP_DIVIDE: case SDLK_9: update_volume(cur_stream, -1, SDL_VOLUME_STEP); break; case SDLK_s: // S: Step to next frame step_to_next_frame(cur_stream); break; case SDLK_a: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_AUDIO); break; case SDLK_v: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_VIDEO); break; case SDLK_c: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_VIDEO); stream_cycle_channel(cur_stream, AVMEDIA_TYPE_AUDIO); stream_cycle_channel(cur_stream, AVMEDIA_TYPE_SUBTITLE); break; case SDLK_t: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_SUBTITLE); break; case SDLK_w: #if CONFIG_AVFILTER if (cur_stream->show_mode == SHOW_MODE_VIDEO && cur_stream->vfilter_idx vfilter_idx >= nb_vfilters) cur_stream->vfilter_idx = 0; } else { cur_stream->vfilter_idx = 0; toggle_audio_display(cur_stream); } #else toggle_audio_display(cur_stream); #endif break; case SDLK_PAGEUP: if (cur_stream->ic->nb_chapters ic->nb_chapters video_stream >= 0) pos = frame_queue_last_pos(&cur_stream->pictq); if (pos audio_stream >= 0) pos = frame_queue_last_pos(&cur_stream->sampq); if (pos ic->pb); if (cur_stream->ic->bit_rate) incr *= cur_stream->ic->bit_rate / 8.0; else incr *= 180000.0; pos += incr; stream_seek(cur_stream, pos, incr, 1); } else { pos = get_master_clock(cur_stream); if (isnan(pos)) pos = (double)cur_stream->seek_pos / AV_TIME_BASE; pos += incr; if (cur_stream->ic->start_time != AV_NOPTS_VALUE && pos ic->start_time / (double)AV_TIME_BASE) pos = cur_stream->ic->start_time / (double)AV_TIME_BASE; stream_seek(cur_stream, (int64_t)(pos * AV_TIME_BASE), (int64_t)(incr * AV_TIME_BASE), 0); } break; default: break; } break; case SDL_MOUSEBUTTONDOWN: if (exit_on_mousedown) { do_exit(cur_stream); break; } if (event.button.button == SDL_BUTTON_LEFT) { static int64_t last_mouse_left_click = 0; if (av_gettime_relative() - last_mouse_left_click force_refresh = 1; last_mouse_left_click = 0; } else { last_mouse_left_click = av_gettime_relative(); } } case SDL_MOUSEMOTION: if (cursor_hidden) { SDL_ShowCursor(1); cursor_hidden = 0; } cursor_last_shown = av_gettime_relative(); if (event.type == SDL_MOUSEBUTTONDOWN) { if (event.button.button != SDL_BUTTON_RIGHT) break; x = event.button.x; } else { if (!(event.motion.state & SDL_BUTTON_RMASK)) break; x = event.motion.x; } if (seek_by_bytes || cur_stream->ic->duration ic->pb); stream_seek(cur_stream, size*x/cur_stream->width, 0, 1); } else { int64_t ts; int ns, hh, mm, ss; int tns, thh, tmm, tss; tns = cur_stream->ic->duration / 1000000LL; thh = tns / 3600; tmm = (tns % 3600) / 60; tss = (tns % 60); frac = x / cur_stream->width; ns = frac * tns; hh = ns / 3600; mm = (ns % 3600) / 60; ss = (ns % 60); av_log(NULL, AV_LOG_INFO, \"Seek to %2.0f%% (%2d:%02d:%02d) of total duration (%2d:%02d:%02d) \\n\", frac*100, hh, mm, ss, thh, tmm, tss); ts = frac * cur_stream->ic->duration; if (cur_stream->ic->start_time != AV_NOPTS_VALUE) ts += cur_stream->ic->start_time; stream_seek(cur_stream, ts, 0, 0); } break; case SDL_WINDOWEVENT: switch (event.window.event) { case SDL_WINDOWEVENT_SIZE_CHANGED: screen_width = cur_stream->width = event.window.data1; screen_height = cur_stream->height = event.window.data2; if (cur_stream->vis_texture) { SDL_DestroyTexture(cur_stream->vis_texture); cur_stream->vis_texture = NULL; } case SDL_WINDOWEVENT_EXPOSED: cur_stream->force_refresh = 1; } break; case SDL_QUIT: case FF_QUIT_EVENT: do_exit(cur_stream); break; default: break; } } } static int opt_frame_size(void *optctx, const char *opt, const char *arg) { av_log(NULL, AV_LOG_WARNING, \"Option -s is deprecated, use -video_size.\\n\"); return opt_default(NULL, \"video_size\", arg); } static int opt_width(void *optctx, const char *opt, const char *arg) { screen_width = parse_number_or_die(opt, arg, OPT_INT64, 1, INT_MAX); return 0; } static int opt_height(void *optctx, const char *opt, const char *arg) { screen_height = parse_number_or_die(opt, arg, OPT_INT64, 1, INT_MAX); return 0; } static int opt_format(void *optctx, const char *opt, const char *arg) { file_iformat = av_find_input_format(arg); if (!file_iformat) { av_log(NULL, AV_LOG_FATAL, \"Unknown input format: %s\\n\", arg); return AVERROR(EINVAL); } return 0; } static int opt_frame_pix_fmt(void *optctx, const char *opt, const char *arg) { av_log(NULL, AV_LOG_WARNING, \"Option -pix_fmt is deprecated, use -pixel_format.\\n\"); return opt_default(NULL, \"pixel_format\", arg); } static int opt_sync(void *optctx, const char *opt, const char *arg) { if (!strcmp(arg, \"audio\")) av_sync_type = AV_SYNC_AUDIO_MASTER; else if (!strcmp(arg, \"video\")) av_sync_type = AV_SYNC_VIDEO_MASTER; else if (!strcmp(arg, \"ext\")) av_sync_type = AV_SYNC_EXTERNAL_CLOCK; else { av_log(NULL, AV_LOG_ERROR, \"Unknown value for %s: %s\\n\", opt, arg); exit(1); } return 0; } static int opt_seek(void *optctx, const char *opt, const char *arg) { start_time = parse_time_or_die(opt, arg, 1); return 0; } static int opt_duration(void *optctx, const char *opt, const char *arg) { duration = parse_time_or_die(opt, arg, 1); return 0; } static int opt_show_mode(void *optctx, const char *opt, const char *arg) { show_mode = !strcmp(arg, \"video\") ? SHOW_MODE_VIDEO : !strcmp(arg, \"waves\") ? SHOW_MODE_WAVES : !strcmp(arg, \"rdft\" ) ? SHOW_MODE_RDFT : parse_number_or_die(opt, arg, OPT_INT, 0, SHOW_MODE_NB-1); return 0; } static void opt_input_file(void *optctx, const char *filename) { if (input_filename) { av_log(NULL, AV_LOG_FATAL, \"Argument '%s' provided as input filename, but '%s' was already specified.\\n\", filename, input_filename); exit(1); } if (!strcmp(filename, \"-\")) filename = \"pipe:\"; input_filename = filename; } static int opt_codec(void *optctx, const char *opt, const char *arg) { const char *spec = strchr(opt, ':'); if (!spec) { av_log(NULL, AV_LOG_ERROR, \"No media specifier was specified in '%s' in option '%s'\\n\", arg, opt); return AVERROR(EINVAL); } spec++; switch (spec[0]) { case 'a' : audio_codec_name = arg; break; case 's' : subtitle_codec_name = arg; break; case 'v' : video_codec_name = arg; break; default: av_log(NULL, AV_LOG_ERROR, \"Invalid media specifier '%s' in option '%s'\\n\", spec, opt); return AVERROR(EINVAL); } return 0; } static int dummy; static const OptionDef options[] = { CMDUTILS_COMMON_OPTIONS { \"x\", HAS_ARG, { .func_arg = opt_width }, \"force displayed width\", \"width\" }, { \"y\", HAS_ARG, { .func_arg = opt_height }, \"force displayed height\", \"height\" }, { \"s\", HAS_ARG | OPT_VIDEO, { .func_arg = opt_frame_size }, \"set frame size (WxH or abbreviation)\", \"size\" }, { \"fs\", OPT_BOOL, { &is_full_screen }, \"force full screen\" }, { \"an\", OPT_BOOL, { &audio_disable }, \"disable audio\" }, { \"vn\", OPT_BOOL, { &video_disable }, \"disable video\" }, { \"sn\", OPT_BOOL, { &subtitle_disable }, \"disable subtitling\" }, { \"ast\", OPT_STRING | HAS_ARG | OPT_EXPERT, { &wanted_stream_spec[AVMEDIA_TYPE_AUDIO] }, \"select desired audio stream\", \"stream_specifier\" }, { \"vst\", OPT_STRING | HAS_ARG | OPT_EXPERT, { &wanted_stream_spec[AVMEDIA_TYPE_VIDEO] }, \"select desired video stream\", \"stream_specifier\" }, { \"sst\", OPT_STRING | HAS_ARG | OPT_EXPERT, { &wanted_stream_spec[AVMEDIA_TYPE_SUBTITLE] }, \"select desired subtitle stream\", \"stream_specifier\" }, { \"ss\", HAS_ARG, { .func_arg = opt_seek }, \"seek to a given position in seconds\", \"pos\" }, { \"t\", HAS_ARG, { .func_arg = opt_duration }, \"play \\\"duration\\\" seconds of audio/video\", \"duration\" }, { \"bytes\", OPT_INT | HAS_ARG, { &seek_by_bytes }, \"seek by bytes 0=off 1=on -1=auto\", \"val\" }, { \"seek_interval\", OPT_FLOAT | HAS_ARG, { &seek_interval }, \"set seek interval for left/right keys, in seconds\", \"seconds\" }, { \"nodisp\", OPT_BOOL, { &display_disable }, \"disable graphical display\" }, { \"noborder\", OPT_BOOL, { &borderless }, \"borderless window\" }, { \"alwaysontop\", OPT_BOOL, { &alwaysontop }, \"window always on top\" }, { \"volume\", OPT_INT | HAS_ARG, { &startup_volume}, \"set startup volume 0=min 100=max\", \"volume\" }, { \"f\", HAS_ARG, { .func_arg = opt_format }, \"force format\", \"fmt\" }, { \"pix_fmt\", HAS_ARG | OPT_EXPERT | OPT_VIDEO, { .func_arg = opt_frame_pix_fmt }, \"set pixel format\", \"format\" }, { \"stats\", OPT_BOOL | OPT_EXPERT, { &show_status }, \"show status\", \"\" }, { \"fast\", OPT_BOOL | OPT_EXPERT, { &fast }, \"non spec compliant optimizations\", \"\" }, { \"genpts\", OPT_BOOL | OPT_EXPERT, { &genpts }, \"generate pts\", \"\" }, { \"drp\", OPT_INT | HAS_ARG | OPT_EXPERT, { &decoder_reorder_pts }, \"let decoder reorder pts 0=off 1=on -1=auto\", \"\"}, { \"lowres\", OPT_INT | HAS_ARG | OPT_EXPERT, { &lowres }, \"\", \"\" }, { \"sync\", HAS_ARG | OPT_EXPERT, { .func_arg = opt_sync }, \"set audio-video sync. type (type=audio/video/ext)\", \"type\" }, { \"autoexit\", OPT_BOOL | OPT_EXPERT, { &autoexit }, \"exit at the end\", \"\" }, { \"exitonkeydown\", OPT_BOOL | OPT_EXPERT, { &exit_on_keydown }, \"exit on key down\", \"\" }, { \"exitonmousedown\", OPT_BOOL | OPT_EXPERT, { &exit_on_mousedown }, \"exit on mouse down\", \"\" }, { \"loop\", OPT_INT | HAS_ARG | OPT_EXPERT, { &loop }, \"set number of times the playback shall be looped\", \"loop count\" }, { \"framedrop\", OPT_BOOL | OPT_EXPERT, { &framedrop }, \"drop frames when cpu is too slow\", \"\" }, { \"infbuf\", OPT_BOOL | OPT_EXPERT, { &infinite_buffer }, \"don't limit the input buffer size (useful with realtime streams)\", \"\" }, { \"window_title\", OPT_STRING | HAS_ARG, { &window_title }, \"set window title\", \"window title\" }, { \"left\", OPT_INT | HAS_ARG | OPT_EXPERT, { &screen_left }, \"set the x position for the left of the window\", \"x pos\" }, { \"top\", OPT_INT | HAS_ARG | OPT_EXPERT, { &screen_top }, \"set the y position for the top of the window\", \"y pos\" }, #if CONFIG_AVFILTER { \"vf\", OPT_EXPERT | HAS_ARG, { .func_arg = opt_add_vfilter }, \"set video filters\", \"filter_graph\" }, { \"af\", OPT_STRING | HAS_ARG, { &afilters }, \"set audio filters\", \"filter_graph\" }, #endif { \"rdftspeed\", OPT_INT | HAS_ARG| OPT_AUDIO | OPT_EXPERT, { &rdftspeed }, \"rdft speed\", \"msecs\" }, { \"showmode\", HAS_ARG, { .func_arg = opt_show_mode}, \"select show mode (0 = video, 1 = waves, 2 = RDFT)\", \"mode\" }, { \"default\", HAS_ARG | OPT_AUDIO | OPT_VIDEO | OPT_EXPERT, { .func_arg = opt_default }, \"generic catch all option\", \"\" }, { \"i\", OPT_BOOL, { &dummy}, \"read specified file\", \"input_file\"}, { \"codec\", HAS_ARG, { .func_arg = opt_codec}, \"force decoder\", \"decoder_name\" }, { \"acodec\", HAS_ARG | OPT_STRING | OPT_EXPERT, { &audio_codec_name }, \"force audio decoder\", \"decoder_name\" }, { \"scodec\", HAS_ARG | OPT_STRING | OPT_EXPERT, { &subtitle_codec_name }, \"force subtitle decoder\", \"decoder_name\" }, { \"vcodec\", HAS_ARG | OPT_STRING | OPT_EXPERT, { &video_codec_name }, \"force video decoder\", \"decoder_name\" }, { \"autorotate\", OPT_BOOL, { &autorotate }, \"automatically rotate video\", \"\" }, { \"find_stream_info\", OPT_BOOL | OPT_INPUT | OPT_EXPERT, { &find_stream_info }, \"read and decode the streams to fill missing information with heuristics\" }, { \"filter_threads\", HAS_ARG | OPT_INT | OPT_EXPERT, { &filter_nbthreads }, \"number of filter threads per graph\" }, { NULL, }, }; static void show_usage(void) { av_log(NULL, AV_LOG_INFO, \"Simple media player\\n\"); av_log(NULL, AV_LOG_INFO, \"usage: %s [options] input_file\\n\", program_name); av_log(NULL, AV_LOG_INFO, \"\\n\"); } void show_help_default(const char *opt, const char *arg) { av_log_set_callback(log_callback_help); show_usage(); show_help_options(options, \"Main options:\", 0, OPT_EXPERT, 0); show_help_options(options, \"Advanced options:\", OPT_EXPERT, 0, 0); printf(\"\\n\"); show_help_children(avcodec_get_class(), AV_OPT_FLAG_DECODING_PARAM); show_help_children(avformat_get_class(), AV_OPT_FLAG_DECODING_PARAM); #if !CONFIG_AVFILTER show_help_children(sws_get_class(), AV_OPT_FLAG_ENCODING_PARAM); #else show_help_children(avfilter_get_class(), AV_OPT_FLAG_FILTERING_PARAM); #endif printf(\"\\nWhile playing:\\n\" \"q, ESC quit\\n\" \"f toggle full screen\\n\" \"p, SPC pause\\n\" \"m toggle mute\\n\" \"9, 0 decrease and increase volume respectively\\n\" \"/, * decrease and increase volume respectively\\n\" \"a cycle audio channel in the current program\\n\" \"v cycle video channel\\n\" \"t cycle subtitle channel in the current program\\n\" \"c cycle program\\n\" \"w cycle video filters or show modes\\n\" \"s activate frame-step mode\\n\" \"left/right seek backward/forward 10 seconds or to custom interval if -seek_interval is set\\n\" \"down/up seek backward/forward 1 minute\\n\" \"page down/page up seek backward/forward 10 minutes\\n\" \"right mouse click seek to percentage in file corresponding to fraction of width\\n\" \"left double-click toggle full screen\\n\" ); } /* Called from the main */ int main(int argc, char **argv) { int flags; VideoState *is; init_dynload(); av_log_set_flags(AV_LOG_SKIP_REPEATED); parse_loglevel(argc, argv, options); /* register all codecs, demux and protocols */ #if CONFIG_AVDEVICE avdevice_register_all(); #endif avformat_network_init(); init_opts(); signal(SIGINT , sigterm_handler); /* Interrupt (ANSI). */ signal(SIGTERM, sigterm_handler); /* Termination (ANSI). */ show_banner(argc, argv, options); parse_options(NULL, argc, argv, options, opt_input_file); if (!input_filename) { show_usage(); av_log(NULL, AV_LOG_FATAL, \"An input file must be specified\\n\"); av_log(NULL, AV_LOG_FATAL, \"Use -h to get full help or, even better, run 'man %s'\\n\", program_name); exit(1); } if (display_disable) { video_disable = 1; } flags = SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER; if (audio_disable) flags &= ~SDL_INIT_AUDIO; else { /* Try to work around an occasional ALSA buffer underflow issue when the * period size is NPOT due to ALSA resampling by forcing the buffer size. */ if (!SDL_getenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\")) SDL_setenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\",\"1\", 1); } if (display_disable) flags &= ~SDL_INIT_VIDEO; if (SDL_Init (flags)) { av_log(NULL, AV_LOG_FATAL, \"Could not initialize SDL - %s\\n\", SDL_GetError()); av_log(NULL, AV_LOG_FATAL, \"(Did you set the DISPLAY variable?)\\n\"); exit(1); } SDL_EventState(SDL_SYSWMEVENT, SDL_IGNORE); SDL_EventState(SDL_USEREVENT, SDL_IGNORE); av_init_packet(&flush_pkt); flush_pkt.data = (uint8_t *)&flush_pkt; if (!display_disable) { int flags = SDL_WINDOW_HIDDEN; if (alwaysontop) #if SDL_VERSION_ATLEAST(2,0,5) flags |= SDL_WINDOW_ALWAYS_ON_TOP; #else av_log(NULL, AV_LOG_WARNING, \"Your SDL version doesn't support SDL_WINDOW_ALWAYS_ON_TOP. Feature will be inactive.\\n\"); #endif if (borderless) flags |= SDL_WINDOW_BORDERLESS; else flags |= SDL_WINDOW_RESIZABLE; window = SDL_CreateWindow(program_name, SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED, default_width, default_height, flags); SDL_SetHint(SDL_HINT_RENDER_SCALE_QUALITY, \"linear\"); if (window) { renderer = SDL_CreateRenderer(window, -1, SDL_RENDERER_ACCELERATED | SDL_RENDERER_PRESENTVSYNC); if (!renderer) { av_log(NULL, AV_LOG_WARNING, \"Failed to initialize a hardware accelerated renderer: %s\\n\", SDL_GetError()); renderer = SDL_CreateRenderer(window, -1, 0); } if (renderer) { if (!SDL_GetRendererInfo(renderer, &renderer_info)) av_log(NULL, AV_LOG_VERBOSE, \"Initialized %s renderer.\\n\", renderer_info.name); } } if (!window || !renderer || !renderer_info.num_texture_formats) { av_log(NULL, AV_LOG_FATAL, \"Failed to create window or renderer: %s\", SDL_GetError()); do_exit(NULL); } } is = stream_open(input_filename, file_iformat); if (!is) { av_log(NULL, AV_LOG_FATAL, \"Failed to initialize VideoState!\\n\"); do_exit(NULL); } event_loop(is); /* never returns */ return 0; } "},"pages/ffmpeg/ffplay_source_code_main_function_do_exit_function.html":{"url":"pages/ffmpeg/ffplay_source_code_main_function_do_exit_function.html","title":"ffplay源码-main函数，do_exit函数","keywords":"","body":"ffplay源码-main函数，do_exit函数 iOS工程代码 源代码一览 #import #include \"libavutil/avstring.h\" #include \"libavutil/eval.h\" #include \"libavutil/mathematics.h\" #include \"libavutil/pixdesc.h\" #include \"libavutil/imgutils.h\" #include \"libavutil/dict.h\" #include \"libavutil/parseutils.h\" #include \"libavutil/samplefmt.h\" #include \"libavutil/avassert.h\" #include \"libavutil/time.h\" #include \"libavformat/avformat.h\" #include \"libavdevice/avdevice.h\" #include \"libswscale/swscale.h\" #include \"libavutil/opt.h\" #include \"libavcodec/avfft.h\" #include \"libswresample/swresample.h\" #include #include const char program_name[] = \"ffplay\"; /* options specified by the user */ static AVInputFormat *file_iformat; static const char *input_filename; static int default_width = 640; static int default_height = 480; /* current context */ static AVPacket flush_pkt; static SDL_Window *window; static SDL_Renderer *renderer; static SDL_RendererInfo renderer_info = {0}; typedef struct VideoState { } VideoState; static void stream_close(VideoState *is) { // TODO: stream_close } static void do_exit(VideoState *is) { if (is) { stream_close(is); } if (renderer) SDL_DestroyRenderer(renderer); if (window) SDL_DestroyWindow(window); // uninit_opts(); //#if CONFIG_AVFILTER // av_freep(&vfilters_list); //#endif // avformat_network_deinit(); // if (show_status) // printf(\"\\n\"); SDL_Quit(); av_log(NULL, AV_LOG_QUIET, \"%s\", \"\"); exit(0); } static VideoState *stream_open(const char *filename, AVInputFormat *iformat) { // TODO: stream_open return NULL; } /* handle an event sent by the GUI */ static void event_loop(VideoState *cur_stream) { // TODO: event_loop } int main(int argc, char *argv[]) { int flags; VideoState *is; // 动态加载的初始化，这是Windows平台的dll库相关处理； // https://blog.csdn.net/ericbar/article/details/79541420 // init_dynload(); // 设置打印的标记，AV_LOG_SKIP_REPEATED表示对于重复打印的语句，不重复输出； // https://blog.csdn.net/ericbar/article/details/79541420 av_log_set_flags(AV_LOG_SKIP_REPEATED); // 使命令行'-loglevel'生效 // parse_loglevel(argc, argv, options); /* register all codecs, demux and protocols */ //#if CONFIG_AVDEVICE // 在使用libavdevice之前，必须先运行avdevice_register_all()对设备进行注册，否则就会出错 // https://blog.csdn.net/leixiaohua1020/article/details/41211121 avdevice_register_all(); //#endif // 打开网络流的话，前面要加上函数 // avformat_network_init(); // Initialize the cmdutils option system, in particular allocate the *_opts contexts. // 初始化 cmdutils 选项系统，特别是分配 *_opts 上下文。 // init_opts(); // signal(SIGINT , sigterm_handler); /* Interrupt (ANSI). */ // signal(SIGTERM, sigterm_handler); /* Termination (ANSI). */ // 将程序横幅打印到 stderr。 横幅内容取决于当前版本的存储库和程序使用的 libav* 库。 // Print the program banner to stderr. The banner contents depend // on the current version of the repository and of the libav* libraries used by // the program. // show_banner(argc, argv, options); // parse_options(NULL, argc, argv, options, opt_input_file); // input_filename：命令行 -i 指定，视频路径 NSString *inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"mov\"]; input_filename = [inPath UTF8String]; if (!input_filename) { // show_usage(); // av_log(NULL, AV_LOG_FATAL, \"An input file must be specified\\n\"); // av_log(NULL, AV_LOG_FATAL, // \"Use -h to get full help or, even better, run 'man %s'\\n\", program_name); exit(1); } // display_disable：命令行 -nodisp 指定，不渲染画面不播放声音 // if (display_disable) { // video_disable = 1; // } flags = SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER; // audio_disable：命令行 -an 指定，渲染画面不播放声音 // if (audio_disable) // flags &= ~SDL_INIT_AUDIO; // else { // /* Try to work around an occasional ALSA buffer underflow issue when the // * period size is NPOT due to ALSA resampling by forcing the buffer size. */ // if (!SDL_getenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\")) // SDL_setenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\",\"1\", 1); // } // if (display_disable) // flags &= ~SDL_INIT_VIDEO; // 指定flags，SDL初始化 if (SDL_Init (flags)) { av_log(NULL, AV_LOG_FATAL, \"Could not initialize SDL - %s\\n\", SDL_GetError()); av_log(NULL, AV_LOG_FATAL, \"(Did you set the DISPLAY variable?)\\n\"); exit(1); } // 禁用一些事件 SDL_EventState(SDL_SYSWMEVENT, SDL_IGNORE); SDL_EventState(SDL_USEREVENT, SDL_IGNORE); av_init_packet(&flush_pkt); flush_pkt.data = (uint8_t *)&flush_pkt; if (1/**!display_disable*/) { int flags = SDL_WINDOW_HIDDEN; // if (alwaysontop) #if SDL_VERSION_ATLEAST(2,0,5) flags |= SDL_WINDOW_ALWAYS_ON_TOP; #else av_log(NULL, AV_LOG_WARNING, \"Your SDL version doesn't support SDL_WINDOW_ALWAYS_ON_TOP. Feature will be inactive.\\n\"); #endif // borderless：命令行 -noborder 指定，没有边框 // if (borderless) // flags |= SDL_WINDOW_BORDERLESS; // else // 可以自由拉伸 flags |= SDL_WINDOW_RESIZABLE; window = SDL_CreateWindow(program_name, SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED, default_width, default_height, flags); // \"0\" or \"nearest\" - Nearest pixel sampling // \"1\" or \"linear\" - Linear filtering (supported by OpenGL and Direct3D) // \"2\" or \"best\" - Currently this is the same as \"linear\" SDL_SetHint(SDL_HINT_RENDER_SCALE_QUALITY, \"linear\"); if (window) { renderer = SDL_CreateRenderer(window, -1, SDL_RENDERER_ACCELERATED | SDL_RENDERER_PRESENTVSYNC); if (!renderer) { av_log(NULL, AV_LOG_WARNING, \"Failed to initialize a hardware accelerated renderer: %s\\n\", SDL_GetError()); renderer = SDL_CreateRenderer(window, -1, 0); } if (renderer) { if (!SDL_GetRendererInfo(renderer, &renderer_info)) av_log(NULL, AV_LOG_VERBOSE, \"Initialized %s renderer.\\n\", renderer_info.name); } } if (!window || !renderer || !renderer_info.num_texture_formats) { av_log(NULL, AV_LOG_FATAL, \"Failed to create window or renderer: %s\", SDL_GetError()); do_exit(NULL); } } is = stream_open(input_filename, file_iformat); if (!is) { av_log(NULL, AV_LOG_FATAL, \"Failed to initialize VideoState!\\n\"); do_exit(NULL); } event_loop(is); /* never returns */ return 0; } "},"pages/ffmpeg/ffplay_source_code_event_loop_function_refresh_loop_wait_event_function.html":{"url":"pages/ffmpeg/ffplay_source_code_event_loop_function_refresh_loop_wait_event_function.html","title":"ffplay源码-event_loop函数、refresh_loop_wait_event函数","keywords":"","body":"ffplay源码-event_loop函数、refresh_loop_wait_event函数 iOS工程代码 源代码一览 // // main.m // iOSFFmpegSDLFastForwardAndBackward // // Created by 陈长青 on 2022/5/8. // #import #include \"libavutil/avstring.h\" #include \"libavutil/eval.h\" #include \"libavutil/mathematics.h\" #include \"libavutil/pixdesc.h\" #include \"libavutil/imgutils.h\" #include \"libavutil/dict.h\" #include \"libavutil/parseutils.h\" #include \"libavutil/samplefmt.h\" #include \"libavutil/avassert.h\" #include \"libavutil/time.h\" #include \"libavformat/avformat.h\" #include \"libavdevice/avdevice.h\" #include \"libswscale/swscale.h\" #include \"libavutil/opt.h\" #include \"libavcodec/avfft.h\" #include \"libswresample/swresample.h\" #include #include const char program_name[] = \"ffplay\"; /* Step size for volume control in dB */ #define SDL_VOLUME_STEP (0.75) /* polls for possible required screen refresh at least this often, should be less than 1/fps */ #define REFRESH_RATE 0.01 #define CURSOR_HIDE_DELAY 1000000 /* options specified by the user */ static AVInputFormat *file_iformat; static const char *input_filename; static int default_width = 640; static int default_height = 480; static int screen_width = 0; static int screen_height = 0; static int cursor_hidden = 0; static int64_t cursor_last_shown; /* current context */ // 命令行 -fs 指定，控制是否全屏显示 static int is_full_screen; static AVPacket flush_pkt; #define FF_QUIT_EVENT (SDL_USEREVENT + 2) static SDL_Window *window; static SDL_Renderer *renderer; static SDL_RendererInfo renderer_info = {0}; // MARK: 视频状态 typedef struct VideoState { int force_refresh; int paused; int seek_req; int seek_flags; int64_t seek_pos; int64_t seek_rel; AVFormatContext *ic; int muted; int width, height, xleft, ytop; int step; // 命令行 -showmode 指定 enum ShowMode { SHOW_MODE_NONE = -1, SHOW_MODE_VIDEO = 0, SHOW_MODE_WAVES, SHOW_MODE_RDFT, SHOW_MODE_NB } show_mode; SDL_Texture *vis_texture; SDL_cond *continue_read_thread; } VideoState; // MARK: 关闭码流 static void stream_close(VideoState *is) { // TODO: stream_close } // MARK: 打开码流 static VideoState *stream_open(const char *filename, AVInputFormat *iformat) { // TODO: stream_open VideoState *is; is = av_mallocz(sizeof(VideoState)); if (!is) return NULL; return is; } // MARK: 切换码流 static void stream_cycle_channel(VideoState *is, int codec_type) { // TODO: stream_cycle_channel } // MARK: 退出 static void do_exit(VideoState *is) { if (is) { stream_close(is); } if (renderer) SDL_DestroyRenderer(renderer); if (window) SDL_DestroyWindow(window); // uninit_opts(); //#if CONFIG_AVFILTER // av_freep(&vfilters_list); //#endif // avformat_network_deinit(); // if (show_status) // printf(\"\\n\"); SDL_Quit(); av_log(NULL, AV_LOG_QUIET, \"%s\", \"\"); exit(0); } // MARK: 刷新视频 /* called to display each frame */ static void video_refresh(void *opaque, double *remaining_time) { // TODO: video_refresh // av_log(NULL, AV_LOG_INFO, \"player，刷新视频\\n\"); } static void toggle_full_screen(VideoState *is) { is_full_screen = !is_full_screen; SDL_SetWindowFullscreen(window, is_full_screen ? SDL_WINDOW_FULLSCREEN_DESKTOP : 0); } // MARK: 主时钟 /* get the current master clock value */ static double get_master_clock(VideoState *is) { // TODO: get_master_clock return 0; } // MARK: Seek /* seek in the stream */ static void stream_seek(VideoState *is, int64_t pos, int64_t rel, int seek_by_bytes) { if (!is->seek_req) { is->seek_pos = pos; is->seek_rel = rel; is->seek_flags &= ~AVSEEK_FLAG_BYTE; if (seek_by_bytes) is->seek_flags |= AVSEEK_FLAG_BYTE; is->seek_req = 1; SDL_CondSignal(is->continue_read_thread); } } // MARK: 暂停2 /* pause or resume the video */ static void stream_toggle_pause(VideoState *is) { // TODO: stream_toggle_pause } // MARK: 暂停 static void toggle_pause(VideoState *is) { stream_toggle_pause(is); is->step = 0; } // MARK: 禁音 static void toggle_mute(VideoState *is) { is->muted = !is->muted; } // MARK: 调声音 static void update_volume(VideoState *is, int sign, double step) { // TODO: update_volume } // MARK: 进入下一帧 static void step_to_next_frame(VideoState *is) { /* if the stream is paused unpause it, then step */ if (is->paused) stream_toggle_pause(is); is->step = 1; } // MARK: SDL事件 /** * SDL 事件 * * 循环检测并优先处理用户输入事件 * 内置刷新率控制，约10ms刷新一次 * https://blog.csdn.net/qq_36783046/article/details/88706162 */ static void refresh_loop_wait_event(VideoState *is, SDL_Event *event) { double remaining_time = 0.0; /* 从输入设备收集事件并放到事件队列中 */ SDL_PumpEvents(); /** * SDL_PeepEvents * 从事件队列中提取事件，由于这里使用的是SDL_GETEVENT, 所以获取事件时会从队列中移除 * 如果有事件发生，返回事件数量，则while循环不执行。 * 如果出错，返回负数的错误码，则while循环不执行。 * 如果当前没有事件发生，且没有出错，返回0，进入while循环。 */ while (!SDL_PeepEvents(event, 1, SDL_GETEVENT, SDL_FIRSTEVENT, SDL_LASTEVENT)) { /* 隐藏鼠标指针， CURSOR_HIDE_DELAY = 1s */ if (!cursor_hidden && av_gettime_relative() - cursor_last_shown > CURSOR_HIDE_DELAY) { SDL_ShowCursor(0); cursor_hidden = 1; } /* 默认屏幕刷新率控制，REFRESH_RATE = 10ms */ if (remaining_time > 0.0) av_usleep((int64_t)(remaining_time * 1000000.0)); remaining_time = REFRESH_RATE; /* 显示视频 */ if (is->show_mode != SHOW_MODE_NONE && (!is->paused || is->force_refresh)) video_refresh(is, &remaining_time); /* 再次检测输入事件 */ SDL_PumpEvents(); } } // MARK: 事件循环 /* handle an event sent by the GUI */ static void event_loop(VideoState *cur_stream) { SDL_Event event; double incr, pos, frac; for (;;) { double x; refresh_loop_wait_event(cur_stream, &event); switch (event.type) { // 按键按下事件 case SDL_KEYDOWN: // 按esc,q退出 if (1/**exit_on_keydown*/ || event.key.keysym.sym == SDLK_ESCAPE || event.key.keysym.sym == SDLK_q) { do_exit(cur_stream); break; } // If we don't yet have a window, skip all key events, because read_thread might still be initializing... if (!cur_stream->width) continue; switch (event.key.keysym.sym) { // 按F键，全屏 case SDLK_f: toggle_full_screen(cur_stream); // 调用video_refresh()刷新视频 cur_stream->force_refresh = 1; break; // 按P、SPACE键，暂停 case SDLK_p: case SDLK_SPACE: toggle_pause(cur_stream); break; // 按M键，静音 case SDLK_m: toggle_mute(cur_stream); break; // 按+、0键，增加音量 // https://blog.csdn.net/huzhifei/article/details/112682390 case SDLK_KP_MULTIPLY: case SDLK_0: update_volume(cur_stream, 1, SDL_VOLUME_STEP); break; // 按-、9键，减小音量 case SDLK_KP_DIVIDE: case SDLK_9: update_volume(cur_stream, -1, SDL_VOLUME_STEP); break; // 按S键，下一帧 case SDLK_s: // S: Step to next frame step_to_next_frame(cur_stream); break; // 按A键，切换音频流 case SDLK_a: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_AUDIO); break; // 按V键，切换视频流 case SDLK_v: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_VIDEO); break; // 按C键，循环切换节目（切换音频、视频、字幕流） case SDLK_c: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_VIDEO); stream_cycle_channel(cur_stream, AVMEDIA_TYPE_AUDIO); stream_cycle_channel(cur_stream, AVMEDIA_TYPE_SUBTITLE); break; // 按T键，切换字幕流 case SDLK_t: stream_cycle_channel(cur_stream, AVMEDIA_TYPE_SUBTITLE); break; // 按W键，循环切换过滤器或显示模式 case SDLK_w: //#if CONFIG_AVFILTER // if (cur_stream->show_mode == SHOW_MODE_VIDEO && cur_stream->vfilter_idx vfilter_idx >= nb_vfilters) // cur_stream->vfilter_idx = 0; // } else { // cur_stream->vfilter_idx = 0; // toggle_audio_display(cur_stream); // } //#else // toggle_audio_display(cur_stream); //#endif break; // mac上好像没找到这个键 case SDLK_PAGEUP: // 如果只有一个视频则向前10分钟 // if (cur_stream->ic->nb_chapters ic->nb_chapters video_stream >= 0) // pos = frame_queue_last_pos(&cur_stream->pictq); // if (pos audio_stream >= 0) // pos = frame_queue_last_pos(&cur_stream->sampq); // if (pos ic->pb); // if (cur_stream->ic->bit_rate) // incr *= cur_stream->ic->bit_rate / 8.0; // else // incr *= 180000.0; // pos += incr; // stream_seek(cur_stream, pos, incr, 1); // } else { pos = get_master_clock(cur_stream); if (isnan(pos)) pos = (double)cur_stream->seek_pos / AV_TIME_BASE; pos += incr; if (cur_stream->ic->start_time != AV_NOPTS_VALUE && pos ic->start_time / (double)AV_TIME_BASE) pos = cur_stream->ic->start_time / (double)AV_TIME_BASE; stream_seek(cur_stream, (int64_t)(pos * AV_TIME_BASE), (int64_t)(incr * AV_TIME_BASE), 0); // } break; default: break; } break; case SDL_MOUSEBUTTONDOWN: // exit_on_mousedown：命令行 -exitonmousedown 指定，鼠标单击左键退出 // if (exit_on_mousedown) { // do_exit(cur_stream); // break; // } // 双击左键全屏 if (event.button.button == SDL_BUTTON_LEFT) { static int64_t last_mouse_left_click = 0; if (av_gettime_relative() - last_mouse_left_click force_refresh = 1; last_mouse_left_click = 0; } else { last_mouse_left_click = av_gettime_relative(); } } // 鼠标移动事件，执行Seek（暂时不会用） case SDL_MOUSEMOTION: // if (cursor_hidden) { // SDL_ShowCursor(1); // cursor_hidden = 0; // } // cursor_last_shown = av_gettime_relative(); // if (event.type == SDL_MOUSEBUTTONDOWN) { // if (event.button.button != SDL_BUTTON_RIGHT) // break; // x = event.button.x; // } else { // if (!(event.motion.state & SDL_BUTTON_RMASK)) // break; // x = event.motion.x; // } // if (seek_by_bytes || cur_stream->ic->duration ic->pb); // stream_seek(cur_stream, size*x/cur_stream->width, 0, 1); // } else { // int64_t ts; // int ns, hh, mm, ss; // int tns, thh, tmm, tss; // tns = cur_stream->ic->duration / 1000000LL; // thh = tns / 3600; // tmm = (tns % 3600) / 60; // tss = (tns % 60); // frac = x / cur_stream->width; // ns = frac * tns; // hh = ns / 3600; // mm = (ns % 3600) / 60; // ss = (ns % 60); // av_log(NULL, AV_LOG_INFO, // \"Seek to %2.0f%% (%2d:%02d:%02d) of total duration (%2d:%02d:%02d) \\n\", frac*100, // hh, mm, ss, thh, tmm, tss); // ts = frac * cur_stream->ic->duration; // if (cur_stream->ic->start_time != AV_NOPTS_VALUE) // ts += cur_stream->ic->start_time; // stream_seek(cur_stream, ts, 0, 0); // } break; case SDL_WINDOWEVENT: switch (event.window.event) { case SDL_WINDOWEVENT_SIZE_CHANGED: screen_width = cur_stream->width = event.window.data1; screen_height = cur_stream->height = event.window.data2; if (cur_stream->vis_texture) { SDL_DestroyTexture(cur_stream->vis_texture); cur_stream->vis_texture = NULL; } case SDL_WINDOWEVENT_EXPOSED: cur_stream->force_refresh = 1; } break; case SDL_QUIT: case FF_QUIT_EVENT: do_exit(cur_stream); break; default: break; } } } // MARK: 入口函数 int main(int argc, char *argv[]) { int flags; VideoState *is; // 动态加载的初始化，这是Windows平台的dll库相关处理； // https://blog.csdn.net/ericbar/article/details/79541420 // init_dynload(); // 设置打印的标记，AV_LOG_SKIP_REPEATED表示对于重复打印的语句，不重复输出； // https://blog.csdn.net/ericbar/article/details/79541420 av_log_set_flags(AV_LOG_SKIP_REPEATED); // 使命令行'-loglevel'生效 // parse_loglevel(argc, argv, options); /* register all codecs, demux and protocols */ //#if CONFIG_AVDEVICE // 在使用libavdevice之前，必须先运行avdevice_register_all()对设备进行注册，否则就会出错 // https://blog.csdn.net/leixiaohua1020/article/details/41211121 avdevice_register_all(); //#endif // 打开网络流的话，前面要加上函数 // avformat_network_init(); // Initialize the cmdutils option system, in particular allocate the *_opts contexts. // 初始化 cmdutils 选项系统，特别是分配 *_opts 上下文。 // init_opts(); // signal(SIGINT , sigterm_handler); /* Interrupt (ANSI). */ // signal(SIGTERM, sigterm_handler); /* Termination (ANSI). */ // 将程序横幅打印到 stderr。 横幅内容取决于当前版本的存储库和程序使用的 libav* 库。 // Print the program banner to stderr. The banner contents depend // on the current version of the repository and of the libav* libraries used by // the program. // show_banner(argc, argv, options); // parse_options(NULL, argc, argv, options, opt_input_file); // input_filename：命令行 -i 指定，视频路径 NSString *inPath = [[NSBundle mainBundle] pathForResource:@\"test\" ofType:@\"mov\"]; input_filename = [inPath UTF8String]; if (!input_filename) { // show_usage(); // av_log(NULL, AV_LOG_FATAL, \"An input file must be specified\\n\"); // av_log(NULL, AV_LOG_FATAL, // \"Use -h to get full help or, even better, run 'man %s'\\n\", program_name); exit(1); } // display_disable：命令行 -nodisp 指定，不渲染画面不播放声音 // if (display_disable) { // video_disable = 1; // } flags = SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER; // audio_disable：命令行 -an 指定，渲染画面不播放声音 // if (audio_disable) // flags &= ~SDL_INIT_AUDIO; // else { // /* Try to work around an occasional ALSA buffer underflow issue when the // * period size is NPOT due to ALSA resampling by forcing the buffer size. */ // if (!SDL_getenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\")) // SDL_setenv(\"SDL_AUDIO_ALSA_SET_BUFFER_SIZE\",\"1\", 1); // } // if (display_disable) // flags &= ~SDL_INIT_VIDEO; // 指定flags，SDL初始化 if (SDL_Init (flags)) { av_log(NULL, AV_LOG_FATAL, \"Could not initialize SDL - %s\\n\", SDL_GetError()); av_log(NULL, AV_LOG_FATAL, \"(Did you set the DISPLAY variable?)\\n\"); exit(1); } // 禁用一些事件 SDL_EventState(SDL_SYSWMEVENT, SDL_IGNORE); SDL_EventState(SDL_USEREVENT, SDL_IGNORE); av_init_packet(&flush_pkt); flush_pkt.data = (uint8_t *)&flush_pkt; if (1/**!display_disable*/) { int flags = SDL_WINDOW_HIDDEN; // if (alwaysontop) #if SDL_VERSION_ATLEAST(2,0,5) flags |= SDL_WINDOW_ALWAYS_ON_TOP; #else av_log(NULL, AV_LOG_WARNING, \"Your SDL version doesn't support SDL_WINDOW_ALWAYS_ON_TOP. Feature will be inactive.\\n\"); #endif // borderless：命令行 -noborder 指定，没有边框 // if (borderless) // flags |= SDL_WINDOW_BORDERLESS; // else // 可以自由拉伸 flags |= SDL_WINDOW_RESIZABLE; window = SDL_CreateWindow(program_name, SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED, default_width, default_height, flags); // \"0\" or \"nearest\" - Nearest pixel sampling // \"1\" or \"linear\" - Linear filtering (supported by OpenGL and Direct3D) // \"2\" or \"best\" - Currently this is the same as \"linear\" SDL_SetHint(SDL_HINT_RENDER_SCALE_QUALITY, \"linear\"); if (window) { renderer = SDL_CreateRenderer(window, -1, SDL_RENDERER_ACCELERATED | SDL_RENDERER_PRESENTVSYNC); if (!renderer) { av_log(NULL, AV_LOG_WARNING, \"Failed to initialize a hardware accelerated renderer: %s\\n\", SDL_GetError()); renderer = SDL_CreateRenderer(window, -1, 0); } if (renderer) { if (!SDL_GetRendererInfo(renderer, &renderer_info)) av_log(NULL, AV_LOG_VERBOSE, \"Initialized %s renderer.\\n\", renderer_info.name); } } if (!window || !renderer || !renderer_info.num_texture_formats) { av_log(NULL, AV_LOG_FATAL, \"Failed to create window or renderer: %s\", SDL_GetError()); do_exit(NULL); } } is = stream_open(input_filename, file_iformat); if (!is) { av_log(NULL, AV_LOG_FATAL, \"Failed to initialize VideoState!\\n\"); do_exit(NULL); } event_loop(is); /* never returns */ return 0; } "},"pages/opengl/01_OpenGL_rendering_structure_and_commonly_used_primitives.html":{"url":"pages/opengl/01_OpenGL_rendering_structure_and_commonly_used_primitives.html","title":"1.OpenGL渲染结构与常用图元","keywords":"","body":"1.OpenGL渲染结构与常用图元 学习内容 掌握OpenGL渲染基础架构 如何使用7种OpenGL几何图元 如何使⽤存储着⾊器 如何使用Uniform属性 如何使用GLBatch帮助类创建⼏何图形 一、OpenGL 与 着色器 在OpenGL 3.0之前，OpenGL 包含一个固定功能的管线，它可以在不使用 着⾊器的情况下处理理⼏何与像素数据。在3.1版本开始，固定管线从核心 模式去掉。因此现在需要使用着色器来完成⼯作。 使⽤用OpenGL 来说，我们会使用GLSL,(OpenGL Shading Langruage，它是 在OpenGL 2.0版本发布的)。 语法与“C、C++”类似。 二、基础图形管线 OpenGL 中的图元只不过是顶点的集合以预定义的方式结合一起罢了。 例如:⼀个单独的点就是⼀个图元。它只需要⼀个顶点 2.1 OpenGL 渲染管线简化版本 客户机、服务器 管线分为上下2部分，上部分是客户端，而下半部分则是服务端。 客户端是存储在CPU存储器中的，并且在应用程序中执行，或者在主系 统内存的驱动程序中执⾏。驱动程序会将渲染命令和数组组合起来，发送给服务器器执⾏!(在⼀台典型的个人计算机上，服务器就是实际上就 是图形加速卡上的硬件和内存) 服务器 和 客户机在功能上也是异步的。 它们是各⾃独立的软件块或硬件块。我们是希望它们2个端都尽量在不停的工作。客户端不断的把数据块和命令块组合在一起输送到缓冲区，然后缓冲区就会发送到服务器 执行。 如果服务器停止⼯作等待客户机，或者客户机停止工作来等待服务器做 好接受更多的命令和准备，我们把这种情况成为管线停滞。 着色器 上图的Vertex Shader(顶点着⾊器) 和 Fragment Shader(⽚段着色器)。 着⾊器是使⽤用GLSL编写的程序，看起来与C语⾔非常类似。 着⾊器必 须从源代码中编译和链接在一起。最终准备就绪的着⾊器程序 顶点着⾊器-->处理从客户机输⼊的数据、应⽤变换、进行其他的类型 的数学运算来计算关照效果、位移、颜色值等。(**为了渲染共有3个顶点的三⻆形，顶点着⾊器将执行3次，也就是为了每个顶点执行⼀次)在⽬前的硬件上有多个执行单元同时运行，就意味着所有的3个顶点可以同时进⾏处理! 图上(primitive Assembly 说明的是:3个顶点已经组合在一起，⽽三⻆ 形已经逐个片段的进⾏了光栅化。每个⽚段通过执⾏⽚元着⾊器进行 填充。⽚元着⾊器会输出我们将屏幕上看到的最终颜色值。 重点! 我们必须在这之前为着⾊色器器提供数据，否则什什么都⽆无法实现! 有3种向OpenGL 着⾊色器器传递渲染数据的⽅方法可供我们选择 1.属性 2.uniform 值 3.纹理理 三、属性、uniform值、纹理理、输出 3.1 属性 属性:就是对每⼀个顶点都要作改变的数据元素。实际上，顶点位置本身 就是⼀个属性。属性值可以是浮点数、整数、布尔数据。 属性总是以四维向量量的形式进行内部存储的，即使我们不会使用所有的 4个分量量。⼀个顶点位置可能存储(x,y,z)，将占有4个分量中的3个。 实际上如果是在平⾯情况下:只要在xy平⾯上就能绘制，那么Z分量就 会自动设置为0; 属性还可以是:纹理坐标、颜⾊值、光照计算表⾯法线 在顶点程序(shader渲染)可以代表你想要的任何意义。因为都是你设 定的。 属性会从本地客户机内存中复制存储在图形硬件中的⼀个缓冲区上。这些属性只提供给顶点着⾊器使用，对于⽚元着色器⽊有太大意义。 声明:这些属性对每个顶点都要做改变，但并不意味着它们的值不能重复。通常情况下，它们都是不一样的，但有可能整个数组都是同一值的 情况。 3.2 Uniform值 属性是⼀种对整个批次属性都取统一值的单⼀值。它是不变的。通过设置uniform变量就紧接着发送一个图元批次命令，Uniform变量实际上可以无数次限制地使⽤用，设置一个应用于整个表⾯的单个颜色值，还可以设置⼀ 个时间值。在每次渲染某种类型的顶点动画时修改它。 注意:这⾥的uniform变量每个批次改变一次，⽽不是每个顶点改变一次。 uniform变量最常见的应⽤是在顶点渲染中设置变换矩阵 后⾯的课程会详细讲解 与属性相同点:可以是浮点值、整数、布尔值 与属性不同点:顶点着⾊器和片元着色器都可以使用uniform变量。uniform 变量还可以是标量类型、⽮量类型、uniform矩阵。 3.3 纹理 传递给着⾊器的第三种数据类型:纹理数据 现在就教大家如果处理纹理数据并将其传递给着⾊器的细节还为时过早。 我们先在前⾯的课程中，了解什么叫做纹理! 在顶点着⾊器、⽚段着⾊器中都可以对纹理数据进⾏采样和筛选。 典型的应⽤场景:⽚段着⾊器对⼀个纹理值进行采样，然后在⼀个三⻆形表⾯应⽤渲染纹理数据。 纹理数据，不仅仅表现在图形，很多图形⽂件格式都是以无符号字节(每个颜⾊通道8位)形式对颜⾊分量进⾏存储的。 3.4 输出 在图表中第四种数据类型是输出(out);输出数据是作为⼀个阶段着⾊ 器的输出定义的，⼆后续阶段的着⾊器则作为输⼊定义。 输出数据可以简单的从⼀个阶段传递到下⼀个阶段，也可以⽤不同的⽅式插⼊。 客户端的代码接触不到这些内部变量 我们的OpenGL开发暂时接触不到。 四、创建坐标系 4.1 正投影 这就是⼀个正投影的例子，在所在3个轴(X,Y,Z)中，它们的范围都是 从-100到+100。这个视景体将包括所有的⼏何图形。 如果你指定了视景体外的⼏何图形，就会被裁减掉!(它将沿着视景体的边界进行剪切) 在正投影中，所有在这个空间范围内的所有东⻄都将被呈现在屏幕上。⽽ 不存在照相机或视点坐标系的概念。 4.2 透视投影 透视投影会进行透视除法对距离观察者很远的对象进⾏缩短和收缩。在投 影到屏幕之后，视景体背⾯与视景体正⾯的宽度测量标准不同。 上图所示:平截头体(frustum)的⼏何体，它的观察⽅向是从⾦字塔的 尖端到宽阔端。观察者的视点与⾦字塔的尖端拉开⼀定距离。 GLFrustum类通过setPerspective⽅法为我们构建⼀个平截头体。 CLFrustum::SetPerspective(float fFov,float fAspect,float fNear ,float fFar); 参数: fFov:垂直⽅向上的视场⻆度 fAspect:窗⼝的宽度与⾼度的纵横⽐ fNear:近裁剪⾯距离 fFar:远裁剪⾯距离 纵横⽐比 = 宽(w)/⾼(h) 五、使⽤存储着⾊器 5.1 使⽤背景 在OpenGL核⼼框架中，并没有提供任何内建渲染管线，在提交一个⼏何图形进行渲染之前，必须实现⼀个着⾊器。 在前⾯的课程可以使用存储着⾊器。这些存储着⾊器由GLTools的C++类 GLShaderManager管理。它们能够满⾜进行基本渲染的基本要求。要求不高的程序员，这些存储着⾊器已经⾜以满足他们的需求。但是，随着时间和经验的提升，⼤部分开发者可能不满⾜于此。 会开始⾃自己着⼿去写着⾊器。 5.2 存储着⾊器的使⽤ GLShaderManager 的初始化 // GLShaderManager 的初始化 GLShaderManager shaderManager; shaderManager.InitializeStockShaders() GLShaderManager 属性 存储着⾊器为每⼀个变量都使用一致的内部变量命名规则和相同的属性 槽。以上就是存储着色器的属性列列表 GLShanderManager 的 uniform值 ⼀般情况，要对⼏何图形进行渲染，我们需要给对象递属性矩阵，⾸ 先要绑定我们想要使⽤的着⾊程序上，并提供程序的uniform值。但是GLShanderManager 类可以暂时为我们完成工作。 userStockShader函数会选择⼀个存储着⾊器并提供这个着⾊器的 uniform值。 GLShaderManager::UserStockShader(GLeunm shader...); 单位(Identity 着⾊器) GLShaderManager::UserStockShader(GLT_ATTRIBUTE_VERTEX,GLfloat vColor[4]); 单位着⾊器:只是简单地使⽤默认笛卡尔坐标系(坐标范围(-1.0， 1.0))。所有的⽚段都应⽤同一种颜色，几何图形为实⼼和未渲染的。 需要设置存储着色器⼀个属性: GLT_ATTRIBUTE_VERTEX(顶点分量) 参数2:vColor[4],你需要的颜色。 平⾯着⾊器 GLShaderManager::UserStockShader(GLT_SHADER_FLAT,GLfloat mvp[1 6],GLfloat vColor[4]); 参数1:平⾯着⾊器 参数2:允许变化的4*4矩阵 参数3:颜⾊ 它将统⼀着⾊器器进行了拓展。允许为⼏何图形变换指定一个 4 * 4 变换矩 阵。经常被称为“模型视图投影矩阵” 上⾊着⾊器 GLShaderManager::UserStockShader(GLT_SHADER_SHADED,GLfloat mvp [16]); 在⼏几何图形中应⽤用的变换矩阵。 需要设置存储着色器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_COLOR(颜⾊色分量量) 2个属性。颜⾊值将被平滑地插⼊顶点之间(平滑着色) 默认光源着⾊器 GLShaderManager::UserStockShader(GLT_SHADER_DEFAULT_LIGHT,GLfloat mvMatrix[16],GLfloat pMatrix[16],GLfloat vColor[4]); 参数1:默认光源着色器 参数2:模型视图矩阵 参数3:投影矩阵 参数4:颜色值 这种着⾊器，是对象产⽣阴影和光照的效果。需要设置存储着⾊器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_NORMAL(表⾯面法线) 点光源着⾊色器器 GLShaderManager::UserStockShader(GLT_SHADER_DEFAULT_LIGHT_DIEF ,GLfloat mvMatrix[16],GLfloat pMatrix[16],GLfloat vLightPos[3] ,GLfloat vColor[4]); 参数1:点光源着⾊器 参数2:模型视图矩阵 参数3:投影矩阵 参数4:视点坐标光源位置 参数5:颜⾊值 点光源着⾊器和默认光源着色器很相似，区别在于:光源位置是特定的。 同样需要设置存储着色器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_NORMAL(表⾯法线) 纹理替换矩阵 GLShaderManager::UserStockShader(GLT_SHADER_TEXTURE_REPLACE,GL float mvMatrix[16],GLint nTextureUnit); 着⾊器通过给定的模型视图投影矩阵，使⽤绑定到 nTextureUnit (纹理单元) 指定纹理单元的纹理对几何图形进行变化。 ⽚段颜色:是直接从纹理样本中直接获取的。 需要设置存储着⾊器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_NORMAL(表⾯法线) 纹理调整着⾊器 GLShaderManager::UserStockShader(GLT_SHADER_TEXTURE_MODULATE,GLfloat mvMatrix[16],GLfloat vColor[4],GLint nTextureUnit); 将⼀个基本⾊乘以一个取⾃纹理单元 nTextureUnit 的纹理。 需要设置存储着色器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_TEXTURE0(纹理坐标) 纹理光源着⾊器 GLShaderManager::UserStockShader(GLT_SHADER_TEXTURE_POINT_LIGH T_DIEF,GLfloat mvMatrix[16],GLfloat pMatrix[16],GLfloat vLight Pos[3],GLfloat vBaseColor[4],GLint nTextureUnit); 参数1:纹理光源着⾊器 参数2:投影矩阵 参数3:视觉空间中的光源位置 参数4:⼏何图形的基本⾊ 参数5:将要使用的纹理单元 将⼀个纹理通过漫反射照明计算机进行调整(相乘)。光线在视觉空间中的位置是给定的。 需要设置存储着⾊器的 GLT_ATTRIBUTE_VERTEX(顶点分量) 和 GLT_ATTRIBUTE_TEXTURE0(纹理坐标)、GLT_ATTRIBUTE_NORMAL(表⾯法线) "},"pages/opengl/02_OpenGL_basic_rendering.html":{"url":"pages/opengl/02_OpenGL_basic_rendering.html","title":"2.OpenGL基础渲染","keywords":"","body":"2.OpenGL基础渲染 This browser does not support PDFs. Please download the PDF to view it: Download PDF. "},"pages/opengl/03_How_to_deal_with_OpenGL_rendering_problems.html":{"url":"pages/opengl/03_How_to_deal_with_OpenGL_rendering_problems.html","title":"3.OpenGL渲染问题的处理方法","keywords":"","body":"3.OpenGL渲染问题的处理方法 学习内容 渲染过程中可能产⽣的问题 油画渲染 正⾯面&背⾯面剔除 深度测试 多边形模型 多边形偏移 裁剪 一、在渲染过程中可能产⽣的问题 在绘制3D场景的时候,我们需要决定哪些部分是对观察者 可⻅的,或者哪些部分是对观察者不可⻅的.对于不可⻅的 部分,应该及早丢弃.例如在⼀个不透明的墙壁后,就不应该 渲染.这种情况叫做“隐藏⾯消除”(Hidden surface elimination). 二、油画算法 2.1 解释 先绘制场景中的离观察者较远的物体,再绘制较近的物体. 例如下⾯的图例：先绘制红⾊部分,再绘制⻩⾊部分,最后再绘制灰色部分,即可解决隐藏⾯消除的 问题。 2.2 弊端 使⽤油画算法,只要将场景按照物理距离观察者的距离远近排序,由远及近的绘制即可.那么会出现什么问题? 如果三个三角形是叠加的情况,油画算法将⽆法处理. 三、正背⾯剔除(Face Culling) 3.1 思考 ⼀个3D图形,你从任何⼀个⽅向去观察,最多可以看到几个面? 答案：最多3⾯. 从⼀个⽴⽅体的任意位置和⽅向上看,你用过不可能看到多于3个面. 那么思考? 我们为何要多余的去绘制那根本看不到的3个面? 如果我们能以某种方式去丢弃这部分数据,OpenGL在渲染的性能即可提⾼超过50%. 如何知道某个面在观察者的视􏰀中不会出现? 任何平⾯都有2个面,正面/背面.意味着你⼀个时刻只能看到一⾯.OpenGL 可以做到检查所有正面朝向观察者的面,并渲染它们.从⽽丢弃背⾯朝向的⾯. 这样可以 节约⽚元着⾊器的性能. 如果告诉OpenGL你绘制的图形,哪个面是正面,哪个面是背面? 答案：通过分析顶点数据的顺序 3.2 分析顶点顺序 正⾯: 按照逆时针顶点连接顺序的三⻆形⾯ 背⾯: 按照顺时针顶点连接顺序的三⻆形⾯ 3.3 分析⽴方体中的正背⾯ 分析 左侧三⻆形顶点顺序为: 1—> 2—> 3; 右侧三⻆形的顶点顺序为: 1—> 2—> 3 . 当观察者在右侧时,则右边的三⻆形⽅向为逆时针⽅方向判定为正⾯,⽽左侧的三⻆形为顺时针则为背面 当观察者在左侧时,则左边的三角形⽅向为逆时针⽅方向判定为正⾯,⽽右侧的三⻆形为顺时针则为背面 总结 正⾯和背面是由三⻆形的顶点定义顺序和观察者方向共同决定的.随着观察者的⻆度⽅向的改变,正面背面也 会跟着改变。 3.4 开启正背面剔除函数 开启表⾯剔除(默认背⾯剔除) void glEnable(GL_CULL_FACE); 关闭表面剔除(默认背⾯剔除) void glDisable(GL_CULL_FACE); ⽤户选择剔除那个面(正面/背面) // mode参数为: GL_FRONT,GL_BACK,GL_FRONT_AND_BACK ,默认GL_BACK void glCullFace(GLenum mode); ⽤户指定绕序哪个为正⾯ // mode参数为: GL_CW,GL_CCW,默认值:GL_CCW oid glFrontFace(GLenum mode); 例如,剔除正面实现(1) glCullFace(GL_BACK); glFrontFace(GL_CW); 例如,剔除正面实现(1) glCullFace(GL_FRONT); 四、深度测试 深度缓冲区(DepthBuffer)和颜⾊缓存区(ColorBuffer)是对应的.颜⾊缓存区存储像素的颜色信息,⽽深度缓冲区存储像素的深度信息.在决定是否绘制一个物体表⾯时,首先要将表⾯面对应的像素的深度值与当前深度缓冲区中的值进⾏比较.如果⼤于深度缓冲区中的值,则丢弃这部分.否则利用这个像素对应的深度值和颜⾊值.分别更新深度缓冲区和颜色缓存区. 这个过程称为”深度测 试”。 4.1 什么是深度？ 深度其实就是该像素点在3D世界中距离摄像机的距离,Z值 4.2 什么是深度缓冲区? 深度缓存区,就是一块内存区域,专⻔存储着每个像素点(绘制在屏幕上的)深度值.深度值(Z值)越⼤, 则离摄像机就越远. 4.3 为什么需要深度缓冲区? 在不使⽤深度测试的时候,如果我们先绘制一个距离⽐较近的物体,再绘制距离较远的物体,则距离远的位图因为后绘制,会把距离近的物体覆盖掉.有了深度缓冲区后,绘制物体的顺序就不那么􏰀重要的. 实际上,只要存在深度缓冲区,OpenGL都会把像素的深度值写⼊到缓冲区中. 除非调用 glDepthMask(GL_FALSE).来禁⽌写入. 4.4 使⽤深度测试 深度缓冲区,⼀般由窗⼝管理理系统,GLFW创建.深度值⼀般由16位,24位,32位值表示.通常是24位.数越高,深度精确度更好. 开启深度测试glEnable(GL_DEPTH_TEST); 在绘制场景前,清除颜⾊缓存区,深度缓冲glClearColor(0.0f,0.0f,0.0f,1.0f); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); 清除深度缓冲区默认值为1.0,表示最大的深度值,深度值的范围为(0,1)之间.值越小表示越靠近观察者,值越大表示越远离观察者 指定深度测试判断模式 打开/阻断深度缓存区写⼊//value : GL_TURE 开启深度缓冲区写⼊入; GL_FALSE 关闭深度缓冲区写⼊入 void glDepthMask(GLBool value); 4.5 ZFighting闪烁问题出现 因为开启深度测试后,OpenGL就不会再去绘制模型被遮挡的部分.这样实现的显示更加真实.但是 由于深度缓冲区精度的限制对于深度相差⾮常小的情况下.(例如在同一平⾯上进行2次绘制),OpenGL就可能出现不能正确判断两者的深度值,会导致深度测试的结果不可预测.显示出来的现象时交错闪烁的前面2个画面,交错出现. 4.6 ZFighting闪烁问题解决 第⼀步: 启⽤用 Polygon Offset ⽅式解决 让深度值之间产⽣生间隔.如果2个图形之间有间隔,是不是意味着就不会产⽣干涉.可以理解为在执⾏深度测试前将⽴方体的深度值做⼀些细微的增加.于是就能将􏰀叠的2个图形深度值之间有所区分. // 启⽤用Polygon Offset⽅式 // 参数列列表: // GL_POLYGON_OFFSET_POINT GL_POLYGON_OFFSET_LINE GL_POLYGON_OFFSET_FILL // 对应光栅化模式: GL_POINT // 对应光栅化模式: GL_LINE // 对应光栅化模式: GL_FILL glEnable(GL_POLYGON_OFFSET_FILL) 第⼆步: 指定偏移量 通过glPolygonOffset来指定.glPolygonOffset需要2个参数: factor , unitsvoid glPolygonOffset(Glfloat factor,Glfloat units); 每个Fragment的深度值都会增加如下所示的偏移量Offset = ( m * factor ) + ( r * units) m(深度值(Z值)) : 多边形的深度的斜率的最⼤值,理解一个多边形越是与近裁剪面平⾏,m 就越接近于0. r(使得深度缓冲区产⽣生变化的最小值) : 能产⽣于窗⼝坐标系的深度值中可分辨的差异最小值.r是由具体OpenGL平台指定的⼀个常量. ⼀个⼤于0的Offset 会把模型推到离你(摄像机)更远的位置,相应的一个⼩于0的Offset会把模型拉近 ⼀般⽽言,只需要将-1.0 和 0.0 这样简单赋值给glPolygonOffset基本可以满⾜需求. 第三步: 关闭Polygon Offset glDisable(GL_POLYGON_OFFSET_FILL) 4.7 ZFighting闪烁问题预防 不要将两个物体靠的太近，避免渲染时三⻆形叠在⼀起。这种方式要求对场景中物体插入⼀个少量的偏移，那么就可能避免ZFighting现象。例如上面的⽴方体和平⾯问题中，将平⾯下移0.001f就可以解决这个问题。当然⼿动去插⼊这个⼩的偏移是要付出代价的。 尽可能将近裁剪⾯设置得离观察者远⼀些。上⾯我们看到，在近裁剪平面附近，深度的精确度是很高的，因此尽可能让近裁剪⾯远⼀些的话，会使整个裁剪范围内的精确度变高⼀些。但是这种⽅式会使离观察者较近的物体被裁减掉，因此需要调试好裁剪⾯参数。 使⽤更⾼位数的深度缓冲区，通常使⽤的深度缓冲区是24位的，现在有一些硬件使用32位的缓冲区，使精确度得到提⾼ 五、裁剪 在OpenGL中提高渲染的一种⽅式.只刷新屏幕上发生变化的部分.OpenGL允许将要进行渲染的窗口只 去指定一个裁剪框. 基本原理:⽤于渲染时限制绘制区域，通过此技术可以再屏幕(帧缓冲)指定一个矩形区域。启⽤剪裁测试之后，不在此矩形区域内的⽚元被丢弃，只有在此矩形区域内的⽚元才有可能进入帧缓冲。因此实际达到的效果就是在屏幕上开辟了了一个⼩窗口，可以再其中进⾏指定内容的绘制。 //1 开启裁剪测试 glEnable(GL_SCISSOR_TEST); //2.关闭裁剪测试 glDisable(GL_SCISSOR_TEST); //3.指定裁剪窗⼝口 void glScissor(Glint x,Glint y,GLSize width,GLSize height); x,y:指定裁剪框左下⻆角位置; width , height:指定裁剪尺⼨ 5.1 理解窗口 就是显示界⾯ 5.2 视口 就是窗⼝中⽤来显示图形的一块矩形区域，它可以和窗口等大，也可以⽐窗口⼤或者小。只有绘制在视口区域中的图形才能被显示，如果图形有一部分超出了视口区域，那么那一部分是看不到的。 5.3 裁剪区域（平行投影） 就是视⼝矩形区域的最小最大x坐标(left,right)和最小最大y坐标 (bottom,top），⽽不是窗口的最小最大x坐标和y坐标。通过glOrtho()函数设置，这个函数还需指定最近 最远z坐标，形成⼀个立体的裁剪区域。 六、混合 我们把OpenGL渲染时会把颜色值存在颜⾊缓存区中，每个⽚段的深度值也是放在深度缓冲区。当深度缓冲区被关闭时，新的颜色将简单的覆盖原来颜色缓存区存在的颜色值，当深度缓冲区再次打开时，新的颜色⽚段只是当它们⽐原来的值更接近邻近的裁剪平⾯才会替换原来的颜色⽚段。 glEnable(GL_BlEND); 6.1 组合颜⾊ ⽬标颜色:已经存储在颜⾊缓存区的颜⾊值 源颜⾊:作为当前渲染命令结果进⼊颜色缓存区的颜⾊值 当混合功能被启动时，源颜⾊和⽬标颜⾊的组合⽅式是混合方程式控制的。在默认情况下， 混合方程式如下所示: // Cf: 最终计算参数的颜⾊色 // Cs: 源颜⾊色 // Cd:⽬目标颜⾊色 // S: 源混合因⼦子 // D: ⽬目标混合因⼦子 Cf = (Cs * S) + (Cd * D) 6.2 设置混合因⼦ 函数介绍 设置混合因⼦子，需要⽤用到glBlendFun函数 // S:源混合因⼦子 // D:⽬目标混合因⼦子 glBlendFunc(GLenum S,GLenum D); 表中R、G、B、A 分别代表 红、绿、蓝、alpha。 表中下标S、D，分别代表源、⽬目标。 表中C 代表常量量颜⾊色(默认⿊黑⾊色)。 函数使用 下⾯面通过⼀个常⻅的混合函数组合来说明问题: glBlendFunc(GL_SRC_ALPHA,GL_ONE_MINUS_SRC_ALPHA); 如果颜⾊缓存区已经有一种颜色红色(1.0f,0.0f,0.0f,0.0f),这个目标颜色Cd，如果在这上面用⼀种alpha为0.6的蓝色(0.0f,0.0f,1.0f,0.6f) Cd (⽬标颜色) = (1.0f,0.0f,0.0f,0.0f); Cs (源颜⾊色) = (0.0f,0.0f,1.0f,0.6f); S = 源alpha值 = 0.6f D = 1 - 源alpha值= 1-0.6f = 0.4f 方程式Cf = (Cs * S) + (Cd * D) 等价于 = (Blue 0.6f) + (Red 0.4f) 总结 最终颜色是以原先的红色(目标颜色)与 后来的蓝色(源颜色)进行组合。源颜⾊的alpha值越⾼，添加的蓝⾊色颜色成分越高，⽬标颜⾊所保留的成分就会越少。混合函数经常⽤于实现在其他一些不透明的物体前⾯绘制一个透明物体的效果。 6.3 改变组合⽅程式 默认混合⽅方程式: Cf = (CsS)+(CdD) 实际上远不止这一种混合⽅程式，我们可以从5个不同的方程式中进行选择 选择混合⽅方程式的函数: glbBlendEquation(GLenum mode); 6.4 glBlendFuncSeparate 函数 除了能使⽤glBlendFunc来设置混合因子，还可以有更灵活的选择。 // strRGB: 源颜⾊的混合因⼦ // dstRGB: ⽬标颜⾊的混合因⼦ // strAlpha: 源颜色的Alpha因⼦ // dstAlpha: ⽬标颜色的Alpha因⼦ void glBlendFuncSeparate(GLenum strRGB,GLenum dstRGB ,GLenum strAlpha,GLenum dstAlpha); glBlendFunc 指定源和⽬标RGBA值的混合函数;但是glBlendFuncSeparate函数则允许为RGB 和 Alpha 成分单独指定混合函数。 在混合因⼦子表中，GL_CONSTANT_COLOR,GL_ONE_MINUS_CONSTANT_COLOR,GL_CONSTANT_ALPHA,GL_ONE_MINUS_CONSTANT值允许混合方程式中引⼊一个常量混合颜⾊。 6.5 常量混合颜⾊ 常量混合颜⾊色，默认初始化为⿊⾊(0.0f,0.0f,0.0f,0.0f)，但是还是可以修改这个常量混合颜色。 void glBlendColor(GLclampf red ,GLclampf green ,GLclampf blue ,GLclampf alpha); 七、案例 源码 7.1 深度测试+正背面剔除 //演示了OpenGL背面剔除，深度测试，和多边形模式 #include \"GLTools.h\" #include \"GLMatrixStack.h\" #include \"GLFrame.h\" #include \"GLFrustum.h\" #include \"GLGeometryTransform.h\" #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif ////设置角色帧，作为相机 GLFrame viewFrame; //使用GLFrustum类来设置透视投影 GLFrustum viewFrustum; GLTriangleBatch torusBatch; GLMatrixStack modelViewMatix; GLMatrixStack projectionMatrix; GLGeometryTransform transformPipeline; GLShaderManager shaderManager; //标记：背面剔除、深度测试 int iCull = 0; int iDepth = 0; //右键菜单栏选项 void ProcessMenu(int value) { switch(value) { case 1: iDepth = !iDepth; break; case 2: iCull = !iCull; break; case 3: glPolygonMode(GL_FRONT_AND_BACK, GL_FILL); break; case 4: glPolygonMode(GL_FRONT_AND_BACK, GL_LINE); break; case 5: glPolygonMode(GL_FRONT_AND_BACK, GL_POINT); break; } glutPostRedisplay(); } // 召唤场景 void RenderScene(void) { //清除窗口和深度缓冲区 glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); //根据设置iCull标记来判断是否开启背面剔除 if(iCull) { glEnable(GL_CULL_FACE); glFrontFace(GL_CCW); glCullFace(GL_BACK); } else glDisable(GL_CULL_FACE); //根据设置iDepth标记来判断是否开启深度测试 if(iDepth) glEnable(GL_DEPTH_TEST); else glDisable(GL_DEPTH_TEST); //把摄像机矩阵压入模型矩阵中 modelViewMatix.PushMatrix(viewFrame); GLfloat vRed[] = { 1.0f, 0.0f, 0.0f, 1.0f }; //使用平面着色器 //参数1：平面着色器 //参数2：模型视图投影矩阵 //参数3：颜色 //shaderManager.UseStockShader(GLT_SHADER_FLAT, transformPipeline.GetModelViewProjectionMatrix(), vRed); //使用默认光源着色器 //通过光源、阴影效果跟提现立体效果 //参数1：GLT_SHADER_DEFAULT_LIGHT 默认光源着色器 //参数2：模型视图矩阵 //参数3：投影矩阵 //参数4：基本颜色值 shaderManager.UseStockShader(GLT_SHADER_DEFAULT_LIGHT, transformPipeline.GetModelViewMatrix(), transformPipeline.GetProjectionMatrix(), vRed); //绘制 torusBatch.Draw(); //出栈 modelViewMatix.PopMatrix(); glutSwapBuffers(); } // 这个函数不需要初始化渲染 // context. 图像上下文 void SetupRC() { // 设置背景颜色 glClearColor(0.3f, 0.3f, 0.3f, 1.0f ); //初始化着色器管理器 shaderManager.InitializeStockShaders(); //将相机向后移动7个单元：肉眼到物体之间的距离 viewFrame.MoveForward(7.0); //创建一个甜甜圈 //void gltMakeTorus(GLTriangleBatch& torusBatch, GLfloat majorRadius, GLfloat minorRadius, GLint numMajor, GLint numMinor); //参数1：GLTriangleBatch 容器帮助类 //参数2：外边缘半径 //参数3：内边缘半径 //参数4、5：主半径和从半径的细分单元数量 gltMakeTorus(torusBatch, 1.0f, 0.3f, 52, 26); //点的大小 glPointSize(4.0f); } //键位设置，通过不同的键位对其进行设置 //控制Camera的移动，从而改变视口 void SpecialKeys(int key, int x, int y) { if(key == GLUT_KEY_UP) viewFrame.RotateWorld(m3dDegToRad(-5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_DOWN) viewFrame.RotateWorld(m3dDegToRad(5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_LEFT) viewFrame.RotateWorld(m3dDegToRad(-5.0), 0.0f, 1.0f, 0.0f); if(key == GLUT_KEY_RIGHT) viewFrame.RotateWorld(m3dDegToRad(5.0), 0.0f, 1.0f, 0.0f); //重新刷新window glutPostRedisplay(); } void ChangeSize(int w, int h) { //防止h变为0 if(h == 0) h = 1; //设置视口窗口尺寸 glViewport(0, 0, w, h); //setPerspective函数的参数是一个从顶点方向看去的视场角度（用角度值表示） // 设置透视模式，初始化其透视矩阵 viewFrustum.SetPerspective(35.0f, float(w)/float(h), 1.0f, 100.0f); //把透视矩阵加载到透视矩阵对阵中 projectionMatrix.LoadMatrix(viewFrustum.GetProjectionMatrix()); // 初始化渲染管线 transformPipeline.SetMatrixStacks(modelViewMatix, projectionMatrix); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH | GLUT_STENCIL); glutInitWindowSize(800, 600); glutCreateWindow(\"Geometry Test Program\"); glutReshapeFunc(ChangeSize); glutSpecialFunc(SpecialKeys); glutDisplayFunc(RenderScene); // Create the Menu glutCreateMenu(ProcessMenu); glutAddMenuEntry(\"Toggle depth test\",1); glutAddMenuEntry(\"Toggle cull backface\",2); glutAddMenuEntry(\"Set Fill Mode\", 3); glutAddMenuEntry(\"Set Line Mode\", 4); glutAddMenuEntry(\"Set Point Mode\", 5); glutAttachMenu(GLUT_RIGHT_BUTTON); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"GLEW Error: %s\\n\", glewGetErrorString(err)); return 1; } SetupRC(); glutMainLoop(); return 0; } 7.2 裁剪 //demo OpenGL 裁剪 #include \"GLTools.h\" #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif //召唤场景 void RenderScene(void) { //设置清屏颜色为蓝色 glClearColor(0.0f, 0.0f, 1.0f, 0.0f); glClear(GL_COLOR_BUFFER_BIT); //1.现在剪成小红色分区 //(1)设置裁剪区颜色为红色 glClearColor(1.0f, 0.0f, 0.0f, 0.0f); //(2)设置裁剪尺寸 glScissor(100, 100, 600, 400); //(3)开启裁剪测试 glEnable(GL_SCISSOR_TEST); //(4)开启清屏，执行裁剪 glClear(GL_COLOR_BUFFER_BIT); // 2.裁剪一个绿色的小矩形 //(1).设置清屏颜色为绿色 glClearColor(0.0f, 1.0f, 0.0f, 0.0f); //(2).设置裁剪尺寸 glScissor(200, 200, 400, 200); //(3).开始清屏执行裁剪 glClear(GL_COLOR_BUFFER_BIT); //关闭裁剪测试 glDisable(GL_SCISSOR_TEST); //强制执行缓存区 glutSwapBuffers(); } void ChangeSize(int w, int h) { //保证高度不能为0 if(h == 0) h = 1; // 将视口设置为窗口尺寸 glViewport(0, 0, w, h); } //程序入口 int main(int argc, char* argv[]) { glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGB); glutInitWindowSize(800,600); glutCreateWindow(\"OpenGL Scissor\"); glutReshapeFunc(ChangeSize); glutDisplayFunc(RenderScene); glutMainLoop(); return 0; } 7.3 颜色混合 //颜色组合 #include \"GLTools.h\" #include \"GLShaderManager.h\" #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLBatch squareBatch; GLBatch greenBatch; GLBatch redBatch; GLBatch blueBatch; GLBatch blackBatch; GLShaderManager shaderManager; GLfloat blockSize = 0.2f; GLfloat vVerts[] = { -blockSize, -blockSize, 0.0f, blockSize, -blockSize, 0.0f, blockSize, blockSize, 0.0f, -blockSize, blockSize, 0.0f}; void SetupRC() { glClearColor(1.0f, 1.0f, 1.0f, 1.0f ); shaderManager.InitializeStockShaders(); //绘制1个移动矩形 squareBatch.Begin(GL_TRIANGLE_FAN, 4); squareBatch.CopyVertexData3f(vVerts); squareBatch.End(); //绘制4个固定矩形 GLfloat vBlock[] = { 0.25f, 0.25f, 0.0f, 0.75f, 0.25f, 0.0f, 0.75f, 0.75f, 0.0f, 0.25f, 0.75f, 0.0f}; greenBatch.Begin(GL_TRIANGLE_FAN, 4); greenBatch.CopyVertexData3f(vBlock); greenBatch.End(); GLfloat vBlock2[] = { -0.75f, 0.25f, 0.0f, -0.25f, 0.25f, 0.0f, -0.25f, 0.75f, 0.0f, -0.75f, 0.75f, 0.0f}; redBatch.Begin(GL_TRIANGLE_FAN, 4); redBatch.CopyVertexData3f(vBlock2); redBatch.End(); GLfloat vBlock3[] = { -0.75f, -0.75f, 0.0f, -0.25f, -0.75f, 0.0f, -0.25f, -0.25f, 0.0f, -0.75f, -0.25f, 0.0f}; blueBatch.Begin(GL_TRIANGLE_FAN, 4); blueBatch.CopyVertexData3f(vBlock3); blueBatch.End(); GLfloat vBlock4[] = { 0.25f, -0.75f, 0.0f, 0.75f, -0.75f, 0.0f, 0.75f, -0.25f, 0.0f, 0.25f, -0.25f, 0.0f}; blackBatch.Begin(GL_TRIANGLE_FAN, 4); blackBatch.CopyVertexData3f(vBlock4); blackBatch.End(); } //上下左右键位控制移动 void SpecialKeys(int key, int x, int y) { GLfloat stepSize = 0.025f; GLfloat blockX = vVerts[0]; GLfloat blockY = vVerts[7]; if(key == GLUT_KEY_UP) blockY += stepSize; if(key == GLUT_KEY_DOWN) blockY -= stepSize; if(key == GLUT_KEY_LEFT) blockX -= stepSize; if(key == GLUT_KEY_RIGHT) blockX += stepSize; if(blockX (1.0f - blockSize * 2)) blockX = 1.0f - blockSize * 2;; if(blockY 1.0f) blockY = 1.0f; vVerts[0] = blockX; vVerts[1] = blockY - blockSize*2; vVerts[3] = blockX + blockSize*2; vVerts[4] = blockY - blockSize*2; vVerts[6] = blockX + blockSize*2; vVerts[7] = blockY; vVerts[9] = blockX; vVerts[10] = blockY; squareBatch.CopyVertexData3f(vVerts); glutPostRedisplay(); } //召唤场景 void RenderScene(void) { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT | GL_STENCIL_BUFFER_BIT); //定义4种颜色 GLfloat vRed[] = { 1.0f, 0.0f, 0.0f, 0.5f }; GLfloat vGreen[] = { 0.0f, 1.0f, 0.0f, 1.0f }; GLfloat vBlue[] = { 0.0f, 0.0f, 1.0f, 1.0f }; GLfloat vBlack[] = { 0.0f, 0.0f, 0.0f, 1.0f }; //召唤场景的时候，将4个固定矩形绘制好 //使用 单位着色器 //参数1：简单的使用默认笛卡尔坐标系（-1，1），所有片段都应用一种颜色。GLT_SHADER_IDENTITY //参数2：着色器颜色 shaderManager.UseStockShader(GLT_SHADER_IDENTITY, vGreen); greenBatch.Draw(); shaderManager.UseStockShader(GLT_SHADER_IDENTITY, vRed); redBatch.Draw(); shaderManager.UseStockShader(GLT_SHADER_IDENTITY, vBlue); blueBatch.Draw(); shaderManager.UseStockShader(GLT_SHADER_IDENTITY, vBlack); blackBatch.Draw(); //组合核心代码 //1.开启混合 glEnable(GL_BLEND); //2.开启组合函数 计算混合颜色因子 glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); //3.使用着色器管理器 //*使用 单位着色器 //参数1：简单的使用默认笛卡尔坐标系（-1，1），所有片段都应用一种颜色。GLT_SHADER_IDENTITY //参数2：着色器颜色 shaderManager.UseStockShader(GLT_SHADER_IDENTITY, vRed); //4.容器类开始绘制 squareBatch.Draw(); //5.关闭混合功能 glDisable(GL_BLEND); //同步绘制命令 glutSwapBuffers(); } void ChangeSize(int w, int h) { glViewport(0, 0, w, h); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH); glutInitWindowSize(800, 600); glutCreateWindow(\"移动矩形，观察颜色\"); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"Error: %s\\n\", glewGetErrorString(err)); return 1; } glutReshapeFunc(ChangeSize); glutDisplayFunc(RenderScene); glutSpecialFunc(SpecialKeys); SetupRC(); glutMainLoop(); return 0; } "},"pages/opengl/04_OpenGL_rendering_skills.html":{"url":"pages/opengl/04_OpenGL_rendering_skills.html","title":"4.OpenGL渲染技巧","keywords":"","body":"4.OpenGL渲染技巧 学习内容 抗锯齿 多重采样 OpenGL基本变化 投影矩阵-正投影 投影矩阵-透视投影 一、抗锯齿 抗锯齿混合的2大功能:颜色组合、抗锯齿。 //开启混合处理理 glEnable(GL_BLEND); //指定混合因⼦子 GLBlendFunc(GL_SRC_ALPHA,GL_ONE_MINUS_SRC_ALPHA); //指定混合⽅方程式 glBlendEquation(GL_FUNC_ADD); //对点进⾏行行抗锯⻮齿处理理 glEnable(GL_POINT_SMOOTH); //对线进⾏行行抗锯⻮齿处理理 glEnable(GL_LINE_SMOOTH); //对多边形进⾏行行抗锯⻮齿处理理 glEnable(GL_POLYGON_SMOOTH); 未开启抗锯齿 开启抗锯齿 二、多重采样 // 1.可以调⽤用 glutInitDisplayMode 添加采样缓存区 glutInitDisplayMode(GLUT_MULTISAMPLE); // 2.可以使⽤用glEnable| glDisable组合使⽤用GLUT_MULTISAMPLE 打开|关闭 多重采 glEnable(GLUT_MULTISAMPLE); glDisable(GLUT_MULTISAMPLE); // 3. 多重采样只正对多边形，点和线使用混合 glEnable(GL_POINT_SMOOTH); glDisable(GL_POINT_SMOOTH); glEnable(GL_LINE_SMOOTH); glDisable(GL_LINE_SMOOTH); 多重采样缓存区在默认情况下使用⽚段RGB值，并不包含颜色的alpha成分，我们可以通过调⽤用glEnable来修改这个行为: GL_SAMPLE_ALPHA_TO_COVERAGE 使⽤用alpha值 GL_SAMPLE_ALPHA_TO_ON 使⽤alpha值并设为1，并使⽤它。 GL_SAMPLE_COVERAGE 使⽤glSampleCoverage所设置的值。 当启⽤ GL_SAMPLE_COVERAGE 时，可以使⽤glSampleCoverage函数允许指定一个特定的值，它是与⽚段覆盖值进⾏按位与操作的结果。 三、OpenGL基础变化 This browser does not support PDFs. Please download the PDF to view it: Download PDF. 四、数学知识 M3DVector3f,三维向量(x,y,z) M3DVector4f,思维向量(x,y,z,w).w = 1.0 //定义2个向量V1,V2 M3DVector3f v1 = {1.0,0.0,0.0}; M3DVector3f v2 = {0.0,1.0,0.0}; //方法1：获取V1,V2的点积，获取夹角的cos值 GLfloat value1 = m3dDotProduct3(v1, v2); printf(\"V1V2 余弦值：%f\\n\",value1); //通过acos()，获取value1的弧度值 GLfloat value2 = acos(value1); printf(\"V1V2 弧度：%f\\n\",value2); //方法2：获取V1，V2的夹角弧度值 GLfloat value3 = m3dGetAngleBetweenVectors3(v1, v2); printf(\"V1V2 弧度：%f\\n\",value3); //m3dRadToDeg 弧度->度数 //m3dDegToRad 度数->弧度 GLfloat value4 = m3dRadToDeg(value3); GLfloat value5 = m3dDegToRad(90); printf(\"V1V2角度：%f\\n\",value4); printf(\"弧度：%f\\n\",value5); //定义向量vector2 M3DVector3f vector2 = {0.0f,0.0f,0.0f}; //实现矩阵叉乘：结果，向量1，向量2 // 获得一个与v1v2所在平面垂直的新向量 // 叉乘的结果与v1v2的相乘顺序有关 m3dCrossProduct3(vector2, v1, v2); printf(\"%f,%f,%f\",vector2[0],vector2[1],vector2[2]); 四、案例 源码 4.1 抗锯齿+多重采样 #include \"GLTools.h\" #include \"GLFrustum.h\" #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLShaderManager shaderManager; GLFrustum viewFrustum; GLBatch smallStarBatch; GLBatch mediumStarBatch; GLBatch largeStarBatch; GLBatch mountainRangeBatch; GLBatch moonBatch; #define SMALL_STARS 100 #define MEDIUM_STARS 40 #define LARGE_STARS 15 #define SCREEN_X 800 #define SCREEN_Y 600 // 选择菜单 void ProcessMenu(int value) { switch(value) { case 1: //打开抗锯齿，并给出关于尽可能进行最佳的处理提示 //设置混合因子 glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); glEnable(GL_BLEND); glEnable(GL_POINT_SMOOTH); glHint(GL_POINT_SMOOTH_HINT, GL_NICEST); glEnable(GL_LINE_SMOOTH); glHint(GL_LINE_SMOOTH_HINT, GL_NICEST); glEnable(GL_POLYGON_SMOOTH); glHint(GL_POLYGON_SMOOTH_HINT, GL_NICEST); break; case 2: //关闭抗锯齿 glDisable(GL_BLEND); glDisable(GL_LINE_SMOOTH); glDisable(GL_POINT_SMOOTH); glDisable(GL_POLYGON_SMOOTH); break; default: break; } // 触发重新绘制 glutPostRedisplay(); } //场景召唤 void RenderScene(void) { //执行clear（颜色缓存区、深度缓冲区） glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); //定义白色 GLfloat vWhite [] = { 1.0f, 1.0f, 1.0f, 1.0f }; //使用存储着色管理器中的平面着色器 //参数1：平面着色器 //参数2: 模型视图投影矩阵 //参数3: 颜色，白色 shaderManager.UseStockShader(GLT_SHADER_FLAT, viewFrustum.GetProjectionMatrix(), vWhite); //针对多边形开始多重采样 glEnable(GLUT_MULTISAMPLE); //绘制小星星 //点的大小 glPointSize(1.0f); smallStarBatch.Draw(); //绘制中星星 glPointSize(4.0f); mediumStarBatch.Draw(); //绘制大星星 glPointSize(8.0f); largeStarBatch.Draw(); // 绘制遥远的地平线 glLineWidth(3.5); mountainRangeBatch.Draw(); //绘制月亮 moonBatch.Draw(); //关闭多重采样 glDisable(GLUT_MULTISAMPLE); // 交换缓冲区 glutSwapBuffers(); } //对渲染进行必要的初始化。 void SetupRC() { M3DVector3f vVerts[SMALL_STARS]; int i; shaderManager.InitializeStockShaders(); // 小星星 smallStarBatch.Begin(GL_POINTS, SMALL_STARS); for(i = 0; i 4.2 图形移动（矩阵变换） /* 案例：实现矩阵的移动，利用矩阵的平移、旋转、综合变化等 */ #include \"GLTools.h\" #include \"GLShaderManager.h\" #include \"math3d.h\" #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLBatch squareBatch; GLShaderManager shaderManager; GLfloat blockSize = 0.1f; GLfloat vVerts[] = { -blockSize, -blockSize, 0.0f, blockSize, -blockSize, 0.0f, blockSize, blockSize, 0.0f, -blockSize, blockSize, 0.0f}; GLfloat xPos = 0.0f; GLfloat yPos = 0.0f; void SetupRC() { //背景颜色 glClearColor(0.0f, 0.0f, 1.0f, 1.0f ); shaderManager.InitializeStockShaders(); // 加载三角形 squareBatch.Begin(GL_TRIANGLE_FAN, 4); squareBatch.CopyVertexData3f(vVerts); squareBatch.End(); } //移动（移动只是计算了X,Y移动的距离，以及碰撞检测） void SpecialKeys(int key, int x, int y) { GLfloat stepSize = 0.025f; if(key == GLUT_KEY_UP) yPos += stepSize; if(key == GLUT_KEY_DOWN) yPos -= stepSize; if(key == GLUT_KEY_LEFT) xPos -= stepSize; if(key == GLUT_KEY_RIGHT) xPos += stepSize; // 碰撞检测 if(xPos (1.0f - blockSize)) xPos = 1.0f - blockSize; if(yPos (1.0f - blockSize)) yPos = 1.0f - blockSize; glutPostRedisplay(); } void RenderScene(void) { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT | GL_STENCIL_BUFFER_BIT); GLfloat vRed[] = { 1.0f, 0.0f, 0.0f, 1.0f }; M3DMatrix44f mFinalTransform, mTranslationMatrix, mRotationMatrix; //平移 xPos,yPos m3dTranslationMatrix44(mTranslationMatrix, xPos, yPos, 0.0f); // 每次重绘时，旋转5度 static float yRot = 0.0f; yRot += 5.0f; m3dRotationMatrix44(mRotationMatrix, m3dDegToRad(yRot), 0.0f, 0.0f, 1.0f); //将旋转和移动的结果合并到mFinalTransform 中 m3dMatrixMultiply44(mFinalTransform, mTranslationMatrix, mRotationMatrix); //将矩阵结果提交到固定着色器（平面着色器）中。 shaderManager.UseStockShader(GLT_SHADER_FLAT, mFinalTransform, vRed); squareBatch.Draw(); // 执行缓冲区交换 glutSwapBuffers(); } void ChangeSize(int w, int h) { glViewport(0, 0, w, h); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH); glutInitWindowSize(600, 600); glutCreateWindow(\"Move Block with Arrow Keys\"); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"Error: %s\\n\", glewGetErrorString(err)); return 1; } glutReshapeFunc(ChangeSize); glutDisplayFunc(RenderScene); glutSpecialFunc(SpecialKeys); SetupRC(); glutMainLoop(); return 0; } 4.3 正交投影 #include \"GLTools.h\" #include \"GLMatrixStack.h\" #include \"GLFrame.h\" #include \"GLFrustum.h\" #include \"GLGeometryTransform.h\" #include \"GLBatch.h\" #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLFrame viewFrame; GLFrustum viewFrustum; GLBatch tubeBatch; GLBatch innerBatch; //GLMatrixStack 堆栈矩阵 GLMatrixStack modelViewMatix; GLMatrixStack projectionMatrix; //几何变换的管道 GLGeometryTransform transformPipeline; GLShaderManager shaderManager; // 召唤场景 void RenderScene(void) { // 清屏、深度缓存 glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); //glEnable(GL_CULL_FACE); //开启深度测试 glEnable(GL_DEPTH_TEST); //绘制前压栈，将数据保存进去 modelViewMatix.PushMatrix(viewFrame); GLfloat vRed[] = { 1.0f, 0.0f, 0.0f, 1.0f }; GLfloat vGray[] = { 0.75f, 0.75f, 0.75f, 1.0f }; //默认光源着色器 //参数1：类型 //参数2：模型视图矩阵 //参数3：投影矩阵 //参数4：颜色 shaderManager.UseStockShader(GLT_SHADER_DEFAULT_LIGHT, transformPipeline.GetModelViewMatrix(), transformPipeline.GetProjectionMatrix(), vRed); tubeBatch.Draw(); //默认光源着色器 //参数1：类型 //参数2：模型视图矩阵 //参数3：投影矩阵 //参数4：颜色 shaderManager.UseStockShader(GLT_SHADER_DEFAULT_LIGHT, transformPipeline.GetModelViewMatrix(), transformPipeline.GetProjectionMatrix(), vGray); innerBatch.Draw(); //绘制完出栈，还原开始的数据 modelViewMatix.PopMatrix(); glutSwapBuffers(); } //对图形上下文初始化 void SetupRC() { //设置清屏颜色 glClearColor(0.0f, 0.0f, 0.75f, 1.0f ); // glEnable(GL_CULL_FACE); glEnable(GL_DEPTH_TEST); //初始化着色器管理器 shaderManager.InitializeStockShaders(); tubeBatch.Begin(GL_QUADS, 200); float fZ = 100.0f; float bZ = -100.0f; //左面板的颜色、顶点、光照数据 //颜色值 tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); //光照法线 //接受3个表示坐标的值，指定一条垂直于三角形表面的法线向量 tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); //顶点数据 tubeBatch.Vertex3f(-50.0f, 50.0f, 100.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f,50.0f,bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f,50.0f,bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, bZ); tubeBatch.End(); //内壁 innerBatch.Begin(GL_QUADS, 40); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f,35.0f,bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, bZ); innerBatch.End(); } void SpecialKeys(int key, int x, int y) { if(key == GLUT_KEY_UP) viewFrame.RotateWorld(m3dDegToRad(-5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_DOWN) viewFrame.RotateWorld(m3dDegToRad(5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_LEFT) viewFrame.RotateWorld(m3dDegToRad(-5.0), 0.0f, 1.0f, 0.0f); if(key == GLUT_KEY_RIGHT) viewFrame.RotateWorld(m3dDegToRad(5.0), 0.0f, 1.0f, 0.0f); //刷新窗口 glutPostRedisplay(); } //启动demo，就会调用这个方法 void ChangeSize(int w, int h) { if(h == 0) h = 1; // 设置视图窗口的尺寸 glViewport(0, 0, w, h); //设置正投影矩阵 /* void SetOrthographic(GLfloat xMin, GLfloat xMax, GLfloat yMin, GLfloat yMax, GLfloat zMin, GLfloat zMax) */ viewFrustum.SetOrthographic(-130.0f, 130.0f, -130.0f, 130.0f, -130.0f, 130.0f); //1.获取投影矩阵 viewFrustum.GetProjectionMatrix() //2.将投影矩阵加载到projectionMatrix中 projectionMatrix.LoadMatrix(viewFrustum.GetProjectionMatrix()); //设置变换管线以使用两个矩阵堆栈 transformPipeline.SetMatrixStacks(modelViewMatix, projectionMatrix); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH | GLUT_STENCIL); glutInitWindowSize(800, 600); glutCreateWindow(\"Orthographic Projection Example\"); glutReshapeFunc(ChangeSize); glutSpecialFunc(SpecialKeys); glutDisplayFunc(RenderScene); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"GLEW Error: %s\\n\", glewGetErrorString(err)); return 1; } SetupRC(); glutMainLoop(); return 0; } 4.4 透视投影 #include \"GLTools.h\" #include \"GLMatrixStack.h\" #include \"GLFrame.h\" #include \"GLFrustum.h\" #include \"GLGeometryTransform.h\" #include \"GLBatch.h\" #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLFrame viewFrame; GLFrustum viewFrustum; GLBatch tubeBatch; GLBatch innerBatch; GLMatrixStack modelViewMatix; GLMatrixStack projectionMatrix; GLGeometryTransform transformPipeline; GLShaderManager shaderManager; void RenderScene(void) { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glEnable(GL_DEPTH_TEST); modelViewMatix.PushMatrix(viewFrame); GLfloat vRed[] = { 1.0f, 0.0f, 0.0f, 1.0f }; GLfloat vGray[] = { 0.75f, 0.75f, 0.75f, 1.0f }; shaderManager.UseStockShader(GLT_SHADER_DEFAULT_LIGHT, transformPipeline.GetModelViewMatrix(), transformPipeline.GetProjectionMatrix(), vRed); tubeBatch.Draw(); shaderManager.UseStockShader(GLT_SHADER_DEFAULT_LIGHT, transformPipeline.GetModelViewMatrix(), transformPipeline.GetProjectionMatrix(), vGray); innerBatch.Draw(); modelViewMatix.PopMatrix(); glutSwapBuffers(); } void SetupRC() { glClearColor(0.0f, 0.0f, 0.75f, 1.0f ); glEnable(GL_DEPTH_TEST); shaderManager.InitializeStockShaders(); viewFrame.MoveForward(450.0f); tubeBatch.Begin(GL_QUADS, 200); float fZ = 100.0f; float bZ = -100.0f; tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, 100.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,fZ); // Right Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,fZ); // Top Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f,fZ); // Bottom Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,fZ); // Top length section //////////////////////////// // Normal points up Y axis tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f,50.0f,bZ); // Bottom section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); // Left section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); // Right Section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); // Pointing straight out Z // Left Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,fZ); // Right Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,fZ); // Top Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f,fZ); // Bottom Panel tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,fZ); // Top length section //////////////////////////// // Normal points up Y axis tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, 1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f,50.0f,bZ); // Bottom section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(0.0f, -1.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); // Left section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); // Right Section tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, fZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Normal3f(-1.0f, 0.0f, 0.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, fZ); // Left Panel tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f,50.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-50.0f, 50.0f, bZ); // Right Panel tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f,-50.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(50.0f, 50.0f, bZ); // Top Panel tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, 35.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 35.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, 50.0f, bZ); // Bottom Panel tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -35.0f,bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -50.0f, bZ); tubeBatch.Normal3f(0.0f, 0.0f, -1.0f); tubeBatch.Color4f(1.0f, 0.0f, 0.0f, 1.0f); tubeBatch.Vertex3f(-35.0f, -35.0f, bZ); tubeBatch.End(); innerBatch.Begin(GL_QUADS, 40); // Insides ///////////////////////////// // Normal points up Y axis innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f,35.0f,bZ); // Bottom section innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(0.0f, 1.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, fZ); // Left section innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, 35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(-35.0f, -35.0f, fZ); // Right Section innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, fZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, -35.0f, bZ); innerBatch.Color4f(0.75f, 0.75f, 0.75f, 1.0f); innerBatch.Normal3f(-1.0f, 0.0f, 0.0f); innerBatch.Vertex3f(35.0f, 35.0f, bZ); innerBatch.End(); } void SpecialKeys(int key, int x, int y) { if(key == GLUT_KEY_UP) viewFrame.RotateWorld(m3dDegToRad(-5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_DOWN) viewFrame.RotateWorld(m3dDegToRad(5.0), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_LEFT) viewFrame.RotateWorld(m3dDegToRad(-5.0), 0.0f, 1.0f, 0.0f); if(key == GLUT_KEY_RIGHT) viewFrame.RotateWorld(m3dDegToRad(5.0), 0.0f, 1.0f, 0.0f); // Refresh the Window glutPostRedisplay(); } void ChangeSize(int w, int h) { if(h == 0) h = 1; glViewport(0, 0, w, h); //设置正投影矩阵 //viewFrustum.SetOrthographic(-130.0f, 130.0f, -130.0f, 130.0f, -130.0f, 130.0f); viewFrustum.SetPerspective(35.0f, float(w)/float(h), 1.0f, 1000.0f); projectionMatrix.LoadMatrix(viewFrustum.GetProjectionMatrix()); transformPipeline.SetMatrixStacks(modelViewMatix, projectionMatrix); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH | GLUT_STENCIL); glutInitWindowSize(800, 600); glutCreateWindow(\"Perspective Projection Example\"); glutReshapeFunc(ChangeSize); glutSpecialFunc(SpecialKeys); glutDisplayFunc(RenderScene); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"GLEW Error: %s\\n\", glewGetErrorString(err)); return 1; } SetupRC(); glutMainLoop(); return 0; } "},"pages/opengl/05_OpenGL_triangle_batch_class_drawing_case.html":{"url":"pages/opengl/05_OpenGL_triangle_batch_class_drawing_case.html","title":"5.OpenGL三角形批次类绘制案例","keywords":"","body":"5.OpenGL三角形批次类绘制案例 源码 // Objects.cpp // OpenGL SuperBible, Chapter 4 // Demonstrates GLTools built-in objects // Program by Richard S. Wright Jr. #include \"GLTools.h\" // OpenGL toolkit #include \"GLMatrixStack.h\" #include \"GLFrame.h\" #include \"GLFrustum.h\" #include \"GLBatch.h\" #include \"GLGeometryTransform.h\" #include \"StopWatch.h\" #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLShaderManager shaderManager; GLMatrixStack modelViewMatrix; GLMatrixStack projectionMatrix; //观察者位置 GLFrame cameraFrame; //世界坐标位置 GLFrame objectFrame; //视景体，用来构造投影矩阵 GLFrustum viewFrustum; //三角形批次类 GLTriangleBatch CC_Triangle; //球 GLTriangleBatch sphereBatch; //环 GLTriangleBatch torusBatch; //圆柱 GLTriangleBatch cylinderBatch; //锥 GLTriangleBatch coneBatch; //磁盘 GLTriangleBatch diskBatch; GLGeometryTransform transformPipeline; M3DMatrix44f shadowMatrix; GLfloat vGreen[] = { 0.0f, 1.0f, 0.0f, 1.0f }; GLfloat vBlack[] = { 0.0f, 0.0f, 0.0f, 1.0f }; int nStep = 0; // 将上下文中，进行必要的初始化 void SetupRC() { // Black background glClearColor(0.7f, 0.7f, 0.7f, 1.0f ); //初始化固定着色管理器 shaderManager.InitializeStockShaders(); //开启深度测试 glEnable(GL_DEPTH_TEST); //通过GLGeometryTransform管理矩阵堆栈 //使用transformPipeline 管道管理模型视图矩阵堆栈 和 投影矩阵堆栈 transformPipeline.SetMatrixStacks(modelViewMatrix, projectionMatrix); //将观察者坐标位置Z移动往屏幕里移动15个单位位置 //表示离屏幕之间的距离 负数，是往屏幕后面移动；正数，往屏幕前面移动 cameraFrame.MoveForward(-15.0f); //利用三角形批次类构造图形对象 // 球 /* gltMakeSphere(GLTriangleBatch& sphereBatch, GLfloat fRadius, GLint iSlices, GLint iStacks); 参数1：sphereBatch，三角形批次类对象 参数2：fRadius，球体半径 参数3：iSlices，从球体底部堆叠到顶部的三角形带的数量；其实球体是一圈一圈三角形带组成 参数4：iStacks，围绕球体一圈排列的三角形对数 建议：一个对称性较好的球体的片段数量是堆叠数量的2倍，就是iStacks = 2 * iSlices; 绘制球体都是围绕Z轴，这样+z就是球体的顶点，-z就是球体的底部。 */ gltMakeSphere(sphereBatch, 3.0, 10, 20); // 环面 /* gltMakeTorus(GLTriangleBatch& torusBatch, GLfloat majorRadius, GLfloat minorRadius, GLint numMajor, GLint numMinor); 参数1：torusBatch，三角形批次类对象 参数2：majorRadius,甜甜圈中心到外边缘的半径 参数3：minorRadius,甜甜圈中心到内边缘的半径 参数4：numMajor,沿着主半径的三角形数量 参数5：numMinor,沿着内部较小半径的三角形数量 */ gltMakeTorus(torusBatch, 3.0f, 0.75f, 15, 15); // 圆柱 /* void gltMakeCylinder(GLTriangleBatch& cylinderBatch, GLfloat baseRadius, GLfloat topRadius, GLfloat fLength, GLint numSlices, GLint numStacks); 参数1：cylinderBatch，三角形批次类对象 参数2：baseRadius,底部半径 参数3：topRadius,头部半径 参数4：fLength,圆形长度 参数5：numSlices,围绕Z轴的三角形对的数量 参数6：numStacks,圆柱底部堆叠到顶部圆环的三角形数量 */ gltMakeCylinder(cylinderBatch, 2.0f, 2.0f, 3.0f, 15, 2); //锥 /* void gltMakeCylinder(GLTriangleBatch& cylinderBatch, GLfloat baseRadius, GLfloat topRadius, GLfloat fLength, GLint numSlices, GLint numStacks); 参数1：cylinderBatch，三角形批次类对象 参数2：baseRadius,底部半径 参数3：topRadius,头部半径 参数4：fLength,圆形长度 参数5：numSlices,围绕Z轴的三角形对的数量 参数6：numStacks,圆柱底部堆叠到顶部圆环的三角形数量 */ //圆柱体，从0开始向Z轴正方向延伸。 //圆锥体，是一端的半径为0，另一端半径可指定。 gltMakeCylinder(coneBatch, 2.0f, 0.0f, 3.0f, 13, 2); // 磁盘 /* void gltMakeDisk(GLTriangleBatch& diskBatch, GLfloat innerRadius, GLfloat outerRadius, GLint nSlices, GLint nStacks); 参数1:diskBatch，三角形批次类对象 参数2:innerRadius,内圆半径 参数3:outerRadius,外圆半径 参数4:nSlices,圆盘围绕Z轴的三角形对的数量 参数5:nStacks,圆盘外网到内围的三角形数量 */ gltMakeDisk(diskBatch, 1.5f, 3.0f, 13, 3); } void DrawWireFramedBatch(GLTriangleBatch* pBatch) { //平面着色器，绘制三角形 shaderManager.UseStockShader(GLT_SHADER_FLAT, transformPipeline.GetModelViewProjectionMatrix(), vGreen); //传过来的参数，对应不同的图形Batch pBatch->Draw(); // 画出黑色轮廓 glPolygonOffset(-1.0f, -1.0f); //开启处理线 glEnable(GL_LINE_SMOOTH); //开启混合功能 glEnable(GL_BLEND); //颜色混合 //表示源颜色乘以自身的alpha 值，目标颜色乘以1.0减去源颜色的alpha值，这样一来，源颜色的alpha值越大，则产生的新颜色中源颜色所占比例就越大，而目标颜色所占比例则减 小。这种情况下，我们可以简单的将源颜色的alpha值理解为“不透明度”。这也是混合时最常用的方式。 glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); //通过程序点大小模式来设置点的大小 glEnable(GL_POLYGON_OFFSET_LINE); //多边形模型(背面、线) 将多边形背面设为线框模式 glPolygonMode(GL_FRONT_AND_BACK, GL_LINE); //线条宽度 glLineWidth(2.5f); //平面着色器绘制线条 shaderManager.UseStockShader(GLT_SHADER_FLAT, transformPipeline.GetModelViewProjectionMatrix(), vBlack); pBatch->Draw(); // 恢复多边形模式和深度测试 glPolygonMode(GL_FRONT_AND_BACK, GL_FILL); glDisable(GL_POLYGON_OFFSET_LINE); glLineWidth(1.0f); glDisable(GL_BLEND); glDisable(GL_LINE_SMOOTH); } //召唤场景 void RenderScene(void) { //用当前清除颜色清除窗口背景 glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT | GL_STENCIL_BUFFER_BIT); //模型视图矩阵栈堆，压栈 modelViewMatrix.PushMatrix(); //获取摄像头矩阵 M3DMatrix44f mCamera; //从camereaFrame中获取矩阵到mCamera cameraFrame.GetCameraMatrix(mCamera); //模型视图堆栈的 矩阵与mCamera矩阵 相乘之后，存储到modelViewMatrix矩阵堆栈中 modelViewMatrix.MultMatrix(mCamera); //创建矩阵mObjectFrame M3DMatrix44f mObjectFrame; //从ObjectFrame 获取矩阵到mOjectFrame中 objectFrame.GetMatrix(mObjectFrame); //将modelViewMatrix 的堆栈中的矩阵 与 mOjbectFrame 矩阵相乘，存储到modelViewMatrix矩阵堆栈中 modelViewMatrix.MultMatrix(mObjectFrame); //使用平面着色器 //参数1：类型 //参数2：通过transformPipeline获取模型视图矩阵 //参数3：颜色 shaderManager.UseStockShader(GLT_SHADER_FLAT, transformPipeline.GetModelViewProjectionMatrix(), vBlack); //判断你目前是绘制第几个图形 switch(nStep) { case 0: DrawWireFramedBatch(&sphereBatch); break; case 1: DrawWireFramedBatch(&torusBatch); break; case 2: DrawWireFramedBatch(&cylinderBatch); break; case 3: DrawWireFramedBatch(&coneBatch); break; case 4: DrawWireFramedBatch(&diskBatch); break; } modelViewMatrix.PopMatrix(); // Flush drawing commands glutSwapBuffers(); } //上下左右，移动图形 void SpecialKeys(int key, int x, int y) { if(key == GLUT_KEY_UP) //移动世界坐标系，而不是去移动物体。 //将世界坐标系在X方向移动-5.0 objectFrame.RotateWorld(m3dDegToRad(-5.0f), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_DOWN) objectFrame.RotateWorld(m3dDegToRad(5.0f), 1.0f, 0.0f, 0.0f); if(key == GLUT_KEY_LEFT) objectFrame.RotateWorld(m3dDegToRad(-5.0f), 0.0f, 1.0f, 0.0f); if(key == GLUT_KEY_RIGHT) objectFrame.RotateWorld(m3dDegToRad(5.0f), 0.0f, 1.0f, 0.0f); glutPostRedisplay(); } //点击空格，切换渲染图形 void KeyPressFunc(unsigned char key, int x, int y) { if(key == 32) { nStep++; if(nStep > 4) nStep = 0; } switch(nStep) { case 0: glutSetWindowTitle(\"Sphere\"); break; case 1: glutSetWindowTitle(\"Torus\"); break; case 2: glutSetWindowTitle(\"Cylinder\"); break; case 3: glutSetWindowTitle(\"Cone\"); break; case 4: glutSetWindowTitle(\"Disk\"); break; } glutPostRedisplay(); } void ChangeSize(int w, int h) { glViewport(0, 0, w, h); //透视投影 viewFrustum.SetPerspective(35.0f, float(w) / float(h), 1.0f, 500.0f); //projectionMatrix 矩阵堆栈 加载透视投影矩阵 projectionMatrix.LoadMatrix(viewFrustum.GetProjectionMatrix()); //modelViewMatrix 矩阵堆栈 加载单元矩阵 modelViewMatrix.LoadIdentity(); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH | GLUT_STENCIL); glutInitWindowSize(800, 600); glutCreateWindow(\"Sphere\"); glutReshapeFunc(ChangeSize); glutKeyboardFunc(KeyPressFunc); glutSpecialFunc(SpecialKeys); glutDisplayFunc(RenderScene); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"GLEW Error: %s\\n\", glewGetErrorString(err)); return 1; } SetupRC(); glutMainLoop(); return 0; } "},"pages/opengl/06_OpenGL_model_view_matrix_drawing_case.html":{"url":"pages/opengl/06_OpenGL_model_view_matrix_drawing_case.html","title":"6.OpenGL模型视图矩阵绘制案例","keywords":"","body":"6.OpenGL模型视图矩阵绘制案例 源码 #include \"GLTools.h\" #include \"GLMatrixStack.h\" #include \"GLFrame.h\" #include \"GLFrustum.h\" #include \"GLGeometryTransform.h\" #include \"GLBatch.h\" #include \"StopWatch.h\" #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif GLFrustum viewFrustum; GLShaderManager shaderManager; GLTriangleBatch torusBatch; // 设置视口和投影矩阵 void ChangeSize(int w, int h) { //防止除以零 if(h == 0) h = 1; //将视口设置为窗口尺寸 glViewport(0, 0, w, h); //设置透视投影 viewFrustum.SetPerspective(35.0f, float(w)/float(h), 1.0f, 1000.0f); } //召唤场景 void RenderScene(void) { //建立基于时间变化的动画 static CStopWatch rotTimer; //当前时间 * 60s float yRot = rotTimer.GetElapsedSeconds() * 60.0f; //清除屏幕、深度缓存区 glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); // 矩阵变量 M3DMatrix44f mTranslate, mRotate, mModelview, mModelViewProjection; //创建一个4*4矩阵变量，将花托沿着Z轴负方向移动2.5个单位长度 m3dTranslationMatrix44(mTranslate, 0.0f, 0.0f, -2.5f); //创建一个4*4矩阵变量，将花托在Y轴上渲染yRot度，yRot根据经过时间设置动画帧率 m3dRotationMatrix44(mRotate, m3dDegToRad(yRot), 0.0f, 1.0f, 0.0f); // m3dRotationMatrix44(mRotate, m3dDegToRad(yRot), 1.0f, 0.0f, 0.0f); //为mModerView 通过矩阵旋转矩阵、移动矩阵相乘，将结果添加到mModerView上 m3dMatrixMultiply44(mModelview, mTranslate, mRotate); // 将模型视图矩阵的投影矩阵， // 将投影矩阵乘以模型视图矩阵，将变化结果通过矩阵乘法应用到mModelViewProjection矩阵上 m3dMatrixMultiply44(mModelViewProjection, viewFrustum.GetProjectionMatrix(),mModelview); // 将这个已完成的矩阵传递给着色器，并渲染这个圆环面。 //绘图颜色 GLfloat vBlack[] = { 0.0f, 0.0f, 0.0f, 1.0f }; //通过平面着色器提交矩阵，和颜色。 //平面着色器的工作只是使用提供矩阵来顶点来进行转换，并且使用指定的颜色对几何图形进行着色以得到实心几何图形 shaderManager.UseStockShader(GLT_SHADER_FLAT, mModelViewProjection, vBlack); //开始绘图 torusBatch.Draw(); // 交换缓冲区，并立即刷新 glutSwapBuffers(); glutPostRedisplay(); } void SetupRC() { glClearColor(0.8f, 0.8f, 0.8f, 1.0f ); glEnable(GL_DEPTH_TEST); shaderManager.InitializeStockShaders(); // 形成一个圆环 gltMakeTorus(torusBatch, 0.4f, 0.15f, 30, 30); //形成一个球 gltMakeSphere(torusBatch, 0.4f, 10, 20); glPolygonMode(GL_FRONT_AND_BACK, GL_LINE); } int main(int argc, char* argv[]) { gltSetWorkingDirectory(argv[0]); glutInit(&argc, argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH | GLUT_STENCIL); glutInitWindowSize(800, 600); glutCreateWindow(\"ModelViewProjection Example\"); glutReshapeFunc(ChangeSize); glutDisplayFunc(RenderScene); GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr, \"GLEW Error: %s\\n\", glewGetErrorString(err)); return 1; } SetupRC(); glutMainLoop(); return 0; } "},"pages/opengl/07_OpenGL_vertex_change_pipeline_detailed_explanation.html":{"url":"pages/opengl/07_OpenGL_vertex_change_pipeline_detailed_explanation.html","title":"7.OpenGL顶点变化管线详解","keywords":"","body":"7.OpenGL顶点变化管线详解 一、顶点变化管线 二、使用矩阵堆栈 // 类型 GLMatrixStack::GLMatrixStack(int iStackDepth = 64); // 在堆栈顶部载⼊一个单元矩阵 void GLMatrixStack::LoadIdentity(void); // 在堆栈顶部载⼊任何矩阵 // 参数:4*4矩阵 void GLMatrixStack::LoadMatrix(const M3DMatrix44f m); // 矩阵乘以矩阵堆栈顶部矩阵，相乘结果存储到堆栈的顶部 void GLMatrixStack::MultMatrix(const M3DMatrix44f); // 获取矩阵堆栈顶部的值 GetMatrix 函数 // 为了适应GLShaderMananger的使⽤，或者获取顶部矩阵的副本 const M3DMatrix44f & GLMatrixStack::GetMatrix(void); void GLMatrixStack::GetMatrix(M3DMatrix44f mMatrix); 三、压栈、出栈 压栈: 存储⼀个状态 出栈: 恢复⼀个状态 // 将当前矩阵压⼊堆栈 void GLMatrixStack::PushMatrix(void); // 将M3DMatrix44f 矩阵对象压入当前矩阵堆栈 void PushMatrix(const M3DMatrix44f mMatrix); // 将GLFame对象压入矩阵对象 void PushMatrix(GLFame &frame); // 出栈(出栈指的是移除顶部的矩阵对象) void GLMatrixStack::PopMatrix(void); 四、仿射变换 //Rotate 函数angle参数是传递的度数，⽽不是弧度 void MatrixStack::Rotate(GLfloat angle,GLfloat x,GLfloat y,GLfloat z); void MatrixStack::Translate(GLfloat x,GLfloat y,GLfloat z); void MatrixStack::Scale(GLfloat x,GLfloat y,GLfloat z); 五、GLFrame 使⽤照相机(摄像机) 和 ⻆色帧 进⾏移动 class GLFrame { protected: // Where am I? M3DVector3f vOrigin; // Where am I going? M3DVector3f vForward; // Which way is up? M3DVector3f vUp; } // 将堆栈的顶部压入任何矩阵 void GLMatrixStack::LoadMatrix(GLFrame &frame); // 矩阵乘以矩阵堆栈顶部的矩阵。相乘结果存储在堆栈的顶部 void GLMatrixStack::MultMatrix(GLFrame &frame); // 将当前的矩阵压栈 void GLMatrixStack::PushMatrix(GLFrame &frame); // GLFrame函数，这个函数⽤来检索条件适合的照相矩阵 void GetCameraMatrix(M3DMatrix44f m,bool bRotationOnly = flase); "},"pages/opengl/08_OpenGL_revolution_and_rotation_case.html":{"url":"pages/opengl/08_OpenGL_revolution_and_rotation_case.html","title":"8.OpenGL公转自转案例","keywords":"","body":"8.OpenGL公转自转案例 源码 #include \"GLTools.h\" #include \"GLShaderManager.h\" #include \"GLFrustum.h\" #include \"GLBatch.h\" #include \"GLMatrixStack.h\" #include \"GLGeometryTransform.h\" #include \"StopWatch.h\" #include #include #ifdef __APPLE__ #include #else #define FREEGLUT_STATIC #include #endif //**4、添加附加随机球 #define NUM_SPHERES 50 GLFrame spheres[NUM_SPHERES]; GLShaderManager shaderManager; // 着色器管理器 GLMatrixStack modelViewMatrix; // 模型视图矩阵 GLMatrixStack projectionMatrix; // 投影矩阵 GLFrustum viewFrustum; // 视景体 GLGeometryTransform transformPipeline; // 几何图形变换管道 GLTriangleBatch torusBatch; // 花托批处理 GLBatch floorBatch; // 地板批处理 //**2、定义公转球的批处理（公转自转）** GLTriangleBatch sphereBatch; //球批处理 //**3、角色帧 照相机角色帧（全局照相机实例） GLFrame cameraFrame; void SetupRC() { // 初始化着色器管理器 shaderManager.InitializeStockShaders(); //开启深度测试 glEnable(GL_DEPTH_TEST); //开启多边形模型 //**4、关闭线框渲染模式 //glPolygonMode(GL_FRONT_AND_BACK, GL_LINE); //设置清屏颜色到颜色缓存区 glClearColor(0.0f, 0.0f, 0.0f, 1.0f); //绘制甜甜圈 gltMakeTorus(torusBatch, 0.4f, 0.15f, 30, 30); //**2、 绘制球(公转自转)** gltMakeSphere(sphereBatch, 0.1f, 26, 13); //往地板floorBatch批处理中添加顶点数据 floorBatch.Begin(GL_LINES, 324); for(GLfloat x = -20.0; x "},"pages/git/related_links.html":{"url":"pages/git/related_links.html","title":"相关链接","keywords":"","body":"相关链接 git如何删除远端不存在的本地分支？ 关于Git协作，除了知道merge，你可能还需要知道rebase git写错分支，如何将一个分支上的修改转移到另一个分支上 Xcode连接git@osc Rebase git rebase 还是 merge的使用场景最通俗的解释 git rebase -i合并多次提交 GIT上fork的项目获取最新源代码 Git 命令收集 Git 删除某一次提交 Git rebase合并多条commit记录 git cherry-pick 教程 git stash 用法总结和注意点 "},"pages/git/rebase_use.html":{"url":"pages/git/rebase_use.html","title":"rebase使用","keywords":"","body":"rebase使用 查看日志 git log --oneline -10 git reflog 拉取远程 git fetch team 基变 git rebase team/feature/v3.7.0 git rebase --continue git rebase -i cea405e6f 强推 git push origin feature/v3.7.0 -f git reflog git cherry-pick a89dbd17f git config --global pull.rebase true "},"pages/security/01_Commonly_used_tools.html":{"url":"pages/security/01_Commonly_used_tools.html","title":"1.常用工具","keywords":"","body":"1.常用工具 Alfred 3.2 Mac破解版 安装地址 “XXX.app 已损坏，打不开。您应该将它移到废纸篓”，Mac应用程序无法打开或文件损坏的处理方法 5分钟上手Mac效率神器Alfred以及Alfred常用操作 Setting the Terminal/Shell to \"Custom\" (iTerm). Mac OS 终端利器 iTerm2 安装教程 How do I hide the “user@hostname” info #39 MAC TERMINAL终端或ITERM2出现问号解决方案 git - 警告:不建议使用此脚本，请参阅git-completion.zsh SwitchHosts 安装地址 如何解决类似 curl: (7) Failed to connect to raw.githubusercontent.com port 443: Connection refused 的问题 #10 OpenInTerminal 安装教程 ios-app-signer 下载地址 idapro IDA Pro 7.0.zip（MAC版本，下载下来可直接打开即可，在10.15.4测试通过） IDA Pro Mac版 V7.0-pc6 IDA Pro 7.0 for Mac(静态反编译软件) v7.0.170914中文版 IDA Pro 7.0 macOS 10.15安装 cycript 安装Cycript报错找不到libruby.2.0.0.dylib 安装cycript遇到的问题 iOS 逆向必备工具和安装过程 csrutil disable sudo mkdir -p /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/ sudo ln -s /System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/lib/libruby.2.6.dylib /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/lib/libruby.2.0.0.dylib mac下安装ruby2.2.0 MacOS 下安装 Ruby pod update报错： Traceback (most recent call last): 2: from /usr/local/bin/pod:23:in `' 1: from /Library/Ruby/Site/2.6.0/rubygems.rb:296:in `activate_bin_path' 使用如下命令解决，参考链接： sudo gem install cocoapods "},"pages/security/02_MonkeyDev.html":{"url":"pages/security/02_MonkeyDev.html","title":"2.MonkeyDev","keywords":"","body":"2.MonkeyDev 一、安装 安装步骤 xcode 12 Types.xcspec not found #266 MonkeyDev插件的安装 二、运行工程 使用Monkey创建工程，导入.app包之后，配置好工程证书后，点击运行发现如下错误： error: Signing for “monkeyTestDylib” requires a development team 解决办法： 选中“monkeytestDylib”Target->Build Settings->搜索Code Sign Style->Manual。 Development Team再设置下。 源码 "},"pages/other/Essential_for_installation.html":{"url":"pages/other/Essential_for_installation.html","title":"装机必备","keywords":"","body":"装机必备 一、升级系统 Mac系统如何显示隐藏文件 mac软件网址 xcode下载 如何生成 SSH 密钥 二、触摸板设置 轻点来点按：不用按压触摸板即可点击 三指拖拽：辅助功能->指针控制->触摸板选项->鼠标与触摸板->启用拖动->三指拖移 三、软件安装 Amphetamine：阻止mac睡眠 Sourcetree：代码管理 The Unarchiver：解压缩 Google Chrome：浏览器 ClashX：翻墙 DevCleaner：清理Xcode Xcode：开发 Sumblime Text：文本编辑 OmniDiskSweeper TeamViewer royaltsx：服务器上传 AE/AI/DW/InDesign/PR/Sketch/Principle/字魂：设计类 Word/Excel/PPT：微软 MenuMeters：监控CPU aifred3：红帽子 oh-my-zsh/iTerm：命令行 SwitchHosts：切Host My Hosts 185.199.108.133 raw.githubusercontent.com 199.232.68.133 user-images.githubusercontent.com 199.232.68.133 avatars2.githubusercontent.com 199.232.68.133 avatars1.githubusercontent.com Postman：请求测试 SwiftFormat for Xcode OBS：录屏幕 MachOView/iOS App Signer：越狱 DeepL：翻译 Wacom：数位板 Woodpecker：iOS屏幕测试 libreoffice：打开word Paintbrush：画板 Vysor：Android手机投屏电脑 Android File Transfer：Android数据传输 VLC media player Charles WiresShark 四、安装brew macos如何科学的安装homebrew跟npm? brew update 报错 \"fatal: Could not resolve HEAD to a revision\" 安装brew的正确姿势 Mac安装git，brew出现的问题 五、安装node MAC 安装BREW跟NODE mac管理及更新node版本 六、安装gitbook GitBook 安装以及使用 安装Gitbook GitBook快速生成PDF 配置ebook问题 $ sudo ln -s ~/Applications/calibre.app/Contents/MacOS/ebook-convert /usr/bin 七、安装CocoaPods ruby管理 iOS 安装CocoaPods 临时解决GitHub的raw.githubusercontent.com无法连接问题 八、安装PS Adobe Photoshop 2021 For Mac v22.3 PS中文破解版 九、Flutter flutter2.2.1 "},"pages/other/Common_links.html":{"url":"pages/other/Common_links.html","title":"常用链接","keywords":"","body":"常用链接 iOS·问题汇总 UITextField PlaceHolder居中显示问题 iOS 10、设置导航栏全透明 两种iOS隐藏导航栏的正确方法 让超出父视图范围的子视图响应事件，在UIView范围外响应点击 iOS时间的处理 ios 消除 字符串 首尾空格 iOS优美的侧滑返回FDFullscreenPopGesture iOS 设置屏幕常亮，延长休眠时间 arm64、armv7、armv7s iOS获取手机型号 Cannot find protocol definition for XXX ios – 奇怪的UIView-Encapsulated-Layout-Height错误 fix:iOS 13.1.3,textView点击回调3次 \"Double-quoted\" issue in latest version of Firebase rem布局在webview中页面错乱解决办法 解決UIWebView target = '_blank'不能彈出 Cocoapod with optional subspecs is not installing, if subspecs in the Podfile and another dependent Cocoapod podspec doesn't match Can I have an init func in a protocol? 构造方法不触发didSet iOS9中调用其他APP时出现This app is not allowed to query 微信怎么唤起app? facebook链接中分享的图片 line链接中分享的图片 Swift - encode URL Class Only Protocols In Swift 5 git删除远程分支报错:remote ref does not exist ERROR | [iOS] unknown: Encountered an unknown error (/usr/bin/xcrun simctl list -j devices Mac解疑：处理 Adobe Genuine Software Integrity Service assigning to ‘NSString *_strong’ from ‘const NSString’ discards qualifiers iOS10 NSLog限制为1024个字符字符串 Auto property synthesis will not synthesize property ‘delegate’;it will be implemented by its superc NSPredicate 谓词 更改 macOS 用户帐户和个人文件夹的名称 苹果 iOS 13 新增的 sign with Apple API 是如何实现隐私保护的？ Xcode 11新建项目多了Scenedelegate 使用 Xcode 13.2.1 和 iOS 15.4 设备 圆角和阴影并存（Swift和OC） Swiftlint warning : For Where Violation: where clauses are preferred over a single if inside a for. (for_where) 【ios】_UITemporaryLayoutWidth是什麼，為什麼它打破了我的約束？ Git 提交错了不用慌，这三招帮你修改记录 解决Xcode14 pod签名问题 iOS UILabel 添加查看更多 UILabel中的NSAttributedString尾部截断 UITableView或UICollectionView的cell中嵌套UICollectionView后,第二层的CollectionViewCell点击无响应的问题 ScrollView嵌套 iOS开发实战 - 解决UIScrollView嵌套滑动手势冲突 LYEmbedScrollView JXCategoryView MXParallaxHeader 关于嵌套滚动现实的讨论 iOS·新知识 使用 Swift Package Manager 集成依赖库 Swift化零为整：Reduce方法详解 swift3.0中fileprivate，private使用 Swift与泛型编程第四弹：类型擦除 iOS 插件化开发(动态库研究) 在 iOS 上下载文件 iOS归档看这篇就够了 ios - Swift URL查询字符串获取参数 Swift打印变量内存地址 swift小知识点之打印对象的地址 Swift 如何声明某个属性已过期 swift5.0 字符串截取 iOS开发之字符串(NSString)的拼接 iOS - NSDate分类-判断时间是否为今天,昨天,一周内,年月日 IOS 保留小数点后几位 Swift开发小技巧系列 - 浮点型数据的四舍五入 UIPresentationController简介 不要用子类！Swift的核心是面向协议 APP速度优化--启动优化 Swift只有上半部分圆角View XCode模拟器屏幕录制/录屏 UIView的autoresizingMask 关于Swift：如何对数组进行分组 SnapKit 关于数组Array的扩展—— 自适应宽度、垂直、水平、九宫格布局 do while妙用 iOS中取消延迟执行函数 iOS 阿拉伯语 RTL适配 Swift - 实现图片（UIImage）的水平翻转(镜像)，垂直翻转 UIView动画 Swift 图片拉伸，屡试不爽！！！ 为什么我们应该避免在 Swift 结构中使用闭包？ CocoaPods清理本地缓存 ExclusiveTouch OC中基于Runtime机制hook函数的多种方法 iOS自定义TableView索引 自定义模板 iOS 高效开发之 - 3分钟实现自定义 Xcode 初始化的模板 Xcode 自定义模板 iOS·审核 检查iOS项目中是否还有使用UIWebView iOS APP内测邀请之TestFlight iOS·多线程 实现自定义NSOperation 开始使用Operation Queue吧 iOS开发 自定义NSOPeration NSOperation property overrides (isExecuting / isFinished) 并发教程 iOS 多线程 - Operation iOS·性能优化 iOS 关于后台持续运行 iOS性能优化 iOS按钮倒计时在进入后台不继续计时的处理 iOS 后台数据处理 background fetch 动画 iOS动画事物（CATransaction） 工具 熊猫压缩 远程 udid获取 JSON格式化 代码格式化插件 App图标在线制作 压缩一个PDF文件 UIImage 二分压缩图片 Woodpecke‪r‬ 图片裁剪 临时邮箱、临时邮、临时电子邮箱、24小时邮箱 JSON格式化 Woodpecker使用 Xcode自定义代码块 时间戳cuo google play store 颜色代码表 阿里网盘资源 在线工具 语法检查 排序算法 Gitbook 新版gitbook导出pdf gitbook教程 推荐12个实用的gitbook插件 mermaid插件 mermaid-gb3 mermaid文档 GitBook 插件 PlantUML 简介 GitBookW3C Gitbook详解（四）-配置和说明详解 定义右面页面的宽度 npmjs GitBook 本地使用排雷，及导出基本可用的 PDF 版本 案例 jim GITBOOK npm的官方网站-gitbook-plugin npm的官方网站-gitbook-theme GitBook插件官网 百度统计 Gitbook 静态站点 加入谷歌分析等网站统计代码 百度统计助手 百度统计后台 Markdown 用 Markdown 制作简历 Markdown Guide MarkDown代码块高亮 在 markdown 中生成并导出思维导图的 Gitbook 插件 gitbook 3.2.3及之后流程图解决方案 Markdown 中文文档 markdown编辑 学生、工作人士如何才能更优雅地记笔记、写文档?（Markdown教程，详细到超乎你想象） 使用 Markdown 时，如何为文字加下划线？ Markdown 简介 MarkDown中如何换行？ 基础配置 简历 iOS程序员简历模板 好看的博客 bawn YouXianMing 笑忘书店 八年iOS架构师教你如何一举拿下iOS工程师的Offer,（附面试技巧） Cocoapods CocoaPods安装方法-2020.05.25 XCode 10中修改cocoapods中的源码编译不生效的解决方法 Cocoapods整理（三）——编写podspec文件 装机步骤 iOS 安装CocoaPods Mac系统如何显示隐藏文件 临时解决GitHub的raw.githubusercontent.com无法连接问题 Mac安装git，brew出现的问题 macos如何科学的安装homebrew跟npm? 安装brew的正确姿势 GitBook 安装以及使用 mac管理及更新node版本 安装Gitbook mac软件网址 xcode下载 开源项目 iOSExperience functionList GitBook 上有哪些十分优秀的已经完成的书？ 滴滴的哆啦A梦组件 iOS 打点 闪屏demo QGTextField URLNavigator 网络监控 Swift-30-Projects iOSKeyPointExploration 语法糖 TTBaseUIKit GAppFramework SwiftTheme NerdyUI Tangram-iOS YNSearch IOS网络请求的简单封装设计 YTKNetwork QMUI_iOS 代码分类 好库编程网 SDUserDefaults LCHelper geniusDemo 2018 iOS 三方库(仅供方面查看) macos 12.3.1 Xcode 13.3 最新objc 838源码更新 国家icon Stevia MVVM MVVMReactiveCocoa BigShow1949 Monkey MHDevelopExample_Objective_C MVVMDemo 地图 map BMKLocationKit iOS在APP中调用第三方地图地图（苹果,高德，百度，腾讯） 经纬度查询 学习 尚德 自考 浙江工业大学·自考 潭州课堂 swift51 csdn 苹果官方文档 苹果官方文档-cn 苹果官方文档-en ios-resolution 查询序列号 OpenGL iOS-OpenGL-Tutorials OpenGLES RxSwift Swift - RxSwift的使用详解1（基本介绍、安装配置） RxSwift的学习之路（二）——Subjects 使用 RxTest 来建立基于 RxSwift 的自动化测试 RxSwift中文文档 RxSwift学习 - share(replay:scope:) RXSwift中Driver的使用 RxSwift+Moya网络请求之项目实战 RxMultiCounter RxSwift Error Handling RxDataSource 使用套路与解释 RxSwift 中的老司机 VPN cx vpn·周 vpn·李 有免费的梯子软件 导航栏 iOS系统中导航栏的转场解决方案与最佳实践 IOS-FDFullscreenPopGesture的使用 我的iOS开发笔记——右滑返回手势失效怎么办？ iOS 导航栏的那些事儿 Swift - 导航栏滑动透明渐变效果的实现（透明度随视图滚动而改变） iOS·UI 详解intrinsicContentSize 及 约束优先级／content Hugging／content Compression Resistance iOS开发笔记常用工具之文本宽度和高度计算 iOS 获取webview高度小结 iOS Swift 判断手机机型 已更新 至iPhone12 iOS判断刘海屏幕机型 所有机型 iOS 设置圆角、指定位置圆角及 iOS 11圆角设置 iOS9 Programming - Autolayout (I) 脚本 sudo pip install openpyxl 快捷键 注释：command+option+/ 打开xcode文档：command+shift+0 现在在Xcode中切换标签的快捷方式？ Android Studio常用快捷键汇总（mac） Flutter Flutter 开发文档 其它 医院等级查询 快速记 shell Shell 函数 shell脚本中判断上一个命令是否执行成功 shell bash判断文件或文件夹是否存在 shell脚本：删除当前目录下除了某几个文件之外的其他文件 ruby Ruby 教程 webview缓存 iOS html5使用缓存并及时更新方案总结 Cache-control iOS webView缓存，保证加载最新html iOS: 聊聊 UIWebView 缓存 H5 和移动端 WebView 缓存机制解析与实战 iOS-WKWebView缓存并保证实时性 iOS代码混淆工具 webview开启无图 iOS定制NSURLProtocol实现离线缓存 web离线技术原理 iOS 开发中使用 NSURLProtocol 拦截 HTTP 请求 使用 NSURLProtocol 拦截 APP 内的网络请求 移动 H5 首屏秒开优化方案探讨 JWNetAutoCache tableView https://www.jianshu.com/p/af4bc69839d8 优化UITableViewCell高度计算的那些事 代码规范 《Effective Objective-C》干货三部曲（二）：规范篇 Swift 如何声明某个属性已过期 单元测试 iOS 单元测试--异步测试 OC实现类似泛型效果的json数据解析 架构 iOS应用架构谈 网络层设计方案 iOS - AFNetWorking打印请求时的相关信息 一键登录 IOS客户端接入 gcd iOS GCD处理多个网络请求并发问题 iOS-GCD的串行队列和并行队列的任务及实现 iOS GCD线程同步以及AFNetworking多次请求线程依赖 c++ C++ 中的 inline 用法 在字符串string中实现一个判断函数，该函数功能是统计某一字符串类对象(仅由单词和空格组成)有多少个单词 IM聊天 环信·IM 环信·iOS SDK 快速集成 环信·集成 iOS SDK 前的准备工作 电影 你的名字 后端 docker 腾讯云 算法 算法学习笔记（目录） 菜鸟算法 Swift - Codable Swift - Codable使用小记 Swift - Codable textview iOS开发－UITextView文字排版 UITextView换行问题解决办法 android studio 用户指南 阿语适配 iOS 国际化 - 阿拉伯语的RTL 踩坑心得 Working with Unicode code points in Swift Swift进阶九:字符串 完整的Unicode地址 iOS检测字符串的语言（用于翻译之前检测） Detect Language of NSString 解决关于swift的Array repeating 初始化多个对象问题 简历优化 iOS开发求职者，写一份成功的简历？ 7年iOS开发经验，教你写一份脱颖而出的简历，进入大厂机会翻3倍！ 怎么样在简历中体现出自己的能力与价值? IOS开发工程师简历范文，【工作经历+项目经验+自我评价】怎么写 iOS高级面试简历指导 图片 头像 表 CJIABA这个牌子的手表一般的价格是多少 casio 浪琴efco是什么档次 https://wen.baidu.com/question/1839889959679620900.html Longines 浪琴 tianbo 天博手表 机械 wilon手表 机械 威斯登 swatch 劳力士（ROLEX） 美格尔(MEGIR)手表 moosie rosdn leier fila 统计 百度统计 数据结构 数据结构导论 JAVA 老杜语雀笔记：https://www.yuque.com/docs/share/866abad4-7106-45e7-afcd-245a733b073f?# 《Spring6》密码：mg9bJAVA java多文件上传 java解析cvs Java List排序的几种方式整理 JUnit4中@Before、@After、@Test等注解的作用 JUnit知识点三则 Mockito 应用指南 Servlet 生命周期 单元测试 - JUnit4 详解 Java List排序的几种方式整理 Java 日期加一天或是日期往后减一天 java中比较两个日期的大小 Java 实例 - 字符串分割 The method getSubmittedFileName() is undefined JAVA打包 geowa4/install.sh Linux下 tomcat war自动部署脚本 自动部署war包脚本 接入广告 Google AdSense 申请Google AdSense三次提示“网站已下线或无法访问” SwiftUI Button in SwiftUI SwiftUI text-alignment SwiftUI 给图片或View添加边框和圆角 Icons & Image in SwiftUI iOS SwiftUI 自己动手做个图片与照片选择器Picker 扩展SwiftUI的Color支持十六进制 如何在SwiftUI中给字体加粗 SwiftUI. How to change the placeholder color of the TextField? RandomColor.swift SwiftUI极简教程13:NavigationView导航栏使用 How to Use PhotosPicker in SwiftUI Combine convert Just to AnyPublisher MVVM with SwiftUI + Combine H2Database https://github.com/h2database/h2database Java Connect to H2 Database Examples java ClassNotFoundException for org.h2.Driver [duplicate] H2数据库的导入、导出(CSV) 数据类型 tutorial H2: how to tell if table exists? H2 DataBase入门+整合hibernate+整合mybatis+整合 "},"pages/other/todo.html":{"url":"pages/other/todo.html","title":"TODO","keywords":"","body":"TODO 操作系统概论 第一章 操作系统简介 第二章 进程管理 第一节 进程的描述 第二节 进程的控制 第三节 操作系统内核 第四、五节 进程同步、通信 第六节 线程 第三章 进程调度与死锁 第一、二节 进程调度的功能与时机、算法 第三、四、五节 实时系统中的调度、进程切换、多处理器调度 第六节 死锁 第四章 内存管理 第一、二、三节 存储器的层次结构 第四节 基本分页存储管理方式 第五节 基于分页的虚拟存储系统 第五章 文件系统 第一节 文件 第二节 目录 第三节 文件系统的实现 第六章 I/O设备管理 第一节 I/O系统的组成 第二节 I/O控制方式 第三节 缓冲管理 第四节 设备分配 第五节 I/O软件管理 第六节 磁盘管理 计算机网络原理 1.1.精讲1 1.2.精讲2 1.3.精讲3 1.5.精讲5 1.6.精讲6 信息系统开发与管理 第一章 管理信息系统导论 第一节：管理信息系统概念及其发展 第二节：管理信息系统的分类 第三节：管理信息系统的结构 第四节：管理信息系统的几种典型应用 第二章 管理信息系统的基本知识 第一节：管理的基本知识 第二节：信息的基本知识 第三节：系统的基本知识 第四节：信息技术的基本知识 第三章 系统开发方法概述 第一节：管理信息系统开发的基本问题 第二节：管理信息系统的开发方法 第三节：结构化开发方法的开发过程 第四节：开发过程组织与管理方法 第四章 总体规划 第一节：总体规划的目的和步骤 第二节：系统规划的主要步骤 第五章 系统分析 第一节：系统分析概论 第二节：详细调查 第三节：业务流程分析 第四节：数据流程分析的概念 第五节：新系统逻辑模型 第六节：系统分析报告 现代汉语 第一章 第二章 游戏UI设计 平面构成的基本形与骨骼 LFLiveKit LFLiveKit源码之LFLivePreview C++程序设计 第一章 C++语言简介 第二章 面向对象的基本概念 第三章 类和对象进阶 第四章 运算符重载 第五章 类的继承与派生 第六章 多态与虚函数 第七章 输入/输出流 第八章 文件操作 第九章 函数模版与类模板 RxSwift系列 4.Observer - 自定义可绑定属性 5.Subjects、Variables 8.条件和布尔操作符：amb、takeWhile、skipWhile等 9.结合操作符：startWith、merge、zip等 10.算数&聚合操作符：toArray、reduce、concat 11.连接操作符：connect、publish、replay、multicast 12.其他操作符：delay、materialize、timeout等 13.错误处理操作 14.调试操作 15.特征序列1：Single、Completable、Maybe 16.特征序列2：Driver 17.特征序列3：ControlProperty、ControlEvent 18.调度器、subscribeOn、observeOn 19.UI控件扩展1：UILabel 20.UI控件扩展2：UITextField、UITextView 21.UI控件扩展3：UIButton、UIBarButtonItem 22.UI控件扩展4：UISwitch、UISegmentedControl 23.UI控件扩展5：UIActivityIndicatorView、UIApplication 24.UI控件扩展6：UISlider、UIStepper .md\">25.双向绑定： 26.UI控件扩展7：UIGestureRecognizer 27.UI控件扩展8：UIDatePicker 28.UITableView的使用1：基本用法 29.UITableView的使用2：RxDataSources 30.UITableView的使用3：刷新表格数据 31.UITableView的使用5：可编辑表格 32.UITableView的使用6：不同类型的单元格混用 33.UITableView的使用7：样式修改 34.UICollectionView的使用1：基本用法 35.UICollectionView的使用2：RxDataSources 36.UICollectionView的使用3：刷新集合数据 37.UICollectionView的使用4：样式修改 38.UIPickerView的使用 39.URLSession的使用1：请求数据 40.URLSession的使用2：结果处理、模型转换 41.结合RxAlamofire使用1：数据请求 42.结合RxAlamofire使用2：结果处理、模型转换 43.结合RxAlamofire使用3：文件上传 44.结合RxAlamofire使用4：文件下载 45.结合Moya使用1：数据请求 46.结合Moya使用2：结果处理、模型转换 47.MVVM架构演示2：使用Observable样例 48.MVVM架构演示3：使用Driver样例 49.一个用户注册样例1：基本功能实现 50.结合MJRefresh使用1：下拉刷新 51.DelegateProxy样例1：获取地理定位信息 52.DelegateProxy样例2：图片选择功能 53.DelegateProxy样例3：应用生命周期的状态变化 54.sendMessage和methodInvoked的区别 55.订阅UITableViewCell里的按钮点击事件 56.通知NotificationCenter的使用 57.键值观察KVO的使用 58.表格图片加载优化 59.检测当前值与初始值是否相同：isEqualOriginValue 60.重复执行某个操作序列：repeatWhen 61.监听滚动条滚动到底部的行为：reachedBottom 62.RxFeedback架构1：安装配置、基本用法 63.RxFeedback架构2：一个用户注册样例 64.RxFeedback架构3：GitHub资源搜索样例 65.ReactorKit架构1：安装配置、基本用法 66.ReactorKit架构2：一个用户注册样例 67.ReactorKit架构3：GitHub资源搜索样例 "},"pages/interview/01_Interview_questions_collection.html":{"url":"pages/interview/01_Interview_questions_collection.html","title":"面试问题集锦1","keywords":"","body":"1.面试问题集锦 1. weak和assign的区别 一、什么情况使用 weak 关键字？ 在 ARC 中，在有可能出现循环引用的时候，往往要通过让其中一端使用 weak 来解决，比如： delegate 代理属性。 自身已经对它进行一次强引用，没有必要再强引用一次，此时也会使用 weak ，自定义 IBOutlet 控件属性一般也使用 weak ；当然，也可以使用 strong 。 二、区别 2.1. 修饰变量类型的区别 weak 只可以修饰对象。如果修饰基本数据类型，编译器会报错-“Property with ‘weak’ attribute must be of object type”。 assign 可修饰对象，和基本数据类型。当需要修饰对象类型时，MRC时代使用 unsafeunretained 。当然， unsafe_unretained 也可能产生野指针，所以它名字是 unsafe 。 2.2. 是否产生野指针的区别 weak 不会产生野指针问题。因为 weak 修饰的对象释放后（引用计数器值为0），指针会自动被置nil，之后再向该对象发消息也不会崩溃。 weak是安全的。 assign 如果修饰对象，会产生野指针问题；如果修饰基本数据类型则是安全的。修饰的对象释放后，指针不会自动被置空，此时向对象发消息会崩溃。 三、相同 都可以修饰对象类型，但是 assign 修饰对象会存在问题。 四、总结 assign 适用于基本数据类型如 int，float，struct 等值类型，不适用于引用类型。因为值类型会被放入栈中，遵循先进后出原则，由系统负责管理栈内存。而引用类型会被放入堆中，需要我们自己手动管理内存或通过 ARC 管理。weak 适用于 delegate 和 block 等引用类型，不会导致野指针问题，也不会循环引用，非常安全。 五、参考文章 iOS开发中 weak 和 assign 的区别 2. 为什么要用 Copy 修饰 Block 一、栈区和堆区概念 内存的栈区：由编译器自动分配释放，存放函数的参数值，局部变量的值等。 其操作方式类似于数据结构中的栈。 内存的堆区：一般由程序员分配释放，若程序员不释放，程序结束时可能由OS回收。注意它与数据结构中的堆是两回事， 分配方式倒是类似于链表。 二、Block 的三种类型 iOS 内存分布，一般分为：栈区、堆区、全局区、常量区、代码区。其实 Block 也是一个 Objective-C 对象，常见的有以下三种 Block ： NSGlobalBlock：全局的静态 Block 没有访问外部变量。 NSStackBlock：保存在栈中的 Block ，没有用copy去修饰并且访问了外部变量，会在函数调用结束被销毁（需要在MRC）。 NSMallocBlock：保存在堆中的 Block ， 此类型 Block 是用 Copy 修饰出来的 Block ，它会随着对象的销毁而销毁，只要对象不销毁，我们就可以调用的到在堆中的 Block 。 三、回答 Block 引用了普通外部变量，都是创建在栈区的；对于分配在栈区的对象，我们很容易会在释放之后继续调用，导致程序奔溃，所以我们使用的时候需要将栈区的对象移到堆区，来延长该对象的生命周期。对于这个问题，得区分 MRC 环境和 ARC 环境： 对于 MRC 环境，使用 Copy 修饰 Block，会将栈区的 Block 拷贝到堆区。 对于 ARC 环境，使用 Strong、Copy 修饰 Block，都会将栈区的 Block 拷贝到堆区。 所以，Block 不是一定要用 Copy 来修饰的，在 ARC 环境下面 Strong 和 Copy 修饰效果是一样的。 四、参考文章 iOS block 为什么用copy修饰 为什么要用copy修饰Block ·iOS 面试题·Block 的原理，Block 的属性修饰词为什么用 copy，使用 Block 时有哪些要注意的？ 3. 怎么用 Copy 关键字？ NSString、NSArray、NSDictionary 等等经常使用 Copy 关键字。因为他们有对应的可变类型：NSMutableString、NSMutableArray、NSMutableDictionary，他们之间可能进行赋值操作，为确保对象中的字符串值不会无意间变动，应该在设置新属性值时拷贝一份。 Block 也经常使用 Copy 关键字。Block 使用 Copy 是从 MRC 遗留下来的“传统”，在 MRC 中，方法内部的 Block 是在栈区的，使用 Copy 可以把它放到堆区。 4. 这个写法会出什么问题：@property (copy) NSMutableArray *array; 添加，删除，修改数组内的元素的时候，程序会因为找不到对应的方法而崩溃.因为 copy 就是复制一个不可变 NSArray 的对象； 比如下面的代码就会发生崩溃 // .h文件 // http://weibo.com/luohanchenyilong/ // https://github.com/ChenYilong // 下面的代码就会发生崩溃 @property (nonatomic， copy) NSMutableArray *mutableArray; // .m文件 // http://weibo.com/luohanchenyilong/ // https://github.com/ChenYilong // 下面的代码就会发生崩溃 NSMutableArray *array = [NSMutableArray arrayWithObjects:@1，@2，nil]; self.mutableArray = array; [self.mutableArray removeObjectAtIndex:0]; 接下来就会奔溃： -[__NSArrayI removeObjectAtIndex:]: unrecognized selector sent to instance 0x7fcd1bc30460 使用了 atomic 属性会严重影响性能； 该属性使用了互斥锁（atomic 的底层实现，老版本是自旋锁，iOS10开始是互斥锁--spinlock底层实现改变了。），会在创建时生成一些额外的代码用于帮助编写多线程程序，这会带来性能问题，通过声明 nonatomic 可以节省这些虽然很小但是不必要额外开销。 5. 如何让自己的类用 copy 修饰符？如何重写带 copy 关键字的 setter？ 若想令自己所写的对象具有拷贝功能，则需实现 NSCopying 协议。如果自定义的对象分为可变版本与不可变版本，那么就要同时实现 NSCopying 与 NSMutableCopying 协议。 具体步骤： 需声明该类遵从 NSCopying 协议 实现 NSCopying 协议。该协议只有一个方法: - (id)copyWithZone:(NSZone *)zone; 案例： .h文 // 件 // http://weibo.com/luohanchenyilong/ // https://github.com/ChenYilong // 以第一题《风格纠错题》里的代码为例 typedef NS_ENUM(NSInteger， CYLSex) { CYLSexMan， CYLSexWoman }; @interface CYLUser : NSObject @property (nonatomic， readonly， copy) NSString *name; @property (nonatomic， readonly， assign) NSUInteger age; @property (nonatomic， readonly， assign) CYLSex sex; - (instancetype)initWithName:(NSString *)name age:(NSUInteger)age sex:(CYLSex)sex; + (instancetype)userWithName:(NSString *)name age:(NSUInteger)age sex:(CYLSex)sex; - (void)addFriend:(CYLUser *)user; - (void)removeFriend:(CYLUser *)user; @end // .m文件 // http://weibo.com/luohanchenyilong/ // https://github.com/ChenYilong // @implementation CYLUser { NSMutableSet *_friends; } - (void)setName:(NSString *)name { _name = [name copy]; } - (instancetype)initWithName:(NSString *)name age:(NSUInteger)age sex:(CYLSex)sex { if(self = [super init]) { _name = [name copy]; _age = age; _sex = sex; _friends = [[NSMutableSet alloc] init]; } return self; } - (void)addFriend:(CYLUser *)user { [_friends addObject:user]; } - (void)removeFriend:(CYLUser *)user { [_friends removeObject:user]; } - (id)copyWithZone:(NSZone *)zone { CYLUser *copy = [[[self class] allocWithZone:zone] initWithName:_name age:_age sex:_sex]; copy->_friends = [_friends mutableCopy]; return copy; } - (id)deepCopy { CYLUser *copy = [[[self class] alloc] initWithName:_name age:_age sex:_sex]; copy->_friends = [[NSMutableSet alloc] initWithSet:_friends copyItems:YES]; return copy; } @end 至于如何重写带 copy 关键字的 setter这个问题， 如果抛开本例来回答的话，如下： - (void)setName:(NSString *)name { //[_name release]; _name = [name copy]; } 那如何确保 name 被 copy？在初始化方法(initializer)中做： - (instancetype)initWithName:(NSString *)name age:(NSUInteger)age sex:(CYLSex)sex { if(self = [super init]) { _name = [name copy]; _age = age; _sex = sex; _friends = [[NSMutableSet alloc] init]; } return self; } 6. @property 的本质是什么？ivar、getter、setter 是如何生成并添加到这个类中的 @property 的本质是什么？ @property = ivar + getter + setter; 下面解释下： “属性” (property)有两大概念：ivar（实例变量）、存取方法（access method ＝ getter + setter）。 “属性” (property)作为 Objective-C 的一项特性，主要的作用就在于封装对象中的数据。 Objective-C 对象通常会把其所需要的数据保存为各种实例变量。实例变量一般通过“存取方法”(access method)来访问。其中，“获取方法” (getter)用于读取变量值，而“设置方法” (setter)用于写入变量值。这个概念已经定型，并且经由“属性”这一特性而成为 Objective-C 2.0 的一部分。 而在正规的 Objective-C 编码风格中，存取方法有着严格的命名规范。 正因为有了这种严格的命名规范，所以 Objective-C 这门语言才能根据名称自动创建出存取方法。其实也可以把属性当做一种关键字，其表示: 编译器会自动写出一套存取方法，用以访问给定类型中具有给定名称的变量。 所以你也可以这么说： @property = getter + setter; 例如下面这个类： @interface Person : NSObject @property NSString *firstName; @property NSString *lastName; @end 上述代码写出来的类与下面这种写法等效 @interface Person : NSObject - (NSString *)firstName; - (void)setFirstName:(NSString *)firstName; - (NSString *)lastName; - (void)setLastName:(NSString *)lastName; @end 源码分析 property在runtime中是 objc_property_t 定义如下: typedef struct objc_property *objc_property_t; 而 objc_property 是一个结构体，包括name和attributes，定义如下： struct property_t { const char *name; const char *attributes; }; 而attributes本质是 objc_property_attribute_t，定义了property的一些属性，定义如下： /// Defines a property attribute typedef struct { const char *name; /** 而attributes的具体内容是什么呢？其实，包括：类型，原子性，内存语义和对应的实例变量。 例如：我们定义一个string的property@property (nonatomic， copy) NSString *string;，通过 property_getAttributes(property)获取到attributes并打印出来之后的结果为T@\"NSString\"，C，N，V_string 其中T就代表类型，可参阅Type Encodings，C就代表Copy，N代表nonatomic，V就代表对应的实例变量。 ivar、getter、setter 是如何生成并添加到这个类中的? “自动合成”( autosynthesis) 完成属性定义后，编译器会自动编写访问这些属性所需的方法，此过程叫做“自动合成”(autosynthesis)。需要强调的是，这个过程由编译 器在编译期执行，所以编辑器里看不到这些“合成方法”(synthesized method)的源代码。除了生成方法代码 getter、setter 之外，编译器还要自动向类中添加适当类型的实例变量，并且在属性名前面加下划线，以此作为实例变量的名字。在前例中，会生成两个实例变量，其名称分别为 _firstName 与 _lastName。也可以在类的实现代码里通过 @synthesize 语法来指定实例变量的名字. @implementation Person @synthesize firstName = _myFirstName; @synthesize lastName = _myLastName; @end 我为了搞清属性是怎么实现的，曾经反编译过相关的代码，他大致生成了五个东西 OBJCIVAR$类名$属性名称 ：该属性的“偏移量” (offset)，这个偏移量是“硬编码” (hardcode)，表示该变量距离存放对象的内存区域的起始地址有多远。 setter 与 getter 方法对应的实现函数 ivar_list ：成员变量列表 method_list ：方法列表 prop_list ：属性列表 也就是说我们每次在增加一个属性，系统都会在 ivar_list 中添加一个成员变量的描述，在 method_list 中增加 setter 与 getter 方法的描述，在属性列表中增加一个属性的描述，然后计算该属性在对象中的偏移量，然后给出 setter 与 getter 方法对应的实现，在 setter 方法中从偏移量的位置开始赋值，在 getter 方法中从偏移量开始取值，为了能够读取正确字节数，系统对象偏移量的指针类型进行了类型强转。 7. @protocol 和 category 中如何使用 @property 在 protocol 中使用 property 只会生成 setter 和 getter 方法声明,我们使用属性的目的,是希望遵守我协议的对象能实现该属性 category 使用 @property 也是只会生成 setter 和 getter 方法的声明,如果我们真的需要给 category 增加属性的实现,需要借助于运行时的两个函数： objc_setAssociatedObject objc_getAssociatedObject 8. runtime 如何实现 weak 属性 要实现 weak 属性，首先要搞清楚 weak 属性的特点： weak 此特质表明该属性定义了一种“非拥有关系” (nonowning relationship)。为这种属性设置新值时，设置方法既不保留新值，也不释放旧值。此特质同 assign 类似， 然而在属性所指的对象遭到摧毁时，属性值也会清空(nil out)。 那么 runtime 如何实现 weak 变量的自动置nil？ runtime 对注册的类， 会进行布局，对于 weak 对象会放入一个 hash 表中。 用 weak 指向的对象内存地址作为 key，当此对象的引用计数为0的时候会 dealloc，假如 weak 指向的对象内存地址是a，那么就会以a为键， 在这个 weak 表中搜索，找到所有以a为键的 weak 对象，从而设置为 nil。 9. @property中有哪些属性关键字？/ @property 后面可以有哪些修饰符？ 原子性--- nonatomic 特质 读/写权限---readwrite(读写)、readonly (只读) 内存管理语义---assign、strong、 weak、unsafe_unretained、copy 方法名---getter= 、setter= getter=的样式： @property (nonatomic, getter=isOn) BOOL on; setter=一般用在特殊的情境下，比如： 在数据反序列化、转模型的过程中，服务器返回的字段如果以 init 开头，所以你需要定义一个 init 开头的属性，但默认生成的 setter 与 getter 方法也会以 init 开头，而编译器会把所有以 init 开头的方法当成初始化方法，而初始化方法只能返回 self 类型，因此编译器会报错。 这时你就可以使用下面的方式来避免编译器报错： @property(nonatomic, strong, getter=p_initBy, setter=setP_initBy:)NSString *initBy; 另外也可以用关键字进行特殊说明，来避免编译器报错： @property(nonatomic, readwrite, copy, null_resettable) NSString *initBy; - (NSString *)initBy __attribute__((objc_method_family(none))); 不常用的：nonnull,null_resettable,nullable 10. weak属性需要在dealloc中置nil么？ 不需要。 在ARC环境无论是强指针还是弱指针都无需在 dealloc 设置为 nil ， ARC 会自动帮我们处理 11. @synthesize和@dynamic分别有什么作用？ @property有两个对应的词，一个是 @synthesize，一个是 @dynamic。如果 @synthesize和 @dynamic都没写，那么默认的就是@syntheszie var = _var; @synthesize 的语义是如果你没有手动实现 setter 方法和 getter 方法，那么编译器会自动为你加上这两个方法。 @dynamic 告诉编译器：属性的 setter 与 getter 方法由用户自己实现，不自动生成。（当然对于 readonly 的属性只需提供 getter 即可）。假如一个属性被声明为 @dynamic var，然后你没有提供 @setter方法和 @getter 方法，编译的时候没问题，但是当程序运行到 instance.var = someVar，由于缺 setter 方法会导致程序崩溃；或者当运行到 someVar = var 时，由于缺 getter 方法同样会导致崩溃。编译时没问题，运行时才执行相应的方法，这就是所谓的动态绑定。 12. ARC下，不显式指定任何属性关键字时，默认的关键字都有哪些？ 对应基本数据类型默认关键字是 atomic readwrite assign 对于普通的 Objective-C 对象 atomic readwrite strong 13. 用@property声明的NSString（或NSArray，NSDictionary）经常使用copy关键字，为什么？如果改用strong关键字，可能造成什么问题？ 因为父类指针可以指向子类对象,使用 copy 的目的是为了让本对象的属性不受外界影响,使用 copy 无论给我传入是一个可变对象还是不可对象,我本身持有的就是一个不可变的副本. 如果我们使用是 strong ,那么这个属性就有可能指向一个可变对象,如果这个可变对象在外部被修改了,那么会影响该属性. copy 此特质所表达的所属关系与 strong 类似。然而设置方法并不保留新值，而是将其“拷贝” (copy)。 当属性类型为 NSString 时，经常用此特质来保护其封装性，因为传递给设置方法的新值有可能指向一个 NSMutableString 类的实例。这个类是 NSString 的子类，表示一种可修改其值的字符串，此时若是不拷贝字符串，那么设置完属性之后，字符串的值就可能会在对象不知情的情况下遭人更改。所以，这时就要拷贝一份“不可变” (immutable)的字符串，确保对象中的字符串值不会无意间变动 成实例变量的规则是什么？假如property名为foo，存在一个名为_foo的实例变量，那么还会自动合成新变量么？ 如果指定了成员变量的名称,会生成一个指定的名称的成员变量 如果这个成员已经存在了就不再生成了 如果是 @synthesize foo; 还会生成一个名称为foo的成员变量，也就是说 如果没有指定成员变量的名称会自动生成一个属性同名的成员变量 如果是 @synthesize foo = _foo; 就不会生成成员变量了。 14. 在有了自动合成属性实例变量之后，@synthesize还有哪些使用场景？ 回答这个问题前，我们要搞清楚一个问题，什么情况下不会autosynthesis（自动合成）？ 1. 同时重写了 setter 和 getter 时 2. 重写了只读属性的 getter 时 3. 使用了 @dynamic 时 4. 在 @protocol 中定义的所有属性 5. 在 category 中定义的所有属性 6. 重写（overridden）的属性 当你在子类中重写（overridden）了父类中的属性，你必须 使用 @synthesize 来手动合成ivar。 当你同时重写了 setter 和 getter 时，系统就不会生成 ivar（实例变量/成员变量）。这时候有两种选择： 手动创建 ivar 使用@synthesize foo = _foo; ，关联 @property 与 ivar。 15. objc中向一个nil对象发送消息将会发生什么？ 在 Objective-C 中向 nil 发送消息是完全有效的——只是在运行时不会有任何作用: 如果一个方法返回值是一个对象，那么发送给nil的消息将返回0(nil)。例如： Person * motherInlaw = [[aPerson spouse] mother]; 如果 spouse 对象为 nil，那么发送给 nil 的消息 mother 也将返回 nil。 如果方法返回值为指针类型，其指针大小为小于或者等于sizeof(void*)，float，double，long double 或者 long long 的整型标量，发送给 nil 的消息将返回0。 如果方法返回值为结构体,发送给 nil 的消息将返回0。结构体中各个字段的值将都是0。 如果方法的返回值不是上述提到的几种情况，那么发送给 nil 的消息的返回值将是未定义的。 具体原因如下： objc是动态语言，每个方法在运行时会被动态转为消息发送，即：objc_msgSend(receiver, selector)。 objc在向一个对象发送消息时，runtime库会根据对象的isa指针找到该对象实际所属的类，然后在该类中的方法列表以及其父类方法列表中寻找方法运行，然后在发送消息的时候，objc_msgSend方法不会返回值，所谓的返回内容都是具体调用时执行的。 那么，回到本题，如果向一个nil对象发送消息，首先在寻找对象的isa指针时就是0地址返回了，所以不会出现任何错误。 16. objc中向一个对象发送消息[obj foo]和objc_msgSend()函数之间有什么关系？ 该方法编译之后就是objc_msgSend()函数调用. ((void ()(id, SEL))(void )objc_msgSend)((id)obj, sel_registerName(\"foo\")); 17. 什么时候会报unrecognized selector的异常？ 简单来说： 当调用该对象上某个方法,而该对象上没有实现这个方法的时候， 可以通过“消息转发”进行解决。 简单的流程如下，在上一题中也提到过： objc是动态语言，每个方法在运行时会被动态转为消息发送，即：objc_msgSend(receiver, selector)。 objc在向一个对象发送消息时，runtime库会根据对象的isa指针找到该对象实际所属的类，然后在该类中的方法列表以及其父类方法列表中寻找方法运行，如果，在最顶层的父类中依然找不到相应的方法时，程序在运行时会挂掉并抛出异常unrecognized selector sent to XXX 。但是在这之前，objc的运行时会给出三次拯救程序崩溃的机会： Method resolution objc运行时会调用+resolveInstanceMethod:或者 +resolveClassMethod:，让你有机会提供一个函数实现。如果你添加了函数，那运行时系统就会重新启动一次消息发送的过程，否则 ，运行时就会移到下一步，消息转发（Message Forwarding）。 Fast forwarding 如果目标对象实现了 -forwardingTargetForSelector:，Runtime 这时就会调用这个方法，给你把这个消息转发给其他对象的机会。 只要这个方法返回的不是nil和self，整个消息发送的过程就会被重启，当然发送的对象会变成你返回的那个对象。否则，就会继续Normal Fowarding。 这里叫Fast，只是为了区别下一步的转发机制。因为这一步不会创建任何新的对象，但下一步转发会创建一个NSInvocation对象，所以相对更快点。 Normal forwarding 这一步是Runtime最后一次给你挽救的机会。首先它会发送 -methodSignatureForSelector: 消息获得函数的参数和返回值类型。如果 -methodSignatureForSelector: 返回nil，Runtime则会发出 -doesNotRecognizeSelector: 消息，程序这时也就挂掉了。如果返回了一个函数签名，Runtime就会创建一个NSInvocation对象并发送 -forwardInvocation: 消息给目标对象。 18. 一个objc对象如何进行内存布局？（考虑有父类的情况） 所有父类的成员变量和自己的成员变量都会存放在该对象所对应的存储空间中. 每一个对象内部都有一个isa指针,指向他的类对象,类对象中存放着本对象的 对象方法列表（对象能够接收的消息列表，保存在它所对应的类对象中） 成员变量的列表, 属性列表 它内部也有一个isa指针指向元对象(meta class),元对象内部存放的是类方法列表,类对象内部还有一个superclass的指针,指向他的父类对象。 19. 一个objc对象的isa的指针指向什么？有什么作用？ isa 顾名思义 is a 表示对象所属的类。 isa 指向他的类对象，从而可以找到对象上的方法。 同一个类的不同对象，他们的 isa 指针是一样的 20. 下面的代码输出什么？ @implementation Son : Father - (id)init { self = [super init]; if (self) { NSLog(@\"%@\", NSStringFromClass([self class])); NSLog(@\"%@\", NSStringFromClass([super class])); } return self; } @end 都输出 Son 21. runtime如何通过selector找到对应的IMP地址？（分别考虑类方法和实例方法） 每一个类对象中都一个方法列表，方法列表中记录着方法的名称、方法实现、以及参数类型，其实selector 本质就是方法名称，通过这个方法名称就可以在方法列表中找到对应的方法实现。 参考 NSObject 上面的方法： - (IMP)methodForSelector:(SEL)aSelector; + (IMP)instanceMethodForSelector:(SEL)aSelector; 22. 使用runtime Associate方法关联的对象，需要在主对象dealloc的时候释放么？ 无论在MRC下还是ARC下均不需要。 23. objc中的类方法和实例方法有什么本质区别和联系？ 类方法： 类方法是属于类对象的 类方法只能通过类对象调用 类方法中的self是类对象 类方法可以调用其他的类方法 类方法中不能访问成员变量 类方法中不能直接调用对象方法 实例方法： 实例方法是属于实例对象的 实例方法只能通过实例对象调用 实例方法中的self是实例对象 实例方法中可以访问成员变量 实例方法中直接调用实例方法 实例方法中也可以调用类方法(通过类名) "},"pages/interview/02_Interview_questions_collection.html":{"url":"pages/interview/02_Interview_questions_collection.html","title":"面试问题集锦2","keywords":"","body":"1. _objc_msgForward函数是做什么的，直接调用它将会发生什么？ _objc_msgForward是 IMP 类型，用于消息转发的：当向一个对象发送一条消息，但它并没有实现的时候，_objc_msgForward会尝试做消息转发。 2. 能否向编译后得到的类中增加实例变量？能否向运行时创建的类中添加实例变量？为什么？ 不能向编译后得到的类中增加实例变量； 能向运行时创建的类中添加实例变量； 解释下： 因为编译后的类已经注册在 runtime 中，类结构体中的 objc_ivar_list 实例变量的链表 和 instance_size 实例变量的内存大小已经确定，同时runtime 会调用 class_setIvarLayout 或 class_setWeakIvarLayout 来处理 strong weak 引用。所以不能向存在的类中添加实例变量； 运行时创建的类是可以添加实例变量，调用 class_addIvar 函数。但是得在调用 objc_allocateClassPair 之后，objc_registerClassPair 之前，原因同上。 3. runloop和线程有什么关系？ 总的说来，Run loop，正如其名，loop表示某种循环，和run放在一起就表示一直在运行着的循环。实际上，run loop和线程是紧密相连的，可以这样说run loop是为了线程而生，没有线程，它就没有存在的必要。Run loops是线程的基础架构部分， Cocoa 和 CoreFundation 都提供了 run loop 对象方便配置和管理线程的 run loop （以下都以 Cocoa 为例）。每个线程，包括程序的主线程（ main thread ）都有与之相应的 run loop 对象。 主线程的run loop默认是启动的。 iOS的应用程序里面，程序启动后会有一个如下的main()函数 int main(int argc, char * argv[]) { @autoreleasepool { return UIApplicationMain(argc, argv, nil, NSStringFromClass([AppDelegate class])); } } 重点是UIApplicationMain()函数，这个方法会为main thread设置一个NSRunLoop对象，这就解释了：为什么我们的应用可以在无人操作的时候休息，需要让它干活的时候又能立马响应。 对其它线程来说，run loop默认是没有启动的，如果你需要更多的线程交互则可以手动配置和启动，如果线程只是去执行一个长时间的已确定的任务则不需要。 在任何一个 Cocoa 程序的线程中，都可以通过以下代码来获取到当前线程的 run loop NSRunLoop *runloop = [NSRunLoop currentRunLoop]; 4. runloop的mode作用是什么？ model 主要是用来指定事件在运行循环中的优先级的，分为： NSDefaultRunLoopMode（kCFRunLoopDefaultMode）：默认，空闲状态 UITrackingRunLoopMode：ScrollView滑动时 UIInitializationRunLoopMode：启动时 NSRunLoopCommonModes（kCFRunLoopCommonModes）：Mode集合 苹果公开提供的 Mode 有两个： NSDefaultRunLoopMode（kCFRunLoopDefaultMode） NSRunLoopCommonModes（kCFRunLoopCommonModes） 5. 以+ scheduledTimerWithTimeInterval...的方式触发的timer，在滑动页面上的列表时，timer会暂定回调，为什么？如何解决？ RunLoop只能运行在一种mode下，如果要换mode，当前的loop也需要停下重启成新的。利用这个机制，ScrollView滚动过程中NSDefaultRunLoopMode（kCFRunLoopDefaultMode）的mode会切换到UITrackingRunLoopMode来保证ScrollView的流畅滑动：只能在NSDefaultRunLoopMode模式下处理的事件会影响ScrollView的滑动。 如果我们把一个NSTimer对象以NSDefaultRunLoopMode（kCFRunLoopDefaultMode）添加到主运行循环中的时候, ScrollView滚动过程中会因为mode的切换，而导致NSTimer将不再被调度。 同时因为mode还是可定制的，所以： Timer计时会被scrollView的滑动影响的问题可以通过将timer添加到NSRunLoopCommonModes（kCFRunLoopCommonModes）来解决。代码如下： //将timer添加到NSDefaultRunLoopMode中 [NSTimer scheduledTimerWithTimeInterval:1.0 target:self selector:@selector(timerTick:) userInfo:nil repeats:YES]; //然后再添加到NSRunLoopCommonModes里 NSTimer *timer = [NSTimer timerWithTimeInterval:1.0 target:self selector:@selector(timerTick:) userInfo:nil repeats:YES]; [[NSRunLoop currentRunLoop] addTimer:timer forMode:NSRunLoopCommonModes]; 6. 猜想runloop内部是如何实现的？ 一般来讲，一个线程一次只能执行一个任务，执行完成后线程就会退出。如果我们需要一个机制，让线程能随时处理事件但并不退出，通常的代码逻辑 是这样的： function loop() { initialize(); do { var message = get_next_message(); process_message(message); } while (message != quit); } 或使用伪代码来展示下: int main(int argc, char * argv[]) { //程序一直运行状态 while (AppIsRunning) { //睡眠状态，等待唤醒事件 id whoWakesMe = SleepForWakingUp(); //得到唤醒事件 id event = GetEvent(whoWakesMe); //开始处理事件 HandleEvent(event); } return 0; } 7. objc使用什么机制管理对象内存？ 通过 retainCount 的机制来决定对象是否需要释放。 每次 runloop 的时候，都会检查对象的 retainCount，如果 retainCount 为 0，说明该对象没有地方需要继续使用了，可以释放掉了。 8. ARC通过什么方式帮助开发者管理内存？ ARC相对于MRC，不是在编译时添加retain/release/autorelease这么简单。应该是编译期和运行期两部分共同帮助开发者管理内存。 在编译期，ARC用的是更底层的C接口实现的retain/release/autorelease，这样做性能更好，也是为什么不能在ARC环境下手动retain/release/autorelease，同时对同一上下文的同一对象的成对retain/release操作进行优化（即忽略掉不必要的操作）；ARC也包含运行期组件，这个地方做的优化比较复杂，但也不能被忽略。 8. 不手动指定autoreleasepool的前提下，一个autorealese对象在什么时刻释放？（比如在一个vc的viewDidLoad中创建） 手动干预释放时机--指定 autoreleasepool 就是所谓的：当前作用域大括号结束时释放。 系统自动去释放--不手动指定 autoreleasepool Autorelease对象出了作用域之后，会被添加到最近一次创建的自动释放池中，并会在当前的 runloop 迭代结束时释放。 如果在一个vc的viewDidLoad中创建一个 Autorelease对象，那么该对象会在 viewDidAppear 方法执行前就被销毁了。 9. BAD_ACCESS在什么情况下出现？ 访问了悬垂指针，比如对一个已经释放的对象执行了release、访问已经释放对象的成员变量或者发消息。 死循环 10. 苹果是如何实现autoreleasepool的？ autoreleasepool 以一个队列数组的形式实现,主要通过下列三个函数完成. objc_autoreleasepoolPush objc_autoreleasepoolPop objc_autorelease 看函数名就可以知道，对 autorelease 分别执行 push，和 pop 操作。销毁对象时执行release操作。 11. 使用block时什么情况会发生引用循环，如何解决？ 一个对象中强引用了 block，在 block 中又强引用了该对象，就会发生循环引用。 ARC 下的解决方法是： 将该对象使用 weak 修饰符修饰之后再在 block 中使用。 id weak weakSelf = self; 或者 weak typeof(&*self)weakSelf = self 该方法可以设置宏 __weak ：不会产生强引用，指向的对象销毁时，会自动让指针置为 ni1 使用 unsafe_unretained 关键字，用法与 __weak 一致。 unsafe_unretained 不会产生强引用，不安全，指向的对象销毁时，指针存储的地址值不变。 也可以使用 block 来解决循环引用问题，用法为： block id weakSelf = self;，但不推荐使用。因为必须要调用该 block 方案才能生效，因为需要及时的将 __block 变量置为 nii。 12. 在block内如何修改block外部变量？ 先描述下问题： 默认情况下，在block中访问的外部变量是复制过去的，即：写操作不对原变量生效。但是你可以加上 __block 来让其写操作生效，示例代码如下: __block int a = 0; void (^foo)(void) = ^{ a = 1; }; foo(); //这里，a的值被修改为1 \"将 auto 从栈 copy 到堆\" “将 auto 变量封装为结构体(对象)” 13. 使用系统的某些block api（如UIView的block版本写动画时），是否也考虑引用循环问题？ 14. GCD的队列（dispatch_queue_t）分哪两种类型 串行队列Serial Dispatch Queue 并发队列Concurrent Dispatch Queue 15. 如何用GCD同步若干个异步调用？（如根据若干个url异步加载多张图片，然后在都下载完成后合成一张整图） 使用Dispatch Group追加block到Global Group Queue,这些block如果全部执行完毕，就会执行Main Dispatch Queue中的结束处理的block。 16. dispatch_barrier_async的作用是什么？ 在并发队列中，为了保持某些任务的顺序，需要等待一些任务完成后才能继续进行，使用 barrier 来等待之前任务完成，避免数据竞争等问题。 dispatch_barrier_async 函数会等待追加到Concurrent Dispatch Queue并发队列中的操作全部执行完之后，然后再执行 dispatch_barrier_async 函数追加的处理，等 dispatch_barrier_async 追加的处理执行结束之后，Concurrent Dispatch Queue才恢复之前的动作继续执行。 打个比方：比如你们公司周末跟团旅游，高速休息站上，司机说：大家都去上厕所，速战速决，上完厕所就上高速。超大的公共厕所，大家同时去，程序猿很快就结束了，但程序媛就可能会慢一些，即使你第一个回来，司机也不会出发，司机要等待所有人都回来后，才能出发。 dispatch_barrier_async 函数追加的内容就如同 “上完厕所就上高速”这个动作。 （注意：使用 dispatch_barrier_async ，该函数只能搭配自定义并发队列 dispatch_queue_t 使用。不能使用： dispatch_get_global_queue ，否则 dispatch_barrier_async 的作用会和 dispatch_async 的作用一模一样。 ） 17. 苹果为什么要废弃dispatch_get_current_queue？ dispatch_get_current_queue函数的行为常常与开发者所预期的不同。 由于派发队列是按层级来组织的，这意味着排在某条队列中的块会在其上级队列里执行。 队列间的层级关系会导致检查当前队列是否为执行同步派发所用的队列这种方法并不总是奏效。dispatch_get_current_queue函数通常会被用于解决由不可以重入的代码所引发的死锁，然后能用此函数解决的问题，通常也可以用\"队列特定数据\"来解决。 18. 以下代码运行结果如何？ - (void)viewDidLoad { [super viewDidLoad]; NSLog(@\"1\"); dispatch_sync(dispatch_get_main_queue(), ^{ NSLog(@\"2\"); }); NSLog(@\"3\"); } 只输出：1 。发生主线程锁死。 19. 如何手动触发一个value的KVO 所谓的“手动触发”是区别于“自动触发”： 自动触发是指类似这种场景：在注册 KVO 之前设置一个初始值，注册之后，设置一个不一样的值，就可以触发了。 想知道如何手动触发，必须知道自动触发 KVO 的原理： 键值观察通知依赖于 NSObject 的两个方法: willChangeValueForKey: 和 didChangevlueForKey: 。在一个被观察属性发生改变之前， willChangeValueForKey: 一定会被调用，这就 会记录旧的值。而当改变发生后， observeValueForKey:ofObject:change:context: 会被调用，继而 didChangeValueForKey: 也会被调用。如果可以手动实现这些调用，就可以实现“手动触发”了。 那么“手动触发”的使用场景是什么？一般我们只在希望能控制“回调的调用时机”时才会这么做。 20. 若一个类有实例变量 NSString *_foo ，调用setValue:forKey:时，可以以foo还是 _foo 作为key？ 都可以 21. KVC的keyPath中的集合运算符如何使用？ 必须用在集合对象上或普通对象的集合属性上 简单集合运算符有@avg， @count ， @max ， @min ，@sum 格式 @\"@sum.age\"或 @\"集合属性.@max.age\" 22. KVC和KVO的keyPath一定是属性么？ KVC 支持实例变量，KVO 只能手动支持手动设定实例变量的KVO实现监听 23. 如何关闭默认的KVO的默认实现，并进入自定义的KVO实现？ 《如何自己动手实现 KVO》 KVO for manually implemented properties 24. apple用什么方式实现对一个对象的KVO？ 当你观察一个对象时，一个新的类会被动态创建。这个类继承自该对象的原本的类，并重写了被观察属性的 setter 方法。重写的 setter 方法会负责在调用原 setter 方法之前和之后，通知所有观察对象：值的更改。最后通过 isa 混写（isa-swizzling） 把这个对象的 isa 指针 ( isa 指针告诉 Runtime 系统这个对象的类是什么 ) 指向这个新创建的子类，对象就神奇的变成了新创建的子类的实例 25. IBOutlet连出来的视图属性为什么可以被设置成weak? Should IBOutlets be strong or weak under ARC? 文章告诉我们： 因为既然有外链那么视图在xib或者storyboard中肯定存在，视图已经对它有一个强引用了。 不过这个回答漏了个重要知识，使用storyboard（xib不行）创建的vc，会有一个叫_topLevelObjectsToKeepAliveFromStoryboard 的私有数组强引用所有 top level 的对象，所以这时即便outlet声明成weak也没关系 26. IB中User Defined Runtime Attributes如何使用？ 它能够通过KVC的方式配置一些你在interface builder 中不能配置的属性。当你希望在IB中作尽可能多得事情，这个特性能够帮助你编写更加轻量级的viewcontroller 27. 如何调试BAD_ACCESS错误 重写object的respondsToSelector方法，现实出现EXEC_BAD_ACCESS前访问的最后一个object 通过 Zombie 设置全局断点快速定位问题代码所在行 Xcode 7 已经集成了BAD_ACCESS捕获功能：Address Sanitizer。 28. lldb（gdb）常用的调试命令？ breakpoint 设置断点定位到某一个函数 n 断点指针下一步 po打印对象 "},"pages/interview/Get_something.html":{"url":"pages/interview/Get_something.html","title":"取东西","keywords":"","body":"取东西 测试一下 // 初始化每行数量 var row1Count = 3; var row2Count = 5; var row3Count = 7; // 获取行剩余数量 function getRemainCount(rowNum) { switch(parseInt(rowNum)) { case 1: return row1Count; case 2: return row2Count; case 3: return row3Count; default: return 0; } } // 更新行数量 // rowNum: 第几行（1,2,3） // getCount: 取数量 function updateRemainCount(rowNum, getCount) { switch(parseInt(rowNum)) { case 1: row1Count -= getCount; break; case 2: row2Count -= getCount; break; case 3: row3Count -= getCount; break; default: break; } }; // 判断是否取完所有 function judgeFinish() { return (row1Count + row2Count + row3Count) 0) { if (getCount > remainCount) { return -3;// 第x行数量不足 } else { // 更新行数量 updateRemainCount(rowNum, getCount); // 判断是否取完 if (judgeFinish()) { return -1;// 你是输家 } return 0;// 成功取出 } } else { return -2;// 第x行空 } } // 执行回合 // getCount: 取数量 // rowNum: 第几行（1,2,3） function round(rowNum, getCount) { var result = getThing(rowNum, getCount); switch(result) { case 0: alert(\"成功取出\"); break; case -1: alert(\"你是输家\"); break; case -2: alert(\"第\"+rowNum+\"行空\"); break; case -3: alert(\"第\"+rowNum+\"行数量不足\"); break; default: break; } }; // 运行 function run() { var rowNum = prompt(\"取第几行:\",\"\"); var getCount = prompt(\"取多少:\",\"\"); round(rowNum, getCount); } 15个任意物品（可以是火柴牙签poker） 以下按牙签为例 将15根牙签 分成三行 每行自上而下（其实方向不限）分别是3、5、7根 安排两个玩家，每人可以在一轮内，在任意行拿任意根牙签，但不能跨行 拿最后一根牙签的人即为输家 题目 请用你最擅长的语言，以你觉得最优雅的方式写一个符合以上游戏规则的程序。完成后把写好的代码和简历同时发到以下邮箱（备注姓名+岗位），并加上一段简短的文字描述一下你的想法 （请使用javascript，typescript或C#的其中一种语言完成测试题） "},"pages/interview/sizeof.html":{"url":"pages/interview/sizeof.html","title":"sizeof","keywords":"","body":"sizeof 面试官：定义一个空的类型，里面没有任何成员变量和成员函数。对 该类型求sizeof，得到的结果是多少？应聘者：答案是1。 面试官：为什么不是0？应聘者：空类型的实例中不包含任何信息，本来求sizeof应该是0，但是当我们声明该类型的实例的时候，它必须在内存中占有一定的空间，否则无法使用这些实例。至于占用多少内存，由编译器决定。在Visual Studio中，每个空类型的实例占用1字节的空间。 面试官：如果在该类型中添加一个构造函数和析构函数，再对该类型求sizeof,得到的结果又是多少？应聘者：和前面一样，还是1。调用构造函数和析构函数只需要知道函数的地址即可，而这些函数的地址只与类型相关，而与类型的实例无关，编译器也不会因为这两个函数而在实例内添加任何额外的信息。 面试官：那如果把析构函数标记为虚函数呢？应聘者：C+的编译器一旦发现一个类型中有虚函数，就会为该类型生成虚函数表，并在该类型的每一个实例中添加一个指向虚函数表的指针。在32位的机器上，一个指针占4字节的空间，因此求sizeof得到4；如果是64位的机器，则一个指针占8字节的空间，因此求sizeof得到8。 测试代码： #include using namespace std; class X { }; class Y: public virtual X { }; class Z: public virtual X { }; class A: public Y, public Z { }; int main() { int x = 0; x = sizeof(X); cout 在Xcode上的执行结果： x：1 y：8 z：8 a：16 参考链接：https://blog.csdn.net/zhuiqiuzhuoyue583/article/details/92846054 "},"pages/interview/copy_constructor.html":{"url":"pages/interview/copy_constructor.html","title":"复制构造函数","keywords":"","body":"复制构造函数 比如，面试官递给应聘者一张有如下代码的A4打印纸要求他分析编译运行的结果，并提供3个选项：A.编译错误；B.编译成功，运行时程序崩溃；C.编译运行正常，输出10。 在上述代码中，复制构造函数A(A other)传入的参数是A的一个实例。由于是传值参数，我们把形参复制到实参会调用复制构造函数。因此，如果允许复制构造函数传值，就会在复制构造函数内调用复制构造函数，就会形成永无休止的递归调用从而导致栈溢出。因此，C+的标准不允许复制构造函数传值参数，在Visual Studio和GCC中，都将编译出错。要解决这个问题，我们可以把构造函数修改为A(const A&other),也就是把传值参数改成常量引用。 "},"pages/interview/Assignment_operator_function.html":{"url":"pages/interview/Assignment_operator_function.html","title":"赋值运算符函数","keywords":"","body":"赋值运算符函数 题目：如下为类型CMyString的声明，请为该类型添加赋值运算符函数。 class CMyString { public: CMyString(char* pData = nullptr); CMyString(const CMyString& str); ~CMyString(void); CMyString& operator = (const CMyString& str); void Print(); private: char* m_pData; }; 当面试官要求应聘者定义一个赋值运算符函数时，他会在检查应聘者写出的代码时关注如下几点： 是否把返回值的类型声明为该类型的引用，并在函数结束前返回实例自身的引用(*this)。只有返回一个引用，才可以允许连续赋值。否则，如果函数的返回值是void,则应用该赋值运算符将不能进行连续赋值。假设有3个CMyString的对象：strl、str2和str3,在程序中语句str1=str2=str3将不能通过编译。 是否把传入的参数的类型声明为常量引用。如果传入的参数不是引用而是实例，那么从形参到实参会调用一次复制构造函数。把参数声明为引用可以避免这样的无谓消耗，能提高代码的效率。同时，我们在赋值运算符函数内不会改变传入的实例的状态，因此应该为传入的引用参数加上const关键字。 是否释放实例自身已有的内存。如果我们忘记在分配新内存之前释放自身已有的空间，则程序将出现内存泄漏。 判断传入的参数和当前的实例(*this)是不是同一个实例。如果是同一个，则不进行赋值操作，直接返回。如果事先不判断就进行赋值，那么在释放实例自身内存的时候就会导致严重的问题：当*this和传入的参数是同一个实例时，一旦释放了自身的内存，传入的参数的内存也同时被释放了，因此再也找不到需要赋值的内容了。 经典的解法，适用于初级程序员 当我们完整地考虑了上述4个方面之后，可以写出如下的代码： CMyString& CMyString::operator = (const CMyString& str) { if(this == &str) return *this; delete []m_pData; m_pData = nullptr; m_pData = new char[strlen(str.m_pData) + 1]; strcpy(m_pData, str.m_pData); return *this; } 考虑异常安全性的解法，高级程序员必备 在前面的函数中，我们在分配内存之前先用delete释放了实例m_pData的内存。如果此时内存不足导致new char抛出异常，则m_pData将是一个空指针，这样非常容易导致程序崩溃。也就是说，一旦在赋值运算符函数内部抛出一个异常，CMyString的实例不再保持有效的状态，这就违背了异常安全性(Exception Safety)原则。 要想在赋值运算符函数中实现异常安全性，我们有两种方法。一种简单的办法是我们先用new分配新内容，再用delete释放已有的内容。这样只在分配内容成功之后再释放原来的内容，也就是当分配内存失败时我们能确保CMyString的实例不会被修改。我们还有一种更好的办法，即先创建一个临时实例，再交换临时实例和原来的实例。下面是这种思路的参考代码： CMyString& CMyString::operator = (const CMyString& str) { if(this != &str) { CMyString strTemp(str); char*pTemp=strTemp.m_pData; strTemp.m_pData=m_pData; m_pData=pTemp; } return *this; } 在这个函数中，我们先创建一个临时实例strTemp,接着把strTemp.m_pData和实例自身的m_pData进行交换。由于strTemp是一个局部变量，但程序运行到if的外面时也就出了该变量的作用域，就会自动调用strTemp的析构函数，把strTemp.m_pData所指向的内存释放掉。由于strTemp.m_pData指向的内存就是实例之前m_pData的内存，这就相当于自动调用析构函数释放实例的内存。 在新的代码中，我们在CMyString的构造函数里用new分配内存。如果由于内存不足抛出诸如bad alloc等异常，但我们还没有修改原来实例的状态，因此实例的状态还是有效的，这也就保证了异常安全性。 源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/01_AssignmentOperator "},"pages/interview/Implement_Singleton_pattern.html":{"url":"pages/interview/Implement_Singleton_pattern.html","title":"实现Singleton模式","keywords":"","body":"实现Singleton模式 题目：设计一个类，我们只能生成该类的一个实例： 不好的解法一：只适用于单线程环境 由于要求只能生成一个实例，因此我们必须把构造函数设为私有函数以禁止他人创建实例。我们可以定义一个静态的实例，在需要的时候创该实例。下面定义类型Singleton1就是基于这个思路的实现： public sealed class Singleton1 { private Singleton1() { } private static Singleton1 instance = null; public static Singleton1 Instance { get { if (instance == null) instance = new Singleton1(); return instance; } } } 上述代码在Singleton1的静态属性instance中，只有在instance为的时候才创建个实例以避免重复创建。同时我们把构造函数定义为私有函数，这样就能确保只创建一个实例。 不好的解法二：虽然在多线程环境中能工作，但效率不高 解法一中的代码在单线程的时候工作正常，但在多线程的情况下就有问题了。设想如果两个线程同时运行到判断instance是否为null的if语句，并且instance的确没有创建时，那么两个线程都会创建一个实例，此时类型Singleton1就不再满足单例模式的要求了。为了保证在多线程环境下我们还是只能得到类型的一个实例，需要加上一个同步锁。把Singleton1稍作修改得到了如下代码： public sealed class Singleton2 { private Singleton2() { } private static readonly object syncObj = new object(); private static Singleton2 instance = null; public static Singleton2 Instance { get { lock (syncObj) { if (instance == null) instance = new Singleton2(); } return instance; } } } 我们还是假设有两个线程同时想创建一个实例。由于在一个时刻只有一个线程能得到同步锁，当第一个线程加上锁时，第二个线程只能等待。当第一个线程发现实例还没有创建时，它创建出一个实例。接着第一个线程释放同步锁，此时第二个线程可以加上同步锁，并运行接下来的代码。这时候由于实例已经被第一个线程创建出来了，第二个线程就不会重复创建实例了，这样就保证了我们在多线程环境中也只能得到一个实例。 但是类型Singleton2还不是很完美。我们每次通过属性Instance得到Singleton2的实例，都会试图加上一个同步锁，而加锁是一个非常耗时的操作，在没有必要的时候我们应该尽量避免。 可行的解法：加同步锁前后两次判断实例是否已存在 我们只是在实例还没有创建之前需要加锁操作，以保证只有一个线程创建出实例。而当实例已经创建之后，我们已经不需要再执行加锁操作了。于是我们可以把解法二中的代码再做进一步的改进： public sealed class Singleton3 { private Singleton3() { } private static object syncObj = new object(); private static Singleton3 instance = null; public static Singleton3 Instance { get { if (instance == null) { lock (syncObj) { if (instance == null) instance = new Singleton3(); } } return instance; } } } Singleton3中只有当instance为null即没有创建时，需要加锁操作。当instance已经创建出来之后，则无须加锁。因为只在第一次的时候instance为null，因此只在第一次试图创建实例的时候需要加锁。这样Singleton3的时间效率比Singleton2要好很多。 Singleton3用加锁机制来确保在多线程环境下只创建一个实例，并且用两个if判断来提高效率。这样的代码实现起来比较复杂，容易出错，我们还有更加优秀的解法。 强烈推荐的解法一：利用静态构造函数 C#的语法中有一个函数能够确保只调用一次，那就是静态构造函数，我们可以利用C#的这个特性实现单例模式。 public sealed class Singleton4 { private Singleton4() { Console.WriteLine(\"An instance of Singleton4 is created.\"); } public static void Print() { Console.WriteLine(\"Singleton4 Print\"); } private static Singleton4 instance = new Singleton4(); public static Singleton4 Instance { get { return instance; } } } Singleton4的实现代码非常简洁。我们在初始化静态变量instance的时候创建一个实例。由于C#是在调用静态构造函数时初始化静态变量，.NET运行时能够确保只调用一次静态构造函数，这样我们就能够保证只初始化一次instance。 C#中调用静态构造函数的时机不是由程序员掌控的，而是当.NET运行时发现第一次使用一个类型的时候自动调用该类型的静态构造函数。因此在Singleton4中，实例instance并不是在第一次调用属性Singleton4.Instance的时候被创建的，而是在第一次用到Singleton4的时候就会被创建。假设我们在Singleton4中添加一个静态方法，调用该静态函数是不需要创建一个实例的，但如果按照Singleton4的方式实现单例模式，则仍然会过早地创建实 例，从而降低内存的使用效率。 强烈推荐的解法二：实现按需创建实例 最后一个实现Singleton5则很好地解决了Singleton4中的实例创建时机过早的问题。 public sealed class Singleton5 { Singleton5() { Console.WriteLine(\"An instance of Singleton5 is created.\"); } public static void Print() { Console.WriteLine(\"Singleton5 Print\"); } public static Singleton5 Instance { get { return Nested.instance; } } class Nested { static Nested() { } internal static readonly Singleton5 instance = new Singleton5(); } } 在上述Singleton5的代码中，我们在内部定义了一个私有类型Nested。当第一次用到这个嵌套类型的时候，会调用静态构造函数创建Singleton5的实例instance。类型Nested只在属性Singleton5.Instance中被用到，由于其私有属性，他人无法使用Nested类型。因此，当我们第一次试图通过属性Singleton5.Instance得到Singleton5的实例时，会自动调用Nested的静态构造函数创建实例instance。如果我们不调用属性Singleton5.Instance，就不会触发.NET运行时调用Nested，也不会创建实例，这样就真正做到了按需创建。 解法比较 在前面的5种实现单例模式的方法中，第一种方法在多线程环境能正常工作，第二种模式虽然能在多线程环境中正常工作，但时间效率很低，都不是面试官期待的解法。在第三种方法中，我们通过两次判断一次加锁确保在多线程环境中能高效率地工作。第四种方法利用C#的静态构造函数的特性，确保只创建一个实例。第五种方法利用私有嵌套类型的特性，做到只在真正需要的时候才会创建实例，提高空间使用效率。如果在面试中给出第四种或者第五种解法，则毫无疑问会得到面试官的青睐。 源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/02_Singleton "},"pages/interview/sorting_algorithm.html":{"url":"pages/interview/sorting_algorithm.html","title":"排序算法","keywords":"","body":"排序算法 冒泡排序 import Foundation func bubbleSort (arr: inout [Int]) { for i in 0.. arr[j+1] { arr.swapAt(j, j+1) } } } } // 测试调用 func testSort () { // 生成随机数数组进行排序操作 var list:[Int] = [] for _ in 0...99 { list.append(Int(arc4random_uniform(100))) } print(\"\\(list)\") bubbleSort(arr:&list) print(\"\\(list)\") } testSort() 选择排序 /// 选择排序 /// /// - Parameter list: 需要排序的数组 func selectionSort(_ list: inout [Int]) -> Void { for j in 0.. list[i] { minIndex = i } } list.swapAt(j, minIndex) } } 插入排序 func insertSort(list: inout [Int]) { for i in 1.. temp { list.swapAt(j, j+1) } } } } 希尔排序 public func insertSort(_ list: inout[Int], start: Int, gap: Int) { for i in stride(from: (start + gap), to: list.count, by: gap) { let currentValue = list[i] var pos = i while pos >= gap && list[pos - gap] > currentValue { list[pos] = list[pos - gap] pos -= gap } list[pos] = currentValue } } public func shellSort(_ list: inout [Int]) { var sublistCount = list.count / 2 while sublistCount > 0 { for pos in 0.. 快速排序 func quicksort(_ a: [T]) -> [T] { guard a.count > 1 else { return a } let pivot = a[a.count/2] let less = a.filter { $0 pivot } return quicksort(less) + equal + quicksort(greater) } 归并排序 func mergeSort(_ array: [Int]) -> [Int] { guard array.count > 1 else { return array } // 1 let middleIndex = array.count / 2 // 2 let leftArray = mergeSort(Array(array[0.. 堆排序 func heapSort(_ array : inout Array){ //1、构建大顶堆 //从二叉树的一边的最后一个结点开始 for i in (0...(array.count/2-1)).reversed() { //从第一个非叶子结点从下至上，从右至左调整结构 SortSummary.adjustHeap(&array, i, array.count) } //2、调整堆结构+交换堆顶元素与末尾元素 for j in (1...(array.count-1)).reversed() { //将堆顶元素与末尾元素进行交换 array.swapAt(0, j) //重新对堆进行调整 SortSummary.adjustHeap(&array, 0, j) } } //调整大顶堆（仅是调整过程，建立在大顶堆以构建的基础上） func adjustHeap(_ array : inout Array, _ i : Int, _ length : Int){ var i = i //取出当前元素i let tmp = array[i] var k = 2*i+1 //从i结点的左子节点开始，也就是2i+1处开始 while k tmp { array[i] = array[k] //记录当前结点 i = k }else{ break } //下一个结点 k = k*2+1 } //将tmp值放到最终的位置 array[i] = tmp } 参考 Swift算法俱乐部-希尔排序Swift算法俱乐部-归并排序swift算法之排序：（四）堆排序 十大经典排序算法常用算法面试题Swift算法俱乐部-快速排序Sort "},"pages/interview/RSA.html":{"url":"pages/interview/RSA.html","title":"RSA","keywords":"","body":"RSA 面试官：如何保证用户模块的数据安全？说说你的解决方案RSA算法介绍如何加密传输和存储用户密码 "},"pages/interview/Interview_question_link.html":{"url":"pages/interview/Interview_question_link.html","title":"面试题链接","keywords":"","body":"面试题链接 iOSInterviewsAndDevNotes iOS开发中 weak和assign的区别 做了快5年iOS，这份面试题让我从15K变成了30K 《招聘一个靠谱的iOS》面试题参考答案 iOS性能优化 iOS]NSHashTable和NSMapTable用法 Swift中的unowned和weak "},"pages/interview/Arrays_and_pointers.html":{"url":"pages/interview/Arrays_and_pointers.html","title":"数组与指针","keywords":"","body":"数组与指针 在C/C++中，数组和指针是既相互关联又有区别的两个概念。当我们声明一个数组时，其数组的名字也是一个指针，该指针指向数组的第一个元素。我们可以用一个指针来访问数组。但值得注意的是，C/C++没有记录数组的大小，因此在用指针访问数组中的元素时，程序员要确保没有超出数组的边界。下面通过一个例子来了解数组和指针的区别。运行下面的代码，请问输出是什么？ int GetSize(int data[]) { return sizeof(data); } int main(int argc, const char * argv[]) { int data1[] = {1,2,3,4,5}; int size1 = sizeof(data1); int* data2 = data1; int size2 = sizeof(data2); int size3 = GetSize(data1); printf(\"%d,%d,%d\",size1,size2,size3); return 0; } 答案是输出“20,4,4”。data1是一个数组，sizeof(data1)是求数组的大小。这个数组包含5个整数，每个整数占4字节，因此共占用20字节。data2声明为指针，尽管它指向了数组data1的第一个数字，但它的本质仍然是一个指针。在32位系统上，对任意指针求sizeof，得到的结果都是4。在C/C++中，当数组作为函数的参数进行传递时，数组就自动退化为同类型的指针。因此，尽管函数GetSize的参数data被声明为数组，但它会退化为指针，size3的结果仍然是4。 剑指 Offer P38 "},"pages/interview/Repeated_numbers_in_array.html":{"url":"pages/interview/Repeated_numbers_in_array.html","title":"数组中重复的数字","keywords":"","body":"数组中重复的数字 题目一：找出数组中重复的数字。 在一个长度为n的数组里的所有数字都在0~n-1的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是重复的数字2或者3。 解决这个问题的一个简单的方法是先把输入的数组排序。从排序的数组中找出重复的数字是一件很容易的事情，只需要从头到尾扫描排序后的数组就可以了。排序一个长度为n的数组需要O(nlogn)的时间。 还可以利用哈希表来解决这个问题。从头到尾按顺序扫描数组的每个数字，每扫描到一个数字的时候，都可以用O(1)的时间来判断哈希表里是否已经包含了该数字。如果哈希表里还没有这个数字，就把它加入哈希表。如果哈希表里已经存在该数字，就找到一个重复的数字。这个算法的时间复杂度是O(n)，但它提高时间效率是以一个大小为O(n)的哈希表为代价的。我们再看看有没有空间复杂度是O(1)的算法。 我们注意到数组中的数字都在0~n-1的范围内。如果这个数组中没有重复的数字，那么当数组排序之后数字i将出现在下标为i的位置。由于数组中有重复的数字，有些位置可能存在多个数字，同时有些位置可能没有数字。 现在让我们重排这个数组。从头到尾依次扫描这个数组中的每个数字。当扫描到下标为i的数字时，首先比较这个数字（用m表示）是不是等于i。如果是，则接着扫描下一个数字；如果不是，则再拿它和第m个数字进行比较。如果它和第m个数字相等，就找到了一个重复的数字（该数字在下标为i和m的位置都出现了)；如果它和第m个数字不相等，就把第i个数字和第m个数字交换，把m放到属于它的位置。接下来再重复这个比较、交换的过程，直到我们发现一个重复的数字。 以数组{2,3,1,0,2,5,3}为例来分析找到重复数字的步骤。数组的第0个数字（从0开始计数，和数组的下标保持一致）是2，与它的下标不相等，于是把它和下标为2的数字1交换。交换之后的数组是{1,3,2,0,2,5,3}。此时第0个数字是1，仍然与它的下标不相等，继续把它和下标为1的数字3交换，得到数组{3,1,2,0,2,5,3}。接下来继续交换第0个数字3和第3个数字0，得到数组{0,1,2,3,2,5,3}。此时第0个数字的数值为0，接着扫描下一个数字。在接下来的几个数字中，下标为1、2、3的3个数字分别为1、2、3，它们的下标和数值都分别相等，因此不需要执行任何操作。接下来扫描到下标为4的数字2。由于它的数值与它的下标不相等，再比较它和下标为2的数字。注意到此时数组中下标为2的数字也是2，也就是数字2在下标为2和下标为4的两个位置都出现了，因此找到一个重复的数字。 上述思路可以用如下代码实现： // 参数: // numbers: 一个整数数组 // length: 数组的长度 // duplication: (输出) 数组中的一个重复的数字 // 返回值: // true - 输入有效，并且数组中存在重复的数字 // false - 输入无效，或者数组中没有重复的数字 bool duplicate(int numbers[], int length, int* duplication) { if(numbers == nullptr || length length - 1) return false; } for(int i = 0; i 在上述代码中，找到的重复数字通过参数duplication传给函数的调用者，而函数的返回值表示数组中是否有重复的数字。当输入的数组中存在 重复的数字时，返回true；否则返回false。代码中尽管有一个两重循环，但每个数字最多只要交换两次就能找到属于它自己的位置，因此总的时间复杂度是O(n)。另外，所有的操作步骤都是在输入数组上进行的，不需要额外分配内存，因此空间复杂度为O(1)。 剑指 Offer P39，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/03_01_DuplicationInArray "},"pages/interview/Repeated_numbers_in_array_2.html":{"url":"pages/interview/Repeated_numbers_in_array_2.html","title":"数组中重复的数字2","keywords":"","body":"数组中重复的数字2 题目二，不修改数组找出重复的数字。 在一个长度为n+1的数组里的所有数字部在1~n的范面内，所以数组中至少有一个数字是重复的，请找出数组中任意一个重复的数字，但不能修改输入的数组。例如，如果输入长度为8的数组{2,3,5,4,3,2,6,7}，那么对应的输出是重复的数字2或者3。 这一题看起来和上面的面试题类似。由于题目要求不能修改输入的数组，我们可以创建一个长度为n+1的辅助数组，然后逐一把原数组的每个数字复制到辅助数组。如果原数组中被复制的数字是m，则把它复制到辅助数组中下标为m的位置。这样很容易就能发现哪个数字是重复的。由于需要创建一个数组，该方案需要O(n)的辅助空间。 接下来我们尝试避免使用O(n)的辅助空间。为什么数组中会有重复的数字？假如没有重复的数字，那么在从1~n的范围里只有n个数字。由于数组里包含超过n个数字，所以一定包含了重复的数字。看起来在某范围里数字的个数对解决这个问题很重要。 我们把从1~n的数字从中间的数字m分为两部分，前面一半为1~m，后面一半为m+1~n。如果1~m的数字的数目超过m,那么这一半的区间里一定包含重复的数字；否则，另一半m+1~n的区间里一定包含重复的数字。我们可以继续把包含重复数字的区间一分为二，直到找到一个重复的数字。这个过程和二分查找算法很类似，只是多了一步统计区间里数字的数目。 我们以长度为8的数组{2,3,5,4,3,2,6,7}为例分析查找的过程。根据题目要求，这个长度为8的所有数字都在1~7的范围内。中间的数字4把1~7的范围分为两段，一段是1~4，另一段是5~7。接下来我们统计1～4这4个数字在数组中出现的次数，它们一共出现了5次，因此这4个数字中一定有重复的数字。 接下来我们再把1~4的范围一分为二，一段是1、2两个数字，另一段是3、4两个数字。数字1或者2在数组中一共出现了两次。我们再统计数字3或者4在数组中出现的次数，它们一共出现了三次。这意味着3、4两个数字中一定有一个重复了。我们再分别统计这两个数字在数组中出现的次数。接着我们发现数字3出现了两次，是一个重复的数字。 上述思路可以用如下代码实现： int countRange(const int* numbers, int length, int start, int end); // 参数: // numbers: 一个整数数组 // length: 数组的长度 // 返回值: // 正数 - 输入有效，并且数组中存在重复的数字，返回值为重复的数字 // 负数 - 输入无效，或者数组中没有重复的数字 int getDuplication(const int* numbers, int length) { if(numbers == nullptr || length = start) { // 0000 0010 >> 左移1位 0000 0101 int middle = ((end - start) >> 1) + start; int count = countRange(numbers, length, start, middle);// 查找落在二分左区间内个数 //cout 1) return start; else break; } if(count > (middle - start + 1))// 如果落在左区间的个数大于区间范围，则这里面一定有重复，否则就去右区间看看 end = middle; else start = middle + 1; } return -1; } int countRange(const int* numbers, int length, int start, int end) { if(numbers == nullptr) return 0; int count = 0; for(int i = 0; i = start && numbers[i] 上述代码按照二分查找的思路，如果输入长度为n的数组，那么函数countRange将被调用O(logn)次，每次需要O(n)的时间，因此总的时间复杂度是O(nlogn)，空间复杂度为O(1)。和最前面提到的需要O(n)的辅助空间的算法相比，这种算法相当于以时间换空间。 需要指出的是，这种算法不能保证找出所有重复的数字。例如，该算法不能找出数组{2,3,5,4,3,2,6,7}中重复的数字2。这是因为在1~2的范围里有1和2两个数字，这个范围的数字也出现2次，此时我们用该算法不能确定是每个数字各出现一次还是某个数字出现了两次。 从上述分析中我们可以看出，如果面试官提出不同的功能要求（找出任意一个重复的数字、找出所有重复的数字)或者性能要求（时间效率优先、空间效率优先)，那么我们最终选取的算法也将不同。这也说明在面试中和面试官交流的重要性，我们一定要在动手写代码之前弄清楚面试官的需求。 剑指 Offer P41，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/03_02_DuplicationInArrayNoEdit "},"pages/interview/Search_in_two_dimensional_array.html":{"url":"pages/interview/Search_in_two_dimensional_array.html","title":"二维数组中的查找","keywords":"","body":"二维数组中的查找 题目：在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的个二维数组和一个整数，判断数组中是否含有该整数。例如下面的二维数组就是每行、每列都递增排序。 如果在这个数组中查找数字7，则返回true;如果查找数字5，由于数组不含有该数字，则返回false。 1 2 8 9 2 4 9 1 4 7 9 12 6 8 11 15 在分析这个问题的时候，很多应聘者都会把二维数组画成矩形，然后从数组中选取一个数字，分3种情况来分析查找的过程。当数组中选取的数字刚好和要查找的数字相等时，就结束查找过程。如果选取的数字小于要查找的数字，那么根据数组排序的规则，要查找的数字应该在当前选取位置的右边或者下边，如图2.1(a)所示。同样，如果选取的数字大于要查找的数字，那么要查找的数字应该在当前选取位置的上边或者左边，如图2.1(b)所示。 图2.1二维数组中的查找 注：在数组中间选择一个数（深色方格），根据它的大小判断要查找的数字可能出现的区域（阴影部分）。 在上面的分析中，由于要查找的数字相对于当前选取的位置有可能在两个区域中出现，而且这两个区域还有重叠，这问题看起来就复杂了，于是很多人就卡在这里束手无策了。 当我们需要解决一个复杂的问题时，一个很有效的办法就是从一个具体的问题入手，通过分析简单具体的例子，试图寻找普遍的规律。针对这个问题，我们不妨也从一个具体的例子入手。下面我们以在题目中给出的数组中查找数字7为例来一步步分析查找的过程。 前面我们之所以遇到难题，是因为我们在二维数组的中间选取一个数字来和要查找的数字进行比较，这就导致下一次要查找的是两个相互重叠的区域。如果我们从数组的一个角上选取数字来和要查找的数字进行比较，那么情况会不会变简单呢？ 首先我们选取数组右上角的数字9。由于9大于7，并且9还是第4列的第一个（也是最小的）数字，因此7不可能出现在数字9所在的列。于是我们把这一列从需要考虑的区域内剔除，之后只需要分析剩下的3列，如图2.2(a)所示。在剩下的矩阵中，位于右上角的数字是8。同样8大于7，因此8所在的列我们也可以剔除。接下来我们只要分析剩下的两列即可，如图2.2(b)所示。 在由剩余的两列组成的数组中，数字2位于数组的右上角。2小于7，那么要查找的7可能在2的右边，也可能在2的下边。在前面的步骤中，我们已经发现2右边的列都已经被剔除了，也就是说7不可能出现在2的右边，因此7只有可能出现在2的下边。于是我们把数字2所在的行也剔除，只分析剩下的三行两列数字，如图2.2(c)所示。在剩下的数字中，数字4位于右上角，和前面一样，我们把数字4所在的行也删除，最后剩下两行两列数字，如图2.2（d)所示。 在剩下的两行两列4个数字中，位于右上角的刚好就是我们要查找的数字7，于是查找过程就可以结束了。 注：矩阵中加阴影的区域是下一步查找的范围。 总结上述杏找的讨程，我们发现如下规律：首先选取数组中右上角的数字。如果该数字等于要查找的数字，则查找过程结束；如果该数字大于要查找的数字，则剔除这个数字所在的列；如果该数字小于要查找的数字，则剔除这个数字所在的行。也就是说，如果要查找的数字不在数组的右上角，则每一次都在数组的查找范围中剔除一行或者一列，这样每一步都可以缩小查找的范围，直到找到要查找的数字，或者查找范围为空。 把整个查找过程分析清楚之后，我们再写代码就不是一件很难的事情了。下面是上述思路对应的参考代码： bool Find(int* matrix, int rows, int columns, int number) { bool found = false; if(matrix != nullptr && rows > 0 && columns > 0) { int row = 0; int column = columns - 1; while(row =0) { if(matrix[row * columns + column] == number) { found = true; break; } else if(matrix[row * columns + column] > number) -- column; else ++ row; } } return found; } 在前面的分析中，我们每次都选取数组查找范围内的右上角数字。同样，我们也可以选取左下角的数字。感兴趣的读者不妨自己分析一下每次都选取左下角数字的查找过程。但我们不能选择左上角数字或者右下角数字。以左上角数字为例，最初数字1位于初始数组的左上角，由于1小于7，那么7应该位于1的右边或者下边。此时我们既不能从查找范围内剔除1所在的行，也不能剔除1所在的列，这样我们就无法缩小查找的范围。 剑指 Offer P44，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/04_FindInPartiallySortedMatrix "},"pages/interview/Replace_spaces.html":{"url":"pages/interview/Replace_spaces.html","title":"替换空格","keywords":"","body":"替换空格 题目：请实现一个函数，把字符串中的每个空格替换成\"%20\"。例如，输入“We are happy.”，则输出“We%20are%20happy.”。 在网络编程中，如果URL参数中含有特殊字符，如空格、'#'等，则可能导致服务器端无法获得正确的参数值。我们需要将这些特殊符号转换成服务器可以识别的字符。转换的规则是在'%'后面跟上ASCII码的两位十六进制的表示。比如空格的ASCI码是32，即十六进制的0x20,因此空格被替换成\"%20\"。再比如'#'的ASCⅡ码为35，即十六进制的0x23，它在URL中被替换为\"%23\"。 看到这个题目，我们首先应该想到的是原来一个空格字符，替换之后变成'%'、'2'和'0'这3个字符，因此字符串会变长。如果是在原来的字符串上进行替换，就有可能覆盖修改在该字符串后面的内存。如果是创建新的字符串并在新的字符串上进行替换，那么我们可以自己分配足够多的内存。由于有两种不同的解决方案，我们应该向面试官问清楚，让他明确告诉我们他的需求。假设面试官让我们在原来的字符串上进行替换，并且保证输入的字符串后面有足够多的空余内存。 时间复杂度为O(n²)的解法，不足以拿到Offer 现在我们考虑怎么执行替换操作。最直观的做法是从头到尾扫描字符串，每次碰到空格字符的时候进行替换。由于是把1个字符替换成3个字符，我们必须要把空格后面所有的字符都后移2字节，否则就有两个字符被覆盖了。 举个例子，我们从头到尾把\"We are happy.\"中的每个空格替换成\"%20\"。为了形象起见，我们可以用一个表格来表示字符串，表格中的每个格子表示一个字符，如图2.3(a)所示。 注：(a)字符串\"We are happy.\"。(b)把字符串中的第一个空格替换成%20。灰色背景表示需要移动的字符。(c)把字符串中的第二个空格替换成%20。浅灰色背景表示需要移动一次的字符，深灰色背景表示需要移动两次的字符。 我们替换第一个空格，这个字符串变成图2.3（b）中的内容，表格中灰色背景的格子表示需要进行移动的区域。接着我们替换第二个空格，替换之后的内容如图2.3（c）所示。同时，我们注意到用深灰色背景标注的“happy”部分被移动了两次。 假设字符串的长度是n。对每个空格字符，需要移动后面O(n)个字符，因此对于含有O(n)个空格字符的字符串而言，总的时间效率是O(n²)。 当我们把这种思路阐述给面试官后，他不会就此满意，他将让我们寻找更快的方法。在前面的分析中，我们发现数组中很多字符都移动了很多次，能不能减少移动次数呢？答案是肯定的。我们换一种思路，把从前向后替换改成从后向前替换。 时间复杂度为O(n)的解法，搞定Offer就靠它了 我们可以先遍历一次字符串，这样就能统计出字符串中空格的总并可以由此计算出替换之后的字符串的总长度。每替换一个空格，长度增加2，因此替换以后字符串的长度等于原来的长度加上2乘以空格数目。我们还是以前面的字符串\"We are happy.\"为例。\"We are happy.\"这个字符串的长度是14（包括结尾符号0），里面有两个空格，因此替换之后字符串的长度是18。 我们从字符串的后面开始复制和替换。首先准备两个指针：P1和P2。P1指向原始字符串的末尾，而P2指向替换之后的字符串的末尾，如图2.4(a)所示。接下来我们向前移动指针P1，逐个把它指向的字符复制到P2指向的位置，直到碰到第一个空格为止。此时字符串如图2.4(b)所示，灰色背景的区域是进行了字符复制（移动）的区域。碰到第一个空格之后，把P1向前移动1格，在P2之前插入字符串\"%20\"。由于\"%20\"的长度为3，同时也要把P2向前移动3格，如图2.4(c)所示。 我们接着向前复制，直到碰到第二个空格，如图2.4(d)所示。和上一次一样，我们再把P1向前移动1格，并把P2向前移动3格插入\"%20”，如图2.4（e）所示。此时P1和P2指向同一位置，表明所有空格都已经替换完毕。 从上面的分析中我们可以看出，所有的字符都只复制（移动）一次，因此这个算法的时间效率是O(n)，比第一个思路要快。 注：图中带有阴影的区域表示被移动的字符。(a)把第一个指针指向字符串的末尾，把第二个指针指向替换之后的字符串的末尾。(b)依次复制字符串的内容，直至第一个指针碰到第一个空格。(c)把第一个空格替换成\"%20”，把第一个指针向前移动1格，把第二个指针向前移动3格。(d)依次向前复制字符串中的字符，直至碰到空格。（e）替换字符串中的倒数第二个空格，把第一个指针向前移动1格，把第二个指针向前移动3格。 在面试过程中，我们也可以和前面的分析一样画一两个示意图解释自己的思路，这样既能帮助我们厘清思路，也能使我们和面试官的交流变得更加高效。在面试官肯定我们的思路之后，就可以开始写代码了。下面是参考代码： /*length 为字符数组str的总容量，大于或等于字符串str的实际长度*/ void ReplaceBlank(char str[], int length) { if(str == nullptr && length length) return; int indexOfOriginal = originalLength; int indexOfNew = newLength; while(indexOfOriginal >= 0 && indexOfNew > indexOfOriginal) { if(str[indexOfOriginal] == ' ') { str[indexOfNew --] = '0'; str[indexOfNew --] = '2'; str[indexOfNew --] = '%'; } else { str[indexOfNew --] = str[indexOfOriginal]; } -- indexOfOriginal; } } 剑指 Offer P51，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/05_ReplaceSpaces "},"pages/interview/linked_list.html":{"url":"pages/interview/linked_list.html","title":"链表","keywords":"","body":"链表 链表应该是面试时被提及最频繁的数据结构。链表的结构很简单，它由指针把若干个节点连接成链状结构。链表的创建、插入节点、删除节点等操作都只需要20行左右的代码就能实现，其代码量比较适合面试。而像哈希表、有向图等复杂数据结构，实现它们的一个操作需要的代码量都较大，很难在几十分钟的面试中完成。另外，由于链表是一种动态的数据结构，其需要对指针进行操作，因此应聘者需要有较好的编程功底才能写出完整的操作链表的代码。而且链表这种数据结构很灵活，面试官可以用链表来设计具有挑战性的面试题。基于上述几个原因，很多面试官都特别青睐与链表相关的题目。 我们说链表是一种动态数据结构，是因为在创建链表时，无须知道链表的长度。当插入一个节点时，我们只需要为新节点分配内存，然后调整指针的指向来确保新节点被链接到链表当中。内存分配不是在创建链表时一次性完成的，而是每添加一个节点分配一次内存。由于没有闲置的内存，链表的空间效率比数组高。如果单向链表的节点定义如下： struct ListNode { int m_nValue; ListNode* m_pNext; }; 那么往该链表的末尾添加一个节点的C++代码如下： void AddToTail(ListNode**pHead, int value) { ListNode* pNew = new ListNode(); pNew->m_nValue = value; pNew->m_pNext = nullptr; if(*pHead == nullptr) { *pHead = pNew; } else { ListNode* pNode = *pHead; while(pNode->m_pNext != nullptr) pNode = pNode->m_pNext; pNode->m_pNext = pNew; } } 在上面的代码中，我们要特别注意函数的第一个参数pHead是一个指向指针的指针。当我们往一个空链表中插入一个节点时，新插入的节点就是链表的头指针。由于此时会改动头指针，因此必须把pHead参数设为指向指针的指针，否则出了这个函数pHead仍然是一个空指针。 由于链表中的内存不是一次性分配的，因而我们无法保证链表的内存和数组一样是连续的。因此，如果想在链表中找到它的第i个节点，那么我们只能从头节点开始，沿着指向下一个节点的指针遍历链表，它的时间效率为O(n)。而在数组中，我们可以根据下标在O(1)时间内找到第i个元素。下面是在链表中找到第一个含有某值的节点并删除该节点的代码： void RemoveNode(ListNode** pHead, int value) { if(pHead == nullptr || *pHead == nullptr) return; ListNode* pToBeDeleted == nullptr; if((*pHead)->m_nValue == value) { pToBeDeleted = *pHead; *pHead = (*pHead)->m_pNext; } else { ListNode* pNode = *pHead; while(pNode->m_pNext != nullptr && pNode->m_pNext->m_nValue != value) pNode = pNode->m_pNext; if(pNode->m_pNext != nullptr && pNode->m_pNext->m_nValue == value) { pToBeDeleted = pNode->m_pNext; pNode->m_pNext = pNode->m_pNext->m_pNext; } } if(pToBeDeleted != nullptr) { delete pToBeDeleted; pToBeDeleted = nullptr; } } 剑指 Offer P56 "},"pages/interview/Print_linked_list_from_end_to_beginning.html":{"url":"pages/interview/Print_linked_list_from_end_to_beginning.html","title":"从尾到头打印链表","keywords":"","body":"从尾到头打印链表 题目：输入一个链表的头节点，从尾到头反过来打印出每个节点的值链表节点定义如下： struct ListNode { int m_nKey; ListNode* m_pNext; }; 看到这道题后，很多人的第一反应是从头到尾输出将会比较简单，于是我们很自然地想到把链表中链接节点的指针反转过来，改变链表的方向，然后就可以从头到尾输出了。但该方法会改变原来链表的结构。是否允许在打印链表的时候修改链表的结构？这取决于面试官的要求，因此在面试的时候我们要询问清楚面试官的要求。 面试小提示：在面试中，如果我们打算修改输入的数据，则最好先问面试官是不是允许修改。 通常打印是一个只读操作，我们不希望打印时修改内容。假设面试官也要求这个题目不能改变链表的结构。 接下来我们想到解决这个问题肯定要遍历链表。遍历的顺序是从头到尾，可输出的顺序却是从尾到头。也就是说，第一个遍历到的节点最后一个输出，而最后一个遍历到的节点第一个输出。这就是典型的“后进先出”，我们可以用栈实现这种顺序。每经过一个节点的时候，把该节点放到一个栈中。当遍历完整个链表后，再从栈顶开始逐个输出节点的值，此时输出的节点的顺序己经反转过来了。这种思路的实现代码如下： #include \"..\\Utilities\\List.h\" #include void PrintListReversingly_Iteratively(ListNode* pHead) { std::stack nodes; ListNode* pNode = pHead; while(pNode != nullptr) { nodes.push(pNode); pNode = pNode->m_pNext; } while(!nodes.empty()) { pNode = nodes.top(); printf(\"%d\\t\", pNode->m_nValue); nodes.pop(); } } 既然想到了用栈来实现这个函数，而递归在本质上就是一个栈结构，于是很自然地又想到了用递归来实现。要实现反过来输出链表，我们每访问到一个节点的时候，先递归输出它后面的节点，再输出该节点自身，这样链表的输出结果就反过来了。 基于这样的思路，不难写出如下代码： void PrintListReversingly_Recursively(ListNode* pHead) { if(pHead != nullptr) { if (pHead->m_pNext != nullptr) { PrintListReversingly_Recursively(pHead->m_pNext); } printf(\"%d\\t\", pHead->m_nValue); } } 上面的基于递归的代码看起来很简洁，但有一个问题：当链表非常长的时候，就会导致函数调用的层级很深，从而有可能导致函数调用栈溢出。显然用栈基于循环实现的代码的鲁棒性要好一些。更多关于循环和递归的讨论，详见本书的2.4.1节。 剑指 Offer P58，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/06_PrintListInReversedOrder "},"pages/interview/Interview_preparation.html":{"url":"pages/interview/Interview_preparation.html","title":"面试准备","keywords":"","body":"面试准备 自我介绍 谢谢您今天给我的这次机会，我是Green，我的专业是计算机通信，9年iOS开发经验，3年Java开发经验，担任过iOS团队负责人，有团队管理经验，有组件化开发经验，具备独立开发能力，具备文档编写能力，有良好的代码习惯。 Thank you for giving me the opportunity to be interviewed for this position today. I'm Green. My major is computer communication. I have 9 years of experience in iOS development, 3 years of experience in Java development, and served as the leader of the iOS team. I have experience in team management, component development, independent development, document writing, and good code habits. 我大学的专业是计算机通信，10年来到上海从事Java后端方面的工作，做了几个OA功能的系统，主要使用的技术有JavaWeb相关的技术，像JAVA、SpringMVC、HTMl、JS等。从事Java大概3年多，我自学进入到了iOS行业，期间做过多媒体拍摄项目，RTC直播项目，对自研播放器有所了解。我掌握的技术章有OC、Swift，熟悉C、C++编程，掌握组件化编程，能够独立开发iOS项目。最近也学习了Flutter相关知识，通过阅读开源项目，对Flutter技术栈了解。 我的名字叫陈长青，今年34岁，我的专业是计算机通信，拥有丰富的开发经验。我擅长于使用Swift语言，进行iOS移动开发，同时我还熟悉Java、C、C++等语言，并且我可以使用SwiftUI进行开发。我最近开发的项目使用了Swift语言，是在汇丰驻场进行开发，我很喜欢客户公司环境，我也非常热爱这份工作。 My name is Changqing Chen, I am 34 years old, my major is computer communication, and I have rich development experience. I am good at using Swift language for iOS mobile application, and I am also familiar with languages ​​such as Objc, Java, C, C++, and I can use SwiftUI for development. My last project uses Swift language. I like the client company environment very much, and I love this job very much. 我的名字叫陈长青，今年34岁，我的专业是计算机通信，拥有丰富的开发经验。我擅长于使用Swift语言，进行iOS移动开发，同时我还熟悉Java、C、C++等语言，并且我可以使用SwiftUI进行开发。我熟悉CICD，我会使用flutter，我了解iOS架构，我还熟悉敏捷开发，我还可以使用设计模式。 My name is Changqing Chen, I am 34 years old, my major is computer communication, and I have rich development experience. I am good at using Swift language for iOS mobile application, and I am also familiar with languages such as Objc, Java, C, C++, and I can use SwiftUI for development. I am familiar with CICD, I can use flutter, I understand iOS architecture, and I am familiar with scrum development, and I can use design patterns. 擅长技术 熟练OC、Swift、Java、RxSwift、MVVM、MVC、Cocoapods、GCD、Git Flow。熟悉C、C++、Maven、HTML、JS、CSS、SQL、Mysql、Tomcat、JDBC、SSH。了解Android、Flutter、ReactNative、libRTMP、OpenGLES、FFmpeg。 视频播放器 组件化开发 设计模式 排序算法 数据结构 RSA CICD SwiftUI 敏捷开发 如何学习 在工作中，如果遇到自己不会的问题，通过Google、百度相关资料，然后总结记录。学习一门新技术，我会通过查看其官方教程或找学习视频，同时买一本书籍阅读，然后实践，再根据自己的理解总结写文章记录。 职业规划 结合自身还有目前的职业环境，我有认真想过这个问题。在工作方面，我想通过积极完成工作任务，积累各方面经验，让自己成为这个领域的专业人士，也希望有机会能够带领团队，成为优秀的管理者，为单位作出更大的贡献，实现双赢。在学习方面，打算在iOS专业领域做进一步学习和研究，同时也学习Android、H5等技术，为以后自己成为管理者做下铺垫。 提问环节 项目中会使用SwiftUI吗？ 项目是针对海外客户吗？ 一个功能需求下来，在开发人员开发着前，您最希望它做好哪些开发准备？ 项目中难免存在一些不得不进行重构优化的代码，您是如何看待这个问题的？ 面试问题 函数式编程：函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！ 线程间通信：内存共享、通知、等待、锁。 swift特性：元组、可选、解包、扩展、泛型、枚举、泛型关联、命名空间、权限关键字、协议、闭包。 内存管理：Swift使用自动引用计数（ARC）来简化内存管理，与OC一致。 swift语言、架构能力、block原理、swift特性、项目管理、代码规范。 内存管理 OC 和 Swift 的弱引用源码分析iOS内存分配-栈和堆 多线程 关于iOS多线程，你看我就够了 参考链接 Swift 语言的一些功能特性为何面试时都会问你的职业规划呢？该如何回答呢？IOS面试题(其他) --- 英文自我介绍 凯捷 代码加固** https://zhuanlan.zhihu.com/p/33109826 1.字符串混淆 对应用程序中使用到的字符串进行加密，保证源码被逆向后不能看出字符串的直观含义。 2.类名、方法名混淆 对应用程序的方法名和方法体进行混淆，保证源码被逆向后很难明白它的真正功能。 3.程序结构混淆加密 对应用程序逻辑结构进行打乱混排，保证源码可读性降到最低。 4.反调试、反注入等一些主动保护策略 这是一些主动保护策略，增大破解者调试、分析App的门槛。 文件名重复会有什么影响 https://blog.csdn.net/weixin_33994429/article/details/93696758 duplicate symbol问题 swift和oc的区别 1.Swift和Objective-C共用一套运行时环境，Swift的类型可以桥接到Objective-C。 2.swift是静态语言，有类型推断，更加安全，OC是动态语言。 3.swift支持泛型，OC只支持轻量泛型 4.Swift速度更快，运算性能更高。 5.Swift的访问权限变更。 7.Swift便捷的函数式编程。 8.swift有元组类型、支持运算符重载 9.swift引入了命名空间。 10.swift支持默认参数。 11.swift比oc代码更加简洁。 struct和class的区别 https://blog.csdn.net/baidu_40537062/article/details/108349757 1.struct是值类型（Value Type）,深拷贝。class是引用类型（Reference Type），浅拷贝。 2.类允许被继承，结构体不允许被继承。 3.类中的每一个成员变量都必须被初始化，否则编译器会报错，而结构体不需要，编译器会自动帮我们生成init函数，给变量赋一个默认值。 4.NSUserDefaults：Struct 不能被序列化成 NSData 对象,无法归解档。 5.当你的项目的代码是 Swift 和 Objective-C 混合开发时，你会发现在 Objective-C 的代码里无法调用 Swift 的 Struct。因为要在 Objective-C 里调用 Swift 代码的话，对象需要继承于 NSObject。 6.class像oc的类一样，可以用kvo,kvc,runtime的相关方法，适用runtime系统。这些struct都不具备。 7.内存分配：struct分配在栈中，class分配在堆中。struct比class更“轻量级”（struct是跑车跑得快，class是SUV可以载更多的人和货物）。 验证HTTPS证书 客户端向服务器发送支持的SSL/TSL的协议版本号，以及客户端支持的加密方法，和一个客户端生成的随机数。 服务器确认协议版本和加密方法，向客户端发送一个由服务器生成的随机数，以及数字证书。 客户端验证证书是否有效，有效则从证书中取出公钥，生成一个随机数，然后用公钥加密这个随机数，发给服务器。 服务器用私钥解密，获取发来的随机数。 客户端和服务器根据约定好的加密方法，使用前面生成的三个随机数，生成对话密钥，用来加密接下来的整个对话过程。 作者：阿拉斯加大狗 链接：https://juejin.cn/post/6844903892765900814 来源：稀土掘金 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 H5信息存储在哪 localStorage与？区别 cookie 图片宽高未知的情况下怎么自适应高度 Model层异步下载图片，然后缓存图片及图片宽高。 图片显示先占位，按需加在图片，缓存图片及图片宽高，reload指定Cell。 上传图片告诉服务器图片尺寸。 通过约束让Cell高度随图片自适应。 push很多页面后怎么控制导航栏里面的子控制器 push到相同页面产生递归。 内存回收处理。 灵动岛 https://www.51cto.com/article/742613.html 展示： 紧凑(Compact)、最小化(Minimal)、扩展(Expanded) 开发框架：ActivityKit（实时活动）、SwiftUI（UI）、WidgetKit（小组件） 实时活动权限 静态库制作 https://www.jianshu.com/p/8ea45370a20d XCFramework H5的http拦截 RSA 1024 2048区别 Git分之管理 支付 flutter Flutter 的生命周期 Flutter Redux：带例子的完整教程 Flutter - iOS集成开发 flutter与native通信 "},"pages/interview/video_player.html":{"url":"pages/interview/video_player.html","title":"视频播放器","keywords":"","body":"视频播放器 实现一个播放器SDK，要求播放控制界面可定制，提供播放、暂停、停止、Seek等功能。 I帧、P帧、B帧 I帧表示关键帧，帧画面的完整保留，解码时只需要本帧数据就可以完成。 P帧表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。 B帧是双向差别帧，也就是B帧记录的是本帧与前后帧的差别，换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面，B帧压缩率高，但是解码时CPU会比较累。 视频如何播放？ 视频画面其实是由视频帧组成，分别为I帧、P帧、B帧。也就是说，显示视频画面需要对视频压缩格式进行遍历解码帧数据，遍历的同时对每一帧数据按照播放时间进行显示，每一帧可以理解为一张图片，其实就是对图片数据的显示。 视频如何显示？ 不仅是视频，显示问题无处不在，我们所有在手机端看到的画面都和视频一样，是一个不断刷新绘制的过程，底层都是通过使用OpenGL的API，对GPU硬件发出指令，通过图形渲染管线程序，最终在屏幕的每个像素点显示。OpenGL是跨平台的，iOS上是通过GLKView进行渲染显示。 什么是图形渲染管线？ 顶点着色器 —— 图元装配 —— 几何着色器 —— 光栅化 —— 片段着色器 —— 测试与混合。 着色器是一段运行在GPU中的程序；顶点着色器确定绘制图形的形状；图元装配是将顶点着色器传来的顶点数据组装为图元；光栅化是将一个图元转化为一张二维的图片，而这张图片由若干个片段（fragment）组成；片段着色器计算片段的颜色；测试和混合丢弃一些不需要显示的片段。 如何播放全景视频？ 在图形渲染管线中，顶点着色器和片元着色器是可编程的，也就是说我们可以通过顶点着色器构建任意事物模型顶点，然后通过过GPU进行绘制，再通过片元着色器给事物上色（纹理贴图）。普通的视频播放可以理解为就是在一个二维的面显示视频画面，二维的面就是顶点着色器构建的，而画面的显示则交给了片元着色器，那么全景视频的显示，其实就是在顶点着色器环节构建一个球面即可实现播放全景视频。 iOS上如何对MP4文件播放？ AVFoundation中的AVPlayer提供对视频压缩文件的播放。以下是播放步骤： 1）构建AVPlayerItem实例。2）通过KVO监听AVPlayerItem实例加载的状态，加载成功，否则播放失败，最后移除监听。3）构建播放器AVPlayer实例。4）为AVPlayerItem实例增加AVPlayerItemVideoOutput实例，加入异步队列。5）构建屏幕定时器，通过AVPlayerItemVideoOutput实例获取当前视频帧数据，交给OpengGL渲染。6）监听播放进度，播放结束。7）开始播放。 如何使用FFmpeg对MP4文件播放？ FFmpeg提供ffplay命令可以对几乎所有的视频压缩格式进行播放，包括yuv格式。 播放器设计 播发器类：CPVideoPlayer，提供播放、暂停、停止、Seek等方法。协议一：CPVideoPlayerDelegate，回调播放状态及视频数据。协议二：CPVideoPlayerDataSource，获取视频地址。协议三：CPVideoControlViewProtocol，播放控制界面。 播放器weak引用上面三个协议。 参考链接 一看就懂的OpenGL ES教程——图形渲染管线的那些事YUV图解I帧、P帧、B帧、GOP、IDR 和PTS, DTS之间的关系 "},"pages/interview/Component_development.html":{"url":"pages/interview/Component_development.html","title":"组件化开发","keywords":"","body":"组件化开发 如何优雅的使用CTMeditor？ 我们项目会有多个组件，比如：个人中心（Me），登录（Login）等，现在我想实现如下的写法： Mediator.shared.me Mediator.shared.login 以me为例子，建立Me的结构体，为Me增加扩展，如果Base为Mediator类型，那么可以拥有debugEnvironment方法。 public struct Me { public let base: Base public init(_ base: Base) { self.base = base } } extension Me where Base: Mediator { public func debugEnvironment(_ env: String) -> Bool { var deliverParams: [String: Any] = [\"env\": env] let result = base.performTarget(\"Me\", action: \"DebugEnvironment\", params: deliverParams) as? [String: Any] return (result?[\"result\"] as? Bool) ?? false } } 扩展Mediator，为Mediator增加me的实例 public protocol MeProtocol {} extension Mediator: MeProtocol {} extension MeProtocol { public var me: Me { return Me(self) } public static var me: Me.Type { return Me.self } } 通过以上操作就可以实现Mediator.shared.me.debugEnvironment(\"dev\")的调用。 CTMeditor实现原理？ 以Mediator.shared.me.debugEnvironment(\"dev\")为例分析，通过这段代码Mediator可以找到Target是类名为Target_Me的实例，方法名是debugEnvironment，参数\"dev\"封装成了字典，通过Runtime的perform方法就可以调用到目标逻辑。 蘑菇街组件化方案问题？ 1）蘑菇街没有拆分远程调用和本地间调用。2）蘑菇街以远程调用的方式为本地间调用提供服务。3）蘑菇街的本地间调用无法传递非常规参数，复杂参数的传递方式非常丑陋。4）蘑菇街必须要在app启动时注册URL响应者。5）新增组件化的调用路径时，蘑菇街的操作相对复杂。6）蘑菇街没有针对target层做封装。 组建化方案如何加载xib及其他资源文件？ 通过Class找到Bundle，然后构建Nib的时候传入制定的bundle。 参考链接 iOS应用架构谈 组件化方案 "},"pages/interview/design_pattern.html":{"url":"pages/interview/design_pattern.html","title":"设计模式","keywords":"","body":"设计模式 1) 状态模式 在状态模式（State Pattern）中，类的行为是基于它的状态改变的。这种类型的设计模式属于行为型模式。 public class StatePatternDemo { public static void main(String[] args) { Context context = new Context(); StartState startState = new StartState(); startState.doAction(context); System.out.println(context.getState().toString()); StopState stopState = new StopState(); stopState.doAction(context); System.out.println(context.getState().toString()); } } 参考链接 菜鸟·设计模式[译] 使用 Swift 的 iOS 设计模式（第一部分）MVC，MVP 和 MVVM 的图示 "},"pages/interview/data_structure.html":{"url":"pages/interview/data_structure.html","title":"数据结构","keywords":"","body":"数据结构 24.6.30 update 用数组实现栈 /// 用数组实现栈 class Stack { var stack: [AnyObject] var isEmpty: Bool { stack.isEmpty } var peek: AnyObject? { stack.last } init() { stack = [AnyObject]() } func push(object: AnyObject) { stack.append(object) } func pop() -> AnyObject? { if (!isEmpty) { return stack.removeLast() } else { return nil } } } 字典和集合 给出一个整型数组和一个目标值，判断数组中是否有两个数之和等于目标值。 func twoSum(nums: [Int], _ target: Int) -> Bool { var set = Set() for num in nums { if set.contains(target - num) { return true } set.insert(num) } return false } 给定一个整型数组中有且仅有两个数之和等于目标值，求这两个数在数组中的序列号。 func twoSum2(nums: [Int], _ target: Int) -> [Int] { var dict = [Int: Int]() for (i, num) in nums.enumerated() { if let lastIndex = dict[target - num] { return [lastIndex, i] } else { dict[num] = i } } fatalError(\"No valid output\") } 字符串 给出一个字符串，要求将其按照单词顺序进行反转。比如，如果是“the sky is blue”，那么反转后的结果是“blue is sky the”。 fileprivate func _swap(_ chars: inout [T], _ p: Int, _ q: Int) { (chars[p], chars[q]) = (chars[q], chars[p]) } fileprivate func _reverse(_ chars: inout [T], _ start: Int, _ end: Int) { var start = start, end = end while start String? { guard let s = s else { return nil } var chars = Array(s), start = 0 _reverse(&chars, 0, chars.count - 1) for i in 0 .. 链表 给出一个链表和一个值x，要求将链表中所有小于x的值放在左边，所有大于或等于x的值放到右边，并且原链表的结点顺序不能变。 func partition(_ head: ListNode?, _ x: Int) -> ListNode? { let prevDummy = ListNode(0), postDummy = ListNode(0) var prev = prevDummy, post = postDummy var node = head while node != nil { if node!.val 删除链表中倒数第n个节点，例：1，2，3，4，5，n = 2，返回1，2，3，5。 func removeNthFromEnd(head: ListNode?, _ n: Int) -> ListNode? { guard let head = head else { return nil } let dumy = ListNode(0) dumy.next = head var prev: ListNode? = dumy var post: ListNode? = dumy for _ in 0 .. 栈和队列 /// 栈协议 protocol Stack { associatedtype Element var isEmpty: Bool { get } var size: Int { get } var peek: Element? { get } mutating func push(_ newElement: Element) mutating func pop() -> Element? } /// 栈实现 struct IntegerStack: Stack { typealias Element = Int private var stack = [Element]() var isEmpty: Bool { stack.isEmpty } var size: Int { stack.count } var peek: Int? { stack.last } mutating func push(_ newElement: Int) { stack.append(newElement) } mutating func pop() -> Int? { stack.popLast() } } /// 队列协议 protocol Queue { associatedtype Element var isEmpty: Bool { get } var size: Int { get } var peek: Element? { get } mutating func enqueue(_ newElement: Element) mutating func dequeue() -> Element? } /// 队列实现 struct IntegerQueue: Queue { typealias Element = Int private var left = [Element]() private var right = [Element]() var isEmpty: Bool { left.isEmpty && right.isEmpty } var size: Int { left.count + right.count } var peek: Int? { left.isEmpty ? right.first : left.last } mutating func enqueue(_ newElement: Int) { right.append(newElement) } mutating func dequeue() -> Int? { if left.isEmpty { left = right.reversed() right.removeAll() } return left.popLast() } } 给出一个文件的绝对路径，要求将其简化。举个例子，路径是“/home/”，简化后为“/home”；路径是“/a/./b/../../c”，简化后为“/c”。 func simplifyPath(path: String) -> String { /// 用数组来实现栈的功能 var pathStack = [String]() /// 拆分原路径 let paths = path.components(separatedBy: \"/\") for path in paths { /// 对于“.”我们直接跳过 guard path != \".\" else { continue } /// 对于“..”使用pop操作 if path == \"..\" { if (pathStack.count > 0) { pathStack.removeLast() } /// 对于空数组的特殊情况 } else if path != \"\" { pathStack.append(path) } } /// 将栈中的内容转化为优化后的新路径 let res = pathStack.reduce(\"\") { total, dir in \"\\(total)/\\(dir)\" } /// 注意空路径的结果是“/” return res.isEmpty ? \"/\" : res } 用栈实现队列 /// 用数组实现栈 class Stack { var stack: [AnyObject] var isEmpty: Bool { stack.isEmpty } var peek: AnyObject? { stack.last } var size: Int { stack.count } init() { stack = [AnyObject]() } func push(object: AnyObject) { stack.append(object) } func pop() -> AnyObject? { if (!isEmpty) { return stack.removeLast() } else { return nil } } } /// 用栈实现队列 struct MyQueue { var stackA: Stack var stackB: Stack var isEmpty: Bool { stackA.isEmpty && stackB.isEmpty } var peek: Any? { shift() return stackB.peek } var size: Int { stackA.size + stackB.size } init() { stackA = Stack() stackB = Stack() } fileprivate func shift() { if stackB.isEmpty { while !stackA.isEmpty { stackB.push(object: stackA.pop()!) } } } func enqueue(object: AnyObject) { stackA.push(object: object) } func dequeue() -> AnyObject? { shift() return stackB.pop() } } 用队列实现栈 struct Queue { private var left = [AnyObject]() private var right = [AnyObject]() var isEmpty: Bool { left.isEmpty && right.isEmpty } var size: Int { left.count + right.count } var peek: AnyObject? { left.isEmpty ? right.first : left.last } mutating func enqueue(_ newElement: AnyObject) { right.append(newElement) } mutating func dequeue() -> AnyObject? { if left.isEmpty { left = right.reversed() right.removeAll() } return left.popLast() } } /// 用队列实现栈 class MyStack { var queueA: Queue var queueB: Queue init() { queueA = Queue() queueB = Queue() } var isEmpty: Bool { queueA.isEmpty && queueB.isEmpty } var peek: AnyObject? { shift() let peekObj = queueA.peek queueB.enqueue(queueA.dequeue()!) swap() return peekObj } var size: Int { queueA.size } func push(object: AnyObject) { queueA.enqueue(object) } func pop() -> AnyObject? { shift() let popObject = queueA.dequeue() swap() return popObject } private func shift() { while queueA.size != 1 { queueB.enqueue(queueA.dequeue()!) } } private func swap() { (queueA, queueB) = (queueB, queueA) } } 二叉树 /// 节点 class TreeNode { var val: Int var left: TreeNode? var right: TreeNode? init(_ val: Int) { self.val = val } } /// 计算树的深度 func maxDepth(root: TreeNode?) -> Int { guard let root = root else { return 0 } return max(maxDepth(root: root.left), maxDepth(root: root.right)) + 1 } /// 判断一个二叉树是否为二叉查找树 func isValidBST(root: TreeNode?) -> Bool { _helper(node: root, nil, nil) } private func _helper(node: TreeNode?, _ min: Int?, _ max: Int?) -> Bool { guard let node = node else { return true } /// 所有右子树的值都必须大于根节点的值 if let min = min, node.val = max { return false } return _helper(node: node.left, min, node.val) && _helper(node: node.right, node.val, max) } /// 用栈实现的前序遍历 func preorderTraversal(root: TreeNode?) -> [Int] { var res = [Int]() var stack = [TreeNode]() var node = root while !stack.isEmpty || node != nil { if node != nil { res.append(node!.val) stack.append(node!) node = node!.left } else { node = stack.removeLast().right } } return res } "},"pages/interview/SwiftUI.html":{"url":"pages/interview/SwiftUI.html","title":"SwiftUI","keywords":"","body":"SwiftUI 苹果官方教程swiftui SwiftUI-api SwiftUI-tutorials SwiftUI-互动教程 SwiftUI 基础之05 list 和 searchbar (2020) SwiftUI教程与源码 SwiftUI Search Bar in the Navigation Bar UISearchController Tutorial 弄清 SwiftUI，才看得懂苹果的强大 "},"pages/interview/Agile_development.html":{"url":"pages/interview/Agile_development.html","title":"敏捷开发","keywords":"","body":"敏捷开发 在传统开发中，如果变更需求，需有走需求变更流程，当频繁出现需求时，这个流程走起来时非常辛苦，很有可能导致变更不控制，项目就容易失控，这样是谁都不愿意看到的，新产品开发流程--敏捷开发就这样产生了。 敏捷方法Vs传统方法 传统的写作方式 敏捷的写作方式 确定主题 与读者微博、论坛互动 整理大纲、搭建框架 确定主题 书写内容 与读者互动、收集反馈 设计、排版、校对 试写第一章 出版 与读者互动、收集反馈 与读者见面 试写第二章 收集反馈，在版本 ... 初步设计、排版 与读者互动、收集反馈 出版 传统方法的问题：需求不是一尘不变的，等做完了发现不是产品想要的。 敏捷方法的好处：早期交付，较低成本；增加与客户的交互，降低产品不适用的风险；传统方法开发到一半的时候，代码不可用，而敏捷已交付的可用。 生命周期类型 敏捷型：交付频率高、变更程度高。技术不确定、需求确定或者技术确定、需求不确定的项目适用于敏捷开发。 敏捷思维 4大价值观，12大原则，2000多实践。 敏捷思维模式由价值观定义，以原则为指导，并在许多不同的实践中来体现。敏捷实践者根据自身需求选择不同的实践。 敏捷宣言 通过运用此法及帮助他人运用此法，我们正在探寻更好的软件开发方法。在这项工作中，我们看重“个体以及互动”胜过“流程和工具”，“可工作的软件”胜过“完整的文档”，“客户合作”胜过“合同谈判”，“响应变化”胜过“遵循计划”。 4大价值观 以人为本（个体以及互动）、以价值为导向（可工作的软件）、合作共赢（客户合作）、拥抱变化（响应变化）。 12条原则 1.通过早期和持续交付有价值的软件，实现客户满意度。2.欢迎不断变化的需求，即使是在项目开发的后期。要善于利用需求变更，帮助客户获得竞争优势。3.不断交付可用的软件，周期通常是几周，越短越好。4.项目过程中，业务人员与开发人员必须在一起工作。5.项目必须围绕那些有内在动力的个人而建立，他们应该受到信任。6.面对面交谈是最好的沟通方式。7.可用性是衡量进度的主要指标。8.提倡可持续的开发，保持稳定的进展速度。9.不断关注技术是否优秀，设计是否良好。10.简单性至关重要，尽最大可能减少不必要的工作。11.最好的架构、要求和设计，来自团队内部自发的认识。12.团队要定期反思如何更有效，并相应地进行调整。 Scrum 3个支柱：透明性、检验、适应。3个角色：产品负责人、敏捷教练、敏捷团队。 3个工件：产品代办事项列表、冲刺待办事项列表、可交付产品增量。5个事件：冲刺、冲刺规划会议、每日站会、迭代评审会议、迭代回顾会议。 参考链接 敏捷开发入门教程抖音@希赛项目管理 "},"pages/interview/Rebuild_binary_tree.html":{"url":"pages/interview/Rebuild_binary_tree.html","title":"重建二叉树","keywords":"","body":"重建二叉树 题目：输入某二叉树的前序遍历和中序遍历的结果，请重建该二又树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如，输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建如图2.6所示的二叉树并输出它的头节点。二叉树节点的定义如下： struct BinaryTreeNode { int m_nValue; BinaryTreeNode* m_pLeft; BinaryTreeNode* m_pRight; }; 在二叉树的前序遍历序列中，第一个数字总是树的根节点的值。但在中序遍历序列中，根节点的值在序列的中间，左子树的节点的值位于根节点的值的左边，而右子树的节点的值位于根节点的值的右边。因此我们需要扫描中序遍历序列，才能找到根节点的值。 图2.6 根据前序遍历序列(1,2,4,7,3,5,6,8}和中序遍历序列4,7,2,1,5,3,8,6}重建的二叉树 如图2.7所示，前序遍历序列的第一个数字1就是根节点的值。扫描中序遍历序列，就能确定根节点的值的位置。根据中序遍历的特点，在根节点的值1前面的3个数字都是左子树节点的值，位于1后面的数字都是右子树节点的值。 由于在中序遍历序列中，有3个数字是左子树节点的值，因此左子树共有3个左子节点。同样，在前序遍历序列中，根节点后面的3个数字就是3个左子树节点的值，再后面的所有数字都是右子树节点的值。这样我们就在前序遍历和中序遍历两个序列中分别找到了左、右子树对应的子序列。 图2.7 在二叉树的前序遍历和中序遍历序列中确定根节点的值、左子树节点的值和右子树节点的值 既然我们已经分别找到了左、右子树的前序遍历序列和中序遍历序列，我们可以用同样的方法分别构建左、右子树。也就是说，接下来的事情可以用递归的方法去完成。 在想清楚如何在前序遍历和中序遍历序列中确定左、右子树的子序列之后，我们可以写出如下的递归代码： BinaryTreeNode* ConstructCore(int* startPreorder, int* endPreorder, int* startInorder, int* endInorder); BinaryTreeNode* Construct(int* preorder, int* inorder, int length) { if(preorder == nullptr || inorder == nullptr || length m_nValue = rootValue; root->m_pLeft = root->m_pRight = nullptr; if(startPreorder == endPreorder) { if(startInorder == endInorder && *startPreorder == *startInorder) return root; else throw std::exception(\"Invalid input.\"); } // 在中序遍历中找到根结点的值 int* rootInorder = startInorder; while(rootInorder 0) { // 构建左子树 root->m_pLeft = ConstructCore(startPreorder + 1, leftPreorderEnd, startInorder, rootInorder - 1); } if(leftLength m_pRight = ConstructCore(leftPreorderEnd + 1, endPreorder, rootInorder + 1, endInorder); } return root; } 在函数ConstructCore中，我们先根据前序遍历序列的第一个数字创建根节点，接下来在中序遍历序列中找到根节点的位置，这样就能确定左、右子树节点的数量。在前序遍历和中序遍历序列中划分了左、右子树节点的值之后，我们就可以递归地调用函数ConstructCore去分别构建它的左、右子树。 剑指 Offer P62，本题完整的源代码： https://github.com/zhedahht/CodingInterviewChinese2/tree/master/07_ConstructBinaryTree "},"pages/interview/Next_node_of_binary_tree.html":{"url":"pages/interview/Next_node_of_binary_tree.html","title":"二叉树的下一个节点","keywords":"","body":"二叉树的下一个节点 题目：给定一棵二叉树和其中的一个节点，如何找出中序遍历序列的下一个节点？树中的节点除了有两个分别指向左、右子节点的指针，还有一个指向父节点的指针。 在图2.8中的二叉树的中序遍历序列是{d,b,h,e,i,a,f,c,g}。我们将以这棵树为例来分析如何找出二叉树的下一个节点。 图2.8 一棵有9个节点的二叉树。树中从父节点指向子节点的指针用实线表示，从子节点指向父节点的指针用虚线表示 如果一个节点有右子树，那么它的下一个节点就是它的右子树中的最左子节点。也就是说，从右子节点出发一直沿着指向左子节点的指针，我们就能找到它的下一个节点。例如，图2.8中节点b的下一个节点是h，节点a的下一个节点是f。 接着我们分析一个节点没有右子树的情形。如果节点是它父节点的左子节点，那么它的下一个节点就是它的父节点。例如，图2.8中节点d的下一个节点是b，节点f的下一个节点是c。 如果一个节点既没有右子树，并且它还是它父节点的右子节点，那么这种情形就比较复杂。我们可以沿着指向父节点的指针一直向上遍历，直到找到一个是它父节点的左子节点的节点。如果这样的节点存在，那么这个节点的父节点就是我们要找的下一个节点。 为了找到图2.8中节点i的下一个节点，我们沿着指向父节点的指针向上遍历，先到达节点e。由于节点e是它父节点b的右节点，我们继续向上遍历到达节点b。节点b是它父节点a的左子节点，因此节点b的父节点a就是节点i的下一个节点。 找出节点g的下一个节点的步骤类似。我们先沿着指向父节点的指针到达节点c。由于节点c是它父节点a的右子节点，我们继续向上遍历到达节点a。由于节点a是树的根节点，它没有父节点，因此节点g没有下一个节点。 我们用如下的C++代码从二叉树中找出一个节点的下一个节点： BinaryTreeNode* GetNext(BinaryTreeNode* pNode) { if(pNode == nullptr) return nullptr; BinaryTreeNode* pNext = nullptr; if(pNode->m_pRight != nullptr) { BinaryTreeNode* pRight = pNode->m_pRight; while(pRight->m_pLeft != nullptr) pRight = pRight->m_pLeft; pNext = pRight; } else if(pNode->m_pParent != nullptr) { BinaryTreeNode* pCurrent = pNode; BinaryTreeNode* pParent = pNode->m_pParent; while(pParent != nullptr && pCurrent == pParent->m_pRight) { pCurrent = pParent; pParent = pParent->m_pParent; } pNext = pParent; } return pNext; } 剑指 Offer P65，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/08_NextNodeInBinaryTrees "},"pages/interview/Implement_queue_with_two_stacks.html":{"url":"pages/interview/Implement_queue_with_two_stacks.html","title":"用两个栈实现队列","keywords":"","body":"用两个栈实现队列 题目：用两个栈实现一个队列。队列的声明如下，请实现它的两个函数appendTail和deleteHead，分别完成在队列尾部插入节点和在队列头部删除节点的功能。 template class CQueue { public: CQueue(void); ~CQueue(void); void appendTail(const T& node); T deleteHead(); private: stackstack1; stackstack2; }; 从上述队列的声明中可以看出，一个队列包含了两个栈stack1和stack2，因此这道题的意图是要求我们操作这两个“先进后出”的栈实现一个“先进先出”的队列CQueue。 我们通过一个具体的例子来分析往该队列插入和删除元素的过程。首先插入一个元素a，不妨先把它插入stack1，此时stack1中的元素有{a}，stack2为空。再压入两个元素b和c，还是插入stack1，此时stack1中的元素有{a,b,c}，其中c位于栈顶，而stack2仍然是空的，如图2.9(a)所示。 这时候我们试着从队列中删除一个元素。按照队列先入先出的规则，由于a比b、c先插入队列中，最先被删除的元素应该是a。元素a存储在stack1中，但并不在栈顶上，因此不能直接进行删除。注意到stack2一直没有被使用过，现在是让stack2发挥作用的时候了。如果我们把stack1中的元素逐个弹出并压入stack2，则元素在stack2中的顺序正好和原来在stack1中的顺序相反。因此经过3次弹出stack1和压入stack2的操作之后，stack1为空，而stack2中的元素是{c,b,a}，这时候就可以弹出stack2的栈顶a了。此时的stack1为空，而stack2的元素为{c,b}，其中b在栈顶，如图2.9(b)所示。 如果我们还想继续删除队列的头部应该怎么办呢？剩下的两个元素是b和c，b比c早进入队列，因此b应该先删除。而此时b恰好又在栈顶上，因此直接弹出stack2的栈顶即可。在这次弹出操作之后，stackl仍然为空，而stack2中的元素为{c}，如图2.9(c)所示。 从上面的分析中我们可以总结出删除一个元素的步骤：当stack2不为空时，在stack2中的栈顶元素是最先进入队列的元素，可以弹出。当stack2为空时，我们把stack1中的元素逐个弹出并压入stack2。由于先进入队列的元素被压到stack1的底端，经过弹出和压入操作之后就处于stack2的顶端，又可以直接弹出。 接下来再插入一个元素d。我们还是把它压入stack1,如图2.9(d)所示，这样会不会有问题呢？我们考虑下一次删除队列的头部stack2不为空，直接弹出它的栈顶元素c，如图2.9(e)所示。而c的确比d先进入队列，应该在d之前从队列中删除，因此不会出现任何矛盾。 总结完每一次在队列中插入和删除操作的过程之后，我们就可以开始动手写代码了。参考代码如下： template void CQueue::appendTail(const T& element) { stack1.push(element); } template T CQueue::deleteHead() { if(stack2.size()0) { T& data = stack1.top(); stack1.pop(); stack2.push(data); } } if(stack2.size() == 0) throw new exception(\"queue is empty\"); T head = stack2.top(); stack2.pop(); return head; } 剑指 Offer P68，本题完整的源代码：https://github.com/zhedahht/CodingInterviewChinese2/tree/master/09_QueueWithTwoStacks "},"pages/interview/Interview_questions_compilation.html":{"url":"pages/interview/Interview_questions_compilation.html","title":"面试问题整理","keywords":"","body":"面试问题整理 面向对象 面向对象编程特征有哪些？ “抽象”，把现实世界中的某一类东西，提取出来，用程序代码表示； “封装”，把过程和数据包围起来，对数据的访问只能通过已定义的界面； “继承”，一种联结类的层次模型； “多态”，允许不同类的对象对同一消息做出响应。 语言特性 OC和Swift如何互相调用？ struct和class的区别？ https://juejin.cn/post/6844903799413276685 类属于引用类型，结构体属于值类型 类允许被继承，结构体不允许被继承 类中的每一个成员变量都必须被初始化，否则编译器会报错，而结构体不需要，编译器会自动帮我们生成init函数，给变量赋一个默认值 Swift设置访问权限如何设置？ https://juejin.cn/post/7012087397765054494 open public internal fileprivate private Swift中的逃逸闭包(@escaping )与非逃逸闭包(@noescaping) https://juejin.cn/post/6844903951519727629 概念：一个接受闭包作为参数的函数，该闭包可能在函数返回后才被调用，也就是说这个闭包逃离了函数的作用域，这种闭包称为逃逸闭包。当你声明一个接受闭包作为形式参数的函数时，你可以在形式参数前写@escaping来明确闭包是允许逃逸。 概念：一个接受闭包作为参数的函数， 闭包是在这个函数结束前内被调用。 设计模式 常见设计模式有哪些？ https://www.cnblogs.com/newsouls/archive/2011/07/28/DesignTemplage.html 单例模式 工厂模式 多线程 如何用GCD同步若干个异步调用？ https://cloud.tencent.com/developer/article/1521135 将几个线程加入到group中, 然后利用group_notify来执行最后要做的动作 利用GCD信号量dispatch_semaphore_t来实现 创建线程有几种方式？ pthread 实现多线程操作 NSThread实现多线程 GCD 实现多线程 NSOperation 运行时 KVO的底层实现？ http://chuquan.me/2018/12/12/kvo-principle/ 内存管理 对象在什么时候释放？ https://youle.zhipin.com/questions/4c09e5e18447d9dbtnV809W7FlQ~.html 引用计数小于1的时候释放的。在ARC环境下我们不能直接去操作引用计数的值，但是我们可以跟踪是否有strong指针指向、如果没有strong指针指向、则立即销毁。 这里有一个地方值得关注的事自动缓存池，他会延迟销毁时机，但是实际上也是延迟执行re lease而已。 单元测试 是否编写过单元测试？ 工程结构 如何管理依赖库？ 组件化开发方式有哪些？ 加解密 https是什么？ 如何存放敏感信息？ 对称加密和非对称加密的区别？ 代码管理 如何使用Git？ "},"pages/datastructures/introduction.html":{"url":"pages/datastructures/introduction.html","title":"第一章 概论","keywords":"","body":"第一章 概论（数据结构导论） 引言 数据结构（Data Structure）：是计算机组织数据和存储数据的方式。 计算机解决问题的步骤： 建立数学模型 设计算法 编程实现算法 基本概念和术语 数据、数据元素和数据项 数据：所有被计算机存储、处理的对象。数据元素：数据的基本单位，在程序中作为一个整体而加以考虑和处理。数据项：数据元素由数据项组成。在数据库中数据项又称为字段或域。它是数据的不可分割的最小标识单位。原始数据：实际问题中的数据。 数据的逻辑结构 逻辑结构：数据元素之间的逻辑关系。逻辑结构的类型：集合、线性结构、树形结构、图形结构。线性结构：除了第一个和最后一个数据元素外，每个结点有一个前驱和一个后继。树形结构：除根结点外，最多一个前驱，可以有多个后继。 逻辑结构与数据元素本身形式、内容无关。 逻辑结构与数据元素的相对位置无关。 逻辑结构与所含结点个数无关。 数据的存储结构 存储结构：数据的逻辑结构在计算机中的实现。存储结构包含两部分： 存储数据元素。 数据元素之间的关联方式。 数据元素之间的关联方式： 顺序存储方式 链式存储方式 索引存储方式 散列存储方式 顺序存储方式：指所有存储结点存放在一个连续的存储区里。利用结点在存储器中的相对位置来表示数据元素之间的逻辑关系。链式存储方式：指每个存储结点除了含有一个数据元素外，还包含指针，每个指针指向一个与本结点有逻辑关系的结点，用指针表示数据元素之间的逻辑关系。 运算 建立 查找 读取 插入 删除 算法及描述 算法：规定了求解给定问题所需的处理步骤及其执行顺序，使得给定问题能在有限时间内被求解。 算法分析 评价算法好坏的因素： 正确性：能正确地实现预定的功能，满足具体问题的需要。 易读性：易于阅读、理解和交流，便于调试、修改和扩充。 健壮性：即使输入非法数据，算法也能适当地做出反应或进行处理，不会产生预料不到的运行结果。 时空性：指该算法的时间性能和空间性能。 时间复杂度 算法运算时需要的总步数，通常是问题规模的函数。 如何确定算法的计算量？ 可在算法中合理地选择一种或几种操作作为“基本操作”。 对给定的输入，确定算法共执行了多少次基本操作，可将基本操作次数作为该算法的时间度量。 最坏时间复杂度：对相同输入数据量的不同输入数据，算法时间用量的最大值。平均时间复杂度：对所有相同输入数据量的各种不同输入数据，算法时间用量的平均值。 时间复杂度的计算先定义标准操作，在计算标准操作的次数，得到一个标准操作的次数和问题规模的函数。然后取出函数的主项，就是它的时间复杂度的大O表示。 常数阶O(1)，对数阶O(log2^n)，线性阶O(n)，平方阶O(n^2)，立方阶O(n^3) 空间复杂度 算法执行时所占用的存储空间，通常是问题规模的函数。 空间复杂度：对一个算法在运行过程中临时占用存储空间大小的量度。一个算法在执行期间所需要的存储空间量应包括以下三个部分： 程序代码所占用的空间； 输入数据所占用的空间； 辅助变量所占用的空间； 算法的空间复杂度指的是：算法中除输入数据占用的存储空间之外所需的附加存储空间的大小。 https://www.youtube.com/watch?v=iPp2HPNsuUw&list=PL73Vsv9h2elKkYRYuZKnNeES0x_4hGEdh "},"pages/datastructures/lineartable.html":{"url":"pages/datastructures/lineartable.html","title":"第二章 线性表","keywords":"","body":"第二章 线性表（数据结构导论） 线性表的基本概念 线性表（Linear List）：是一种线性结构，它是由n（n>=0）个数据元素组成的有穷序列，数据元素又称结点。结点个数n称为表长。 基本特征：线性表中结点具有一对一的关系，如果结点数不为零，则除起始结点没有直接前驱外，其他每个结点有且仅有一个直接前驱；除终端结点没有直接后继外，其他每个结点有且仅有一个直接后继。 线性表基本运算有：初始化、求表长、读表元素、定位、插入、删除。 顺序表 顺序表定义 线性表顺序存储的方法是：将表中的结点依次存放在计算机内存中一组连续的存储单元中，数据元素在线性表中的邻接关系决定它们在存储空间中的存储位置，即逻辑结构中相邻的结点其存储位置也相邻。用顺序存储实现的线性表称为顺序表。一般使用数组来表示顺序表。 顺序表的结构定义如下： typedef struct { int num; char name[8]; char sex[2]; int age; int score; } DataType; const int Maxsize = 100; typedef struct { DataType data[Maxsize]; int length; } SeqList; SeqList L; 顺序表插入 首先将结点a(i)~a(n)依次向后移动一个元素的位置，这样空出第i个数据元素的位置；然后将x置入该空位，最后表长加1。 void InsertSeqlist(SeqList L, DataType x, int i) { if (L.length==Maxsize) exit(\"表已满\"); if (iL.length+1) exit(\"位置错\"); for (int j = L.length; j >= i; j--) L.data[j] = L.data[j-1]; L.data[i-1] = x; L.length++; } 顺序表删除 如果i的值合法，当1 void DeleteSeqlist(SeqList L, int i) { if (iL.length) exit(\"非法位置\"); for (int j = i; j 顺序表定位 i从0开始，作为扫描顺序表时的下标。如果表L中有一个结点的值等于x，或i等于L.length，则终止while循环。若while循环终止于i等于L.length，则未找到值为x的元素，返回0，否则返回值为x的元素的位置。 int LocateSeqlist(SeqList L, DataType x) { int i=0; while((i 顺序表实现算法的分析 顺序表的插入、删除算法在时间性能方面是不理想的。 插入 时间复杂度：O(n) 移动次数：n-i+1 平均移动次数：n/2 删除 时间复杂度：O(n) 移动次数：n-i，最坏移动次数：n-1 平均移动次数：(n-1)/2 定位 时间复杂度：O(n) 读表长和读元素 时间复杂度：O(1) 线性表的链接存储 线性表的链接存储时指它的存储结构时链式的。线性表常见的链式存储结构有单链表、循环链表和双向循环链表，其中最简单的事单链表。 单链表 单链表定义 data部分称为数据域，用于存储线性表的一个数据元素，next部分称为指针域或链域，用于存放一个指针，该指针指向本结点所含数据元素的直接后继结点。 所有结点通过指针链接形成链表（Link List）。head称为头指针变量，该变量的值是指向单链表的第一个结点的指针。 链表中第一个数据元素结点称为链表的首结点。尾结点指针域的值NULL称为空指针，它不止向任何结点，表示链表结束。 如果head等于NULL，则表示该链表无任何结点，是空单链表。 单链表的类型定义： typedef struct node { DataType data; struct node *next; } Node, *LinkList; LinkList head; struct node表示链表的结点，结点包含两个域：数据域（data）和指针域（next）。数据域的类型为DataType，指针域存放该结点的直接后继结点的地址，类型为指向struct node的指针。 定义中通过typedef语句把struct node类型定义为Node，把struct node指针类型定义为LinkList，LinkList的实质是该链表的头指针类型。 单链表初始化 空表由一个头指针和一个头结点组成。初始化一个单链表首先需要创建一个头结点并将其指针域设为NULL，然后用一个LinkList类型的变量指向新创建的结点。 LinkList InitateLinkList() { LinkList head; head=malloc(sizeof(Node)); head->next=NULL; return head; } 单链表求表长 在单链表存储结构中，线性表的表长等于单链表中数据原属的结点个数，即除了头结点以外的结点的个数。 通过结点的指针域来从头到为访问每一个结点，让工作指针p通过指针域逐个结点向尾结点移动，工作指针没向尾部移动一个结点，让计数器加1，直到工作指针p->next为NULL时，说明已经走到了表的尾部。计数器cnt的值即是表的长度。 int LengthLinkList(LinkList head) { Node *p=head; int cnt=0; while(p->next!=NULL) { p=p->next; cnt++; } return cnt; } 单链表读表元素 通常给定一个序号i，查找线性表的第i个元素。从头指针出发，一直往后移动，直到第i个结点。 Node * GetLinkList(LinkList head, int i) { Node *p; p=head->next; int c=1; while((cnext;c++; if (i==c) reutrn p; else reuturn NULL: } 单链表定位 线性表的定位运算，就是对给定表元素的值，找出这个元素的位置。从头到尾访问链表，直到找到需要的结点，返回其序号。若未找到，返回0。 int LocateLinkList(LinkList head, DataType x) { Node *p=head; p=p->next; int i=0; while(p!=NULL && p->data!=x) { i++; p=p->next; } if (p!=NULL) return i+1; else return 0; } 单链表插入 单链表的插入运算是将给定值为x的元素插入到链表head的第i个结点之前。 先找到链表的第i-1个结点q，然后，生成一个值为x的新结点p，p的指针域指向q的值接后继结点，q的指针域指向p。 void InsertLinkList(LinkList head, DataType x, int i) { Node *p, *q; if (i==1) q=head; else q=GetLinkList(head, i-1); if (q==NULL) exit(\"找不到插入的位置\"); else { p=malloc(sizeof(Node)); p->data=x; p->next=q->next; q->next=p; } } 单链表删除 删除运算是给定一个值i，将链表中第i个结点从链表中移除。 将a（i）结点移出后，需要修改改结点的直接前驱结点a（i-1）的指针域，使其指向移出结点a（i）的直接后继结点。 void DeleteLinkList(LinkList head, int i) { Node *q; if (i==1) q=head; else q=GetLinkList(head, i-1); if (q!=NULL && q->next!=NUll) { p=q->next; q->next=p->next; free(p); } else ext(\"找不到要删除的结点\"); } 其他运算在单链表上的实现 建表（尾插法一） 三步：首先建立带头结点的空表；其次建立一个新结点，然后将新结点链接到头结点之后；重复建立新结点和将新结点链接到表尾这两个步骤，直到线性表中所有元素链接到单链表中。 LinkList CreateLinkList1() { LinkList head; int x,i; head=InitateLinkList(); i=1; scanf(\"%d\", &x); while(x!=0) { InsertLinkList(head,x,i); i++; scanf(\"%d\", &x); } return head; } 时间复杂度：O（n^2) 建表（尾插法二） 上面的算法由于每次插入都从表头开始查找，比较浪费时间。因为每次都是把新的结点链接到表尾，我们可以用一个指针指向尾结点，这样就为下一个新结点指明了插入位置。 LinkList CreateLinkList2() { LinkList head; Node *q,*t; int x; head=malloc(sizeof(Node)); q=head; scanf(\"%d\", &x); while(x!=0) { t=malloc(sizeof(Node)); t->data=x; q->next=t; q=t; scanf(\"%d\",&x); } } 时间复杂度：O（n) 建表（头插法） 该方法始终将新增加的结点插入到头结点之后，第一个数据结点之前，可称之为“前插”操作。 LinkList CreateLinkList3() { LinkList head; Node *p; int x; head=malloc(sizeof(Node)); head->next=NULL; scandf(\"%d\",&x); while(x) { p=malloc(sizeof(Node)); p->data=x; p->next=head->next; head->next=p; scandf(\"%d\",&x); } return head; } 删除重复结点 void PurgeLinkList(LinkList head) { Node *p,*q,*r; q=head->next; while(q!=NULL) { p=q; while(p->next!=NULL) if(p->next->data==q->data) { r=p->next; p->next=r->next; free(r); } else p=p->next; q=q->next; } } 循环链表 在单链表中，如果让最后一个结点的指针域指向第一个结点可以构成循环链表。在循环链表中，从任意结点出发能够扫描整个链表。 双向循环链表 在单链表的每个结点中再设置一个指向其直接前驱结点的指针域prior，这样每个结点有两个指针。prior与next类型相同，它指向直接前驱结点。头结点的prior指向最后一个结点，最后一个结点的next指向头结点，由这种结点构成的链表称为双向循环链表。 struct dbnode { DataType data; struct dbnode *prior,*next; } 双向循环链表的对称性可以用下列等式表示： p=p->next=p->next->prior 删除 p->prior->next=p->next; p->next->prior=p->prior; free(p); 插入 t->prior=p; t->next=p->next; p->next->prior=t; p->next=t; 顺序实现与链式实现的比较 查找：顺序表是随机存取，时间复杂度为O（1）。单链表需要对表元素进行扫描，它时间复杂度为O（n）。 定位：时间复杂度都是O（n）。 插入、删除：在顺序表和链表中，都需要进行定位。在顺序表中，其基本操作是元素的比较和结点的移动，平均时间复杂度为O（n）。在单链表中，由于需要定位，基本操作是元素的比较，尽管不需要移动结点，其平均时间复杂度仍然为O（n）。 单链表的每个结点包括数据域与指针域，指针域需要占用额外空间。 顺序表要预先分配存储空间，如果预先分配得过大，将造成浪费，若分配得过小，又将发生上溢；单链表不需要预先分配空间，只需要内存空间没有耗尽，单链表中的结点个数就没有限制。 参考 手把手教你数据结构c语言实现 数据结构 C语言数据结构-顺序栈 "},"pages/datastructures/stacks-queues.html":{"url":"pages/datastructures/stacks-queues.html","title":"第三章 栈、队列和数组","keywords":"","body":"第三章 栈、队列和数组 栈 栈的基本概念 栈是运算受限的线性表，这种线性表上的插入和删除运算限定在表的某一端进行。允许进行插入和删除的一端称为栈顶，另一端称为栈底。不含任何数据元素的栈称为空栈。处于栈顶位置的数据元素称为栈顶元素。 栈的修改原则是后进先出，栈又称为后进先出线性表，简称后进先出表。 栈的基本运算： 初始化 InitStack(S)：构建一个空栈S； 判栈空 EmptyStack(S)：若栈S为空栈，则结果为1，否则结果为0； 进栈 Push(S, x)：将元素x插入栈S中，使x称为栈S的栈顶元素； 出栈Pop(S)：删除栈顶元素； 取栈顶 GetTop(S)：返回栈顶元素。 栈的顺序实现 栈的顺序存储结构式用一组连续的存储但愿依次存放栈中的每个元素，并用始端作为栈底。栈的顺序实现称为顺序栈。通常用一个一维数组和一个记录栈顶位置的变量来实现栈的顺序存储。 顺序栈用类C语言定义： const int maxsize=6; typedef struct seqstack { DataType data[maxsize]; int top; } SeqStk; maxsize为顺序栈的容量。data[maxsize]为存储栈中数据元素的数组。top为标志栈顶位置的变量，常用整型表示，范围0～(maxsize-1)。 初始化 int InitStack(SeqStk *stk) { stk->top=0; return 1; } 判栈空 int EmptyStack(SeqStk *stk) { if (stk->top==0) return 1; else return 0; } 进栈 int Push(SeqStk *stk, DataType x) { if (stk->top==maxsize-1) { error(\"栈已满\");return 0; } else { stk->top++; stk->data[stk->top]=x; return 1; } } 出栈 int Pop(SeqStk *stk) { if (EmptyStack(stk)) { error(\"下溢\");return 0; } else { stk->top--; return 1; } } 取栈顶元素 DataType GetTop(SeqStk *stk) { if (EmptyStack(stk)) { return NULL; } else { return stk->data[stk->top]; } } 栈的链接实现 栈的链接实现称为链栈，链栈可以用带头结点的单链表来实现。各个结点通过链域的链接组成栈，由于每个结点空间都是动态分配产生，链栈不用预先考虑容量的大小。 链栈用类C语言定义： typedef struct node { DataType data; struct node *next; }LkStk; 初始化 void InitStack(LkStk *LS) { LS = (LkStk *)malloc(sizeof(LkStk)); LS->next=NULL; } 生成一个结点，该结点的next域设置为NULL。 判栈空 int EmptyStack(LkStk *LS) { if (LS->next==NULL) { return 1; } else { return 0; } } 进栈 void Push(LkStk *LS, DataType x) { LkStk *temp; temp=(LkStk *)malloc(sizeof(LkStk)); temp->data=x; temp->next=LS->next; LS->next=temp; } 出栈 int Pop(LkStk *LS) { LkStk *temp; if (!EmptyStack(LS)) { temp=LS->next; LS->next=temp->next; free(temp); return 1; } else { return 0; } } 栈的简单应用和递归 递归是一个重要的概念，同时也是一种重要的程序设计方法。简单地说，如果在一个函数或数据结构的定义中又应用了它自身，那么这个函数或数据结构称为递归定义的，简称递归的。 递归定义不能是“循环定义”。为此要求任何递归定义必须同时满足如下两个条件： 被定义项在定义中的应用（即作为定义项的出现）具有更小的“规模”； 被定义项在最小“规模”上的定义是非递归的，这是递归的结束条件； 理解： 递归前进段 递归边界段 递归回归段 阶乘函数的递归算法： long f(int n) { if(n==0)return 1; else return n*f(n-1); } 递归函数的运行引起递归调用。为了保证在不同层次的递归调用能正确的返回，必须将每一次递归调用的参数和返回地址保存起来。由于函数的递归是后进先出的，所以要用栈来保存这些值。 队列 队列的基本概念 队列是有限个同类型数据元素的线性序列，是一种先进先出的线性表，新加入的数据元素插在队列尾端，出队列的数据元素首部被删除。 列表的基本运算： 队列初始化 InitQueue(Q)：设置一个空队列Q； 判队列空 EmptyQueue(Q)：若队列Q为空，则返回值为1，否则返回值为0； 入队列 EnQueue(Q, x)：将数据元素x从对尾一端插入队列，使其成为队列的新尾元素； 出队列：OutQueue(Q)：删除队列首元素； 取队列首元素 GetHead(Q)：返回队列首元素的值。 队列的顺序实现 顺序存储实现的队列称为顺序队列，它由一个一维数组（用于存储队列中元素）及两个分贝指示队列首和队列尾元素的变量组成，这两个变量分别称为“队列首指针”和“队列尾指针”。 用类C语言定义顺序队列类型如下： const int maxsize=20; typedef struct sequeue { DataType data[maxsize]; int front, rear; }SeqQue; SeqQue SQ; 顺序队列结构类型中有三个域：data、front和rear。其中data为一维数组，存储队列中数据元素。front和rear定义为整型变量，实际取值范围是0~(maxsize-1)。为了方便操作，规定front指向队列首元素的前一个单元，rear指向实际的队列元素单元。 假溢出：数组的实际空间并没有沾满，新元素无法进入队列。通过SQ.rear=0，把SQ.data[0]作为新的队列尾，可以解决“假溢出”问题。 用类C语言定义循环队列： typedef struct cycqueue { DataType data[maxsize]; int front, rear; }CycQue; 队列的初始化 void InitQueue(CycQue CQ) { CQ.front=0; CQ.rear=0; } 判队列空 int EmptyQueue(CycQue CQ) { if(CQ.rear==CQ.front) return 1; else return 0; } 入队列 int EnQueue(CycQue CQ, DataType x) { if ((CQ.rear+1)%maxsize==CQ.front) { error(\"队列满\");return 0; } else { CQ.rear=(CQ.rear+1)%maxsize; CQ.data[CQ.rear]=x; return 1; } } 出队列 int OutQueue(CycQue CQ) { if (EmptyQueue(CQ)) { error(\"队列空\");return 0; } else { CQ.front=(CQ.front+1)%maxsize; return 1; } } 取队列首元素 DataType GetHead(Cycle CQ) { if (EmptyQueue(CQ)) { return NULL; } else { return CQ.data[(CQ.front+1)%maxsize]; } } 队列的链接实现 队列的链接实现实际上是使用一个带头结点的单链表来表示队列，称为链队列。头指针指向链表的头结点，单链表的头结点的next域指向队列首结点，尾指针指向队列尾结点，即单链表的最后一个结点。 链接队列用类C语言定义： typedef struct LinkQueueNode { DataType data; struct LinkQueueNode *next; }LkQueNode; typedef struct LinkQueueNode { LkQueNode *front, * rear; }LkQue; LkQue LQ; 队列的初始化 void InitQueue(LkQue *LQ) { LkQueNode *temp; temp=(LkQueNode *)malloc(sizeof(LkQueNode)); LQ->front=temp; LQ->rear=teamp; (LQ->front)->next=NULL; } 判队列空 int EmptyQueue(LkQue LQ) { if(LQ.rear==LQ.front) return 1; else return 0; } 入队列 void EnQueue(LkQue *LQ, DataType x) { LkQueNode *temp; temp=(LkQueNode *)malloc(sizeof(LkQueNode)); temp->data=x; temp->next=NULL; (LQ->rear)->next=temp; LQ->rear=temp; } 出队列 OutQueue(LkQue *LQ) { LkQueNode *temp; if(EmptyQueue(CQ)) { error(\"队空\");return 0; } else { temp=(LQ->front)->next; (LQ->front)->next=temp->next; if(temp->next==NULL) LQ->rear=LQ->front; free(temp); return 1; } } 取队列首元素 DataType GetHead(LkQue LQ) { LkQueNode *temp; if (EmptyQueue(CQ)) { return NULLData; } else { temp=LQ.front->next; return temp->data; } } 队列的应用 银行办理业务： while(1) { 接收命令； 若为‘A’，取号，排队等待； 若为‘N’，队列中的第一个人，即持所报号的人，出队列接收服务； 若为‘Q’，队列中剩余人按按顺序依次接收服务，结束； } 用C语言算法描述： typedef struct LinkQueueNode { int data; struct LinkQueueNode *next; } LkQueNode; typedef struct LinkQueue { LkQueNode *front, *rear; } void GetService() { LkQue LQ; int n; char ch; InitQueue(&LQ); while(1) { printf(\"\\n请输入命令：\"); scanf(\"%c\", &ch); switch(ch) { case 'A': printf(\"客户取号\\n\"); scanf(\"%d\", &n); EnQueue(&LQ, n); break; case 'N': if(!EmptyQueue(LQ)) { n=Gettop(LQ); OutQueue(&LQ); printf(\"号为 %d 的客户接收服务\", n); } else { printf(\"无人等待服务\\n\"); } break; case 'Q': printf(\"排队等候的人一次接受服务\\n\"); break; } if(ch=='Q') { while(!EmptyQueue(LQ)) { n=Gettop(LQ); OutQueue(&LQ); printf(\"号为 %d 的客户接受服务\", n); } break; } } } 数组 数组的逻辑结构和基本运算 数组可以看成线性表的一种推广。以为数组有称向量，它由一组具有相同类型的数据元素组成，并存储在一组连续的存储单元中。若一维数组中的数据元素又是一维数组结构，则称为二维数组；依此类推，若一维数组中的数据元素又是一个二维数组结构，则称为三维数组。 二维数组是n个列向量组成的线性表；二维数组是m个行向量组成的线性表。 数组通常只有两种基本运算 读：给定一组下标，返回该位置的元素内容； 写：给定一组下标，修改改位置的元素内容。 数组的存储结构 一维数组元素的内存单元地址是连续的，二维数组可有两种存储方法： 一种是以列序为主序的存储； 一种是以行序为主序的存储。 矩阵 矩阵是很多科学计算问题研究的对象，矩阵可以用二维数组来表示。在数值分析中经常出现一些高阶矩阵，这些高阶矩阵中有许多值相同的元素或零元素，为了节省存储空间，对这类矩阵采用多个值相同的元素只分配一个存储空间，零元素不存储的策略，这一方法称为矩阵的压缩存储。 对称矩阵 三角矩阵 稀疏矩阵 "},"pages/datastructures/tree.html":{"url":"pages/datastructures/tree.html","title":"第四章 树和二叉树","keywords":"","body":"第三章 树和二叉树 树的基本概念 线性结构中的一个结点至多只有一个直接后继，而树形结构中一个结点可以有一个或多个直接后继。因此，树形结构可以表示更复杂的数据。 树的概念 树是一类重要的数据接哦股，其定义如下： 树是n(n>=0)个结点的有限集合，一棵树满足以下两个条件： 当n=0时，称为空树； 当n>0时，有且仅有一个称为根的结点，除根结点外，其余结点分为m（m>=0)个互不相交的非空集合T1,T2,...，Tm，这些集合中的每一个都是一棵树，称为根的子树。 树的相关术语 结点的度：树上任意结点所拥有的子树的数目称为该结点的度。叶子：度为0的结点称为叶子或终端结点。一个结点结点的子树的根称为该结点的孩子（或称子结点）。相应地该结点称为孩子的双亲（也称父结点）。结点的层次：从根开始算起，根的层次为1，其余结点的层次为其双亲的层次加1。树的高度：一棵树中所有结点层次数的最大值称为该树的高度或深度。有序树：若书中各结点的子树从左到右时有次序的，不能互换，称为有序树。有序树中最左边子树的根称为第1个孩子，左边第i个子树的根称为第i个孩子。无序树：若树中各结点的子树是无次序的，可以互换，则称为无序树。 树的基本运算： 求根 求双亲 求孩子 建树 剪枝 遍历 二叉树 二叉树的基本概念 二叉树（Binary Tree）是n（n>=）个元素的有限集合，该集合或者为空，或者由一个根及两颗互不相交的左子树和右子树组成，其中左子树和右子树也均为二叉树。 二叉树的任一结点都有两颗子树（它们中的任何一个都可以是空子树），并且这两颗子树之间有次序关系，即如果互换了位置就成为一颗不同的二叉树。 二叉树的基本运算： 初始化 求双亲 求左孩子 建二叉树 先序遍历 中序遍历 后序遍历 层次遍历 二叉树的性质 性质一：二叉树第i（i>=1）层上至多有2^(i-1)个结点。性质二：深度为k（k>=1）的二叉树至多有2^(k)-1个结点。性质三：对任何一棵二叉树，若度数为0的结点（叶结点）个数为n0，度数为2的结点个数为n2，则n0=n2+1。满二叉树：深度为k（k>=1）且有2^(k)-1个结点的二叉树称为满二叉树。完全二叉树：如果对满二叉树丛上到下，从左到右的顺序编号，并在最下面一层删去部分结点（删除最后一层仍有结点），如果删除的这些结点的编号是连续的且删除的结点中含有最大编号的结点，那么这颗二叉树就是完全二叉树。 完全二叉树的性质： 性质四：含有n个结点的完全二叉树的深度为log2^(n)+1。性质五：如果将一棵有n个结点的完全二叉树按层编号，按层编号是指：将一棵二叉树中的所有n个结点按从第一层到最大层，每层从左到右的顺序依次标记为1，2，...，n。则对任一编号为i（1 若i=1，则结点A是根；若i>1，则A的双亲Parent（A）的编号为i/2； 若2i>n，则结点A既无左孩子，也无右孩子；否则A的左孩子的编号为2i； 若2i+1>n，则结点A无右孩子；否则，A的右孩子的编号为2i+1。 二叉树的存储结构 二叉树通常有两类存储结构：顺序存储结构和链式存储结构。 二叉树的顺序存储结构 二叉树的顺序存储结构可以用一维数组来实现，二叉树上的结点按某种次序分别存入该数组的各个单元中。 二叉树的链式存储结构 二叉树有不同的链式存储结构，其中最常用的是二叉链表与三叉链表。 二叉链表的类型定义如下： typedef struct btnode { DataType data; struct btnnode *lchild, *rchild; } *BinTree; 三叉链表的类型定义如下： typedef struct ttnode { DataType data; struct ttnode *lchild, *parent, *rchild; } * TBinTree; 二叉树的遍历 二叉树遍历的递归实现 二叉树的遍历是指按某种次序访问二叉树的的所有结点，使每个结点被访问一次且仅被访问一次。 先序遍历 若被遍历的二叉树为空，执行空操作；否则，依次执行下列操作； 访问根结点； 先序遍历左子树； 先序遍历右子树。 void preorder(BinTree bt) { if(bt!=NULL) { visit(bt); preorder(bt->lchild); preorder(bt->rchild); } } 中序遍历 若被遍历的二叉树为空，执行空操作；否则，依次执行下列操作： 中序遍历左子树； 访问根结点； 中序遍历右子树。 void inorder(BinTree bt) { if (bt!=NULL) { inorder(bt->lchild); visit(bt); inorder(bt->rchild); } } 后序遍历 若被遍历的二叉树为空，执行空操作；否则，依次执行下列操作： 后序遍历左子树； 后序遍历右子树； 访问根结点。 void posterder(BinTree bt) { if (bt!=NULL) { posterder(bt->lchild); posterder(bt->rchild); visit(bt); } } 层次遍历 所谓二叉树的层次遍历，是指从二叉树的根结点的这一层开始，逐层向下遍历，在每一层上按从左到右的顺序对结点逐个访问。 树和森林 树的存储结构 孩子链表表示法 孩子链表表示法是树的一种链式存储结构。它的主体是一个数组元素个数和树中结点个数相同的一维数组。树上的一个结点x以及该结点的所有孩子结点组成一个带头结点的单链表，单链表的头结点含有两个域：数据域和指针域。其中，数据域用于存储结点x中的数据元素，指针域用于存储指向x第一个孩子结点的指针。 孩子兄弟链表表示法 存储时每个结点除了数据域外，还有指向该结点的第一个孩子和下一个兄弟结点的指针。 双亲表示法 双亲表示法由一个一维数组构成。数组的每个分量包含两个域：数据域和双亲域。数组域用于存储树上一个结点中数据元素，双亲域用于存储本结点的双亲结点在数组中的序号（下标值）。 树、森林与二叉树的关系 树转换成二叉树 任何一棵树可唯一地与一棵二叉树对应。相应地，一棵二叉树也唯一地对应一棵树，即树与二叉树可以互相转换。 将树转换成二叉树的方法如下： 将所有兄弟结点连接起来； 保留第一个兄弟结点与父结点的连接，断开其他兄弟结点与父结点的连接，然后以根结点为轴心按顺时针的方向旋转45度角。 森林转换成二叉树 将每棵树转换成相应的二叉树； 将1中得到的各棵二叉树的根结点看作是兄弟连接起来。 二叉树转换成森林 树和森林的遍历 先序遍历 后序遍历 层次遍历 森林的遍历 先序遍历 中序遍历 判定树和哈夫曼树 练习 先序遍历ABCDEF，中序遍历CBAEDF，后序遍历结果为： CBEFDA a b d c e f 中序遍历BACDEFGH，后序遍历BCAEDGHF，建立该二叉树 F E H A D B C G "},"pages/datastructures/find.html":{"url":"pages/datastructures/find.html","title":"第六章 查找","keywords":"","body":"第六章 查找 基本概念 查找表（Search Table）是由同一类型的数据元素构成的集合，它是一种以查找为“核心”，同时包括其他运算的非常灵活的数据结构。 关键字，简称键，是数据元素中某个数据项，可以用来标识数据元素，该数据项的值称为键值。 根据给定的某个值，在查找表中寻找一个其键值等于给定值的数据元素。若找到一个这样的数据元素，则称查找成功，此时的运算结果为该数据元素在查找表中的位置；否则，称查找不成功，此时的运算结果为一个特殊标志。 静态查找表是以具有相同特性的数据元素集合为逻辑结构，包括下列三种基本运算（但不包括插入和删除运算）： 建表Create（ST）：操作结果是生成一个由用户给定的若干数据元素组成的静态查找表ST； 查找Search（ST，key）：若ST中存在关键字值等于key的数据元素，操作结果为该数据元素的值，否则操作结果为空； 读表中元素Get（ST，pos）：操作结果是返回ST中pos位置上的元素。 静态查找表 顺序表上的查找 静态查找表最简单的实现方法是以顺序表为存储结构，静态查找表顺序存储结构的类型定义如下： const int Maxsize = 20; typedef struct { KeyType key; ... }TableElem; typedef struct { TableElem ele[Maxsize+1]; int n; }SqTable; 查找算法如下： int SearchSqTable(SqTable T, KeyType key) { T.elem[0].key = key; i = T.n; while(T.elem[i].key! = key) i--; return i; } 对于查找运算，其基本操作是“数据元素的键值与给定值的比较”，所以通常用“数据元素的键值与给定值的比较次数”作为衡量查找算法好坏的依据，称上述比较次数为查找长度。 有序表上的查找 如果顺序表中数据元素是按照键值大小的顺序排列的，则称为有序表。 二分查找的查找过程为每次用给定值与处在表中间位置的数据元素键值进行比较，确定给定值的所在区间，然后逐步缩小查找区间。重复以上过程直至找到或确认找不到该数据元素为止。 int SearchBin(SqTable T, KeyType key) { int low,high; low=1;high=T.n; while(low 索引顺序表上的查找 先确定待查数据元素所在的块； 然后在块内顺序查找。 二叉排序树 实现动态查找的树表，这种树表的结构本身是在查找过程中动态生成的，即对于给定key，若表中存在与key相等的元素，则查找成功，不然插入关键字等于key的元素。 一颗二叉排序树（Binary Sort Tree）（又称二叉查找树）或者一颗空二叉树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上所有结点的键值均小于它的根结点键值； 若它的右子树不空，则右子树上所有结点的键值大于它的根结点键值； 根的左、右子树也分别为二叉排序树。 中序遍历一颗二叉排序树可得到一个键值的升序序列。 二叉排序树的二叉链表的类型定义如下： typedef struct btnode { KeyType key; struct btnode *lchild, *rchild; }BSTNode, *BinTree; BinTree bst; 二叉排序树上的查找 Bintree SearchBST(BinTree bst, KeyType key) { if (bst==NULL) return NULL; else if (key==bst->key) return bst; else if (keykey) return SearchBST(bst->lchild, key); else return SearchBST(bst->rchild, key); } 关键字比较的次数不超过二叉树的深度。 二叉排序树的插入 由于二叉排序树这种动树表是在查找过程中，不断地往树中插入不存在的键值而形成的，所有插入算法必须包含查找过程，并且是在查找不成功时进行插入新结点的操作。在二叉排序树上进行插入的原则是：必须要保证插入一个新结点后，仍为一颗二叉排序树。这个结点是查找不成功时查找路径上访问的最后一个结点的左孩子或右孩子。 Bintree SearchBST(BinTree bst, KeyType key, BSTNode *f) { if (bst==NULL) return NULL; else if (key==bst->key) return bst; else if (keykey) return SearchBST(bst->lchild, key, bst); else return SearchBST(bst->rchild, key, bst); } int InsertBST(BinTree bst, KeyType key) { BSTNode *p, *t, *f; f = NULL; t = SearchBST(bst, key, f); if (t==NULL) { p = malloc(sizeof(btnode)); p->key = key; p->lchild = NULL; p->rchild = NULL; if (f==NULL) bst = p; else if (keykey) f->lchild = p; else f->rchild = p; return 1; } else { return 0; } } 二叉排序树的查找分析 二叉排序树的平均查找长度是介于O（n）和 O（log2^n）之间的，其查找效率与树的形态有关。 散列表 为了使数据元素的存储位置和键值之间建立某种联系，以减少比较次数，本节介绍用散列技术实现动态查找表。 数据元素的键值和存储位置之间建立对应关系H称为散列函数，用键值通过散列函数获取存储位置的这种存储方式构造的存储结构称为散列表（Hash Table），这一映射过程称为散列。如果选定了某个散列函数H及相应的散列表L，则对每个数据元素X，函数值H（X.key）就是X在散列表L中的存储位置，这个存储位置也称为散列地址。 设有散列函数H和键值k1、k2，若k1!=k2，但是H（k1）= H（k2），则称这种现象为冲突，且称k1、k2室相对于H的同义词。 采用散列技术时需要考虑两个问题： 如何构造（选择）“均匀的”散列函数 用什么方法右效地解决冲突 常用散列法 数字分析法 数字分析法又称数字选择法，其方法是收集所有可能出现的键值，排列在一起，对键值的每一位进行分析，选择分布较均匀的若干位组成散列地址。 除留余数法 除留余数法是一种简单有效且最常用的构造方法，其方法是选择一个不大于散列表长n的正整数p，以键值除以p所得的余数作为散列地址。 平方取中法 平方取中法以键值平方的中间几位作为散列地址。 技术转换法 将键值看成另一种进制的数再转换成原来的进制的数，然后选其中几位作为散列地址。 散列表的实现 通常用来解决冲突的方法有以下几种： 线性探测法 二次探测法 链地址法 多重散列法 公共溢出区法 散列表的基本操作算法 链地址法散列表 const int n=20; typedef struct TagNode { KeyType key; struct TagNode *next; ... }*Pointer, Node; typedef Pointer LinkHash[n]; 这种散列表查找的过程是首先计算给定值key的散列地址i，由它到指针向量中找到指向key的同义词子表的表头指针。然后，在该同义词子表中顺序查找键值为key的结点。 Pointer SearchLinkHash(KeyType key, LinkHash HP) { i = H(key); p = HP[i]; if (p==NULL) return NULL; while((p!=NULL) && (p->key != key)) p=p->next; return p; } 散列表上的插入算法： void InsertLinkHash(KeyType key, LinkHash HP) { if((SearchLinkHash(key, HP)) == NULL) { i=H(key); q=Pointer malloc(size(Node)); q->key = key; q->next = HP[i]; HP[i]=q; } } 散列表删除算法： void DeleteLinkHash(KeyType key, LinkHash HP) { i=H(key); if (HP[i]==NULL) return; else { p=HP[i]; if (p->key==key) { HP[i]=p->next; free(p); return; } else { while (p->next!=NULL) { q=p; p=p->next; if(p->key==key) { q->next=p->next; free(p); return; } } } } } 线性探测法散列表 散列表数据元素的类型定义如下： const int MaxSize = 20; typedef struct { KeyType key; ... }Element; typedef Element OpenHash[MaxSize]; 用线性探测法解决冲突的散列表查找运算的实现算法描述如下： int SearchOpenHash(KeyType key, OpenHash HL) { /* 在散列表HL中查找键值为key的结点，成功时返回该位置；不成功时返回标志0，嘉定以线性探测法解决冲突 */ d=H(key); i=d; while((HL[i].key!=NULL) && (HL[i].key!=key)) i=(i+1)%m; if(HL[i].key==key)return i; else return 0; } "},"pages/datastructures/sort.html":{"url":"pages/datastructures/sort.html","title":"第七章 排序","keywords":"","body":"第七章 排序 概述 排序就是将一组对象按照规定的次序重新排列的过程，排序往往是为检索服务的。 相同键值的两个记录在排序前后相对位置的变化情况是排序算法研究中经常关注的一个问题，这个问题称为排序算法的稳定性。稳定性是排序方法本身的特性，与数据无关，换句话说，一种排序方法如果是稳定的，则对所有的数据序列都是稳定的，反过来，如果在一组数据上出现不稳定的现象，则该方法是不稳定的。 排序可分为两大类： 内部排序（Internal Sorting）：待排序的记录全部存放在计算机内存中进行的排序过程； 外部排序（External Sorting）：待排序的记录数量很大，内存不能存储全部记录，需要对外存进行访问的排序过程。 待排序的数据存储结构，用类C语言描述如下： typedef struct { int key; ItemType otheritem; }RecordType; typedef RecordType List[n+1]; 插入排序 常见的插入排序方法： 直接插入排序 折半插入排序 表插入排序 希尔排序 直接插入排序（Straight Insertion Sorting）是一种简单的排序方法，它的基本思想是依次将每个记录插入到一个已排好序的有序表中去，从而得到一个新的、记录数增加1的有序表。 直接插入排序算法描述如下： void StraightInsertSort(List R, int n) // 对顺序表R进行直接插入排序 { int i,j; for(i=2;i 直接插入的算法简单，易于理解，容易实现，时间复杂度为O（n^2），若待排序记录的数量很大时，一般不选用直接插入排序。从空间来看，它只需要一个记录的辅助空间，即空间复杂度O（1）。 直接插入排序方法是稳定的。 交换排序 交换排序的基本思想：比较两个记录键值的大小，如果这两个记录键值的大小出现逆序，则交换这两个记录，这样将键值较小的记录向序列前部移动，键值较大的记录向序列后部移动。 冒泡排序 冒泡排序（Bubble Sorting）是一种交换排序，其过程是首先将第一个记录的键值和第二个记录的键值进行比较，若为逆序（即R[1].key>R[2].key），则将这两个记录交换，然后继续比较第二个和第三个记录的键值。依此类推，直到完成第n-1个记录和第n个记录的键值比较交换为止。 冒泡排序的算法描述如下： void BubbleSort(List R, int n) { int i,j,temp,endsort; for(i=1;iR[j+1].key) // 如果逆序则交换记录 { temp=R[j]; R[j]=R[j+1]; R[j+1]=temp; endsort = 1; } } if(endsort==0) break; } } 该算法的时间复杂度为O（n^2），冒泡排序是稳定的排序方法。 快速排序 快速排序（Quick Sorting）是交换排序的一种，实质上是对冒泡排序的一种改进。它的基本思想：在n个记录中取某一个记录的键值为标准，通常取第一个记录键值为基准，通过一趟排序将待排序的记录分为小于或等于这个键值和大于这个键值的两个独立的部分，这时一部分的记录键值均比另一部分记录的键值小，然后，对这两部分记录继续分别进行快速排序，以达到整个序列有序。 下面给出一趟快速排序的算法： int QuickPartition(List R, int low, int high) { // 对R[low]，R[low+1],...,R[high]子序列进行一趟快速排序 x=R[low];// 置初值 while(low=x.key)) high--; R[low]=R[high];// 自尾端进行比较，将比x键值小的记录移至低端 while((low 完整的快速排序可写成如下递归算法： void QuickSort(List R, int low, int high) { // 对记录序列R[low],R[low+1],...,R[high]进行快速排序 if(low 选择排序 选择排序（Selection Sorting）的基本思想：每一次n-i+1(i=1,2,...,n-1)个记录中选取键值最小的记录作为有序序列的第i个记录。 直接选择排序 直接选择排序算法的基本思想：在第i次选择操作中，通过n-i次键值间比较，从n-i+1个记录中选出键值最小的记录，并和i（1 算法描述如下： void SelectSort(List R, int n) { int min,i,j; for(i=1;i 堆排序 自顶向下的调整过程称为“筛选”，算法描述如下： void Sift(List R, int k, int m) { /* 假设R[k],R[k+1],...,R[m]是以R[k]为根的完全二叉树，R[k]的左、右子树均满足堆的性质。本算法调整R[k]使整个序列R[k],R[k+1],...,R[m]满足堆的性质 */ int i,j,x; List t; i=k;j=2*i; x=R[k].key; t=R[k]; while(jR[j+1].key)) { j++;// 若存在右子树，且右子树根的关键字小，则沿右分支筛选 if(x 堆排序算法描述如下： void HeapSort(List R) { // 对R[n]进行堆排序，排序完成后，R中记录按关键字自大至小有序排列 int i; for(i=n/2;i>=1;i--) Shit(R,i,n);// 从第n/2个记录开始进行筛选建堆 for(i=n;i>=2;i--) { swap(R[1],R[i]);// 将堆顶记录和堆中最后一个记录互换 Sift(R,1,i-1);// 调整R[1]是R[1],...,R[i-1]变成堆 } } 归并排序 要求：若干个有序子序列组成。 归并：将两个或两个以上的有序表合并成一个新的有序表。 合并方法：比较各子序列的第一个记录的键值，最小的一个是排序后序列的第一个记录的键值。取出这个记录，继续比较各子序列现有的第一个记录的键值，便可找出排序后的第二个记录。如此继续下去，最终可以得到排序结果。 归并排序的基础是合并。 有序序列的合并 void Merge(List a, List R, int h, int m, int n) { // 将ah,...,am和am+1,...,an两个有序序列合并成一个有序序列Rh,...,Rn k=h;j=m+1;// k,j置成文件的起始位置 while((hn，将ah,...,am剩余部分插入R的末尾 while(jm，将am+1,...,an剩余部分插入R的末尾 } 二路归并排序 二路归并排序即是将两个有序表合并成一个有序表的排序方法，其基本思想：假设序列有n个记录，可看成n个有序的子序列，每个序列的长度为1。首先将每相邻的两个记录合并，得到n/2个较大的有序子序列，每个子序列包含2个记录，再将上述子序列两两合并，得到n/2/2个有序子序列，如此反复，直至得到一个长度为n的有序序列为止，排序结束。 二路归并算法的核心是每一次的归并操作，所以根据前面介绍的算法Merge，写出执行一次归并的算法MergePass如下： void MergePass(List a,List b,int n, int h) { /* 在含有n个记录的序列a中，将长度各为h的相邻两个有序子序列合并成长度为2h的一个有序序列，并把结果存入b中 */ i=1; while(i 二路归并排序算法描述如下： void MergeSort(List a,int n) { // 将序列a1,a2,...,an按关键字的非递减次序进行排序，b也定义为list类型 m=1;// m为子序列长度，初始值为1 while(m "},"./":{"url":"./","title":"最近更新","keywords":"","body":"自动配置原理 24.1.6 21:59 开始 24.1.7 00:39 更新 @SpringBootApplication @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan( excludeFilters = {@Filter( type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class} ), @Filter( type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class} )} ) @SpringBootConfiguration：也就是@Configuration，代表当前是一个配置类。 @ComponentScan：指定扫描哪些。 @EnableAutoConfiguration：激活自动配置。 @AutoConfigurationPackage// 下面分析 @Import({AutoConfigurationImportSelector.class})// 下面分析 public @interface EnableAutoConfiguration @AutoConfigurationPackage // 给容器中导入一个组件 @Import({AutoConfigurationPackages.Registrar.class}) public @interface AutoConfigurationPackage 利用Registrar给容器中导入一系列组件。 指定的一个包下的所有组件导入进来，默认xxxApplication所在包下。 Registrar： static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports { Registrar() { } public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { AutoConfigurationPackages.register(registry, (String[])(new PackageImports(metadata)).getPackageNames().toArray(new String[0])); } public Set determineImports(AnnotationMetadata metadata) { return Collections.singleton(new PackageImports(metadata)); } } metadata：注解元信息，注解指的是@AutoConfigurationPackage。 @AutoConfigurationPackage是标注在xxxApplication类上。 计算包名：new PackageImports(metadata)).getPackageNames() 所以Registrar将xxxAplication所在包下的所有组件注册了。 @Import @Import({AutoConfigurationImportSelector.class}) public class AutoConfigurationImportSelector { ... @Override public String[] selectImports(AnnotationMetadata annotationMetadata) { if (!isEnabled(annotationMetadata)) { return NO_IMPORTS; } AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations()); } ... } 调用关系 利用getAutoConfigurationEntry(annotationMetadata)给容器中批量导入一些组件。 protected AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata) { if (!isEnabled(annotationMetadata)) { return EMPTY_ENTRY; } AnnotationAttributes attributes = getAttributes(annotationMetadata); // 寻找需要加载的候选配置类数组 List configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); Set exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = getConfigurationClassFilter().filter(configurations); fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationEntry(configurations, exclusions); } 调用getCandidateConfigurations，获取到所有需要导入到容器中的配置类。 List configurations = getCandidateConfigurations(annotationMetadata, attributes); Debug调试，查看有127个候选配置组件： getCandidateConfigurations getCandidateConfigurations调用了ImportCandidates.load： protected List getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { List configurations = ImportCandidates.load(AutoConfiguration.class, getBeanClassLoader()) .getCandidates(); Assert.notEmpty(configurations, \"No auto configuration classes found in \" + \"META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports. If you \" + \"are using a custom packaging, make sure that file is correct.\"); return configurations; } ImportCandidates.load从META-INF/spring/%s.imports位置来加载一个文件： public static ImportCandidates load(Class annotation, ClassLoader classLoader) { Assert.notNull(annotation, \"'annotation' must not be null\"); ClassLoader classLoaderToUse = decideClassloader(classLoader); String location = String.format(\"META-INF/spring/%s.imports\", annotation.getName()); Enumeration urls = findUrlsInClasspath(classLoaderToUse, location); List importCandidates = new ArrayList(); while(urls.hasMoreElements()) { URL url = (URL)urls.nextElement(); importCandidates.addAll(readCandidateConfigurations(url)); } return new ImportCandidates(importCandidates); } 查看spring-boot-autoconfigure-3.2.1jar的META-INF: spring.boot2.7起使用META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports文件来指定需要加载自动配置类 按需开启自动配置项 虽然我们127个场景的所有自动配置启动的时候默认全部加载。xxxxAutoConfiguration 按照条件装配规则（@Conditional），最终会按需配置。 AopAutoConfiguration // 我是一个配置类 @AutoConfiguration // 判断配置文件中是否存在“spring.aop”，且名字为“auto”，且值为true，则配置类生效； // matchIfMissing = true：如果没有配，也认为配置了，并且值为true，所以默认不配也是生效。 @ConditionalOnProperty(prefix = \"spring.aop\", name = \"auto\", havingValue = \"true\", matchIfMissing = true) public class AopAutoConfiguration { // 我是一个配置类 @Configuration(proxyBeanMethods = false) // 如果整个应用没有`Advice`类（org.aspectj.weaver.Advice），则不生效； // 默认没有`Advice`类，所以默认不生效。 @ConditionalOnClass(Advice.class) static class AspectJAutoProxyingConfiguration { ... } @Configuration(proxyBeanMethods = false) // 如果没有“org.aspectj.weaver.Advice”类，则生效； // 默认是没有的，所有默认生效。 @ConditionalOnMissingClass(\"org.aspectj.weaver.Advice\") // 默认开启aop功能 @ConditionalOnProperty(prefix = \"spring.aop\", name = \"proxy-target-class\", havingValue = \"true\", matchIfMissing = true) static class ClassProxyingConfiguration { ... } } CacheAutoConfiguration @AutoConfiguration(after = { CouchbaseDataAutoConfiguration.class, HazelcastAutoConfiguration.class, HibernateJpaAutoConfiguration.class, RedisAutoConfiguration.class }) // 判断是否存在`CacheManager.class`类，不存在，则不生效； // 默认是有的，条件通过； @ConditionalOnClass(CacheManager.class) // 默认是没有的，条件不通过，配置不生效。 @ConditionalOnBean(CacheAspectSupport.class) @ConditionalOnMissingBean(value = CacheManager.class, name = \"cacheResolver\") @EnableConfigurationProperties(CacheProperties.class) @Import({ CacheConfigurationImportSelector.class, CacheManagerEntityManagerFactoryDependsOnPostProcessor.class }) public class CacheAutoConfiguration { ... } DispatcherServletAutoConfiguration @AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE) // 判断是否在`ServletWebServerFactoryAutoConfiguration`配置完成之后 @AutoConfiguration(after = ServletWebServerFactoryAutoConfiguration.class) // 判断是否是原生servlet的应用，还有一种是响应式编程`WebFlux` @ConditionalOnWebApplication(type = Type.SERVLET) // 判断是否导入了`DispatcherServlet`类，因为注册了springmvc的starter，所以导入了。 @ConditionalOnClass(DispatcherServlet.class) public class DispatcherServletAutoConfiguration { ... @Configuration(proxyBeanMethods = false) @Conditional(DefaultDispatcherServletCondition.class) // 判断是否导入`ServletRegistration` @ConditionalOnClass(ServletRegistration.class) // 开启配置绑定功能，配置文件中以“spring.mvc”开头的属性， // 会被绑定至`WebMvcProperties`实例。 @EnableConfigurationProperties(WebMvcProperties.class) protected static class DispatcherServletConfiguration { // 配置`DispatcherServlet`组件，指定名字。 @Bean(name = DEFAULT_DISPATCHER_SERVLET_BEAN_NAME) public DispatcherServlet dispatcherServlet(WebMvcProperties webMvcProperties) { ... return dispatcherServlet; } @SuppressWarnings({ \"deprecation\", \"removal\" }) private void configureThrowExceptionIfNoHandlerFound(WebMvcProperties webMvcProperties, DispatcherServlet dispatcherServlet) { dispatcherServlet.setThrowExceptionIfNoHandlerFound(webMvcProperties.isThrowExceptionIfNoHandlerFound()); } // 配置文件上传解析器组件 @Bean @ConditionalOnBean(MultipartResolver.class) // 容器中没有名字为“multipartResolver”的组件 @ConditionalOnMissingBean(name = DispatcherServlet.MULTIPART_RESOLVER_BEAN_NAME) public MultipartResolver multipartResolver(MultipartResolver resolver) { // 给@Bean标注的方法传入了对象参数，这个参数的值就会从容器中找。 // SpringMVC multipartResolver。防止有些用户配置的文件上传解析器不符合规范 // Detect if the user has created a MultipartResolver but named it incorrectly return resolver; } } ... } HttpEncodingAutoConfiguration @AutoConfiguration // 配置绑定至`ServerProperties`类 @EnableConfigurationProperties(ServerProperties.class) // 是否是Servlet应用 @ConditionalOnWebApplication(type = ConditionalOnWebApplication.Type.SERVLET) // 是否存在`CharacterEncodingFilter`类 @ConditionalOnClass(CharacterEncodingFilter.class) // 默认开启encoding @ConditionalOnProperty(prefix = \"server.servlet.encoding\", value = \"enabled\", matchIfMissing = true) public class HttpEncodingAutoConfiguration { private final Encoding properties; public HttpEncodingAutoConfiguration(ServerProperties properties) { this.properties = properties.getServlet().getEncoding(); } @Bean // 容器中如果没有配CharacterEncodingFilter，自动配置 // SpringBoot默认会在底层配好所有的组件。但是如果用户自己配置了以用户的优先 @ConditionalOnMissingBean public CharacterEncodingFilter characterEncodingFilter() { CharacterEncodingFilter filter = new OrderedCharacterEncodingFilter(); filter.setEncoding(this.properties.getCharset().name()); filter.setForceRequestEncoding(this.properties.shouldForce(Encoding.Type.REQUEST)); filter.setForceResponseEncoding(this.properties.shouldForce(Encoding.Type.RESPONSE)); return filter; } ... } 总结 SpringBoot先加载所有的自动配置类（xxxxxAutoConfiguration）。 每个自动配置类按照条件进行生效，默认都会绑定配置文件指定的值。 xxxxProperties里面拿，xxxProperties和配置文件进行了绑定。 生效的配置类就会给容器中装配很多组件。 只要容器中有这些组件，相当于这些功能就有了。 定制化配置 用户直接自己@Bean替换底层的组件。 用户去看这个组件是获取的配置文件什么值就去修改。 xxxxxAutoConfiguration ---> 组件 ---> xxxxProperties里面拿值 ----> application.properties 视频地址 start：https://www.bilibili.com/video/BV19K4y1L7MT?p=13 end：https://www.bilibili.com/video/BV19K4y1L7MT?p=15 备案号： -->沪ICP备2022002183号-1 "}}